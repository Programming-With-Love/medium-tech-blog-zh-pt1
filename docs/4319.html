<html>
<head>
<title>Create Artistic Effect by Stylizing Image Background — Part 2: TensorFlow Lite Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过风格化图像背景创造艺术效果第2部分:TensorFlow Lite模型</h1>
<blockquote>原文：<a href="https://medium.com/google-developer-experts/create-artistic-effect-by-stylizing-image-background-part-2-tensorflow-lite-models-e614af91944d?source=collection_archive---------1-----------------------#2020-11-24">https://medium.com/google-developer-experts/create-artistic-effect-by-stylizing-image-background-part-2-tensorflow-lite-models-e614af91944d?source=collection_archive---------1-----------------------#2020-11-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8005" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi jc translated"><span class="l jd je jf bm jg jh ji jj jk di">在</span>的<a class="ae jl" rel="noopener" href="/@margaretmz/image-background-stylizer-part-1-project-intro-d68c4547e7e3">上一篇文章</a>中，Margaret介绍了该项目，该项目涵盖了可能有用的不同场景以及其他技术目标。</p><p id="1491" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本帖中，我们将提供更多关于我们使用的模型的细节，TensorFlow Lite (TFLite)中转换过程的一些主要部分，以及模型的基准测试结果。你可以在这里跟随材料<a class="ae jl" href="https://github.com/margaretmz/segmentation-style-transfer/tree/master/ml" rel="noopener ugc nofollow" target="_blank">。</a></p><blockquote class="jm jn jo"><p id="a093" class="ie if jp ig b ih ii ij ik il im in io jq iq ir is jr iu iv iw js iy iz ja jb ha bi translated">这个项目的全部代码可以在这个<a class="ae jl" href="https://github.com/margaretmz/segmentation-style-transfer" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到。如果你想直接跳到项目的Android实现部分，请参考这里的<a class="ae jl" href="https://farmaker47.medium.com/android-part-of-create-artistic-effect-by-stylizing-an-image-segment-2a646da2d39a" rel="noopener"/>。</p></blockquote><h1 id="706e" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">模型转换</h1><p id="752b" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">该项目包括两种类型的模型(语义分割模型和风格化模型)，如intro博客帖子(TODO: update链接)中所述。对于细分和风格化模型，我们有许多不同的模型变量可以从TensorFlow Hub的模型库中选择(<a class="ae jl" href="https://tfhub.dev/s?deployment-format=lite&amp;module-type=image-segmentation" rel="noopener ugc nofollow" target="_blank">细分模型</a>和<a class="ae jl" href="https://tfhub.dev/s?deployment-format=lite&amp;module-type=image-style-transfer" rel="noopener ugc nofollow" target="_blank">风格化模型</a>)。在本节中，我们将讨论转换过程中的一些主要位。</p><h1 id="6750" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">转换语义分割模型</h1><p id="b2ec" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">本节介绍的所有代码都在<a class="ae jl" href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/DeepLabV3/DeepLab_TFLite_COCO.ipynb" rel="noopener ugc nofollow" target="_blank">这个Colab笔记本</a>中演示过。语义分割模型基于<a class="ae jl" href="https://github.com/tensorflow/models/tree/master/research/deeplab" rel="noopener ugc nofollow" target="_blank"> DeepLabV3 </a>。为了执行转换，我们将使用DeepLab作者提供的<a class="ae jl" href="https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md" rel="noopener ugc nofollow" target="_blank">预训练检查点</a>。</p><p id="7be1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">每个不同的DeepLab模型文件都附带以下文件-</p><ul class=""><li id="d0a1" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated"><code class="du lf lg lh li b">frozen_inference_graph.pb</code></li><li id="9281" class="kw kx hh ig b ih lj il lk ip ll it lm ix ln jb lb lc ld le bi translated"><code class="du lf lg lh li b">model.ckpt.data-00000-of-00001</code>，<code class="du lf lg lh li b">model.ckpt.index</code></li></ul><p id="0d1a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将只使用<code class="du lf lg lh li b">frozen_inference_graph.pb</code>，它是一个冻结的推理图。DeepLabV3检查点分布在三个不同的数据集上——PASCAL VOC 2012、CityScapes和ADE20k。你可以从<a class="ae jl" href="https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md" rel="noopener ugc nofollow" target="_blank">这里</a>找到所有可用产品的更多信息。让我们从与PASCAL VOC 2012数据集相关联的<code class="du lf lg lh li b">mobilenetv2_coco_voctrainval</code>模型开始。</p><p id="a70c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">模型文件可以从这个<a class="ae jl" href="http://download.tensorflow.org/models/deeplabv3_pascal_trainval_2018_01_04.tar.gz" rel="noopener ugc nofollow" target="_blank">链接</a>下载。文件解压缩后，下面的代码清单生成TFLite模型</p><figure class="lo lp lq lr fd ls"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="70b9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里有几件重要的事情需要注意。如果我们检查<a class="ae jl" href="https://github.com/lutzroeder/netron" rel="noopener ugc nofollow" target="_blank"> Netron </a>中的<code class="du lf lg lh li b">frozen_inference_graph.pb</code>文件来检查图形的输入和输出张量，我们会看到以下内容</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/239adee7c7228f6c4890cedf28453612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8i7mYq9oQguyiXVA"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Fig 1: Input tensor of the frozen graph</figcaption></figure><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mg"><img src="../Images/135a96810d444cb1e953613a59ab89fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*x1j2Q2pkcMob1XOy"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Fig 2: Output tensor of the frozen graph</figcaption></figure><p id="6005" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们可以看到的，我们在上面的代码清单中指定的输入(<code class="du lf lg lh li b">sub_7</code>)和输出(<code class="du lf lg lh li b">ResizeBilinear_2</code>)张量在我们的SavedModel中与原始图不同。为什么会这样？</p><p id="c70f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你在<code class="du lf lg lh li b">sub_7</code> <strong class="ig hi"> </strong>之前看到图形的一部分，实际上是做了预处理步骤，允许原始模型图形处理动态形状的图像。目前(截至2020年11月)，TFLite模型不支持在GPU代理上处理动态形状。因此，为了加快转换后的TFLite模型的执行速度，输入张量是这样选择的。这也是我们在将输入图像/视频帧输入到TFLite模型之前必须执行这些预处理步骤的原因。下图中，我们可以看到<code class="du lf lg lh li b">sub_7</code> <strong class="ig hi"> </strong>有一个固定的输出形状——</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mh"><img src="../Images/110fe5ebd0b768e3cd9d48c87e63ef30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8on6geiA0ompgTOW"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Fig 3: Input tensor of the SavedModel with a fixed output shape</figcaption></figure><p id="a309" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">同样的原因也适用于我们为什么以那种方式选择TFLite模型的输出张量。在这种情况下，同样，我们需要实现在<code class="du lf lg lh li b">ResizeBilinear_2</code> <strong class="ig hi"> </strong>张量之后的原始模型图中执行的步骤(后处理)。此外，正如我们在图2中看到的，原始模型图的输出张量通过了一个<code class="du lf lg lh li b">ArgMax</code>操作，该操作还不被TF Lite的GPU代理所支持。总之，在准备TFLite模型时，我们确保了以下几点:</p><ul class=""><li id="5b86" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated">排除不能在TFLite委托上运行的操作。</li><li id="8a83" class="kw kx hh ig b ih lj il lk ip ll it lm ix ln jb lb lc ld le bi translated">排除具有动态输出形状的操作。</li></ul><p id="e0e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">再次，<em class="jp">注意，在将输入图像/视频帧馈送到TFLite模型</em>之前，我们需要对它们执行TFLite模型图中没有包括的操作。</p><p id="1a2b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">到目前为止，我们应该已经生成了一个反映上述考虑事项的TFLite模型文件。DeepLab模型的MobileNet变体在转换为完全可用于移动应用程序的TFLite时，大小约为<strong class="ig hi"> 2.3 MB </strong>。</p><h1 id="2f6f" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">转换图像风格化模型</h1><p id="da20" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">在风格化的例子中，实际上有两种模式在起作用</p><ul class=""><li id="e3f1" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated"><strong class="ig hi">风格预测模型</strong>，该模型从风格图像(您想要提取其风格的图像)中计算特征瓶颈。</li><li id="daa4" class="kw kx hh ig b ih lj il lk ip ll it lm ix ln jb lb lc ld le bi translated"><strong class="ig hi">风格转移模型</strong>，从风格图像中提取内容图像和预先计算的特征瓶颈，并实际生成最终的风格化图像。</li></ul><p id="d369" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">TensorFlow Lite主要为模型转换提供三种<a class="ae jl" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank">训练后量化策略</a>—动态范围、浮点16和整数。对于大多数器件，转换过程非常简单，除了整数量化。它要求您提供一个代表性数据集，以便TFLiteConverter可以校准动态激活范围。</p><p id="20b7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jp">你可以参考这个</em> <a class="ae jl" href="https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Magenta_arbitrary_style_transfer_model_conversion.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="jp"> Colab笔记本</em> </a> <em class="jp">跟随下面讨论的代码片段。</em></p><p id="88db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于<strong class="ig hi">风格的预测模型</strong>，这大致转化为下面的代码清单</p><figure class="lo lp lq lr fd ls"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="0a16" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在输入形状前加上前缀，以限制模型接受动态形状。更重要的是，因为我们的最终目标是将这些模型部署到手机上，固定形状的输入通常会带来更好的性能。</p><p id="d63b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于<strong class="ig hi">风格转移模型</strong>，事情可能看起来更复杂，因为它需要两个不同的输入。这些输入中的一个应该直接从风格预测模型中计算出来。为了在这种情况下执行整数量化，我们需要考虑如何生成代表性数据集。在下图中，我们给出了代表性数据集生成的简要示意图</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mi"><img src="../Images/d7673e53795eb78a6ff34af312b8cadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*Tt5hJFaQ0wjXkg_S74ZGsA.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Fig 4: Schematic of the representative dataset generation process for the style transfer model</figcaption></figure><p id="1f0d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在代码中，这看起来像这样-</p><figure class="lo lp lq lr fd ls"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="e173" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在，我们已经有了生成器，我们可以开始实际的模型转换过程-</p><figure class="lo lp lq lr fd ls"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="0c0d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所看到的，我们需要为我们的模型将要处理的两个不同的输入指定指数。这些索引是在模型加载时动态生成的。因此，在转换过程中相应地指定它们是很重要的。</p><p id="fba9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jp">为了在Python中使用</em> <code class="du lf lg lh li b"><em class="jp">TFLiteInterpreter</em></code> <em class="jp">对讨论的模型进行推理，您可以跟随笔记本上提到的</em> <a class="ae jl" href="https://github.com/margaretmz/segmentation-style-transfer/tree/master/ml" rel="noopener ugc nofollow" target="_blank"> <em class="jp">这里的</em> </a> <em class="jp">。</em></p><h1 id="f1f3" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">标杆管理</h1><p id="622f" class="pw-post-body-paragraph ie if hh ig b ih kr ij ik il ks in io ip kt ir is it ku iv iw ix kv iz ja jb ha bi translated">将模型转换为。tflite，我们使用了<a class="ae jl" href="https://www.tensorflow.org/lite/performance/measurementhttps://www.tensorflow.org/lite/performance/measurement" rel="noopener ugc nofollow" target="_blank">基准测试工具</a>来获得关于这些TensorFlow Lite模型的不同设备特定的统计数据，例如平均推断时间、峰值内存使用量等等。</p><p id="865d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图比较了不同细分模型的平均推断时间(毫秒)</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mj"><img src="../Images/aef51eeb8542234951c6aaa29dadb386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m6KhcUoGrki6FLbO"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Fig 5: Inference latency of the segmentation models</figcaption></figure><p id="80f9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从上图中，我们可以了解该模型在特定设备的不同计算能力下的性能。现在，因为我们的最终目标是在移动应用程序中使用这些模型，所以模型大小非常重要。接下来，我们比较它们的大小-</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mk"><img src="../Images/167090a3325fa0e9c841f52b21fa16fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*o5omA53D9KyozBrf"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Fig 6: Model sizes of the segmentation models</figcaption></figure><p id="ef1e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，我们对用于执行语义分割的官方TensorFlow Lite模型进行了基准测试<a class="ae jl" href="https://tfhub.dev/tensorflow/lite-model/deeplabv3/1/metadata/2" rel="noopener ugc nofollow" target="_blank">。该模型基于深度乘数为0.5的MobileNetV2主干网，但经过257x257图像分辨率的训练-</a></p><figure class="lo lp lq lr fd ls er es paragraph-image"><div class="er es ml"><img src="../Images/031fd0076bc4768247abe204dd693673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*xNJzqy-oz3zMVZrD"/></div><figcaption class="mc md et er es me mf bd b be z dx">Fig 7: Model size and inference latency of an official TFLite segmentation model</figcaption></figure><p id="588f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在对风格化模型进行基准测试后，我们得到了-</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mk"><img src="../Images/66ddd504b19b9f3f34f882eb40e7d3e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ExAp73E54W5h7O2J"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Fig 8: Inference latency of the stylization models (click <a class="ae jl" href="https://i.ibb.co/GpJ6SKZ/Benchmarks-Stylization.png" rel="noopener ugc nofollow" target="_blank">here</a> to see an enlarged version)</figcaption></figure><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mk"><img src="../Images/4fe66f0d3e66de61ba62e5cb5bcadf14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pw7wiVBFEsjfbe2y"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Fig 9: Model sizes of the stylization models (click <a class="ae jl" href="https://i.ibb.co/2ZwSv8T/Model-Sizes.png" rel="noopener ugc nofollow" target="_blank">here</a> to see an enlarged version)</figcaption></figure><p id="ceb5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Float16模型具有在GPU上执行更快的明显优势，尽管它们的大小几乎是其他变体的两倍。</p><p id="345b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为您的应用选择模型时，总是要在<em class="jp">“大小(vs)精度(vs)延迟”</em>之间进行权衡。但是由于我们的目标运行时间主要是移动电话，我们希望在那里快速生成有趣的艺术图像<em class="jp"/>我们决定使用最快的型号</p><ul class=""><li id="0adb" class="kw kx hh ig b ih ii il im ip ky it kz ix la jb lb lc ld le bi translated">对于语义分割，我们使用了<a class="ae jl" href="https://tfhub.dev/tensorflow/lite-model/deeplabv3/1/metadata/2" rel="noopener ugc nofollow" target="_blank">DeepLabV3</a>(<code class="du lf lg lh li b">mobilenetv2_dm05_coco_voc_trainaug</code>)的这个变种。</li><li id="1c06" class="kw kx hh ig b ih lj il lk ip ll it lm ix ln jb lb lc ld le bi translated">对于风格化，使用了以下几种——<a class="ae jl" href="https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/fp16/prediction/1" rel="noopener ugc nofollow" target="_blank">预测模型</a>(基于MobileNetV2的主干网)<a class="ae jl" href="https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/fp16/transfer/1" rel="noopener ugc nofollow" target="_blank">传输模型</a>(基于MobileNetV2的主干网)。</li></ul></div><div class="ab cl mm mn go mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ha hb hc hd he"><p id="5c32" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你有兴趣看看我们是如何使用这些模型的，可以参考这篇由<a class="ae jl" href="https://www.linkedin.com/in/george-soloupis/" rel="noopener ugc nofollow" target="_blank"> George </a>撰写的讨论<a class="ae jl" href="https://farmaker47.medium.com/android-part-of-create-artistic-effect-by-stylizing-an-image-segment-2a646da2d39a" rel="noopener"> Android实现</a>的博文，或者这篇讨论iOS实现的博文(即将发布)。</p><p id="5f4a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">感谢Margaret Maynard-Reid对这篇文章的评论。感谢TFLite团队的<a class="ae jl" href="https://twitter.com/khanhlvg" rel="noopener ugc nofollow" target="_blank"> Khanh LeViet </a>和<a class="ae jl" href="https://twitter.com/natrajmeghna" rel="noopener ugc nofollow" target="_blank"> Meghna Natraj </a>的技术支持。</p></div></div>    
</body>
</html>