<html>
<head>
<title>Deploy Machine Learning Models on IoT devices using ONNX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用ONNX在物联网设备上部署机器学习模型</h1>
<blockquote>原文：<a href="https://medium.com/globant/deploy-machine-learning-models-on-iot-devices-using-onnx-dc34cd854e27?source=collection_archive---------0-----------------------#2022-07-29">https://medium.com/globant/deploy-machine-learning-models-on-iot-devices-using-onnx-dc34cd854e27?source=collection_archive---------0-----------------------#2022-07-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="bf5e" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">一个运行时来管理它们</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/88874c07f98a271d7644486b34f16c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fh0PDvRX6xCpJT-W"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source: <a class="ae jm" href="https://unsplash.com/es/fotos/rZKdS0wI8Ks" rel="noopener ugc nofollow" target="_blank">Unsplash </a>(Credits to Vishnu Mohanan)</figcaption></figure><p id="1ecb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在本文中，我们将学习如何使用<a class="ae jm" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>部署机器学习(ML)模型，这是一个生态系统，使我们能够分离ML模型的训练和推理环境。首先，我们将了解为什么以及何时这是有用的，然后概述ONNX的工作原理，最后，我们将通过一个使用计算机视觉的例子。从PyTorch中的训练模型开始，我们将它导出到ONNX，并在Google Colab中执行这两个版本的模型，以验证结果并寻找性能差异，这对于小型物联网设备来说是一个关键因素，如<a class="ae jm" href="https://www.raspberrypi.com/products/raspberry-pi-4-model-b/" rel="noopener ugc nofollow" target="_blank"> Raspberry Pi 4 </a>，我们将在其中运行ONNX版本的模型。</p><p id="d4d9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">要继续学习，你需要具备应用于ML的Python的基础知识，能够访问Google Colab或等效的工作空间，以及Raspberry Pi。</p><h1 id="912f" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated"><strong class="ak">我们什么时候需要改变部署工作流程？</strong></h1><p id="edc6" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">部署ML模型的传统流程需要使用相同的框架来训练模型和进行预测。开始时，这很有意义，但很快就会失去控制，尤其是在满足以下任何条件的情况下:</p><ul class=""><li id="02bf" class="lg lh hh jp b jq jr jt ju jw li ka lj ke lk ki ll lm ln lo bi translated">我们使用不止一个框架来训练模型。</li><li id="71d9" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">生产中模型的目标设备不同于用于培训的设备。</li><li id="f184" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">我们使用的是性能受限的小型设备，因此模型必须针对该平台进行优化。</li></ul><p id="2315" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">有几种方法可以应对这些挑战，但近年来越来越受欢迎的一种方法是分离训练和推理环境。这是因为ML框架是所有行业的千斤顶，我们从培训和优化到部署模型都使用它们，但它们并不是每个任务的最佳工具。</p><p id="44d9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这就是为推理设计的工具派上用场的地方。它可以帮助我们充分利用我们的模型，优化流程并减少执行它们所需的依赖性。考虑到这一点，开放神经网络交换(ONNX)就是这样一种工具。在它的特性中，我们将获得:</p><ul class=""><li id="29bb" class="lg lh hh jp b jq jr jt ju jw li ka lj ke lk ki ll lm ln lo bi translated">一种允许跨框架转换模型的标准格式。</li><li id="df1c" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">在不同设备中执行相同模型的运行时。</li><li id="c65a" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">每个目标平台的运行时优化。</li></ul><h1 id="02e6" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated"><strong class="ak">ONNX生态系统是如何工作的？</strong></h1><p id="a96d" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">ONNX生态系统的第一部分是格式的<a class="ae jm" href="https://github.com/onnx/onnx/blob/main/docs/IR.md" rel="noopener ugc nofollow" target="_blank">规范，包括:</a></p><ul class=""><li id="05a3" class="lg lh hh jp b jq jr jt ju jw li ka lj ke lk ki ll lm ln lo bi translated">代表模型的计算图的定义</li><li id="efdb" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">支持的数据类型</li><li id="0fac" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">内置运算符。</li></ul><p id="a507" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">有了这些功能，就有可能将模型从它们原来的框架转换成ONNX格式或任何其他兼容的框架。</p><p id="919c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">第二部分是ONNX运行时，它在各种目标设备上执行ML模型，而不需要修改模型文件。该功能完善了生态系统，并实现了不同目标设备中模型的互操作性。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lu"><img src="../Images/98a961922cf0e8918c5029ca364d699b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*AW5W3_k7_Yk5hoo5X-3u9w.png"/></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source: <a class="ae jm" href="https://microsoft.github.io/ai-at-edge/docs/onnx/" rel="noopener ugc nofollow" target="_blank">Microsoft</a></figcaption></figure><h1 id="e7ae" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated"><strong class="ak">图像分类模型的例子</strong></h1><p id="a299" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">使用图像分类模型进行推理的标准管道从读取输入图像开始，然后有一个预处理阶段将输入准备为模型所需的格式，这可能包括:调整图像的大小、裁剪或归一化图像。准备好输入后，进行模型推理，根据输出格式，可能需要一个后处理阶段，以使输出具有人类可读的格式。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es lv"><img src="../Images/0d9cebc9102c6ba721db411c8311befc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*qRqok_7qIQ1zwzU9ksczog.png"/></div></figure><p id="35d7" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我们将把这个管道从Pytorch迁移到ONNX，为此我们必须改变模型，并且在大多数情况下，还要改变预处理阶段。这是因为我们通常在训练模型的同一个框架中进行部分图像操作，因此为了消除对框架的依赖性，我们必须改变管道中使用它的所有部分。在本文中，我们将关注代码的关键部分，但是可以查看这个例子在<a class="ae jm" href="https://github.com/Luis-ramirez-r/cv-onnx" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hi">仓库</strong> </a>中的完整文件。</p><p id="0472" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们从克隆回购开始:</p><pre class="ix iy iz ja fd lw lx ly lz aw ma bi"><span id="0e17" class="mb kk hh lx b fi mc md l me mf">!git clone <a class="ae jm" href="https://github.com/Luis-ramirez-r/cv_pytorch2onnx.git" rel="noopener ugc nofollow" target="_blank">https://github.com/Luis-ramirez-r/cv-onnx.git</a></span></pre><p id="0447" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们已经有了完成任务所需的所有文件，让我们打开Main.ipynb笔记本并运行第一行。这将做几件事:首先，在PyTorch中下载我们的启动模型，执行推理管道，最后保存模型。如果一切正常，你将得到狗的品种“萨摩耶”和模型的路径如下:</p><pre class="ix iy iz ja fd lw lx ly lz aw ma bi"><span id="0cc1" class="mb kk hh lx b fi mc md l me mf">&gt;&gt;&gt; !python '1_pytorch_model_test.py'<br/>Samoyed<br/>The model has been saved at models/googlenet.pt</span></pre><p id="438c" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们知道模型正在工作，我们想把它从PyTorch转换成ONNX，为此我们可以参考PyTorch <a class="ae jm" href="Pytorch documentation" rel="noopener ugc nofollow" target="_blank">文档</a></p><p id="62c9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">从这里我们可以看到，有一个使用<strong class="jp hi"> torch.onnx.export </strong>将模型导出为ONNX格式的功能。要使用它，我们需要提供输入:模型对象，也就是我们在脚本1中下载的模型，一个输入样本，为此我们将读取我们的测试图像Dog.jpeg，以及输出文件的路径。对于其余的参数，我们可以保留默认值。还有一个名为<strong class="jp hi"> check_model </strong>的函数，我们将使用它来确认模型是否具有有效的模式，如果文件有错误，该函数将抛出一个错误。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="e77b" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这一步的代码在文件2_export_model.py中</p><pre class="ix iy iz ja fd lw lx ly lz aw ma bi"><span id="2e74" class="mb kk hh lx b fi mc md l me mf">&gt;&gt;&gt; !python '2_export_model.py'<br/>The model has been saved at: models/googlenet.onnx</span></pre><p id="e403" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们准备再次运行管道，但是有一个关键的变化。这次使用ONNX-Runtime代替PyTorch。为了加载ONNX模型，我们需要创建一个推理会话，它需要两个参数:模型的路径和执行提供者(EP)。EP允许运行时优化对平台的模型推断。现在，我们将使用“CPUExecutionProvider”。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="0454" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这一步的代码在3_onnx_model_base_pipeline.py文件中，我们可以用与前面文件相同的方式运行它，输出应该是相同的。</p><pre class="ix iy iz ja fd lw lx ly lz aw ma bi"><span id="4636" class="mb kk hh lx b fi mc md l me mf">&gt;&gt;&gt; !python '3_onnx_model_og_pipeline.py'<br/>Samoyed</span></pre><p id="84d9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这样，我们验证了我们的模型工作正常，但仍有一个问题。在预处理阶段，我们使用PyTorch的transform函数，这意味着此时我们仍然需要安装PyTorch来运行管道。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="ec54" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为了消除PyTorch依赖性，我们必须重写预处理函数，为此我们将使用NumPy:</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="ea6f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们完成了，在这一点上，我们最终执行了没有PyTorch依赖的模型。</p><p id="a21f" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">这个阶段的完整代码在4_onnx_new_pipeline.py文件中，如您所见，不需要导入PyTorch。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="5efd" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">让我们运行脚本:</p><pre class="ix iy iz ja fd lw lx ly lz aw ma bi"><span id="766b" class="mb kk hh lx b fi mc md l me mf">&gt;&gt;&gt; !python '4_onnx_pipeline.py'<br/>Samoyed</span></pre><p id="c741" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">正如所料，我们在PyTorch、ONNX + PyTorch和ONNX only三种情况下都得到了相同的结果，现在让我们看看有什么不同。</p><h2 id="0fc3" class="mb kk hh bd kl mi mj mk kp ml mm mn kt jw mo mp kv ka mq mr kx ke ms mt kz mu bi translated">【ONNX推论与Pytorch相比如何？</h2><p id="6799" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">对于所有的情况，我们从模型中得到相同的预测:<strong class="jp hi">、萨摩耶、</strong>哪一个是输入图像中正确的品种，但是这两个模型之间有什么不同吗？</p><p id="a3b9" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">在性能方面，我们得到了一个更有趣的结果，当我们使用ONNX时，推理时间明显更低，129ms vs .当推理在CPU中进行时，41ms。要使用GPU复制推理，首先我们需要将Colab运行时改为GPU:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mv"><img src="../Images/16c9d2ed928d199ae0e54d737c6759c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/0*1xbWvDwXe51cweT1"/></div></figure><p id="a78d" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">对于PyTorch中的代码，不需要进行任何更改，对于ONNX代码，唯一需要的更改是添加CUDA EP:</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="e9ec" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">一旦为两个框架执行了代码，我们就会得到以下推理时间:PyTorch为10ms，ONNX为6ms。虽然在这种情况下差异较小，但与PyTorch相比，ONNX的速度几乎是前者的两倍。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mg mh l"/></div></figure><h1 id="f65c" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated"><strong class="ak">物联网呢？</strong></h1><p id="efdd" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">到目前为止，我们一直在使用Google Colab上的ONNX来准备和测试管道，现在我们已经准备好了最后一步。部署到物联网设备时，我们会有哪些不同？</p><p id="f49e" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">物联网设备有很多种，但我们可以期待的是更小的尺寸和更低的功耗，这意味着更少的计算能力和更低的成本。这是通过切换到ARM架构来实现的，这与我们的手机中使用的架构相同。这些设备的一个例子是<a class="ae jm" href="https://www.raspberrypi.org/" rel="noopener ugc nofollow" target="_blank"> Raspberry Pi </a>，这就是我们将要使用的设备。在许多情况下，架构的改变可能是一个问题，但是因为ONNX是一个跨平台的运行时，所以这次我们不必担心它。</p><p id="0746" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">我用的是树莓Pi4 4GBs RAM版本。如果你没有访问权限，一个替代方法是使用ARM架构的虚拟机，比如<a class="ae jm" href="https://aws.amazon.com/ec2/graviton/" rel="noopener ugc nofollow" target="_blank"> AWS Graviton实例。</a></p><h2 id="14b9" class="mb kk hh bd kl mi mj mk kp ml mm mn kt jw mo mp kv ka mq mr kx ke ms mt kz mu bi translated"><strong class="ak">树莓Pi 4设置</strong></h2><p id="c298" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">在Colab设置中，大多数库从一开始就是可用的，在这种情况下，我们需要在我们的Raspberry Pi中安装所有的依赖项。希望，因为它是一个推理环境，只有没有很多。</p><p id="e908" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">为了保持整洁，您可以在您的设备中克隆repo，并查找requirements.txt文件，该文件包含我们执行模型所需的所有库，正如您所看到的，只有四个。</p><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="3bb1" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">要在您的Raspberry Pi上安装这些库，只需运行以下代码:</p><pre class="ix iy iz ja fd lw lx ly lz aw ma bi"><span id="b135" class="mb kk hh lx b fi mc md l me mf">pip install -r requirements.txt</span></pre><p id="effb" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">现在我们准备好了，您可以执行我们在Colab 4_onnx_new_pipeline.py中使用的相同文件，您将得到相同的结果，狗繁殖。</p><pre class="ix iy iz ja fd lw lx ly lz aw ma bi"><span id="314a" class="mb kk hh lx b fi mc md l me mf">&gt;&gt;&gt; python '4_onnx_pipeline.py'<br/>Samoyed</span></pre><p id="5b74" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">就这样，我们有了一个推理环境，我们的模型在一个物联网兼容设备上运行。这是使用标准运行时的主要优势之一，跨平台移植模型这种方式更容易，并且在开始时需要做一些额外的工作，因为我们必须导出模型，并且在某些情况下需要修改部分管道，就像我们在预处理阶段所做的那样。</p><h1 id="0e31" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">下一步是什么？</h1><p id="7c51" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">到目前为止，我们讨论了ONNX生态系统的一个用例，将PyTorch模型转换为ONNX，并将其部署在Raspberry Pi上，但这只是冰山一角。如果我们从不同的框架开始或者想要使用另一个目标设备，会发生什么？</p><p id="4d55" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">对于PyTorch，我们可以直接将模型导出为。onnx，但对于许多框架，我们必须使用外部库，如<a class="ae jm" href="https://github.com/onnx/onnxmltools" rel="noopener ugc nofollow" target="_blank"> onnxmltools </a>，它支持scikit-learn、Keras、H2O等。</p><p id="cf91" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">如果您计划在另一个设备上部署您的模型，如<a class="ae jm" href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit" rel="noopener ugc nofollow" target="_blank"> Jetson nano </a>或<a class="ae jm" href="https://www.intel.com/content/www/us/en/developer/tools/neural-compute-stick/overview.html" rel="noopener ugc nofollow" target="_blank"> neural compute stick </a>，请确保安装针对您的平台进行了更好优化的运行时版本和EP，<a class="ae jm" href="https://onnxruntime.ai/" rel="noopener ugc nofollow" target="_blank">这里</a>您可以查看在所有支持的平台上安装ONNX运行时的说明，这里<a class="ae jm" href="https://onnxruntime.ai/docs/execution-providers/" rel="noopener ugc nofollow" target="_blank">是所有可用EP的列表。</a></p><h1 id="9b2b" class="kj kk hh bd kl km kn ko kp kq kr ks kt in ku io kv iq kw ir kx it ky iu kz la bi translated">结论</h1><p id="a8e2" class="pw-post-body-paragraph jn jo hh jp b jq lb ii js jt lc il jv jw ld jy jz ka le kc kd ke lf kg kh ki ha bi translated">在本文中，我们描述了当使用多个框架时，或者当我们有性能限制时，部署ML模型的挑战，并提出ONNX生态系统作为解决这些挑战的替代方案。然后，我们采用了计算机视觉的实际操作方法，我们:</p><ul class=""><li id="e5c3" class="lg lh hh jp b jq jr jt ju jw li ka lj ke lk ki ll lm ln lo bi translated">使用Google Colab作为我们的开发平台</li><li id="72be" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">将模型从PyTorch导出到ONNX，并相应修改预处理阶段</li><li id="9d8b" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">验证我们得到了相同的结果</li><li id="1777" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">比较ONNX和Pytorch的性能</li><li id="9554" class="lg lh hh jp b jq lp jt lq jw lr ka ls ke lt ki ll lm ln lo bi translated">将模型部署到Raspberry Pi</li></ul><p id="a095" class="pw-post-body-paragraph jn jo hh jp b jq jr ii js jt ju il jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">最后，研究进一步工作的一些建议，例如在其他设备上部署模型或使用不同的EP来优化推理时间。</p></div></div>    
</body>
</html>