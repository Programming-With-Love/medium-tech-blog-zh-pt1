<html>
<head>
<title>Highlights from O’Reilly’s TensorFlow World 2019</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">奥莱利2019年张量流世界集锦</h1>
<blockquote>原文：<a href="https://medium.com/compendium/https-medium-com-grensesnittet-tf-world-2019-bf2a352807e3?source=collection_archive---------1-----------------------#2019-11-11">https://medium.com/compendium/https-medium-com-grensesnittet-tf-world-2019-bf2a352807e3?source=collection_archive---------1-----------------------#2019-11-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/b9aa370878226181f5132b379116281a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GbUApbxtnPc7F9StTy02-A.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Ben Lorica and Edd Wilder-James welcoming us to TensorFlow World 2019</figcaption></figure><p id="7607" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在参加了世界上最大的TensorFlow专家和爱好者聚会后，我现在正在返回挪威的路上。这次会议是由奥赖利组织的。前两天用于指导，后两天用于会议的主要项目。请看一下<a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/schedule/2019-10-28" rel="noopener ugc nofollow" target="_blank">会议日程</a>中的项目概述和相关的<a class="ae jr" href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvxcmHHRftsuiO1GyinVAwUg" rel="noopener ugc nofollow" target="_blank"> YouTube频道</a>中的讲座将在大约三周后发布。</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es jz"><img src="../Images/3d80ce97fb047d3356ff40a7a6b32c40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ab1tuINnqpVVxxJKRHQfWg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Me at the registration booth</figcaption></figure><p id="5022" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">会议重点关注以下主题:</p><ul class=""><li id="1bce" class="ke kf hh iv b iw ix ja jb je kg ji kh jm ki jq kj kk kl km bi translated"><a class="ae jr" href="https://www.tensorflow.org/guide/effective_tf2" rel="noopener ugc nofollow" target="_blank"> TensorFlow 2.0 </a></li><li id="a3d2" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated"><a class="ae jr" href="https://www.kubeflow.org/" rel="noopener ugc nofollow" target="_blank"> Kubeflow </a>，Kubernetes的机器学习工具包。</li><li id="f9a3" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated"><a class="ae jr" href="https://www.tensorflow.org/tfx/" rel="noopener ugc nofollow" target="_blank">张量流扩展</a> (TFX)。如果你不熟悉TFX，你可以把它看作是谷歌开源的关于如何建立机器学习管道的一套最佳实践模板。</li></ul><p id="ae87" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">接下来，我将只讨论对我影响最大的几个讲座。这当然是我个人的偏见，因此，鉴于我的个人经历，它只是探索了会议的“一部分”。请看看其他谈论他们在TensorFlow World的经历的帖子，以便更好地了解这个伟大的会议。</p><h1 id="5da8" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/79404" rel="noopener ugc nofollow" target="_blank">延伸TensorFlow的生产ML管道(TFX) </a></h1><p id="f393" class="pw-post-body-paragraph it iu hh iv b iw lq iy iz ja lr jc jd je ls jg jh ji lt jk jl jm lu jo jp jq ha bi translated">这个教程是由Aurélien Géron带领的，他做了一件很棒的工作，带领我们浏览了来自TFX的所有组件，以建立端到端的管道。该研讨会是围绕colab笔记本电脑组织的，您可以在下面的<a class="ae jr" href="https://github.com/tensorflow/workshops/tree/master/tfx_labs" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中亲自尝试。笔记本写得非常好，即使你没有参加辅导，你也应该能够遵循重要的步骤。其他几位演讲者向我们介绍了以下主题(在上面提到的报告中有相应的笔记本和演示):</p><ul class=""><li id="7847" class="ke kf hh iv b iw ix ja jb je kg ji kh jm ki jq kj kk kl km bi translated">TFX的公平指标。他们刚刚在会议期间被释放。这是如何测量张量流偏差的第一步。目前，还没有实现缓解算法，它们只是关注分类问题。IBM已经有了一个很棒的库<a class="ae jr" href="https://github.com/IBM/AIF360" rel="noopener ugc nofollow" target="_blank"> AIF360 </a>，它不仅能检测偏见，还能减轻偏见。我相信，如果这两个项目共同努力，使我们处理公平问题的方式标准化，社区将从中受益。我们需要更少的工具来解决同样的问题，更好地理解如何从抽象的道德原则到公平的正确技术实现。在会议期间，我花了一些时间向这些项目中的Google和IBM团队游说这个想法，看起来他们已经开始一起讨论这些问题了。我真的希望他们能得到卓有成效的合作，从而使整个社区受益。<a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/79001" rel="noopener ugc nofollow" target="_blank">谷歌</a>和<a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/80790" rel="noopener ugc nofollow" target="_blank"> IBM </a>都就人工智能的道德使用进行了讨论。确保在讲座开始时观看！</li><li id="3988" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">如何在Google Cloud中用TFX构建CD/CI管道(此处<a class="ae jr" href="https://github.com/jarokaz/tfx-kfp-demo" rel="noopener ugc nofollow" target="_blank">有代码</a>)。如果您要将TFX集成到CI/CD管道中，这是一个很好的起点。代码带有Terraform脚本，可以在GCP自动完成整个设置。你绝对应该检查代码并尝试一下！</li><li id="e669" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated"><a class="ae jr" href="https://www.tensorflow.org/neural_structured_learning" rel="noopener ugc nofollow" target="_blank">神经结构化学习</a> ( <a class="ae jr" href="https://www.youtube.com/watch?v=2Ucq7a8CY94&amp;list=PLQY2H8rRoyvxcmHHRftsuiO1GyinVAwUg&amp;index=13&amp;t=0s" rel="noopener ugc nofollow" target="_blank">视频</a>)，解决如何在数据中定义结构的问题。这通常是你的特征中没有表现出来的东西，这种技术允许你在你的例子之间建立关系。这种技术的一个应用是当你只有很少的标记数据时增加模型的准确性，或者使你的模型对敌对攻击具有鲁棒性。</li><li id="75a6" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated"><a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/78365" rel="noopener ugc nofollow" target="_blank"> TensorFlow发球</a>。是的，不要用Gunicorn / Flask进行深度学习推理。它不是针对这种工作负载优化的，如果你做错了，你可能会阻塞你的GPU，还有其他几个问题。当它变得可利用的时候看谈话。</li></ul><p id="b04e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">总而言之，这是一个奇妙的教程。但是，让我对没有涵盖的内容进行挑剔，考虑到时间限制，这是可以理解的:</p><ul class=""><li id="e27e" class="ke kf hh iv b iw ix ja jb je kg ji kh jm ki jq kj kk kl km bi translated">超参数调谐。我不太清楚如何对模型进行超参数调整。我猜一种方法是创建一个自定义组件(repo上有一个示例笔记本)。在与TFX团队交谈后，我听说他们正在开发一个标准组件来解决这个问题，并使事情变得更简单。</li><li id="9e80" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">固定随机种子的可重复性。据我所知，随机种子是不暴露在TFX从梁实施的一些组成部分。我可能误解了这里，但我正在与TFX团队讨论，当我知道更多的时候，我会更新这个帖子。</li><li id="356e" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">更好的采样技术。TFX提供了一种<a class="ae jr" href="https://www.tensorflow.org/tfx/guide/statsgen" rel="noopener ugc nofollow" target="_blank">收集数据统计</a>的方法。这些在流水线的下游被用于检测异常，例如特征偏斜、不良训练/测试分离或模型偏差等等。在我看来，对数据进行第一轮处理并计算整个数据集的统计数据是非常有益的。然后，可以有一个标准的机制来在训练/测试/评估中分割数据，给定那些统计数据(并且可能有其他限制来防止数据泄漏)。这将是一种预防性措施，而不是在稍后阶段发现数据的不平衡并重新进行整个过程。据我所知，目前在TFX没有“简单”的方法进行这种取样。但是一个好的起点是使用Beam的<a class="ae jr" href="https://cloud.google.com/blog/products/gcp/keys-to-faster-sampling-in-cloud-dataflow" rel="noopener ugc nofollow" target="_blank">复合转换</a>并构建自己的TFX组件。有人要吗？</li></ul><h1 id="bb03" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/80581" rel="noopener ugc nofollow" target="_blank">使用MLflow模型注册中心管理TensorFlow模型的整个部署生命周期(由Databricks赞助)</a></h1><p id="676c" class="pw-post-body-paragraph it iu hh iv b iw lq iy iz ja lr jc jd je ls jg jh ji lt jk jl jm lu jo jp jq ha bi translated">克莱门斯·梅瓦尔德</p><p id="f1a5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这是一个很好的概述，介绍了MLFlow如何帮助您在构建ML管道时跟踪实验。该讲座对正常软件开发和机器学习的开发工作流程的差异进行了很大的比较。然后，Clemens给了我们一个关于MLFlow如何解决普通软件开发工具没有覆盖的部分的很好的概述。最后，我们得到了MLFlow新的<a class="ae jr" href="https://mlflow.org/docs/latest/registry.html" rel="noopener ugc nofollow" target="_blank">模型注册表</a>的演示，它展示了一个很棒的用户界面，团队可以在这里一起工作，共享、实验、测试和监控ML模型。它还提供了一种与审批和治理工作流集成的方式。总而言之，对我们从业者来说是一个很棒的工具！</p><p id="2cb2" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">据我所知，在TFX或Kubeflow中没有一个组件具有与MLFlow的模型注册中心所提供的功能等同的功能。但是，来自Kubeflow贡献者的会议谣言告诉我们，在这个问题上有一项工作正在进行中，这样一个组件很快就会发布。在我看来，社区必须共同努力来标准化模型治理的可用格式，这样我们就可以跨技术工作并减少碎片化。从我得到的谣言来看，人们已经在沿着这些路线思考了，所以希望事情会进展顺利。</p><h1 id="f23b" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/79318" rel="noopener ugc nofollow" target="_blank">tensor flow 2.0中具有tf.data、tf.function和tf.distribute的高性能、可扩展模型</a></h1><p id="edcd" class="pw-post-body-paragraph it iu hh iv b iw lq iy iz ja lr jc jd je ls jg jh ji lt jk jl jm lu jo jp jq ha bi translated">由<a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/speaker/358954" rel="noopener ugc nofollow" target="_blank">泰勒·罗比</a>和<a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/speaker/327320" rel="noopener ugc nofollow" target="_blank">普里亚·古普塔</a>主持。(<a class="ae jr" href="https://www.youtube.com/watch?v=yH1cF7GnoIo&amp;list=PLQY2H8rRoyvxcmHHRftsuiO1GyinVAwUg&amp;index=7&amp;t=0s" rel="noopener ugc nofollow" target="_blank">视频</a>)</p><p id="1884" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如果你关心如何优化TensorFlow管道以获得最佳性能，你应该看看这个演讲。它提供了许多实用的技巧，你可以在下面的<a class="ae jr" href="https://colab.research.google.com/gist/robieta/9463e86b5501541a441d431b9c4f1a1e/tf_world.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>中尝试，同时<a class="ae jr" href="https://www.youtube.com/watch?v=yH1cF7GnoIo&amp;list=PLQY2H8rRoyvxcmHHRftsuiO1GyinVAwUg&amp;index=7&amp;t=0s" rel="noopener ugc nofollow" target="_blank">观看</a>演讲。</p><h1 id="083c" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/78650" rel="noopener ugc nofollow" target="_blank"> TensorFlow隐私:针对训练数据的差分隐私学习</a></h1><p id="e437" class="pw-post-body-paragraph it iu hh iv b iw lq iy iz ja lr jc jd je ls jg jh ji lt jk jl jm lu jo jp jq ha bi translated">谈话者<a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/speaker/349663" rel="noopener ugc nofollow" target="_blank">乌尔法·厄林松</a></p><p id="a41d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这是整个会议中对我影响最大的一次演讲。它解决了一些我一直想知道的问题。演讲的重点是什么是<a class="ae jr" href="https://en.wikipedia.org/wiki/Differential_privacy" rel="noopener ugc nofollow" target="_blank">差分隐私</a>，如何用隐私训练机器学习模型，以及如何使用<a class="ae jr" href="https://github.com/tensorflow/privacy" rel="noopener ugc nofollow" target="_blank"> tensorflow.privacy </a>库。</p><p id="2607" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">你的第一个想法可能是:“当然，隐私，嗯，我的数据不是敏感的，所以我不需要它”，你就错了！</p><blockquote class="lv lw lx"><p id="b233" class="it iu ly iv b iw ix iy iz ja jb jc jd lz jf jg jh ma jj jk jl mb jn jo jp jq ha bi translated">“……差别隐私是一种稳定性保证，从根本上符合统计学的核心目标，即从关于全体人口而不是特定个人的数据中学习。”<br/><a class="ae jr" href="http://blog.mrtz.org/2015/03/13/practicing-differential-privacy.html" rel="noopener ugc nofollow" target="_blank">http://blog . mrtz . org/2015/03/13/practicing-differential-privacy . html</a></p></blockquote><p id="585b" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">隐私训练将帮助您的模型从总体中学习，以便它能够进行归纳。神经网络倾向于<a class="ae jr" href="https://bair.berkeley.edu/blog/2019/08/13/memorization/" rel="noopener ugc nofollow" target="_blank">记忆训练数据</a>，尤其是位于数据分布尾部的数据。非正式地说，当你进行隐私训练时，你的训练方式是，用某个单独的例子进行训练所得到的模型输出，与你一开始没有用那个例子进行训练所得到的模型输出非常相似。我知道，这听起来像魔法，从某种意义上说，引擎盖下有一些很好的数学咒语！</p><p id="6355" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">该技术的一个好处是，在给定模型如何“看到”数据的情况下，它允许您以一致的方式定义训练数据上的哪些实例是“良好表示的”。这里的直觉是，你可以获得一个“分数”，让你按照“异常值等级”对训练数据进行排序(例如，参见<a class="ae jr" href="https://arxiv.org/abs/1910.13427" rel="noopener ugc nofollow" target="_blank"> arxiv:1910.13427 </a>，第7节)。这需要在启用隐私的情况下反复训练几次。这里的好处是，它告诉你什么样的数据已经足够，什么样的数据你需要收集更多。所以这也可以作为一种可解释性技术来使用(参见<a class="ae jr" href="https://arxiv.org/abs/1910.13427" rel="noopener ugc nofollow" target="_blank"> arxiv:1910.13427 </a>，第4节)！lfar告诉我，即将发布的代码将让我们以更具体的方式来处理这些想法。我真的很期待！等我拿到代码的链接我会更新文章的。</p><p id="a074" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">另一个优势是符合GDPR协议。当你对一个特定的用户数据进行训练，而用户请求删除他/她的数据时会发生什么？是不是一定要扔掉你的模型重新开始？好吧，如果你够聪明，受过隐私保护训练，你什么都不需要做！这个模型真的不记得你用来训练它的任何特定例子。这是因为当你使用差分隐私时，你对你的算法可能泄露的关于其输入的信息设置了界限(见第9.3节<a class="ae jr" href="https://arxiv.org/abs/1802.08232" rel="noopener ugc nofollow" target="_blank"> arxiv:1802.08232 </a>)。</p><p id="234e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">一个很酷的应用是针对敏感数据微调语言模型，比如<a class="ae jr" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>。在启用隐私的情况下进行这种微调，将允许您获得一个尊重用户隐私的模型。</p><p id="aa6a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这里要记住的一个很酷的提示是，如果训练和评估损失函数的曲线图在收敛时彼此非常接近，那么您的模型在启用隐私的情况下训练正确。直觉上，这是因为模型不应该能够区分哪些数据来自训练集，哪些来自评估集。</p><p id="10c7" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">通常，当您使用这种技术时，您会发现性能有所下降。这并不坏，因为这也将保护你的模型架构不至于过度适应给定的问题(见<a class="ae jr" href="https://arxiv.org/abs/1806.00451" rel="noopener ugc nofollow" target="_blank"> arxiv:1806.0045 </a>)。因此，您可以更好地评估您的模型在现实世界中的真实技能。希望你现在和我一样确信在可能的时候使用这种技术。这项技术的一个警告是，你将不得不喝比平时更多的咖啡，因为训练时间可能增加10-100倍(参见第10.1节<a class="ae jr" href="https://arxiv.org/abs/1802.08232" rel="noopener ugc nofollow" target="_blank"> arxiv:1802.08232 </a>了解局限性和未来工作的完整概述)。</p><h1 id="7b49" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/detail/78333" rel="noopener ugc nofollow" target="_blank">使用变压器架构的自然语言处理</a></h1><p id="6941" class="pw-post-body-paragraph it iu hh iv b iw lq iy iz ja lr jc jd je ls jg jh ji lt jk jl jm lu jo jp jq ha bi translated">由<a class="ae jr" href="https://conferences.oreilly.com/tensorflow/tf-ca/public/schedule/speaker/270099" rel="noopener ugc nofollow" target="_blank">奥雷连·盖伦</a>主讲</p><p id="9177" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这次演讲最终给了我深入了解变压器架构的最后动力。Aurélien以一种漂亮而直观的方式解释了这些架构是如何组合在一起的，以及每个组件的作用。如果你一直在等待那个推送，我真的向你推荐他的演讲。</p><figure class="ka kb kc kd fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mc"><img src="../Images/b3562e9b347c05dfed49d29563573ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IG62bNRiyaBU6lAvN3TR4w.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">The conference opened an amusement park for us to celebrate Halloween!</figcaption></figure><h1 id="6b41" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">结论</h1><p id="9264" class="pw-post-body-paragraph it iu hh iv b iw lq iy iz ja lr jc jd je ls jg jh ji lt jk jl jm lu jo jp jq ha bi translated">总而言之，来到TensorFlow World 2019是一次很棒的经历。我与其他参与者、TFX的团队、谷歌、英伟达、IBM和一些演讲者进行了许多精彩的讨论。我回到家里，对社区的知识感到谦卑，并对如何解决我们客户面临的问题充满了最佳实践和新想法。</p><p id="8563" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">最后，我可能对TensorFlow World的其他演讲极不公平。老实说，有很多次我很难选择一个演讲，因为有这么多伟大的演讲者。幸运的是，你可以关注<a class="ae jr" href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvxcmHHRftsuiO1GyinVAwUg" rel="noopener ugc nofollow" target="_blank"> TensorFlow的Youtube频道</a>，选择你感兴趣的讲座。让我在评论中知道哪些其他的谈话对你影响最大。</p><p id="0a18" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如果你想讨论更多关于张量流、TFX、库伯流、可解释性或公平性的问题，我很乐意<a class="ae jr" href="https://www.linkedin.com/in/marcobertaniokland/" rel="noopener ugc nofollow" target="_blank">与你</a>联系。如果您想了解我们如何帮助您的使用案例，您可以访问我们的<a class="ae jr" href="https://computas.com/tjenester/skyteknologi" rel="noopener ugc nofollow" target="_blank">主页</a>并联系我们！</p><h1 id="0de2" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">有用的链接</h1><p id="d58e" class="pw-post-body-paragraph it iu hh iv b iw lq iy iz ja lr jc jd je ls jg jh ji lt jk jl jm lu jo jp jq ha bi translated">我给你们留下了一个列表，里面有一些来自TensorFlow World其他讲座和教程的有趣链接。感谢Rahul Joglekar 提供的一些链接！</p><ul class=""><li id="50be" class="ke kf hh iv b iw ix ja jb je kg ji kh jm ki jq kj kk kl km bi translated">文本分类有https://github.com/lapolonio/text_classification_tutorial<br/>伯特<a class="ae jr" href="https://github.com/lapolonio/text_classification_tutorial" rel="noopener ugc nofollow" target="_blank"><br/>T5</a><a class="ae jr" href="https://github.com/lapolonio?tab=repositories" rel="noopener ugc nofollow" target="_blank">https://github.com/lapolonio?tab=repositories</a></li><li id="8c49" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">零到ML英雄<br/><a class="ae jr" href="https://colab.research.google.com/github/lmoroney/" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/github/lmoroney/</a><br/><a class="ae jr" href="https://codelabs.developers.google.com/codelabs/tensorflow-lab3-convolutions/#0" rel="noopener ugc nofollow" target="_blank">https://codelabs . developers . Google . com/codelabs/tensor flow-lab3-convolutions/# 0</a></li><li id="27e1" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">无博士的递归神经网络<br/><a class="ae jr" href="https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/tree/master/tensorflow-rnn-tutorial" rel="noopener ugc nofollow" target="_blank">https://github . com/Google cloud platform/tensor flow-without-a-PhD/tree/master/tensor flow-rnn-tutorial</a></li><li id="9b44" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">没有博士学位的tensor flow<br/><a class="ae jr" href="https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd" rel="noopener ugc nofollow" target="_blank">https://github . com/Google cloud platform/tensor flow-without a-PhD</a></li><li id="58df" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">雨燕为张量流在3小时内<br/>https://github.com/AIwithSwift/TFWorld2019-SwiftIn3Hours<a class="ae jr" href="https://github.com/AIwithSwift/TFWorld2019-SwiftIn3Hours" rel="noopener ugc nofollow" target="_blank"/></li><li id="f715" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">张量流世界:用张量流进行隐私保护机器学习<br/><a class="ae jr" href="https://github.com/dropoutlabs/tf-world-tutorial" rel="noopener ugc nofollow" target="_blank">https://github.com/dropoutlabs/tf-world-tutorial</a></li><li id="f6da" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">来自https://github.com/teamdatatonic/tf-sampling的数据张量流采样</li><li id="06f6" class="ke kf hh iv b iw kn ja ko je kp ji kq jm kr jq kj kk kl km bi translated">张量流模型优化<br/><a class="ae jr" href="https://www.tensorflow.org/model_optimization" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/model_optimization</a></li></ul><h1 id="9fe7" class="ks kt hh bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">承认</h1><p id="b983" class="pw-post-body-paragraph it iu hh iv b iw lq iy iz ja lr jc jd je ls jg jh ji lt jk jl jm lu jo jp jq ha bi translated">我要感谢<a class="ae jr" href="https://www.linkedin.com/in/bergby/" rel="noopener ugc nofollow" target="_blank"> Ole-Magnus Bergby </a>、<a class="ae jr" href="https://www.linkedin.com/in/michaelgfeller/" rel="noopener ugc nofollow" target="_blank"> Michael Gfeller </a>和<a class="ae jr" href="https://www.linkedin.com/in/josephine-honor%C3%A9-79125047/" rel="noopener ugc nofollow" target="_blank"> Josephine Honoré </a>对本文早期草稿的反馈。</p></div></div>    
</body>
</html>