<html>
<head>
<title>Filtering Vs Enriching Data in Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark中过滤和丰富数据的比较</h1>
<blockquote>原文：<a href="https://medium.com/capital-one-tech/filtering-vs-enriching-data-in-apache-spark-e44108c3a372?source=collection_archive---------1-----------------------#2019-02-05">https://medium.com/capital-one-tech/filtering-vs-enriching-data-in-apache-spark-e44108c3a372?source=collection_archive---------1-----------------------#2019-02-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/8a3e8c9854a05cf1413441d7b5c9d147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xcxRMyYKc8UpZlNbt6vxeQ.jpeg"/></div></div></figure><p id="de26" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我知道这是一个模糊的标题…在更广泛的大数据处理方案中，对于如何处理数据，有各种选项/选择可用，有各种权衡。这篇博客的目的是通过比较我的团队尝试的两个选项来解决这种模糊性，以及为什么我们会选择一个而不是另一个。</p><p id="7555" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据存在于Capital One的DNA中。我们所有的决策都是由数据驱动的，作为一家有很多监管要求的金融公司，我们有义务提供更精细的处理数据，而不是高级细节。</p><p id="476d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是在我们进入主题的更多细节之前，这里是Apache Spark对于初学者的快速要点<strong class="ir hi"> <em class="jn">(如果您已经熟悉Apache Spark，请跳到下一节。)</em> </strong></p><h1 id="c46f" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Apache Spark快速介绍</h1><p id="ac12" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated"><a class="ae kr" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>是一个开源的大数据框架，用于使用计算集群分析大规模数据。Spark的主要优势是内存计算，这提高了应用程序处理的速度。此外，Spark支持各种工作负载，如批处理、机器学习、使用一次建立的相同基础设施的流。Spark最初是加州大学伯克利分校的一个学术项目，后来于2013年6月20日捐赠给了<a class="ae kr" href="https://www.apache.org/foundation/" rel="noopener ugc nofollow" target="_blank"> Apache基金会</a>。截至撰写本文时，Spark的最新版本是2.4</p><h1 id="9289" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">我们的应用用例#1:过滤器</h1><p id="65a8" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">Capital One是Apache Spark批量和流式工作负载的大量用户。我开发的应用程序是核心信用卡交易处理引擎之一，用于为我们的信用卡客户计算奖励。我们的批处理管道有许多Spark作业，这篇博客的重点是名为<strong class="ir hi"> <em class="jn"> Filter的作业的第一个版本。</em>T11】</strong></p><h2 id="c76b" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ja kz la kc je lb lc kg ji ld le kk lf bi translated"><strong class="ak">过滤器的功能</strong></h2><p id="fead" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">Filter将信用卡交易作为输入，应用一堆业务逻辑，过滤掉与赚取奖励无关的交易。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lg"><img src="../Images/6cf02c92e9d5c293e20994389db08a5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V7zpR-U_RZVVOsRTa7hZ-A.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx"><strong class="bd jq">High Level Workflow of Filter Job</strong></figcaption></figure><h2 id="7dcb" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ja kz la kc je lb lc kg ji ld le kk lf bi translated"><strong class="ak">过滤器的问题</strong></h2><p id="9be8" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">我们在生产中使用了这个版本几个月，在调试数据时发现了一个问题。上面管道中提到的每个阶段都是两个数据集之间的一个<strong class="ir hi"> spark内部连接转换</strong>。最终输出保存了合格的交易，并计算了奖励。然而，我们无法追溯到交易级别，为什么/哪种特定的业务逻辑使其没有资格获得奖励。这是因为所有中间连接结果集都是在内存中计算的<strong class="ir hi">并在执行spark操作时传递到下一个阶段。</strong></p><p id="016c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">很快，自然的问题出现了。我们能否在每个阶段结束时进行火花动作计数并缓存数据集？事实上，我们已经在这么做了。但问题是，它只能给我们多少交易被过滤作为阶段的业务逻辑的一部分，而不是每个交易的细节。</p><h2 id="364f" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ja kz la kc je lb lc kg ji ld le kk lf bi translated"><strong class="ak">工作流程的样本数据表示(带计数)</strong></h2><figure class="lh li lj lk fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/52745074c883826c79a0e2368bc28818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uqublL9oMqSoYCxM"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx"><strong class="bd jq">Spark Data Representation of Filter Job</strong></figcaption></figure><p id="2281" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设十个事务作为输入被馈送。在应用业务逻辑(帐户资格)后，只有五个交易进入下一阶段，在下一阶段，同样的事情发生，它过滤掉另外三个交易，最后进行两个合格的交易，并计算奖励。</p><h2 id="ef04" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ja kz la kc je lb lc kg ji ld le kk lf bi translated"><strong class="ak">我们是如何克服这个问题的？</strong></h2><p id="00c6" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">如前所述，核心问题是使用spark内存内部连接的业务逻辑数据过滤。这使得很难进入每个事务的粒度细节，确定为什么/如何应用业务规则进行调试。经过多次讨论，该团队提出了一种不同的设计模式，即丰富数据，而不是在每个步骤中过滤数据。这就把我们带到了…</p><h1 id="db9f" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">我们的应用用例#2:丰富</h1><p id="5b75" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">我们的新工作是<strong class="ir hi"> <em class="jn">浓缩</em> </strong>。对于这项工作，我们分两步完成，而不是一步应用spark内部连接和业务逻辑过滤。</p><ul class=""><li id="4071" class="lq lr hh ir b is it iw ix ja ls je lt ji lu jm lv lw lx ly bi translated">第一步——数据丰富，每个阶段使用spark left outer join从每个业务逻辑数据框架中丰富所需的数据。</li><li id="88d8" class="lq lr hh ir b is lz iw ma ja mb je mc ji md jm lv lw lx ly bi translated">第二步—使用前一步中收集的数据点进行业务逻辑过滤。</li></ul><h2 id="bc83" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ja kz la kc je lb lc kg ji ld le kk lf bi translated"><strong class="ak">使用浓缩的工作流程样本数据表示(带计数)</strong></h2><figure class="lh li lj lk fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/3850d12f980a73ae51bab17007497a04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9iTZJN8wXyVJ_Aeh"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx"><strong class="bd jq">Spark Data Representation of Enrichment Job</strong></figcaption></figure><p id="61b1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">和之前的例子一样。十个事务被作为输入。在使用业务逻辑(帐户资格)应用左外连接之后，它收集所需的列，以便在后面的阶段进行过滤。因此，不过滤事务，而是用每个阶段所需的数据列来丰富原始输入数据集。在最后一个阶段应用业务逻辑过滤后，我们仍然只看到两个合格的事务。然而，不同之处在于沿途获得了更丰富的数据。</p><h2 id="58a3" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ja kz la kc je lb lc kg ji ld le kk lf bi translated"><strong class="ak">浓缩优于过滤</strong></h2><p id="95bd" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">现在，在每个阶段，使用左外连接捕获所需的数据，并将其丰富到原始数据集中。它捕获状态信息，以便稍后进行更详细的分析/调试。相同的数据列/标志用于应用业务逻辑，并且在后面的阶段产生相同的结果，但是粒度更大。使用这种方法，我们能够找到每个事务的每个业务逻辑数据列/标志的状态。</p><h1 id="10c1" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">做出改变</h1><p id="c86c" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">当我们将Enrichment部署到Prod时，我们希望确保输出是相同的。为了进行验证，我们对相同的输入数据集运行了两个作业，并部署了比较作业来比较过滤和丰富的结果。在此期间，我们的比较作业始终在过滤和浓缩作业上给出结果。这给了我们推进新浓缩模式的信心。在成功验证浓缩作业在生产中的性能后，我们更换了过滤器，并已在生产中成功运行浓缩一年。过滤器已从我们的整体处理流程中被淘汰。</p><h1 id="4da6" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">丰富vs过滤？</h1><p id="29c6" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">我们选择Apache Spark进行平台现代化工作，在整体操作的性能和准确性方面肯定产生了良好的结果。我们的平台每天处理<strong class="ir hi">数百万笔交易</strong>，每天向客户奖励<strong class="ir hi">数百万里程、现金、积分</strong>。考虑到这些高数据——就我们对平台的处理和准确性的预期而言——我们倾向于丰富而不是过滤数据并不奇怪。</p><h2 id="2d14" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ja kz la kc je lb lc kg ji ld le kk lf bi translated"><strong class="ak">丰富数据模式的优点</strong></h2><p id="a728" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">保持同一组数据，并按列增加数据集。这种精细的数据跟踪有助于更好地进行调试，并提高识别和报告的准确性</p><h2 id="27a7" class="ks jp hh bd jq kt ku kv ju kw kx ky jy ja kz la kc je lb lc kg ji ld le kk lf bi translated"><strong class="ak">过滤数据模式的优点</strong></h2><p id="596f" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">由于内存处理，计算速度更快，并提供高级指标</p><p id="1429" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">希望这个比较有助于您的用例决策权衡。</p></div><div class="ab cl me mf go mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ha hb hc hd he"><p id="1127" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">披露声明:这些观点是作者的观点。除非本帖中另有说明，否则Capital One不属于所提及的任何公司，也不被其认可。使用或展示的所有商标和其他知识产权均为其各自所有者所有。本文为2019首都一。</em></p></div></div>    
</body>
</html>