<html>
<head>
<title>Lessons from AlphaZero: Connect Four</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AlphaZero的教训:连接四个</h1>
<blockquote>原文：<a href="https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68?source=collection_archive---------0-----------------------#2018-06-13">https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68?source=collection_archive---------0-----------------------#2018-06-13</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div class="er es hf"><img src="../Images/2d68e8e296fd7599ccf9a9e514bd48bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Rx5PGQ2Pj4SFOqjeph_glw.jpeg"/></div></figure><div class=""/><p id="7e5c" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">欢迎回来！在我们的<a class="ae jj" rel="noopener" href="/oracledevs/lessons-from-implementing-alphazero-7e36e9054191">上一期</a>中，我们简要回顾了DeepMind的AlphaZero算法如何快速学会比最好的人类(有时也是现有的最好算法)更好地玩游戏。回想一下，它使用基于神经网络的蒙特卡罗树搜索(MCTS)来与自己对弈，并使用这些自我对弈的结果来进一步训练网络，处于自我强化的循环中。</p><p id="45eb" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在接下来的几个帖子中，我们将分享一些我们从教它玩<a class="ae jj" href="https://en.wikipedia.org/wiki/Connect_Four" rel="noopener ugc nofollow" target="_blank">连接四个</a>中学到的经验。</p><h1 id="0b10" class="jk jl ho bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">为什么连接四个？</h1><p id="e645" class="pw-post-body-paragraph il im ho in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">为了实现和测试AlphaZero，我们首先要选择一个游戏让它玩。与围棋(在难度的一个极端)或井字游戏(在另一个极端)不同，Connect Four似乎提供了足够的复杂性来引起兴趣，同时仍然足够小，可以快速迭代。特别是我们选择了常见的6x7变体(如上图)。</p><p id="6519" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">它还有另一个好处:作为一个完全<a class="ae jj" href="https://en.wikipedia.org/wiki/Solved_game" rel="noopener ugc nofollow" target="_blank">解决的游戏</a>，我们可以根据最优策略测试我们的模型。也就是说，一旦我们使用AlphaZero训练了我们的网络，我们就可以向它输入我们知道正确答案的测试板位置，以获得有意义的“基本事实”准确性。</p><p id="6abe" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">此外，它让我们发现在给定的架构下，我们的网络<em class="kn">能有多完美。让我们来看看它是如何工作的。</em></p><h1 id="4f70" class="jk jl ho bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">培养</h1><p id="5b95" class="pw-post-body-paragraph il im ho in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">培训和评估流程如下:</p><ol class=""><li id="0c3b" class="ko kp ho in b io ip is it iw kq ja kr je ks ji kt ku kv kw bi translated">使用Connect Four <em class="kn">解算器</em>(即，将告诉您任何板位置的正确移动的程序)来生成带标签的训练和测试集。</li><li id="cbc6" class="ko kp ho in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">选择神经网络体系结构。AlphaZero的论文给出了一个很好的起点，但显而易见，不同的游戏将受益于对它的调整。</li><li id="2d2c" class="ko kp ho in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">使用监督学习来训练带有标签的训练集的网络。这给了我们一个关于架构应该能够学习得多好的上限。</li><li id="af48" class="ko kp ho in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">使用相同的网络架构训练AlphaZero，并使用标记的测试集对其进行评估，以查看它与上限的接近程度。</li></ol><p id="716d" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">假设AlphaZero训练的网络达到了90%的测试准确率。这告诉我们什么？</p><ul class=""><li id="d3ef" class="ko kp ho in b io ip is it iw kq ja kr je ks ji lc ku kv kw bi translated">如果同样的架构在监督训练下达到了99%，那么网络<em class="kn">可以</em>学到比AlphaZero教给它的更多的东西，我们应该改进AlphaZero的自玩组件。</li><li id="3e1e" class="ko kp ho in b io kx is ky iw kz ja la je lb ji lc ku kv kw bi translated">如果监督训练只达到91%，那么网络架构本身很可能是限制因素，应该加以改进。</li></ul><p id="672f" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">使用这种策略帮助我们调整网络架构，发现错误，并在我们的MCTS自我游戏中进行其他改进。</p><h1 id="aaa1" class="jk jl ho bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">挑战</h1><p id="6d14" class="pw-post-body-paragraph il im ho in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">当生成带标签的训练数据时，我们将随机的电路板位置输入到一个连接四个解算器中。问题是，什么构成了“随机”</p><p id="40e7" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">因为AlphaZero应该学会只玩完美的游戏，它的网络最终将不再看到玩得不好的位置。因此，在他们身上进行测试是不公平的。另一方面，Connect Four非常简单，完美的游戏屈指可数，因此不可能只使用完美的棋盘位置来构建大型训练集。</p><p id="c468" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">另一个困难是确定什么是一个职位的正确答案。在任何给定的位置，可能会有多次移动导致胜利。赢的越快越好吗？好多少？如果有<em class="kn">而不是</em>一个确定的胜利，你应该选择输得最慢的一步棋(假设对手完美)，还是最有可能让他们犯错的一步棋(如果对手不完美)？你如何做到精确？</p><p id="6b71" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">最后，我们决定使用非常不完美的游戏中的位置来生成我们的标签集，从而使我们经过AlphaZero训练的网络在比较中处于劣势。然而，正如我们很快就会看到的那样，它在这种不利条件下表现得出奇地好。</p><p id="03a8" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们还决定使用两种不同的“最佳行动”定义当生成<strong class="in hp">【强】</strong>训练/测试集时，只有当一步棋导致<em class="kn">最快的</em>可能胜利(或最慢的可能失败)时，我们才会将其标记为正确。在<strong class="in hp">“弱”</strong>的情况下，<em class="kn">所有的</em>赢棋都被赋予同等的权重。为了保持比较的公平性，我们还在强模式和弱模式下训练了AlphaZero，鼓励它在前一种情况下更喜欢更快的胜利(通过根据游戏长度缩放最终结果值)。让我们简单看一下。</p><h2 id="ce2f" class="ld jl ho bd jm le lf lg jq lh li lj ju iw lk ll jy ja lm ln kc je lo lp kg lq bi translated">强对弱AlphaZero</h2><p id="d914" class="pw-post-body-paragraph il im ho in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">在MCTS模拟期间，非终端位置从网络获得一个初始<em class="kn">值</em>，表示预期的游戏结果(在[-1，1]范围内)。另一方面，终端位置不需要这样的估计；我们可以直接给它们赋值+/-1。这对于<em class="kn">弱</em>训练来说没问题，在那里两个获胜的位置被认为是同样好的，不管到达那里需要多长时间。对于<em class="kn">强</em>训练，我们通过游戏长度来衡量终端结果。</p><p id="afc4" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">最快的连四胜是7步，最长的是42步(用尽整个棋盘)。一些快速代数表明，如果<em class="kn"> n </em>是实际的游戏长度，那么表达式1.18-(9 * n/350)<em class="kn"/>将产生一个在[0.1，1]中的值用于获胜。这使得MCTS更喜欢速赢(和慢输)。</p><h1 id="d21b" class="jk jl ho bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">评价</h1><p id="f88e" class="pw-post-body-paragraph il im ho in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">我们进行了两种评估:</p><ul class=""><li id="2ccf" class="ko kp ho in b io ip is it iw kq ja kr je ks ji lc ku kv kw bi translated"><strong class="in hp">网络专用</strong>:给网络一个测试板位置，检查其预测移动是否正确。</li><li id="580d" class="ko kp ho in b io kx is ky iw kz ja la je lb ji lc ku kv kw bi translated"><strong class="in hp"> MCTS </strong>:在网络支持下运行MCTS(有800次模拟)，评估它的首选行动。</li></ul><p id="b0ff" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">回想一下，我们使用了两种不同的培训方法:</p><ul class=""><li id="7281" class="ko kp ho in b io ip is it iw kq ja kr je ks ji lc ku kv kw bi translated"><strong class="in hp"> AlphaZero ("AZ") </strong>训练。</li><li id="c019" class="ko kp ho in b io kx is ky iw kz ja la je lb ji lc ku kv kw bi translated"><strong class="in hp">监督</strong>训练(即使用来自解算器的标记训练数据)。</li></ul><p id="eaba" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这给了我们四个组合来评价:<strong class="in hp">阿兹-网络，阿兹-MCTS，监督-网络，监督-MCTS。</strong></p><p id="fdda" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">最后，有两种训练模式(强对弱)。我们将依次查看每一项。</p><h1 id="9182" class="jk jl ho bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结果</h1><h2 id="6071" class="ld jl ho bd jm le lf lg jq lh li lj ju iw lk ll jy ja lm ln kc je lo lp kg lq bi translated">强劲的结果</h2><ul class=""><li id="fba5" class="ko kp ho in b io ki is kj iw lr ja ls je lt ji lc ku kv kw bi translated">监督网络:<strong class="in hp"> 96.20% </strong></li><li id="dda9" class="ko kp ho in b io kx is ky iw kz ja la je lb ji lc ku kv kw bi translated">受监管-MCTS: <strong class="in hp"> 97.29% </strong></li><li id="c48f" class="ko kp ho in b io kx is ky iw kz ja la je lb ji lc ku kv kw bi translated">AZ-网络:<strong class="in hp"> 95.70% </strong></li><li id="e9dc" class="ko kp ho in b io kx is ky iw kz ja la je lb ji lc ku kv kw bi translated">阿兹-MCTS: <strong class="in hp"> 96.95% </strong></li></ul><p id="af3a" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这告诉我们的是:</p><ol class=""><li id="f4af" class="ko kp ho in b io ip is it iw kq ja kr je ks ji kt ku kv kw bi translated">我们的AlphaZero训练几乎和监督训练一样好(尽管有前面提到的缺点)。</li><li id="bf7a" class="ko kp ho in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">尽管网络本身做得非常好，但是在MCTS使用它进一步降低了大约29%的错误率(从大约4%到大约3%)。</li></ol><h2 id="4ad9" class="ld jl ho bd jm le lf lg jq lh li lj ju iw lk ll jy ja lm ln kc je lo lp kg lq bi translated">微弱的结果</h2><ul class=""><li id="3a3b" class="ko kp ho in b io ki is kj iw lr ja ls je lt ji lc ku kv kw bi translated">监督网络:<strong class="in hp"> 98.93% </strong></li><li id="1645" class="ko kp ho in b io kx is ky iw kz ja la je lb ji lc ku kv kw bi translated">受监管-MCTS: <strong class="in hp"> 99.79% </strong></li><li id="947d" class="ko kp ho in b io kx is ky iw kz ja la je lb ji lc ku kv kw bi translated">阿兹-网络:<strong class="in hp"> 98.83% </strong></li><li id="c0ce" class="ko kp ho in b io kx is ky iw kz ja la je lb ji lc ku kv kw bi translated">阿兹-MCTS: <strong class="in hp"> 99.76% </strong></li></ul><p id="416d" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这一次，AlphaZero训练更加接近监督训练，几乎与之相当。此外，MCTS减少了高达80%的错误率，几乎每个位置都是正确的。</p><p id="6ef1" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在弱测试集中，一个位置平均有4.07次正确移动(相比之下，强测试集为2.16次)。这意味着在7个可能的移动中随机猜测只能得到58.1%。</p><h1 id="6ac0" class="jk jl ho bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结论</h1><p id="a246" class="pw-post-body-paragraph il im ho in b io ki iq ir is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji ha bi translated">Connect Four是一个很好的游戏，可以用来训练你的AlphaZero实现(并找出其中的错误)。它允许快速迭代，并且足够复杂以至于有趣。因为这是一个已解决的游戏，所以也有可能得到一个训练成功程度的客观测量。</p><p id="ccbb" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">当只寻找<em class="kn">任何</em>最优走法(不管它赢得多快，或者避免输得多快)，再加上MCTS，AlphaZero训练过的网络在400个位置中只有一次走错。甚至在必须找到<em class="kn">最快的</em>赢(或不输)，并且单独使用网络预测时，它也只在25个位置中出错一次。相当不错！</p><p id="914c" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在接下来的几周里，我们将看看我们所做的一些调整，以获得最好的结果。到时候见！</p><p id="60f0" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="kn"> Part 3现公布</em> <a class="ae jj" rel="noopener" href="/oracledevs/lessons-from-alphazero-part-3-parameter-tweaking-4dceb78ed1e5"> <em class="kn">此处</em> </a> <em class="kn">。</em></p></div></div>    
</body>
</html>