<html>
<head>
<title>Scalable and reliable data ingestion at Pinterest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pinterest上可扩展且可靠的数据摄取</h1>
<blockquote>原文：<a href="https://medium.com/pinterest-engineering/scalable-and-reliable-data-ingestion-at-pinterest-b921c2ee8754?source=collection_archive---------0-----------------------#2017-05-19">https://medium.com/pinterest-engineering/scalable-and-reliable-data-ingestion-at-pinterest-b921c2ee8754?source=collection_archive---------0-----------------------#2017-05-19</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="63c6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">杨宇| Pinterest工程师，数据</p><p id="45a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在Pinterest，我们使用数据来获得洞察力，以便为所有事情做出决策，并最终改善Pinners的整体体验。每天我们记录超过100的数据。为了有效地使用数据，我们首先必须可靠地接收数据，并为下游使用做好准备。在本帖中，我们将介绍我们的数据接收管道的发展，并展示我们当前的数据接收基础设施。展望未来，我们将有一系列后续文章来更详细地描述每个组件。</p><h2 id="c73b" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated"><strong class="ak">数据摄取概述</strong></h2><p id="2f1c" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">数据接收就是从各种来源收集数据，并将其移动到持久存储中。数据通常以各种格式分布在成千上万的主机上，因此以可靠、高效和可扩展的方式做到这一点是一项具有挑战性和趣味性的任务。</p><p id="4228" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据接收通常有以下要求:</p><ul class=""><li id="3568" class="kc kd hh ig b ih ii il im ip ke it kf ix kg jb kh ki kj kk bi translated">可靠性:数据收集和传输应该可靠，数据丢失最小</li><li id="6392" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">性能:管道应该具有高吞吐量和低延迟</li><li id="52e1" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">高效:应该使用最少的计算和存储资源</li><li id="b91a" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">灵活性:它应该支持各种来源和数据格式</li><li id="f5e3" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">自治:系统应该以最小的操作开销运行</li></ul><p id="8ae1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在Pinterest，数据集主要有两类:在线服务日志和数据库转储。服务日志是由跨越数千台主机的服务生成的连续日志流。收集服务日志的管道通常由三个阶段组成:日志收集、日志传输和日志持久化。数据库转储是逻辑数据库备份，每小时或每天生成一次。数据库接收通常包括数据库转储生成和后期处理。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kq"><img src="../Images/fc0824b62e34cea337a20af5e26e2645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SI7Rl_wE7ZNNgECh."/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Figure 1. Data ingestion infrastructure in 2014</figcaption></figure><p id="c7ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">图1显示了2014年的摄入管道。那时，我们在在线服务端使用卡夫卡作为中央消息传送器。应用服务器直接向Kafka写日志消息。每个Kafka broker上的数据上传器将Kafka日志文件上传到S3。在数据库转储方面，我们有一个定制的Hadoop流作业，从数据库中提取数据并将结果写入S3。但是，这种设置存在一些问题:</p><ul class=""><li id="5b37" class="kc kd hh ig b ih ii il im ip ke it kf ix kg jb kh ki kj kk bi translated">通过从appserver直接写入Kafka，如果Kafka代理出现故障，消息需要在服务器端缓冲到内存中。由于缓冲区大小有限，长时间的Kafka中断会导致数据丢失。</li><li id="e718" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">卡夫卡0.7没有复制。Kafka broker故障会导致数据丢失。</li><li id="6b68" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">为了最小化在线服务的影响，我们只能从从属节点获取数据库转储。我们需要跟踪主从映射，频繁的数据库转储故障转移会增加这些作业的失败。</li><li id="0b99" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">操作开销很高。</li></ul><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lg"><img src="../Images/45a09462bbaa3172c2b8d9302a8f41e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ss74NCTxbB46uwV7."/></div></div><figcaption class="lc ld et er es le lf bd b be z dx">Figure 2. Data Ingestion infrastructure in late 2016</figcaption></figure><p id="9743" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解决这些问题，我们增强了数据接收的各个阶段。图2显示了更新的架构，其中我们对数据接收管道进行了以下更改:</p><ul class=""><li id="58cf" class="kc kd hh ig b ih ii il im ip ke it kf ix kg jb kh ki kj kk bi translated">对于在线服务日志记录，服务将日志消息写入本地磁盘，而不是直接写入Kafka。</li><li id="f1b0" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">我们构建了一个名为<a class="ae lh" href="https://www.slideshare.net/DiscoverPinterest/singer-pinterests-logging-infrastructure" rel="noopener ugc nofollow" target="_blank"> Singer </a>的高性能日志代理，它将日志消息从主机上传到Kafka。Singer支持多种日志格式，并保证日志消息至少发送一次。</li><li id="84f8" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">我们发展了<a class="ae lh" rel="noopener" href="/@Pinterest_Engineering/introducing-pinterest-secor-e868d9400bec#.wwf6vz3pc"> Secor </a>并建立了一个名为Merced的数据持久化服务，将数据从Kafka转移到S3。Merced使用低级消费者来读取来自Kafka的消息，并采用主-工人方法在工人之间分配数据持久化工作负载。默塞德保证从卡夫卡到S3的消息只持续一次。</li><li id="8b09" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">我们为数据清理任务(例如，模式强制检查、重复数据删除、数据策略强制等)添加了一个清理阶段。</li><li id="ea38" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">在数据库接收端，数据库直接将逻辑转储存储到S3，并使用清理框架来生成下游处理所需的数据集，而不是使用Hadoop流从数据库中提取数据。</li><li id="a981" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">我们建立了端到端的审计和数据完整性检查，并提高了管道每个阶段的可见性，以减少运营开销。</li></ul><p id="babf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过这种设置，我们每天能够处理超过1500亿条消息和超过50TB的逻辑数据库转储。随着每月1.75亿用户群的增长，以及1000亿pin图的不断扩大，我们每天都面临着新的挑战:</p><ul class=""><li id="0be2" class="kc kd hh ig b ih ii il im ip ke it kf ix kg jb kh ki kj kk bi translated">由于数据库主服务器故障转移，逻辑CSV转储生成通常会延迟。这阻止了下游及时获得新数据。我们需要以增量方式接收数据库数据，以提高数据库接收的可靠性和性能。</li><li id="01b8" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">需要进一步简化数据管道的创建和启动。</li><li id="0cd0" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">当我们扩大Kafka集群的规模时，Kafka运营就成了一个挑战。</li><li id="03e3" class="kc kd hh ig b ih kl il km ip kn it ko ix kp jb kh ki kj kk bi translated">由于各种原因，更多的事件可能会迟到，我们需要一致地处理迟到的事件。</li></ul><p id="b8fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将继续改进我们的数据接收渠道，并将在后续帖子中分享更多内容。如果这些是让你兴奋的问题，<a class="ae lh" href="https://careers.pinterest.com/" rel="noopener ugc nofollow" target="_blank">加入我们的</a>。</p><p id="27e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="li">鸣谢:Pinterest的许多工程师帮助建立和改进了Pinterest的数据接收基础设施，包括Henry Cai、Roger Wang、Indy Prentice、Shawn Nguyen、Yi Yin、Dan Frankowski、Rob Wultsch、Ernie Souhrada、、Dmitry Chechik和许多其他人。</em></p></div></div>    
</body>
</html>