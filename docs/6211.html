<html>
<head>
<title>Introducing Complete the Look: a scene-based complementary recommendation system</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">介绍完整外观:基于场景的互补推荐系统</h1>
<blockquote>原文：<a href="https://medium.com/pinterest-engineering/introducing-complete-the-look-a-scene-based-complementary-recommendation-system-eb891c3fe88?source=collection_archive---------0-----------------------#2019-06-14">https://medium.com/pinterest-engineering/introducing-complete-the-look-a-scene-based-complementary-recommendation-system-eb891c3fe88?source=collection_archive---------0-----------------------#2019-06-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="73e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Eric Kim &amp; Eileen Li |视觉搜索</p><p id="815a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在Pinterest的视觉搜索团队中，我们一直致力于帮助人们从视觉上发现新想法，即使他们无法用语言来描述他们正在寻找的东西。在传统的图像搜索系统中，目标是返回视觉上类似于查询图像的结果，但是我们正在使用视觉发现引擎，我们需要从更广泛的场景中识别并返回视觉组件，以推荐服装或客厅风格等想法，并在查询中进行区分和个性化。这使得更大的场景与任何给定大头针中的主要部分一样重要。大头针内的每个视觉对象都是一个搜索和发现的机会。</p><p id="baff" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在最新的开发中，为了获得灵感和可购买产品的建议，我们建立了Complete the Look，它利用丰富的场景上下文来推荐时尚和家居装饰别针的视觉兼容结果。完整的外观考虑了服装、体型、季节、室内与室外、各种家具以及房间的整体美学等背景，通过视觉搜索技术提供基于品味的推荐。</p><p id="e9a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在早期测试中，我们发现这项技术的表现明显优于以前的推荐系统。你可以在2019年计算机视觉和模式识别大会(CVPR)上接受的论文中找到更多细节:<a class="ae jc" href="https://arxiv.org/abs/1812.01748" rel="noopener ugc nofollow" target="_blank">完成Look:基于场景的补充产品推荐</a>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/5662f4b229e8c194bb5f2e3d0ae3ecb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jIwpSLdzPy3e_J8r"/></div></div></figure><p id="f4e6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">建模风格的兼容性由于其复杂性和主观性而具有挑战性。现有的工作集中于预测产品图像之间的兼容性(例如，包含t恤的图像和包含牛仔裤的图像)。然而，这些方法忽略了真实世界的“场景”图像，例如街道风格的大头针，这可能带来照明和姿势变化的复杂性，但另一方面，可能潜在地提供关键上下文(例如，用户的体型或季节)以进行更准确的推荐。</p><p id="9b2c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的解决方案是完成外观，这是一种执行视觉补充的新方法。视觉互补系统应该推荐与查询图像互补或相匹配的结果。例如，你可能在视觉上寻找与裙子相配的鞋子。该查询的结果不受视觉相似性的限制，但是可以探索风格相似性的替代维度<strong class="ig hi">。</strong>视觉搭配系统有助于搭配你的服装或寻找与新桌子相配的椅子。</p><h1 id="e4a2" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">完成外观任务</h1><p id="c977" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">在我们讨论CTL模型细节之前，让我们形式化一些术语。我们将<strong class="ig hi">场景图像</strong>定义为“在野外”的真实世界图像，比如一个人在阳光明媚的日子外出或者一个别致的卧室环境。这与<strong class="ig hi">产品图像</strong>形成对比，后者是产品的特写图像，通常带有白色背景。</p><p id="6073" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将CTL任务定义为:给定一幅<strong class="ig hi">场景图像</strong>和一幅<strong class="ig hi">产品图像</strong>，计算距离的定量度量，使得距离度量反映场景和产品之间的<strong class="ig hi">视觉互补性</strong>。二元分类器或重分类器都可以使用这种距离度量。</p><h1 id="9543" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">资料组</h1><p id="1106" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">为了训练我们的模型，我们收集了一个带标签的数据集，我们在这里公开发布了这个数据集。数据集包括场景和产品图像对的正面示例，以及产品类别和边界框注释。每一对都增加了从同一类别中随机抽取的负产品图像。我们的模型在训练期间将这个三元组作为输入。</p><p id="083e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们不想让模型记住确切的产品，所以我们做了一个额外的预处理步骤，从原始场景图像中裁剪出产品:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ks"><img src="../Images/517404ec9d075b59ff32c6b34ce0c277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ooGObsoO9c9HxsAP"/></div></div></figure><p id="46f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个额外的步骤迫使模型严格独立于视觉相似性来学习场景和产品的兼容性。</p><h1 id="be9b" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">模型概述</h1><p id="fe18" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">CTL模型是一个深度卷积前馈神经网络，由两个模块组成:图像特征化器和CTL头。CTL head将全局特征相似性与局部空间注意机制相结合，该机制鼓励模型关注图像的特定区域，以告知其决策。我们使用ResNet50模型架构作为图像特征，在ImageNet上进行了预处理。在所有实验中，我们不微调ResNet50网络。</p><p id="33e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">CTL模型包括三个步骤:</p><p id="9a8d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> (1)特征化场景和产品图片</strong></p><p id="500c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，模型使用ResNet50网络为场景和产品图像生成基本特征。我们使用block4特征图。</p><p id="03c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> (2)计算全局相似度。</strong></p><p id="2264" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，我们计算场景和每个正负产品图像之间的全局相似性度量。</p><p id="0dab" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是通过从ResNet50特征地图中计算场景和产品嵌入并计算两个嵌入之间的L2距离来实现的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/77ce6ef62dc83b394527920cf4117a38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tQ4YMdemTHWQrViz"/></div></div></figure><p id="2a73" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">规范中的两个术语分别是场景嵌入和产品嵌入。</p><p id="d5bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> (3)计算局部相似度。</strong></p><p id="46e4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们计算基于类别的局部注意力显著图，该图鼓励模型关注场景中的细粒度细节，以告知其决策。</p><p id="ed4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，我们将产品嵌入与场景图像的中间特征图中的每个空间区域进行匹配，例如ResNet50基本网络的block3。因为不是所有的场景区域都是同等相关的，所以我们通过基于类别的注意力图对匹配进行加权，注意力图被定义为场景区域嵌入和目标类别嵌入之间的L2距离:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kt"><img src="../Images/ebb09b7bd0567472f5d121016731dbfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/0*GgXH0JbiCogFaDsl"/></div></figure><p id="2954" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中s，p是场景和乘积，c是p的范畴，f_i是区域I的场景嵌入，e_c是范畴c的L2归一化范畴嵌入</p><p id="ac40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">注意力地图是基于类别的，因为不同的项目在兼容性方面关注不同的东西。例如，鞋子与衣服的其他部分搭配很重要，而对于家居装饰来说，抱枕与房间的整体美学相匹配也很重要。</p><p id="2b10" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最终的相似性度量是全局和局部相似性的平均值:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ku"><img src="../Images/9dab4583e17898b955950dc61397e9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HH2vFB9vKnunQU0k"/></div></div></figure><h1 id="ea6d" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">损失函数</h1><p id="0fc4" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们使用<strong class="ig hi">三元组损失公式</strong>训练模型，其中输入三元组为:(场景图像、正图像、负图像)。我们使用铰链损耗，它促使场景和正产品图像之间的距离小于场景和负产品图像之间的距离:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kv"><img src="../Images/a6c55affe3dbb3c60e34ec665c869930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/0*h3IJlnhLaWfhGkLW"/></div></figure><h1 id="d6ae" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">实验</h1><p id="5cf1" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">我们将我们的CTL模型与三个离线评估数据集上的几个基线进行了比较，包括时尚和家居装饰设置。对于二元分类和Top-K精度设置，我们发现我们的CTL模型始终优于基线。</p><p id="4bc7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">二元分类:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kw"><img src="../Images/daa7a8570916e8e64ccc854797a4a2bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OQgrVDKpM4WALShY"/></div></div></figure><p id="006e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有趣的是，直接将ResNet50特性用于CTL任务并不比随机选择好多少。这表明<strong class="ig hi">视觉兼容性</strong>不同于<strong class="ig hi">视觉相似性</strong>，因此有必要从数据中学习兼容性的概念。</p><p id="235d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最高精度:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/10dd7a260256455e693b2e8b4d699f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1R7KqRWM9iuRx55h"/></div></div></figure><h1 id="28fa" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">定性结果</h1><p id="4bfe" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">以下是CTL模型为测试集中的几幅图像提供的建议:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/ce5bbefcd18d4ffb6227a2f384111db6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*j8vMEPm6Bhqr5Nst"/></div></div></figure><p id="4177" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，完整的场景和(地面实况)产品图像仅用于演示，不作为我们系统的输入。</p><p id="d6a9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">定性来说，生成的产品与场景是兼容的。该模型已经学会建议产品不仅在视觉上类似于地面真相(例如相同的颜色)，但也包括具有相同风格的其他颜色(例如极简主义)。</p><h1 id="bad8" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">注意力地图</h1><p id="8d20" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">以下是CTL模型在测试场景图像上生成的注意力图的可视化:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kx"><img src="../Images/4d92682a90350856d73a0db513ecbdea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KeGnm5-Krcbl4fYFTvWPKQ.png"/></div></div></figure><p id="e4e2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">“A”列是我们的注意力地图，而“S”列是一个通用显著对象检测器的输出，<a class="ae jc" href="http://www.zhaoliming.net/research/deepsaliency" rel="noopener ugc nofollow" target="_blank">深度显著性</a>。</p><p id="9c5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在时尚领域，我们的模特学会了在推荐服装时关注模特的着装。相比之下，室内设计领域的注意力地图更加分散，关注许多对象，而不是单一主题。这表明该模型在推荐互补产品时考虑了房间的整体美学，而不是专注于房间中的单个特定对象。</p><h1 id="bf1a" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">摘要</h1><p id="476a" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">“完成外观”是一种新颖的视觉补充方法，它利用场景图像的丰富上下文来提供高度个性化的推荐。这个项目是视觉搜索团队在Pinterest研究的视觉搜索领域中许多令人兴奋的问题之一。我们将继续致力于使用最新的视觉搜索技术在Pinterest上提供推荐。</p><h1 id="656f" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">感谢</h1><p id="4264" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">这项工作是与王合作完成的，当时他是Pinterest视觉搜索领域的博士实习生。我们要感谢Julian McAuley、Jure Leskovec和Charles Rosenberg在项目期间提供的指导。</p><p id="7608" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ky">此外，我们还要感谢何瑞宁、李、Larkin Brown、于哲飞、陈凯丰、Jen Chan、Seth Park、Aimee Rancer、Andrew Zhai、、朱瑞民、、Jean Yang、Zhong、Michael Feng、Dmitry Kislyuk和对我们工作的帮助。</em></p></div></div>    
</body>
</html>