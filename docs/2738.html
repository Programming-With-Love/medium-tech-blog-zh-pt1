<html>
<head>
<title>SONAR Data Classification Using Single Layer Perceptrons</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用单层感知器的声纳数据分类</h1>
<blockquote>原文：<a href="https://medium.com/edureka/perceptron-learning-algorithm-d30e8b99b156?source=collection_archive---------1-----------------------#2017-12-08">https://medium.com/edureka/perceptron-learning-algorithm-d30e8b99b156?source=collection_archive---------1-----------------------#2017-12-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/5e78c6fe2d0d18f335a19a5106b9939e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*gpH4JC6Dqx_hIjrrcrq1Og.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Perceptron learning Algorithm — Edureka</figcaption></figure><p id="4fd4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">众所周知，感知器是创建深度神经网络的基本构件。因此，很明显，我们应该从感知器开始我们的深度学习之旅，并学习如何使用TensorFlow来实现它，以解决不同的问题。以下是本文将涉及的主题:</p><ul class=""><li id="01ae" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">感知器作为线性分类器</li><li id="baaa" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">使用张量流库实现感知器</li><li id="4940" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">使用单层感知器的声纳数据分类</li></ul><h1 id="75f2" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">分类问题的类型</h1><p id="d2c3" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">人们可以将所有可以使用神经网络解决的分类问题分为两大类:</p><ul class=""><li id="3ccc" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi">线性可分问题</strong></li><li id="f3a1" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">非线性可分问题</strong></li></ul><p id="e31b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">基本上，如果你可以用一条线将数据集分成两类，那么这个问题就是线性可分的。<em class="le">例如，从一群猫狗中分离出猫</em>。相反，在非线性可分离问题的情况下，数据集包含多个类，并且需要非线性线来将它们分成各自的类。<em class="le">比如手写数字的分类</em>。让我们通过绘制线性可分问题和非线性问题数据集的图表来形象化两者之间的差异:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/c9ce4bf5b589d56c3f2a433ca7c22f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*6YncLXViz52OubA73zeMgg.jpeg"/></div></figure><p id="5f96" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因为你们都熟悉AND门，我将用它作为例子来解释感知器如何作为线性分类器工作。</p><blockquote class="lk ll lm"><p id="2023" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated"><strong class="ir hi">注:</strong>当您处理更复杂的问题时，例如图像识别，您想要捕捉的数据中的关系变得高度非线性，因此需要一个由多个人工神经元组成的网络，称为人工神经网络。</p></blockquote><h1 id="f6b2" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">感知器作为与门</h1><p id="5ac2" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">众所周知，在所有其他情况下，如果两个输入都是1和0，则与门产生的输出为1。因此，感知器可用作分隔符或判定线，将与门的输入集分为两类:</p><ul class=""><li id="84c4" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi">类别1: </strong>输出为0的输入，位于判定线以下。</li><li id="d23c" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">类别2: </strong>输出为1的输入，位于判定线或分隔符上方。</li></ul><p id="0ef5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下图显示了上述使用感知器对与门输入进行分类的想法:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/51a34adae85b9060a234890725af4032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*B6EwH-smwOQYfIsEZDAgEA.png"/></div></figure><p id="8bd7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">到目前为止，您已经了解线性感知器可以用于将输入数据集分为两类。但是，它是如何对数据进行分类的呢？</p><p id="d732" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在数学上，可以将感知器表示为权重、输入和偏差(垂直偏移)的函数:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/95c24f86bbdf0ff78512d2ffdaa1b5ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*r4UFMln8lJYEzExgD4Omiw.png"/></div></figure><ul class=""><li id="d160" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">感知器接收的每个输入都根据其对获得最终输出的贡献量进行了加权。</li><li id="ce8a" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">偏差允许我们移动决策线，以便它能够最好地将输入分成两类。</li></ul><p id="e321" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">理论讲得够多了，让我们看看这篇博客中关于感知器学习算法的第一个例子，我将从头开始使用感知器实现与门。</p><h1 id="c2a3" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">感知器学习算法:与门的实现</h1><h2 id="fe8e" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">1.导入所有需要的库</h2><p id="2a03" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我将从导入所有需要的库开始。在这种情况下，我只需要导入一个库，即TensorFlow:</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="742d" class="lr kc hh mg b fi mk ml l mm mn">#import required library<br/>import tensorflow as tf</span></pre><h2 id="cb63" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">2.定义输入和输出的向量变量</h2><p id="7bd7" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">现在，我将为我的感知器创建用于存储输入、输出和偏差的变量:</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="a601" class="lr kc hh mg b fi mk ml l mm mn">#input1, input2 and bias<br/>train_in = [<br/>    [1., 1.,1],<br/>    [1., 0,1],<br/>    [0, 1.,1],<br/>    [0, 0,1]]<br/> <br/>#output<br/>train_out = [<br/>[1.],<br/>[0],<br/>[0],<br/>[0]]</span></pre><h2 id="50a7" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated"><strong class="ak"> 3。定义权重变量</strong></h2><p id="e67e" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">现在，我需要定义权重变量，并在开始时给它分配一些随机值。因为这里有三个输入(输入1、输入2和偏差)，所以每个输入需要3个权重值。因此，我将为我们的权重定义一个形状为3×1的张量变量，它将用随机值初始化:</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="2109" class="lr kc hh mg b fi mk ml l mm mn">#weight variable initialized with random values using random_normal()<br/>w = tf.Variable(tf.random_normal([3, 1], seed=12))</span></pre><blockquote class="lk ll lm"><p id="df31" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated"><strong class="ir hi">注意:</strong>在TensorFlow中，变量是处理不断变化的神经网络权重的唯一方式，这些权重随着学习过程而更新。</p></blockquote><h2 id="ab2c" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">4.为输入和输出定义占位符</h2><p id="013d" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">在TensorFlow中，您可以指定占位符，以便在运行时接受外部输入。因此，我将定义两个占位符——x代表输入，y代表输出。稍后，您将理解如何向占位符输入内容。</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="f90d" class="lr kc hh mg b fi mk ml l mm mn">#Placeholder for input and output<br/>x = tf.placeholder(tf.float32,[None,3])<br/>y = tf.placeholder(tf.float32,[None,1])</span></pre><h2 id="31fd" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">5.计算输出和激活函数</h2><p id="96c3" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">如前所述，感知器接收的输入首先乘以各自的权重，然后，所有这些加权的输入相加在一起。然后，将该求和值反馈给activation，以获得最终结果，如下图所示，后面是代码:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/db76225466a37bf5e003c96b3363520a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*j-6hkXCQws3T20CEPyN1og.png"/></div></figure><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="6909" class="lr kc hh mg b fi mk ml l mm mn">#calculate output <br/>output = tf.nn.relu(tf.matmul(x, w))</span></pre><blockquote class="lk ll lm"><p id="8f69" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated"><strong class="ir hi">注意:</strong>在这种情况下，我使用了<strong class="ir hi"> relu </strong>作为我的激活函数。其他重要的激活功能将在这篇关于感知器神经网络的博客中进一步介绍。</p></blockquote><h2 id="41d6" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">6.计算成本或误差</h2><p id="43e7" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">现在，我需要计算误差值w.r.t感知器输出和期望输出。通常，该误差被计算为均方误差，其仅仅是感知器输出和期望输出之差的平方，如下所示:</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="2170" class="lr kc hh mg b fi mk ml l mm mn">#Mean Squared Loss or Error<br/>loss = tf.reduce_sum(tf.square(output - y))</span></pre><h2 id="6993" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">7.最小化误差</h2><p id="5f2c" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">TensorFlow提供了缓慢改变每个变量(权重和偏差)的优化器，以便在连续迭代中最小化损失。最简单的优化器是梯度下降，我将在这种情况下使用。</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="5dae" class="lr kc hh mg b fi mk ml l mm mn">#Minimize loss using GradientDescentOptimizer with a learning rate of 0.01<br/>optimizer = tf.train.GradientDescentOptimizer(0.01)<br/>train = optimizer.minimize(loss)</span></pre><h2 id="c31a" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">8.初始化所有变量</h2><p id="b222" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">当你调用<em class="le"> tf时，变量没有被初始化。变量</em>。因此，我需要使用以下代码显式初始化TensorFlow程序中的所有变量:</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="aab7" class="lr kc hh mg b fi mk ml l mm mn">#Initialize all the global variables<br/>init = tf.global_variables_initializer()<br/>sess = tf.Session()<br/>sess.run(init)</span></pre><h2 id="ccd9" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated"><strong class="ak"> 9。迭代训练感知器</strong></h2><p id="0800" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">现在，我需要训练我们的感知器，即在连续迭代中更新权重和偏差的值，以最小化错误或损失。在这里，我将在1000个纪元中训练我们的感知机。</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="6ff3" class="lr kc hh mg b fi mk ml l mm mn">#Compute output and cost w.r.t to input vector<br/>sess.run(train, {x:train_in,y:train_out})<br/>cost = sess.run(loss,feed_dict={x:train_in,y:train_out})<br/>print('Epoch--',i,'--loss--',cost)</span></pre><p id="5bc4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的代码中，您可以观察到我是如何使用feed_dict将train_in(与门的输入集)和train_out(与门的输出集)分别提供给占位符x和y来计算成本或误差的。</p><h2 id="b61d" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">输出:</h2><p id="1dc6" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">以下是我的感知器模型经过训练后得到的最终输出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mo"><img src="../Images/d0078739aa340fee18f7210f43c6d088.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*g0g7POwHCEOM-vDIk4rUMA.png"/></div></figure><h2 id="182c" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated"><strong class="ak">激活功能</strong></h2><p id="bfc1" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">如前所述，激活函数应用于感知器的输出，如下图所示:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/fd8a474927124bbc5a39514d2366edcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*FspkEmvVk4hzBoCMh-MfmA.png"/></div></figure><p id="8330" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在前面的例子中，我已经向您展示了如何使用具有relu激活功能的线性感知器对AND门的输入集执行线性分类。但是，如果您希望执行的分类本质上是非线性的，该怎么办呢？在这种情况下，您将使用一种非线性激活功能。一些突出的非线性激活函数如下所示:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/3da06493e2348bf0a4303fd240b554a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*EyZSmprAdIYH2-zK3yveZg.png"/></div></figure><p id="c38e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">TensorFlow库提供了应用激活函数的内置函数。下面列出了上述激活功能的内置功能:</p><ul class=""><li id="968b" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi"> tf.sigmoid(x，name =无)</strong></li></ul><ol class=""><li id="34e4" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm mp jt ju jv bi translated">按元素计算x的sigmoid</li><li id="7107" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm mp jt ju jv bi translated">对于元素x，sigmoid的计算公式为-y = 1/(1+exp(-x))</li></ol><ul class=""><li id="a4fd" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi"> tf.nn.relu(功能，名称=无)</strong></li></ul><ol class=""><li id="6b31" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm mp jt ju jv bi translated">将校正后的线性计算为— <em class="le"> max(features，0) </em></li></ol><ul class=""><li id="1c5c" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi"> tf.tanh(x，name =无)</strong></li></ul><ol class=""><li id="6794" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm mp jt ju jv bi translated">计算x元素的双曲正切值</li></ol><p id="3d66" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">到目前为止，您已经学习了感知器如何工作，以及如何使用TensorFlow对其进行编程。因此，是时候向前迈进，应用我们对感知机的理解来解决声纳数据分类的一个有趣用例了。</p><h1 id="f4d9" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">使用单层感知器的声纳数据分类</h1><p id="54c8" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">在这个用例中，我得到了一个声纳数据集，其中包含了208种模式的数据，这些模式是通过在不同条件下以不同角度从金属圆柱体(水雷)和岩石上反射声纳信号而获得的。现在，如你所知，<strong class="ir hi">水雷</strong>是一种放置在水中的独立爆炸装置，用于破坏或摧毁水面舰艇或潜艇。因此，我们的目标是建立一个模型，可以根据我们的数据集预测物体是水雷还是岩石。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/f4d204f9b204e4e7ad93700e6fe2ad11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*1qFoF97FZ8eZeOfcRUIM9Q.png"/></div></figure><p id="2fdf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们看看我们的声纳数据集:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es mq"><img src="../Images/9de2491a9be187d979e433dba996261a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jlDG6SJD0p1dwkigUUGzQQ.png"/></div></div></figure><p id="fde5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，整个基本过程将与AND门的过程相同，差别很小，为了避免混淆，将对其进行讨论。让我向您介绍使用单层感知器对声纳数据集执行线性分类的所有步骤:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es mv"><img src="../Images/28531c5e3d4b93916daa914de8e8a145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1f9Rmrvk2crPvwEdrrZujA.png"/></div></div></figure><p id="d61f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然您已经对这个用例中涉及的所有步骤有了很好的了解，让我们继续使用TensorFlow对模型进行编程:</p><h2 id="aa06" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">1.导入所有需要的库:</h2><p id="7fe8" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">首先，我将从下面列出的所有必需的库开始:</p><ul class=""><li id="fde2" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi"> matplotlib库:</strong>提供绘制图形的函数。</li><li id="e167" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi"> tensorflow库:</strong>提供实现深度学习模型的函数。</li><li id="5b88" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi"> pandas、numpy和sklearn库:</strong>提供数据预处理的功能。</li></ul><h2 id="c382" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">2.读取和预处理数据集:</h2><p id="1ed6" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">在前面的例子中，我定义了输入和输出变量w.r.t. AND Gate，并显式地为它分配了所需的值。但是，在像声纳这样的实际用例中，您将获得需要读取和预处理的原始数据集，以便您可以围绕它训练您的模型。</p><ul class=""><li id="71ae" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">首先，我将使用read_csv()函数读取CSV文件(输入数据集)</li><li id="20f5" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">然后，我将把特征列(自变量)和输出列(因变量)分别分离为X和y</li><li id="303c" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">输出列由字符串分类值“M”和“R”组成，分别表示Rock和Mine。因此，我将它们标记为0和1wr . t . ' M '和' R '</li><li id="9331" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">在我将这些分类值转换成整数标签后，我将使用one_hot_encode()函数应用一个热编码，这将在下一步中讨论。</li></ul><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="a24d" class="lr kc hh mg b fi mk ml l mm mn">#Read the sonar dataset<br/>df = pd.read_csv("sonar.csv")<br/>print(len(df.columns))<br/>X = df[df.columns[0:60]].values<br/>y = df[df.columns[60]]<br/> <br/>#encode the dependent variable as it has two categorical values<br/>encoder = LabelEncoder()<br/>encoder.fit(y)<br/>y = encoder.transform(y)<br/>Y = one_hot_encode(y)</span></pre><h2 id="fbe5" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">3.一个热编码器的功能:</h2><p id="d38a" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">一个Hot编码器根据列中存在的标签数量添加额外的列。在这种情况下，我有两个标签0和1(代表岩石和矿井)。因此，将添加两个额外的列，对应于每个分类值，如下图所示:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/3c0d132b9d8d5200a9e2a5a3fddf7a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*LHksCj68m6v93gTD7POFgg.png"/></div></figure><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="ba33" class="lr kc hh mg b fi mk ml l mm mn">#function for applying one_hot_encoder<br/>def one_hot_encode(labels):<br/>  n_labels = len(labels)<br/>  n_unique_labels = len(np.unique(labels))<br/>  one_hot_encode = np.zeros((n_labels,n_unique_labels))<br/>  one_hot_encode[np.arange(n_labels), labels] = 1<br/>  return one_hot_encode</span></pre><h2 id="93d5" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">4.将数据集划分为训练和测试子集</h2><p id="fe6e" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">在处理任何深度学习项目时，您需要将您的数据集分成两部分，其中一部分用于训练您的深度学习模型，另一部分用于在模型训练完成后验证模型。因此，在这一步中，我还会将数据集分成两个子集:</p><ul class=""><li id="c0bf" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">训练子集:用于训练模型</li><li id="e483" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">测试子集:它用于验证我们训练好的模型</li></ul><p id="2142" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我将使用sklearn库中的train_test_split()函数来划分数据集:</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="fccb" class="lr kc hh mg b fi mk ml l mm mn">#Divide the data in training and test subset<br/>X,Y = shuffle(X,Y,random_state=1)<br/>train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=0.20, random_state=42)</span></pre><h2 id="43be" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">5.定义变量和占位符</h2><p id="2de5" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">这里，我将为以下实体定义变量:</p><ul class=""><li id="d545" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi">学习率:</strong>权重将被调整的量。</li><li id="a076" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">训练时期:</strong>迭代次数</li><li id="c780" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">成本历史:</strong>一个数组，存储连续时期的成本值。</li><li id="2a23" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">权重:</strong>用于存储权重值的张量变量</li><li id="ce33" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">偏差:</strong>用于存储偏差值的张量变量</li></ul><p id="c119" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">除了变量，我还需要可以输入的占位符。因此，我将为我的输入创建占位符，并在稍后用数据集填充它。最后，我会调用global_variable_initializer()来初始化所有的变量。</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="fa3d" class="lr kc hh mg b fi mk ml l mm mn">#define all the variables to work with the tensors<br/>learning_rate = 0.1<br/>training_epochs = 1000<br/> <br/>cost_history = np.empty(shape=[1],dtype=float)<br/> <br/>n_dim = X.shape[1]<br/>n_class = 2<br/> <br/>x = tf.placeholder(tf.float32,[None,n_dim])<br/>W = tf.Variable(tf.zeros([n_dim,n_class]))<br/>b = tf.Variable(tf.zeros([n_class])</span></pre><h2 id="1cef" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">6.计算成本或误差</h2><p id="6295" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">类似于与门实现，我将计算我们的模型产生的成本或误差。在这种情况下，我将使用<strong class="ir hi"> <em class="le">交叉熵</em> </strong>来计算误差，而不是均方误差。</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="05bb" class="lr kc hh mg b fi mk ml l mm mn">y_ = tf.placeholder(tf.float32,[None,n_class])<br/>y = tf.nn.softmax(tf.matmul(x, W)+ b)<br/>cost_function = tf.reduce_mean(-tf.reduce_sum((y_ * tf.log(y)),reduction_indices=[1]))<br/>training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)</span></pre><h2 id="d1e0" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">7.在连续的时期中训练感知器模型</h2><p id="e792" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">现在，我将在连续的纪元中训练我的模型。在每个时期中，计算成本，然后基于该成本，优化器修改权重和偏差变量，以最小化误差。</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="3389" class="lr kc hh mg b fi mk ml l mm mn">#Minimizing the cost for each epoch<br/>for epoch in range(training_epochs):<br/>    sess.run(training_step,feed_dict={x:train_x,y_:train_y})<br/>    cost = sess.run(cost_function,feed_dict={x: train_x,y_: train_y})<br/>    cost_history = np.append(cost_history,cost)<br/>    print('epoch : ', epoch,  ' - ', 'cost: ', cost)</span></pre><h2 id="e392" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">8.基于测试子集的模型验证</h2><p id="be65" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">如前所述，训练模型的准确性是基于测试子集计算的。因此，首先，我将把测试子集输入到我的模型中，并获得输出(标签)。然后，我会将从模型中获得的输出与实际或期望的输出进行比较，最后，我会计算正确预测占测试子集总预测的百分比。</p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="104a" class="lr kc hh mg b fi mk ml l mm mn">#Run the trained model on test subset<br/>pred_y = sess.run(y, feed_dict={x: test_x})<br/> <br/>#calculate the correct predictions<br/>correct_prediction = tf.equal(tf.argmax(pred_y,1), tf.argmax(test_y,1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<br/>print("Accuracy:",sess.run(accuracy))</span></pre><h2 id="f687" class="lr kc hh bd kd ls lt lu kh lv lw lx kl ja ly lz kp je ma mb kt ji mc md kx me bi translated">输出:</h2><p id="bbea" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">以下是培训完成后您将获得的输出:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mw"><img src="../Images/de708c47bcf44deaf9a1877e608a2d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*xm6M84VGDbxL-lrt50DY-Q.png"/></div></figure><p id="fee0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如你所见，我们得到了83.34% 的准确度，这已经足够下降了。现在，让我们通过绘制<strong class="ir hi">成本对时期数量</strong>的图表来观察成本或误差在连续时期中是如何减少的:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/f26b77a95be83c9e8626cb632b3bc6ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*JSbUW_-xAs7c043PU4ZeXA.png"/></div></figure><p id="db20" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">使用单层感知器进行声纳数据分类的完整代码</strong></p><pre class="lg lh li lj fd mf mg mh mi aw mj bi"><span id="282e" class="lr kc hh mg b fi mk ml l mm mn">#import the required libraries<br/>import matplotlib.pyplot as plt<br/>import tensorflow as tf<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.utils import  shuffle<br/>from sklearn.model_selection import train_test_split<br/> <br/>#define the one hot encode function<br/>def one_hot_encode(labels):<br/>    n_labels = len(labels)<br/>    n_unique_labels = len(np.unique(labels))<br/>    one_hot_encode = np.zeros((n_labels,n_unique_labels))<br/>    one_hot_encode[np.arange(n_labels), labels] = 1<br/>    return one_hot_encode<br/> <br/>#Read the sonar dataset<br/>df = pd.read_csv("sonar.csv")<br/>print(len(df.columns))<br/>X = df[df.columns[0:60]].values<br/>y=df[df.columns[60]]<br/>#encode the dependent variable containing categorical values<br/>encoder = LabelEncoder()<br/>encoder.fit(y)<br/>y = encoder.transform(y)<br/>Y = one_hot_encode(y)<br/> <br/>#Transform the data in training and testing<br/>X,Y = shuffle(X,Y,random_state=1)<br/>train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=0.20, random_state=42)<br/> <br/> <br/>#define and initialize the variables to work with the tensors<br/>learning_rate = 0.1<br/>training_epochs = 1000<br/> <br/>#Array to store cost obtained in each epoch<br/>cost_history = np.empty(shape=[1],dtype=float)<br/> <br/>n_dim = X.shape[1]<br/>n_class = 2<br/> <br/>x = tf.placeholder(tf.float32,[None,n_dim])<br/>W = tf.Variable(tf.zeros([n_dim,n_class]))<br/>b = tf.Variable(tf.zeros([n_class]))<br/> <br/>#initialize all variables.<br/>init = tf.global_variables_initializer()<br/> <br/>#define the cost function<br/>y_ = tf.placeholder(tf.float32,[None,n_class])<br/>y = tf.nn.softmax(tf.matmul(x, W)+ b)<br/>cost_function = tf.reduce_mean(-tf.reduce_sum((y_ * tf.log(y)),reduction_indices=[1]))<br/>training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)<br/> <br/>#initialize the session<br/>sess = tf.Session()<br/>sess.run(init)<br/>mse_history = []<br/> <br/>#calculate the cost for each epoch<br/>for epoch in range(training_epochs):<br/>    sess.run(training_step,feed_dict={x:train_x,y_:train_y})<br/>    cost = sess.run(cost_function,feed_dict={x: train_x,y_: train_y})<br/>    cost_history = np.append(cost_history,cost)<br/>    print('epoch : ', epoch,  ' - ', 'cost: ', cost)<br/> <br/>pred_y = sess.run(y, feed_dict={x: test_x})<br/> <br/>#Calculate Accuracy<br/>correct_prediction = tf.equal(tf.argmax(pred_y,1), tf.argmax(test_y,1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<br/>print("Accuracy:",sess.run(accuracy))<br/> <br/>plt.plot(range(len(cost_history)),cost_history)<br/>plt.axis([0,training_epochs,0,np.max(cost_history)])<br/>plt.show()</span></pre><h1 id="f977" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结论</h1><p id="40bb" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">在这篇关于感知器学习算法的文章中，您了解了什么是感知器，以及如何使用TensorFlow库实现它。您还了解了感知器如何用作线性分类器，我演示了如何使用感知器来实现“与”门。最后，我向前迈了一步，应用感知器解决了一个实时用例，在这个用例中，我对声纳数据集进行了分类，以检测出<strong class="ir hi">岩石</strong>和<strong class="ir hi">矿井</strong>之间的差异。</p><p id="6239" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，那么你可以参考<a class="ae my" href="https://www.edureka.co/blog/?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=perceptron-learning-algorithm" rel="noopener ugc nofollow" target="_blank"> Edureka的官方网站。</a></p><p id="6864" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释深度学习的各个其他方面。</p><blockquote class="lk ll lm"><p id="1554" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">1.<a class="ae my" rel="noopener" href="/edureka/tensorflow-tutorial-ba142ae96bca">张量流教程</a></p><p id="6e4a" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">2.<a class="ae my" rel="noopener" href="/edureka/pytorch-tutorial-9971d66f6893"> PyTorch教程</a></p><p id="4dba" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">3.<a class="ae my" rel="noopener" href="/edureka/tensorflow-object-detection-tutorial-8d6942e73adc">tensor flow中的对象检测</a></p><p id="2812" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">4.<a class="ae my" rel="noopener" href="/edureka/neural-network-tutorial-2a46b22394c9">神经网络教程</a></p><p id="0e94" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">5.什么是反向传播？</p><p id="44cc" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">6.<a class="ae my" rel="noopener" href="/edureka/convolutional-neural-network-3f2c5b9c4778">卷积神经网络</a></p><p id="ac3f" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">7.<a class="ae my" rel="noopener" href="/edureka/capsule-networks-d7acd437c9e">胶囊神经网络</a></p><p id="d989" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">8.<a class="ae my" rel="noopener" href="/edureka/recurrent-neural-networks-df945afd7441">递归神经网络</a></p><p id="d999" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">9.<a class="ae my" rel="noopener" href="/edureka/autoencoders-tutorial-cfdcebdefe37">自动编码器教程</a></p><p id="3fde" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">10.<a class="ae my" rel="noopener" href="/edureka/restricted-boltzmann-machine-tutorial-991ae688c154">受限玻尔兹曼机教程</a></p><p id="6821" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">11.<a class="ae my" rel="noopener" href="/edureka/pytorch-vs-tensorflow-252fc6675dd7"> PyTorch vs TensorFlow </a></p><p id="134f" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">12.<a class="ae my" rel="noopener" href="/edureka/deep-learning-with-python-2adbf6e9437d">用Python进行深度学习</a></p><p id="bdf2" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">13.<a class="ae my" rel="noopener" href="/edureka/artificial-intelligence-tutorial-4257c66f5bb1">人工智能教程</a></p><p id="1c49" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">14.<a class="ae my" rel="noopener" href="/edureka/tensorflow-image-classification-19b63b7bfd95">张量流图像分类</a></p><p id="a9fd" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">15.<a class="ae my" rel="noopener" href="/edureka/artificial-intelligence-applications-7b93b91150e3">人工智能应用</a></p><p id="cd37" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">16.<a class="ae my" rel="noopener" href="/edureka/become-artificial-intelligence-engineer-5ac2ede99907">如何成为一名人工智能工程师？</a></p><p id="4a2a" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">17.<a class="ae my" rel="noopener" href="/edureka/q-learning-592524c3ecfc">问学习</a></p><p id="c037" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">18.<a class="ae my" rel="noopener" href="/edureka/apriori-algorithm-d7cc648d4f1e"> Apriori算法</a></p><p id="e859" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">19.<a class="ae my" rel="noopener" href="/edureka/introduction-to-markov-chains-c6cb4bcd5723">马尔可夫链与Python </a></p><p id="1654" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">20.<a class="ae my" rel="noopener" href="/edureka/artificial-intelligence-algorithms-fad283a0d8e2">人工智能算法</a></p><p id="c09f" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">21.<a class="ae my" rel="noopener" href="/edureka/best-laptop-for-machine-learning-a4a5f8ba5b">机器学习的最佳笔记本电脑</a></p><p id="2aa8" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">22.<a class="ae my" rel="noopener" href="/edureka/top-artificial-intelligence-tools-36418e47bf2a">12大人工智能工具</a></p><p id="673b" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">23.<a class="ae my" rel="noopener" href="/edureka/artificial-intelligence-interview-questions-872d85387b19">人工智能(AI)面试问题</a></p><p id="09ce" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">24.<a class="ae my" rel="noopener" href="/edureka/theano-vs-tensorflow-15f30216b3bc"> Theano vs TensorFlow </a></p><p id="3026" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">25.<a class="ae my" rel="noopener" href="/edureka/what-is-a-neural-network-56ae7338b92d">什么是神经网络？</a></p><p id="bcd1" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">26.<a class="ae my" rel="noopener" href="/edureka/pattern-recognition-5e2d30ab68b9">模式识别</a></p><p id="dfc8" class="ip iq le ir b is it iu iv iw ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl jm ha bi translated">27.<a class="ae my" rel="noopener" href="/edureka/alpha-beta-pruning-in-ai-b47ee5500f9a">人工智能中的阿尔法贝塔剪枝</a></p></blockquote></div><div class="ab cl mz na go nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ha hb hc hd he"><p id="1d3d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">原载于2017年12月8日www.edureka.co</em><a class="ae my" href="https://www.edureka.co/blog/perceptron-learning-algorithm/" rel="noopener ugc nofollow" target="_blank"><em class="le"/></a><em class="le">。</em></p></div></div>    
</body>
</html>