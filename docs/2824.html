<html>
<head>
<title>Apache Spark Architecture -Distributed System Architecture Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark架构-分布式系统架构解释</h1>
<blockquote>原文：<a href="https://medium.com/edureka/spark-architecture-4f06dcf27387?source=collection_archive---------0-----------------------#2018-09-28">https://medium.com/edureka/spark-architecture-4f06dcf27387?source=collection_archive---------0-----------------------#2018-09-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/093334e9002319426d46948e96370455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*qZ51Ouc6K6Np61pW0d4FRQ.png"/></div><figcaption class="il im et er es in io bd b be z dx">Apache Spark Architecture — Edureka</figcaption></figure><p id="8099" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Apache Spark是一个开源集群计算框架，它正在点燃大数据世界。与Hadoop相比，Spark的内存性能快100倍，磁盘性能快10倍。在本文中，我将向您简要介绍Spark架构以及Spark架构的基础。</p><p id="c606" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇Spark架构文章中，我将讨论以下主题:</p><ul class=""><li id="6591" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">Spark及其特点</li><li id="f1e9" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">Spark架构概述</li><li id="4ba8" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">星火生态系统</li><li id="33b5" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">弹性分布式数据集</li><li id="a9a0" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">星火建筑的运作</li><li id="952f" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">在Spark Shell中使用Scala的示例</li></ul><h1 id="803c" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">Spark及其特点</h1><p id="9ec7" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">Apache Spark是一个用于实时数据处理的开源集群计算框架。Apache Spark的主要特点是它的<strong class="ir hi"> <em class="le">内存集群计算</em> </strong>提高了应用程序的处理速度。Spark提供了一个接口，通过隐式<strong class="ir hi"> <em class="le">数据并行和容错</em> </strong>对整个集群进行编程。它旨在涵盖广泛的工作负载，如批处理应用程序、迭代算法、交互式查询和流。</p><h1 id="672b" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">Apache Spark的特性:</h1><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/716ce476005abe92aaf1f698e5070d11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xX0UnCg6I_3J3CHkhR1N6A.png"/></div></div></figure><blockquote class="lo lp lq"><p id="fe1e" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><strong class="ir hi">速度:</strong> Spark运行速度比Hadoop MapReduce大规模数据处理快100倍。它还能够通过受控分区来实现这一速度。</p><p id="e66b" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><strong class="ir hi">强大的缓存<br/> </strong>简单的编程层提供了强大的缓存和磁盘持久化能力。</p><p id="adbb" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><strong class="ir hi">部署<br/> </strong>可以通过<strong class="ir hi"> <em class="hh"> Mesos，Hadoop via YARN，或者Spark自带的集群管理器进行部署。</em>T19】</strong></p><p id="b992" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><strong class="ir hi">实时</strong> <br/>由于<strong class="ir hi"> <em class="hh">内存计算，它提供实时计算&amp;低延迟。</em> </strong></p><p id="e0ec" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><strong class="ir hi"> Polyglot </strong> <br/> Spark提供Java、Scala、Python、r的高级API，Spark代码可以用这四种语言中的任意一种编写。它还提供了Scala和Python中的shell。</p></blockquote><h1 id="8cab" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">Spark架构概述</h1><p id="7748" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">Apache Spark有一个定义良好的分层架构，其中所有Spark组件和层都是松散耦合的。该架构进一步与各种扩展和库集成。Apache Spark架构基于两个主要的抽象概念:</p><ul class=""><li id="42a6" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><em class="le">弹性分布式数据集(RDD) </em></li><li id="1cc3" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><em class="le">有向无环图</em></li></ul><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lu"><img src="../Images/9cec64980c2fa78c248bfd24747f77e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*XRVKLtWBGA6hIioVuHF2zg.png"/></div></figure><p id="ca47" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是在深入Spark架构之前，让我解释一下Spark的几个基本概念，比如Spark生态系统和RDD。这将有助于你获得更好的见解。</p><p id="fd29" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我先解释一下什么是星火生态系统。</p><h1 id="2994" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">星火生态系统</h1><p id="444c" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">从下图可以看出，spark生态系统由各种组件组成，如Spark SQL、Spark Streaming、MLlib、GraphX和核心API组件。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/9db91e1a5e54b7b8ac09250386e05776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pxAJ-vxXXLxiaKuDNhnknA.png"/></div></div></figure><h2 id="b910" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak">火花芯</strong></h2><p id="8a29" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">Spark核心是大规模并行和分布式数据处理的基础引擎。此外，构建在核心之上的附加库允许流、SQL和机器学习的不同工作负载。它负责内存管理和故障恢复、调度、分发和监控集群上的作业&amp;与存储系统交互。</p><h2 id="8e97" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak">火花流</strong></h2><p id="639c" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">Spark流是Spark的组件，用于处理实时流数据。因此，它是对核心Spark API的有益补充。它支持实时数据流的高吞吐量和容错流处理。</p><h2 id="3450" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak"> Spark SQL </strong></h2><p id="a15f" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">Spark SQL是Spark中的一个新模块，它将关系处理与Spark的函数式编程API集成在一起。它支持通过SQL或Hive查询语言查询数据。对于那些熟悉RDBMS的人来说，Spark SQL将是从早期工具的简单过渡，在早期工具中，您可以扩展传统关系数据处理的边界。</p><h2 id="4b7f" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak"> GraphX </strong></h2><p id="a4df" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">GraphX是用于图形和图形并行计算的Spark API。因此，它用弹性分布式属性图扩展了火花RDD。在高层次上，GraphX通过引入弹性分布式属性图(每个顶点和边都有属性的有向多图)扩展了Spark RDD抽象。</p><h2 id="a908" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak"> MLlib(机器学习)</strong></h2><p id="088e" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">MLlib代表机器学习库。Spark MLlib用于在Apache Spark中执行机器学习。</p><h2 id="5527" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak"> <em class="mj">火花</em> </strong></h2><p id="bd12" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">它是一个提供分布式数据帧实现的R包。它还支持选择、过滤、聚合等操作，但是是在大型数据集上。</p><p id="8e88" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如您所见，Spark附带了高级库，包括对R、SQL、Python、Scala、Java等的支持。这些标准库增加了复杂工作流程中的无缝集成。此外，它还允许各种服务与其集成，如MLlib、GraphX、SQL +数据帧、流服务等。来增强它的能力。</p><p id="6035" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们讨论Spark的基本数据结构，即RDD。</p><h1 id="4f65" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">弹性分布式数据集(RDD)</h1><p id="7fe5" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">rdd是任何Spark应用程序的构建模块。RDDs代表:</p><ul class=""><li id="bc80" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi"> <em class="le">弹性:</em> </strong>容错，能够在发生故障时重建数据</li><li id="1c77" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi"> <em class="le">分布式:</em> </strong>在一个集群中的多个节点之间分布数据</li><li id="136f" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi"> <em class="le">数据集:</em> </strong>带值的分区数据集合</li></ul><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mk"><img src="../Images/03411fe03eac7204e7e6dd98ea0c9c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9USZKJxz6fz5EYUcB-LXQ.png"/></div></div></figure><p id="27d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它是分布式集合上的抽象数据层。它本质上是不可变的，并且遵循<em class="le">惰性转换</em>。</p><p id="7f07" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在你可能想知道它的工作原理。嗯，RDD中的数据是根据一个键分割成块的。rdd具有很高的弹性，也就是说，当相同的数据块在多个执行器节点上复制时，它们能够从任何问题中快速恢复。因此，即使一个执行器节点失败，另一个仍将处理数据。这允许您通过利用多个节点的能力，非常快速地对数据集执行函数计算。</p><p id="e370" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">此外，一旦你创建了一个RDD，它就变成了不可变的<strong class="ir hi"><em class="le"/></strong>。我所说的不可变的意思是，一个对象的状态在它被创建后不能被修改，但是它们肯定可以被转换。</p><p id="4ddc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">谈到分布式环境，RDD的每个数据集都被划分成逻辑分区，这些分区可以在集群的不同节点上进行计算。因此，您可以并行地对完整的数据执行转换或操作。此外，您也不必担心分发问题，因为Spark会处理这些问题。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/c01e1f0751f70e708cb6e24833e65209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9xg4SATGH0c0zSwWiN9pg.png"/></div></div></figure><p id="d994" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">创建rdd有两种方法:在驱动程序中并行化现有集合，或者通过引用外部存储系统(如共享文件系统、HDFS、HBase等)中的数据集。</p><p id="2ae5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用rdd，您可以执行两种类型的操作:</p><ol class=""><li id="7c4d" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm ml jt ju jv bi translated"><strong class="ir hi"> <em class="le">转换:</em> </strong>它们是应用于创建新RDD的操作。</li><li id="fce0" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm ml jt ju jv bi translated"><strong class="ir hi"> <em class="le">动作:</em> </strong>应用在RDD上，指示Apache Spark应用计算，并将结果返回给驱动程序。</li></ol><p id="f847" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我希望你彻底理解了RDD的概念。现在，让我们进一步了解Spark架构的工作原理。</p><h1 id="0503" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">星火建筑的运作</h1><p id="028c" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">您已经看到了Apache Spark的基本架构概述，现在让我们更深入地研究它的工作原理。</p><p id="5324" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在您的<strong class="ir hi">主节点</strong>中，您有<em class="le">驱动程序</em>，它驱动您的应用程序。您正在编写的代码相当于一个驱动程序，或者如果您正在使用交互式shell，shell相当于驱动程序。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/6a0f53976f1c8e674c12dab93c57fba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*QDFZMNcJEPuTsegcYu1SJA.png"/></div></figure><p id="ddcb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在驱动程序内部，你做的第一件事是，你<em class="le">创建</em>一个<strong class="ir hi"> <em class="le"> Spark上下文。</em> </strong>假设Spark上下文是所有Spark功能的网关。它类似于您的数据库连接。您在数据库中执行的任何命令都要经过数据库连接。同样，你在Spark上做的任何事情都要经过Spark上下文。</p><p id="cb40" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，这个Spark上下文与<strong class="ir hi"> <em class="le">集群管理器</em> </strong>一起管理各种作业。驱动程序&amp; Spark context负责集群内的作业执行。一个作业被分成多个任务，分布在worker节点上。任何时候在Spark上下文中创建一个RDD，它都可以分布在不同的节点上并缓存在那里。</p><p id="6733" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> W <em class="le">工作节点</em>T3】是工作主要是执行任务的从节点。然后，这些任务在worker节点中的分区rdd上执行，并将结果返回给Spark上下文。</strong></p><p id="0fa6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark Context接收作业，将作业分解成任务，并将它们分发到工作节点。这些任务在分区的RDD上工作，执行操作，收集结果并返回到主Spark上下文。</p><p id="db66" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果您增加工作线程的数量，那么您可以将作业分成更多的分区，并在多个系统上并行执行它们。会快很多。</p><p id="680d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随着工作线程数量的增加，内存大小也会增加&amp;您可以缓存作业以更快地执行它。</p><p id="4f64" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">要了解Spark架构的工作流程，可以看看下面的<strong class="ir hi">信息图</strong>:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/7c91c8901beec62253b81788a37772d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jE4fxTxUt_8vh5KazmftTQ.png"/></div></div></figure><h2 id="2533" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak">第一步:</strong></h2><p id="1424" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">客户端提交spark用户应用程序代码。当提交应用程序代码时，驱动程序隐式地将包含转换和动作的用户代码转换成逻辑上<em class="le">有向非循环图</em>，称为<strong class="ir hi"> <em class="le"> DAG。</em> </strong>在这个阶段，它还执行流水线转换等优化。</p><h2 id="4d60" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak">第二步:</strong></h2><p id="66ca" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">之后，它将名为DAG的逻辑图转换成具有多个阶段的物理执行计划。在转换成物理执行计划后，它在每个阶段下创建称为任务的物理执行单元。然后，任务被捆绑并发送到集群。</p><h2 id="de59" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak">第三步:</strong></h2><p id="58d3" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">现在，驱动程序与集群管理器对话并协商资源。集群管理器代表驱动程序启动工作节点中的执行器。此时，驱动程序将根据数据放置将任务发送给执行器。当执行者开始时，他们向司机注册。因此，驱动程序将有一个正在执行任务的执行器的完整视图。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mn"><img src="../Images/a1d7ece7ac12bd69166afe4cd93b2ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yYLOJVxIk6qRTWal2pe8cg.png"/></div></div></figure><h2 id="27d8" class="lv kc hh bd kd lw lx ly kh lz ma mb kl ja mc md kp je me mf kt ji mg mh kx mi bi translated"><strong class="ak">第四步:</strong></h2><p id="8092" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">在任务执行的过程中，驱动程序将监控运行的执行器集合。驱动节点还基于数据放置来调度未来的任务。</p><p id="84ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这都是关于火花建筑。现在，让我们来了解一下火花壳的工作原理。</p><h1 id="722c" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">在Spark shell中使用Scala的示例</h1><p id="2e65" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">首先，让我们通过假设Hadoop和Spark守护进程已经启动并运行来启动Spark shell。<strong class="ir hi"><em class="le">Web UI</em></strong>Spark的端口是<strong class="ir hi"> <em class="le"> localhost:4040。</em> </strong></p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mo"><img src="../Images/c6d018bba6c913d83a2433f7daa39ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O4TlaF0MdNL79D-kkFhphg.png"/></div></div></figure><p id="5a16" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一旦您启动了Spark shell，现在让我们看看如何执行字数统计示例:</p><ol class=""><li id="3337" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm ml jt ju jv bi translated">在本例中，我创建了一个简单的文本文件，并将其存储在hdfs目录中。您也可以使用其他大型数据文件。</li></ol><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/c5c724923f261ee7c92ddcfbd407a2e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XhNz0gsFnTS-XbFkZTLaig.png"/></div></div></figure><p id="81ae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">2.一旦火花壳启动，让我们创建一个RDD。为此，您必须指定输入文件路径并应用转换<strong class="ir hi"> flatMap() </strong>。下面的代码说明了同样的情况:</p><pre class="lg lh li lj fd mp mq mr ms aw mt bi"><span id="7911" class="lv kc hh mq b fi mu mv l mw mx">scala&gt; var map = sc.textFile("<!-- -->hdfs://localhost:9000/Example/sample.txt<!-- -->").flatMap(line =&gt; line.split(" ")).map(word =&gt; (word,1));</span></pre><p id="c39c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">3.在执行这段代码时，将会创建一个RDD，如图所示。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es my"><img src="../Images/7e2194ff71b572f6a965c8456dce596c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ga1ymgNO6SZBwLU7NVDVxw.png"/></div></div></figure><p id="f162" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">4.之后，您需要将动作<strong class="ir hi"> reduceByKey() </strong>应用到创建的RDD。</p><pre class="lg lh li lj fd mp mq mr ms aw mt bi"><span id="d6a4" class="lv kc hh mq b fi mu mv l mw mx">scala&gt; var counts = map.reduceByKey(_+_);</span></pre><p id="7a45" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">应用操作后，执行开始，如下所示。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mz"><img src="../Images/ad791c6b90c4f631c2413ada8df0aecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZHGP8tPnP4aEFCra1eAxFw.png"/></div></div></figure><p id="287a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">5.下一步是将输出保存在文本文件中，并指定存储输出的路径。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es na"><img src="../Images/2dd4e735916fcff9359d36eaaa013545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*d-87QtASdDZLmHZNHH5tPQ.png"/></div></figure><p id="61fd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">6.指定输出路径后，进入<em class="le"> hdfs网页浏览器</em> <strong class="ir hi"> <em class="le"> localhost:50040。</em> </strong>在这里你可以看到“零件”文件中的输出文本，如下所示。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/dae5ca922b553131a25f0e4d24086383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KoN-XzQV9PONV7urMJ89Cw.png"/></div></div></figure><p id="3387" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">7.下图显示了“零件”文件中的输出文本。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/fa15fd689a55ecc6615eb41a39e780cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*Zs-BrPwUK2alaFLiLiWrHA.png"/></div></figure><p id="6420" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我希望您已经理解了如何创建Spark应用程序并获得输出。</p><p id="e2f0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我带您通过Spark的web UI来理解DAG可视化和已执行任务的分区。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/8113eefc91a4eccd320ee472d8335103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qo2uRdYncz_Fjn1hoBQj3w.png"/></div></div></figure><ul class=""><li id="9ede" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">单击您提交的任务，您可以查看已完成作业的有向无环图(DAG)。</li></ul><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/d5104844989595cabf3ba3ce741dbfbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*ncZtPmNZY9IF-MXjRLGejg.png"/></div></figure><ul class=""><li id="44ae" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">此外，您还可以查看已执行任务的摘要指标，如执行任务所用的时间、作业ID、已完成的阶段、主机IP地址等。</li></ul><p id="5242" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们了解一下rdd中的分区和并行性。</p><ul class=""><li id="2c66" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">一个<strong class="ir hi"> <em class="le">分区</em> </strong>是一个<em class="le">逻辑</em> <em class="le">组块</em>的一个<em class="le">大型</em> <em class="le">分布式</em> <em class="le">数据</em> <em class="le">集合。</em></li><li id="2e84" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">默认情况下，Spark试图从<em class="le">节点</em>读取 <em class="le">数据</em> <em class="le">到</em><em class="le"/><em class="le">【RDD】</em>即<em class="le">关闭</em> <em class="le">到</em> <em class="le">它。</em></li></ul><p id="bbab" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们看看如何在shell中执行并行任务。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/5652bc51cf284eaff043387c266a5452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0QimKKlhu9Y0BefihYrX3g.png"/></div></div></figure><ul class=""><li id="e5f8" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">下图显示了创建的RDD上的分区总数。</li></ul><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/10061b653c2e9f296f38fd4afc4956ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3osT6tPcZNjanl4gT96rVw.png"/></div></div></figure><ul class=""><li id="01c3" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">现在，让我向您展示5个不同任务是如何并行执行的。</li></ul><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/7a6de7c6523d3d22f9f6b456880ee727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uYgaPMz-ng8bpKRWTNwoUw.png"/></div></div></figure><p id="c7cc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就把我们带到了Apache Spark架构博客的结尾。我希望这篇博客能给你提供信息，增加你的知识。</p><p id="f941" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">原来就是这样！我希望这篇博客能给你提供信息，增加你的知识。如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="367a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释Spark的各个方面。</p><blockquote class="lo lp lq"><p id="8996" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><em class="hh"> 1。</em> <a class="ae nd" rel="noopener" href="/edureka/spark-tutorial-2a036075a572"> <em class="hh">阿帕奇火花教程</em> </a></p><p id="14b1" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><em class="hh"> 2。</em> <a class="ae nd" rel="noopener" href="/edureka/spark-streaming-92bdcb1d94c4"> <em class="hh">火花串流教程</em> </a></p><p id="9867" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><em class="hh"> 3。</em> <a class="ae nd" rel="noopener" href="/edureka/spark-mllib-e87546ac268"> <em class="hh">火花MLlib </em> </a></p><p id="eef3" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><em class="hh"> 4。</em> <a class="ae nd" rel="noopener" href="/edureka/spark-sql-tutorial-6de1e241bf76"> <em class="hh"> Spark SQL教程</em> </a></p><p id="84a3" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><em class="hh"> 5。</em> <a class="ae nd" rel="noopener" href="/edureka/spark-graphx-f9bd805ac429"> <em class="hh"> Spark GraphX教程</em> </a></p><p id="f0e6" class="ip iq le ir b is it iu iv iw ix iy iz lr jb jc jd ls jf jg jh lt jj jk jl jm ha bi translated"><em class="hh"> 6。</em> <a class="ae nd" rel="noopener" href="/edureka/spark-java-tutorial-cb2f54991c2b"> <em class="hh"> Spark Java教程</em> </a></p></blockquote></div><div class="ab cl ne nf go ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ha hb hc hd he"><p id="c5bf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">原载于2018年9月28日www.edureka.co</em><em class="le">的</em> <a class="ae nd" href="https://www.edureka.co/blog/spark-architecture/" rel="noopener ugc nofollow" target="_blank"> <em class="le">。</em></a></p></div></div>    
</body>
</html>