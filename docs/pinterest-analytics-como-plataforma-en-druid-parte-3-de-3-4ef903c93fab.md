# Pinterest Analytics 作为德鲁伊平台(第 3 部分,共 3 部分)

> 原文：<https://medium.com/pinterest-engineering/pinterest-analytics-como-plataforma-en-druid-parte-3-de-3-4ef903c93fab?source=collection_archive---------5----------------------->

Jian Wang, Jiaqi Gu, Yi Yang, Isabel Tallam, Lakshmi Narayana Namala, Kapil Bajaj | 实时分析团队

*这篇文章最初发表于 英语.Read the English version* [*here*](/pinterest-engineering/pinterests-analytics-as-a-platform-on-druid-part-3-of-3-579406ffa374) *(T7 )*

在本系列文章中,我们将讨论 Pinterest Analytics 作为德鲁伊平台,并分享使用德鲁伊的一些经验教训。这是该系列的第三个版本,我们将讨论我们在实时使用案例优化德鲁伊方面学到的经验教训。

# 针对实时用例优化 Druid 的课程

当我们在 Pinterest 中引入 Druid 时,它主要用于处理批处理数据的查询。随着时间的推移,我们一直在过渡到实时报告系统,以便在指标到达几分钟后即可进行查询。使用案例越来越多地嵌入到 Lambda 架构中,除了真正的批处理源之外,Flink 还具有流式处理进程。这给德鲁伊报告层带来了巨大的挑战,因为我们纳入的更大的实时用例具有以下要求:上游 ETL 流程生成德鲁伊消耗超过 500K QPS 的卡夫卡主题,并且期望德鲁伊的摄入延迟不到一分钟。预期查询的 QPS 约为 1,000,P99 延迟约为 250 毫秒。

## 记忆体中的 bitmap

通过合并使用案例,我们首先遇到瓶颈,因为我们在中间管理员中为在行程中运行的实时段提供服务。最初,增加了相当数量的主机以遵守 SLA,但违规者很快就不再与收益保持一致。此外,由于有如此多的任务计数和副本,Overlord 在处理区段元数据时非常重视处理行人请求。它需要大量的主机来提供足够的线程处理,除非每个线程的工作(分析一个线程)更有效,否则无法减少它们。我们进行了表征,发现实时区段的查询逻辑效率低下:当数据首次被实时层吸收时,它首先被放置在内存中没有反向索引的映射中,而这些索引使 Druid 区段被批量吸收。这意味着对区段的每次实时查询将对每个行执行完整分析,以便添加候选行。对于我们的大多数使用案例,要为每个查询添加的候选行仅占区段中总行数的很小百分比。接下来,我们通过查询非持久性实时段来实现内存中的位图。这导致我们拥有的所有实时用例的中间管理器容量减少了近 70%,同时提供了更好的延迟。

![](img/74c712d91d626e54a162fa04dc5378a0.png)

Figura 1: Reducción de latencia P99 para administradores intermedios (caso de uso 1)

![](img/ab8163059046c4a204ca6e5be12abef6.png)

Figura 2: Latencia P90 (ms) y reducción de costos de infraestructura en administradores intermedios (caso de uso 2)

## 实时分区段

我们在将位图添加到内存中后,在处理单个段时实现了 CPU 减少的改进。尽管如此,我们得出的结论是,由于大量的线程,我们仍然无法处理线程,因为这些线程没有分区。当队列在中间管理器中实时处理段时,以及当历史记录在几个小时后开始压缩作业之前完成并处理段时,这是一个问题。我们拥有的用例具有许多延迟事件的性质,时间戳跨越过去 48 小时。这给德鲁伊带来了沉重的负担,因为每小时要创建的区段数量与我们接受的延迟消息时间窗口数量成正比。经过一些基准评估后,客户同意使用 3 小时的延迟消息接受窗口来捕获大多数事件。这比之前的 48 小时延迟消息窗口要好,但仍然对系统提出了巨大的挑战。当 QPS 很高时,恒定倍增器很重要。假设我们需要 250 个作业才能将摄取延迟维持在 1 分钟以内,而延迟消息窗口为 3 小时,则当前时间的段数至少为 250 × 3 = 750。每个单独的查询必须分析 750 个数据段,如果使用 1,000 QPS,则每秒要分析的数据段数量约为 750 × 1,000 = 750,000。Druid 中的每个区段都由单个线程处理,典型的线程处理量设置为等于主机上可用的 CPU 数量。从理论上讲,我们需要 750,000 / 32 = 23437 个 32 核主机,以便不排队任何段,但违规将是不可控的。

实际上,效率在于事件分散在所有细分市场。对于 Pinterest 中的大多数实时使用案例,每个查询都附带一个要过滤的 ID(例如:, pin_id),因此,如果我们可以根据 ID 拆分区段,我们可以删除区段并限制查询部署。在批量摄取中,分区是可能的,因为我们可以在摄取过程中执行任意混合,而在实时摄取中,每个段中摄取的数据基于分配给每个用户的卡夫卡分区。因此,我们让上游处理器使用自定义的 Kafka 密钥分区器。分区是在基于哈希的机制下进行的,实时分区采用了相应的自定义分区规范,该规范在创建分区的 Kafka 分区中也有元数据。在查询期间,经纪人可以对过滤器中的 ID 进行重新哈希,以确定给定的区段是否可能包含用于识别的数据。对 ID 进行哈希会导致 Kafka 分区,从而导致分段。这样一来,每个区段要分析的区段数量减少了 250 倍,因此要分析/计数任务的原始区段数量从 (750,000) 增加到 750,000/250 = 3,000 个区段,这需要 3,000/32 = 93 个 32 核主机 - 这个数字比以前更易于管理。

![](img/297f4684f85b909f0e0212381c6741f9.png)

作为后续行动,我们还扩展了自定义分区规范,使其具有另一个字段 fanOutSize,以便在多个区段中对给定的分区维度值进行哈希,以解决未来使用案例中的潜在数据不对称问题。

## 实时区段中的 Bloom 过滤索引

使用实时分区,查询分区的部署显著减少,但所需的 CPU 数量仍然很高。上述计算中要查询的 3,000 个区段仅为一个小时。我们的实时用例查询通常会请求从现在到 24 小时前的数据。由于各种原因,需要 8 小时才能开始工作,以压缩 8 小时前的实时段。当时,要查询的区段数量减少了三倍,因为由延迟事件引入的 3 个区段已压缩,因此 8 小时前的区段所需的 CPU 要少得多。另一方面,对于最近的 8 小时段,3,000 × 8 = 24,000 需要 24,000 / 32 = 750 个 32 核主机。

通过对查询模式进行进一步表征,我们发现,虽然过滤器中标识符的基数很大,但并非所有标识符在任何给定时间都有新数据。基于哈希的实时分区没有足够的元数据来执行每个 ID 的级别存在检查,因为哈希会导致至少一个区段可能包含数据,而实际上该区段在大多数情况下没有任何数据可以返回。对于高 QPS 和延迟事件,要分析的区段中的误报非常重要。

我们没有找到一个很好的方法来解决中间管理员的行人所服务的实时段中的上述问题。他们继续有新的数据流,我们不知道特定 ID 的数据是否会到达,直到该段完成并发布到历史记录。同时,由于区段分区规范中基于元数据的删除逻辑在代理中被调用,并且路由器知道有关区段中有哪些数据的最新信息,因此如果没有元数据的准确性,则无法在两个不同组件之间实时同步区段中的特定 ID。由于只有一个小时,我们保留了未完成的部分。

另一方面,通过在行人中完成实时区段,我们可以全面了解区段中的数据。然后,我们将元数据添加到分区规范中:最初,当路由器在中间管理器中为区段提供服务时,代理会忽略这些数据,但后来,当历史记录加载区段的完成版本时,它们会被收集。对于特定的元数据,我们在分区规范中使用 Bloom 过滤器来存储 ID,这是一个很好的大小和精度,具有很高的结构平衡概率。只需几 MB 即可获得区段中百万基数 ID 的 3% 预期误报率。通过此次更改,我们能够在过去 8 小时内将要分析的区段数量减少 5 倍。先前计算所需的 CPU 数量变成了 750 / 5 = 150 个 32 核主机,这比以前更具成本效益。

![](img/66c31aa1faf2d8fc99c57091f90504f6.png)

Figura 3: Cantidad de segmentos a analizar (caso de uso 1)

![](img/8f9f09e3e3a8e4234e7226ea222ac575.png)

Figura 4: Uso de subprocesos de procesamiento del caso de uso 1 en históricos

![](img/b1ba83db6d1fea742398953a262a723f.png)

Tabla 1: Tasa de eliminación de la cantidad de segmentos a analizar (caso de uso 2)

# 未来工作

仍有许多方面需要改进,包括但不限于持续压缩,支持 Kafka 主题缩放事件的实时分区,大量任务计数以及延迟消息窗口可扩展性的改进等。与此同时,我们将更多地致力于我们迄今为止开发的开源软件。

# 致谢

我们从 Druid Guild 与广告数据团队的对话中学到了很多东西,并从开源社区的反馈中学到了很多,因为我们开始为我们的工作做出贡献。我们还要感谢所有与我们合作将其用例集成到统一分析平台的团队:信息团队、核心产品数据团队、度量团队、信任与安全团队、广告数据团队、信号平台团队、广告投放团队等。每个用例都是不同的,并且该平台自成立以来已经发展了很多。

*有关 Pinterest 工程的更多信息,请查看我们的* [*工程博客 [*](https://medium.com/pinterest-engineering) *并访问我们的* [*Pinterest Labs*](https://www.pinterestlabs.com/?utm_source=medium&utm_medium=blog-article-post&utm_campaign=want-et-al-december-20-2021&utm_content=spanish) *网站。如需查看和申请职位,请访问我们的* [*工作*](https://www.pinterestcareers.com/?utm_source=medium&utm_medium=blog-article-post&utm_campaign=want-et-al-december-20-2021&utm_content=spanish) *页面。(T19)*