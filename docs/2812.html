<html>
<head>
<title>PySpark Programming - Integrating Speed With Simplicity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark编程——速度与简单的结合</h1>
<blockquote>原文：<a href="https://medium.com/edureka/pyspark-programming-e007e68fbccb?source=collection_archive---------0-----------------------#2018-08-14">https://medium.com/edureka/pyspark-programming-e007e68fbccb?source=collection_archive---------0-----------------------#2018-08-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/d181fcd3da615c8c1b8b2c62857d65da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*hJqrr7tyTQZH4xXGo7xicg.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">PySpark Programming - Edureka</figcaption></figure><p id="adf0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Python和Apache Spark是分析行业最热门的流行语。Apache Spark是一个流行的开源框架，它确保以闪电般的速度处理数据，并支持各种语言，如Scala、Python、Java和r。然后，它归结为您的语言偏好和工作范围。通过这篇PySpark编程文章，我将讨论Spark和Python，以展示Python如何利用Apache Spark的功能。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jn"><img src="../Images/c99dd34bf46ed85aed0fa6fd7696aca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-2GDktS3fDMKMDgdBBnIQ.jpeg"/></div></div></figure><p id="3069" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们开始PySpark编程之旅之前，让我列出我将在本文中涉及的主题:</p><ul class=""><li id="f8ae" class="jw jx hh ir b is it iw ix ja jy je jz ji ka jm kb kc kd ke bi translated">什么是PySpark</li><li id="753f" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">RDDs</li><li id="7b7c" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">数据帧</li><li id="87ed" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">PySpark SQL</li><li id="f0c3" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">PySpark流</li><li id="c572" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">机器学习(MLlib)</li></ul><p id="395c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，让我们从列表中的第一个主题开始，即PySpark编程。</p><h1 id="12ae" class="kk kl hh bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">PySpark编程</h1><p id="5e35" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">PySpark是Apache Spark和Python的合作。</p><p id="25fd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Apache Spark 是一个开源的集群计算框架，围绕速度、易用性和流分析而构建，而<strong class="ir hi"> Python </strong>是一种通用的高级编程语言。它提供了广泛的库，主要用于机器学习和实时流分析。</p><p id="055e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">换句话说，它是一个用于Spark的Python API，允许您利用Python的简单性和Apache Spark的强大功能来驯服大数据。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ln"><img src="../Images/b763c71bd58aa382ff877df6491872e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTDEt8iL5ck37gWHsJ6_-w.jpeg"/></div></div></figure><p id="d554" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可能想知道，既然有其他语言可用，为什么我选择Python来使用Spark。为了回答这个问题，我列出了Python的一些优势:</p><ul class=""><li id="f9d8" class="jw jx hh ir b is it iw ix ja jy je jz ji ka jm kb kc kd ke bi translated">Python非常容易学习和实现。</li><li id="75f4" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">它提供了简单而全面的API。</li><li id="e619" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">使用Python，代码的可读性、维护性和熟悉性都要好得多。</li><li id="4a86" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">它为数据可视化提供了各种选项，使用Scala或Java很难做到这一点。</li><li id="3f70" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">Python附带了大量的库，如numpy、pandas、scikit-learn、seaborn、matplotlib等。</li><li id="d02c" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated">它得到了一个庞大而活跃的社区的支持。</li></ul><h1 id="a7d9" class="kk kl hh bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">弹性分布式数据集</h1><p id="5131" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated"><a class="ae lo" href="https://www.edureka.co/blog/pyspark-rdd?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=pyspark-programming" rel="noopener ugc nofollow" target="_blank">rdd</a>是任何Spark应用的构建模块。RDDs代表:</p><ul class=""><li id="a205" class="jw jx hh ir b is it iw ix ja jy je jz ji ka jm kb kc kd ke bi translated"><strong class="ir hi"> <em class="lp">弹性:</em> </strong>容错，能够在故障时重建数据。</li><li id="be5d" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated"><strong class="ir hi"> <em class="lp">分布式:</em> </strong>数据分布在一个集群中的多个节点上。</li><li id="cc87" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm kb kc kd ke bi translated"><strong class="ir hi"> <em class="lp">数据集:</em> </strong>带有值的分区数据的集合。</li></ul><p id="a60c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它是分布式集合上的抽象数据层。它本质上是不可变的，并且遵循<em class="lp">惰性转换</em>。</p><p id="cbaa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用rdd，您可以执行两种类型的操作:</p><ol class=""><li id="43ae" class="jw jx hh ir b is it iw ix ja jy je jz ji ka jm lq kc kd ke bi translated">转换:这些操作被用来创建一个新的RDD。</li><li id="cf29" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm lq kc kd ke bi translated"><strong class="ir hi">动作:</strong>这些操作被应用到一个RDD上，以指示Apache Spark应用计算并将结果传递回驱动程序。</li></ol><h1 id="7101" class="kk kl hh bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">数据帧</h1><p id="97da" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">PySpark中的Dataframe是结构化或半结构化数据的分布式集合。Dataframe中的数据存储在命名列下的行中，这类似于关系数据库表或excel表。</p><p id="6a51" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它还与RDD有一些共同的属性，比如本质上不可变，遵循惰性求值，以及本质上是分布式的。它支持多种格式，如JSON、CSV、TXT等等。此外，您可以从现有的rdd或通过编程指定模式来加载它。</p><h1 id="bdde" class="kk kl hh bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">PySpark SQL</h1><p id="deaa" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">PySpark SQL是PySpark核心之上的一个高级抽象模块。它主要用于处理结构化和半结构化数据集。它还提供了一个优化的API，可以从包含不同文件格式的各种数据源中读取数据。因此，使用PySpark，您可以通过使用SQL和HiveQL来处理数据。由于这个特性，PySpark SQL在数据库程序员和Apache Hive用户中慢慢流行起来。</p><h1 id="6488" class="kk kl hh bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">PySpark流</h1><p id="5ea4" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">PySpark流是一个可扩展的容错系统，遵循RDD批处理范式。它基本上以小批量或批量间隔操作，范围从500毫秒到更大的间隔窗口。</p><p id="ff65" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这种情况下，Spark Streaming从诸如<a class="ae lo" href="https://www.edureka.co/blog/apache-flume-tutorial?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=pyspark-programming" rel="noopener ugc nofollow" target="_blank"> Apache Flume </a>、Kinesis、Kafka、TCP sockets等来源接收连续的输入数据流。然后，这些流式数据根据<em class="lp">批处理间隔</em>在内部分解成多个更小的批处理，并转发给Spark引擎。Spark Engine使用复杂的算法处理这些数据批次，这些算法用map、reduce、join和window等高级函数表示。处理完成后，处理后的批处理将被推送到数据库、文件系统和实时仪表板。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ln"><img src="../Images/fdfc9215b99148032bae7a5aab150b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xXWEuaknDWpY98Ee10VLQg.jpeg"/></div></div></figure><p id="46ff" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark流的关键抽象是离散化流(d Stream)。数据流建立在rdd之上，便于Spark开发人员在rdd和批处理的相同环境中工作，以解决数据流问题。此外，Spark Streaming还集成了MLlib、SQL、DataFrames和GraphX，拓宽了您的功能范围。作为一个高级API，Spark Streaming为有状态操作提供了容错的"<em class="lp">恰好一次"</em>语义。</p><blockquote class="lr ls lt"><p id="7cfa" class="ip iq lp ir b is it iu iv iw ix iy iz lu jb jc jd lv jf jg jh lw jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="hh">注意</em> </strong> <em class="hh">:“恰好一次”语义意味着事件将被流应用中的所有操作符“恰好一次”处理，即使发生任何故障。</em></p></blockquote><p id="e73c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下图显示了火花流的基本组件。</p><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ln"><img src="../Images/06457a63088a4559fabe31a48684dde0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v3hJF1Sj1eYeWP35SCoAGg.jpeg"/></div></div></figure><p id="3978" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如您所看到的，数据从各种来源(如Kafka、Flume、Twitter、ZeroMQ、Kinesis或TCP sockets等等)被接收到Spark流中。此外，这些数据还使用复杂的算法进行处理，这些算法用map、reduce、join和window等高级函数表示。最后，这些经过处理的数据被推送到各种文件系统、数据库和实时控制面板，以供进一步利用。</p><p id="e405" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我希望这能让您清楚地了解PySpark流是如何工作的。现在让我们继续这篇PySpark编程文章的最后一个但也是最吸引人的主题，即机器学习。</p><h1 id="6f8f" class="kk kl hh bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">机器学习</h1><p id="d2d9" class="pw-post-body-paragraph ip iq hh ir b is li iu iv iw lj iy iz ja lk jc jd je ll jg jh ji lm jk jl jm ha bi translated">众所周知，Python是一种成熟的语言，多年来一直被大量用于数据科学和机器学习。在PySpark中，机器学习是通过一个名为MLlib(机器学习库)的Python库来实现的。它只不过是PySpark核心上的一个包装器，使用分类、聚类、线性回归等机器学习算法来执行数据分析。</p><p id="4d8b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用PySpark进行机器学习的一个诱人的特性是，它可以在分布式系统上工作，并且高度可伸缩。</p><p id="a712" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">MLlib公开了PySpark的三个核心机器学习功能:</p><ol class=""><li id="3fe4" class="jw jx hh ir b is it iw ix ja jy je jz ji ka jm lq kc kd ke bi translated"><strong class="ir hi">数据准备:</strong> It <strong class="ir hi">提供各种功能，如提取、转换、选择、散列等。</strong></li><li id="d59f" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm lq kc kd ke bi translated"><strong class="ir hi">机器学习算法:</strong>利用一些流行的高级回归、分类、聚类算法进行机器学习。</li><li id="8d44" class="jw jx hh ir b is kf iw kg ja kh je ki ji kj jm lq kc kd ke bi translated"><strong class="ir hi">实用工具:</strong>拥有卡方检验、描述统计、线性代数、模型评估方法等统计方法。</li></ol><p id="be17" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我来给你演示一下如何通过逻辑回归使用<em class="lp">分类</em>实现机器学习。</p><p id="912f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这里，我将对芝加哥市的食品检测数据进行简单的预测分析。</p><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="095a" class="mc kl hh ly b fi md me l mf mg">##Importing the required libraries<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.ml.feature import HashingTF, Tokenizer<br/>from pyspark.sql import Row<br/>from pyspark.sql.functions import UserDefinedFunction<br/>from pyspark.sql.types import *<br/> <br/>##creating a RDD by importing and parsing the input data<br/>def csvParse(s):<br/>import csv<br/>from StringIO import StringIO<br/>sio = StringIO(s)<br/>value = csv.reader(sio).next()<br/>sio.close()<br/>return value<br/> <br/>food_inspections = sc.textFile('file:////home/edureka/Downloads/Food_Inspections_Chicago_data.csv')\<br/>.map(csvParse)<br/> <br/>##Display data format<br/>food_inspections.take(1)</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es mh"><img src="../Images/53bf0a56a3f94715c4b5709f4a88c5ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pqdglektrs4kWgXs2qUaUg.jpeg"/></div></div></figure><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="f904" class="mc kl hh ly b fi md me l mf mg">#Structuring the data<br/>schema = StructType([<br/>StructField("id", IntegerType(), False),<br/>StructField("name", StringType(), False),<br/>StructField("results", StringType(), False),<br/>StructField("violations", StringType(), True)])<br/>#creating a dataframe and a temporary table (Results) required for the predictive analysis. <br/>##sqlContext is used to perform transformations on structured data<br/>ins_df = spark.createDataFrame(food_inspections.map(lambda l: (int(l[0]), l[1], l[12], l[13])) , schema)<br/>ins_df.registerTempTable('Count_Results')<br/>ins_df.show()</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/7ee73e45a7ca81bc4e722d10a24643a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*PeBNLs6C8OKLR41rzHY1TQ.jpeg"/></div></figure><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="cd70" class="mc kl hh ly b fi md me l mf mg">##Let's now understand our dataset<br/>#show the distinct values in the results column<br/>result_data = ins_df.select('results').distinct().show()</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/16064ed04f6270cdf8292966a58fb9c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*41y4-WiY8gHrLC4zgi6B2A.jpeg"/></div></figure><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="87db" class="mc kl hh ly b fi md me l mf mg">##converting the existing dataframe into a new dataframe <br/>###each inspection is represented as a label-violations pair. <br/>####Here 0.0 represents a failure, 1.0 represents a success, and -1.0 represents some results besides those two<br/>def label_Results(s):<br/>if s == 'Fail':<br/>return 0.0<br/>elif s == 'Pass with Conditions' or s == 'Pass':<br/>return 1.0<br/>else:<br/>return -1.0<br/>ins_label = UserDefinedFunction(label_Results, DoubleType())<br/>labeled_Data = ins_df.select(ins_label(ins_df.results).alias('label'), ins_df.violations).where('label &gt;= 0')<br/>labeled_Data.take(1)</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es mk"><img src="../Images/1f70bbce38cea856cad1e53fef351655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJXoeiSF5CoR-Lj7wyDYZQ.jpeg"/></div></div></figure><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="c909" class="mc kl hh ly b fi md me l mf mg">##Creating a logistic regression model from the input dataframe<br/>tokenizer = Tokenizer(inputCol="violations", outputCol="words")<br/>hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")<br/>lr = LogisticRegression(maxIter=10, regParam=0.01)<br/>pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])<br/>model = pipeline.fit(labeled_Data)<br/>## Evaluating with Test Data<br/> <br/>test_Data = sc.textFile('file:////home/edureka/Downloads/Food_Inspections_test.csv')\<br/>.map(csvParse) \<br/>.map(lambda l: (int(l[0]), l[1], l[12], l[13]))<br/>test_df = spark.createDataFrame(test_Data, schema).where("results = 'Fail' OR results = 'Pass' OR results = 'Pass with Conditions'")<br/>predict_Df = model.transform(test_df)<br/>predict_Df.registerTempTable('Predictions')<br/>predict_Df.columns</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div class="er es ml"><img src="../Images/5a36bc378cec6506d2396c87b25de187.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*VmD2hzn2JkQj5NKK6ZUqwg.jpeg"/></div></figure><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="8428" class="mc kl hh ly b fi md me l mf mg">## Printing 1st row<br/>predict_Df.take(1)</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es mm"><img src="../Images/2d19d0ba74c4a807073582e9099068b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0VqsK9oDLxN0talPPc-TFw.jpeg"/></div></div></figure><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="c5fa" class="mc kl hh ly b fi md me l mf mg">## Predicting the final result<br/>numOfSuccess = predict_Df.where("""(prediction = 0 AND results = 'Fail') OR<br/>(prediction = 1 AND (results = 'Pass' OR<br/>results = 'Pass with Conditions'))""").count()<br/>numOfInspections = predict_Df.count()<br/>print "There were", numOfInspections, "inspections and there were", numOfSuccess, "successful predictions"<br/>print "This is a", str((float(numOfSuccess) / float(numOfInspections)) * 100) + "%", "success rate"</span></pre><figure class="jo jp jq jr fd ii er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es mn"><img src="../Images/e67e7e9e9f7a78f4047a41ed0c03eed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*GT8smOm_1dhLKnqbNYt1SQ.jpeg"/></div></div></figure><p id="313c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">至此，我们结束了这篇关于PySpark编程的博客。希望它有助于增加你的知识价值。</p><p id="9791" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，那么你可以参考<a class="ae lo" href="https://www.edureka.co/blog/?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=pyspark-programming" rel="noopener ugc nofollow" target="_blank"> Edureka的官方网站。</a></p><p id="27e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释PySpark的各个方面。</p><blockquote class="lr ls lt"><p id="60fe" class="ip iq lp ir b is it iu iv iw ix iy iz lu jb jc jd lv jf jg jh lw jj jk jl jm ha bi translated">1.<a class="ae lo" rel="noopener" href="/edureka/pyspark-tutorial-87d41dab9657"> PySpark教程</a></p><p id="b279" class="ip iq lp ir b is it iu iv iw ix iy iz lu jb jc jd lv jf jg jh lw jj jk jl jm ha bi translated">2.<a class="ae lo" rel="noopener" href="/edureka/pyspark-dataframe-tutorial-9335f3d09b4"> PySpark数据帧教程</a></p><p id="7e12" class="ip iq lp ir b is it iu iv iw ix iy iz lu jb jc jd lv jf jg jh lw jj jk jl jm ha bi translated">3.<a class="ae lo" rel="noopener" href="/edureka/pyspark-rdd-ef9edd060a25">py spark中的rdd</a></p><p id="e368" class="ip iq lp ir b is it iu iv iw ix iy iz lu jb jc jd lv jf jg jh lw jj jk jl jm ha bi translated">4.<a class="ae lo" rel="noopener" href="/edureka/pyspark-mllib-tutorial-759391dbb08a"> PySpark MLlib教程</a></p></blockquote></div><div class="ab cl mo mp go mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ha hb hc hd he"><p id="b690" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="lp">原载于2018年8月14日</em><a class="ae lo" href="https://www.edureka.co/blog/pyspark-programming/" rel="noopener ugc nofollow" target="_blank"><em class="lp">【www.edureka.co】</em></a><em class="lp">。</em></p></div></div>    
</body>
</html>