<html>
<head>
<title>How To Use Regularization in Machine Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中如何使用正则化？</h1>
<blockquote>原文：<a href="https://medium.com/edureka/regularization-in-machine-learning-4e041bbbdae?source=collection_archive---------6-----------------------#2019-08-07">https://medium.com/edureka/regularization-in-machine-learning-4e041bbbdae?source=collection_archive---------6-----------------------#2019-08-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/9505fb27227aadc64b2489600ea49204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*V4QhPAWs4TWtwwsvVgOMlA.png"/></div></figure><p id="5e86" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">您是否遇到过这样的情况:您的机器学习模型对训练数据的建模非常好，但对测试数据却表现不佳，即无法预测测试数据？这种情况可以用机器学习中的正则化来处理。</p><p id="cbf1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">当模型从训练数据中学习到非常特定的模式和噪音，以至于对我们的模型从训练数据归纳到新的(“看不见的”)数据的能力产生负面影响时，就会发生过度拟合。所谓噪声，我们指的是数据集中不相关的信息或随机性。</p><p id="a235" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">防止过拟合对于提高我们的机器学习模型的性能是非常必要的。</p><p id="96bf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">以下几点将在本文中涉及，并最终帮助我们解决这个问题:</p><ul class=""><li id="a1a7" class="jj jk hh in b io ip is it iw jl ja jm je jn ji jo jp jq jr bi translated">什么是正规化</li><li id="6569" class="jj jk hh in b io js is jt iw ju ja jv je jw ji jo jp jq jr bi translated">正规化是如何运作的</li><li id="37a4" class="jj jk hh in b io js is jt iw ju ja jv je jw ji jo jp jq jr bi translated">正则化技术</li></ul><p id="c456" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">继续这篇关于机器学习中的正则化的文章。</p><h1 id="7f22" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">什么是正规化？</h1><p id="5f41" class="pw-post-body-paragraph il im hh in b io kv iq ir is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji ha bi translated">一般来说，规则化就是让事情变得有规律或者可以接受。这正是我们用它来进行应用机器学习的原因。在机器学习的上下文中，正则化是将系数正则化或缩小到零的过程。简而言之，正则化不鼓励学习更复杂或更灵活的模型，以防止过度拟合。</p><p id="1dda" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">继续这篇关于机器学习中的正则化的文章。</p><h1 id="f5d7" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">正规化是如何运作的？</h1><p id="6ec3" class="pw-post-body-paragraph il im hh in b io kv iq ir is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji ha bi translated">基本思想是惩罚复杂的模型，即增加一个复杂的项，这会给复杂的模型带来更大的损失。为了理解它，让我们考虑一个简单的线性回归关系。数学上表述如下:<br/>y≈w _ 0+w _ 1x _ 1+w _ 2x _(2 )+⋯+w_p x _ p</p><p id="a59d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">其中Y是学习的关系，即要预测的值。<br/> X_1，X_，X \\, P，是决定y值的特征。<br/> W_1，W_，W \\, P，分别是特征X_1，X \\, P的权重。<br/>w0代表偏差。</p><p id="7cfd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，为了拟合精确预测Y值的模型，我们需要一个损失函数和优化参数，即偏差和权重。</p><p id="ff0c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">通常用于线性回归的损失函数称为残差平方和(RSS)。根据上述线性回归关系，可以给出:<br/>RSS = ∑_(j=1)^m(y_i-w_0-∑_(i=1)^n w _ I x _ Ji)</p><p id="37c8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们也可以称RSS为没有正则化的线性回归目标。</p><p id="6300" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，模型将通过这个损失函数来学习。基于我们的训练数据，它会调整权重(系数)。如果我们的数据集有噪声，它将面临过拟合问题，估计的系数不会在看不见的数据上推广。</p><p id="097d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这就是正规化发挥作用的地方。它通过惩罚系数的大小将这些学习到的估计值调整到零。</p><p id="da1e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">但是它是如何给系数分配惩罚的，让我们来探索一下。</p><p id="d945" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">继续这篇关于机器学习中的正则化的文章。</p><h1 id="ee5a" class="jx jy hh bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">正则化技术</h1><p id="46d8" class="pw-post-body-paragraph il im hh in b io kv iq ir is kw iu iv iw kx iy iz ja ky jc jd je kz jg jh ji ha bi translated">有两种主要的正则化技术，即岭回归和套索回归。它们的不同之处在于对系数分配惩罚的方式。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es la"><img src="../Images/e8169f4cada4d109678dfb7c6158bfde.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/0*lKVTHIpweKLlBIfL.png"/></div></figure><p id="d622" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">岭回归(L2正则化)</strong></p><p id="4a00" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这种正则化技术执行L2正则化。它通过添加相当于系数大小的平方的惩罚(收缩量)来修改RSS。<br/>∑_(j=1)^m(y_i-w_0-∑_(i=1)^n w _ I x _ Ji)+α∑_(i=1)^n w _ I = RSS+α∑_(i=1)^n w _ I</p><p id="bf20" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，使用这个修改的损失函数来估计系数。</p><p id="d10d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在上面的等式中，您可能已经注意到了参数α (alpha)以及收缩量。这就是所谓的调整参数，它决定了我们想要惩罚模型的程度。换句话说，调整参数平衡了对最小化RSS和最小化系数平方和的重视程度。</p><p id="bac5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们看看αα的值如何影响岭回归产生的估计值。</p><p id="1e87" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">当</strong> α=0时，惩罚项无效。这意味着它返回平方的残差和作为我们最初选择的损失函数，即我们将获得与简单线性回归相同的系数。</p><p id="ca31" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">当</strong> α=∞时，岭回归系数将为零，因为修改的损失函数将忽略核心损失函数并最小化系数平方，最终将参数值取为0。</p><p id="dac4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">当</strong> 0 &lt; α &lt; ∞时，对于简单线性回归，岭回归系数会在0到1之间。</p><p id="68e1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这就是为什么选择一个好的α值是至关重要的。由岭回归正则化技术产生的系数方法也称为L2范数。</p><p id="1119" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">拉索回归(L1正则化)</strong></p><p id="5e60" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这种正则化技术执行L1正则化。它通过添加相当于系数绝对值之和的罚值(收缩量)来修改RSS。<br/>∑_(j=1)^m(y_i-w_0-∑_(i=1)^n w _ I x _ Ji)+α∑_(i=1)^n | w _ I | = RSS+α∑_(i=1)^n | w _ I |<br/>现在，使用这个修改的损失函数来估计系数。</p><p id="9974" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">套索回归不同于岭回归，因为它使用绝对系数值进行归一化。</p><p id="e25d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">由于损失函数只考虑绝对系数(权重)，优化算法将惩罚高系数。这就是众所周知的L1规范。</p><p id="7c11" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这里，α (alpha)也是一个调整参数，工作方式类似于岭回归，并在平衡系数的RS幅度之间提供一个折衷。</p><p id="0aaf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">像岭回归一样，lasso回归中的α (alpha)可以取如下各种值:</p><p id="6d2e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">当</strong> α=0时，我们将得到与简单线性回归相同的系数。</p><p id="f25b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">当</strong> α=∞时，拉索回归系数为零。</p><p id="3e0d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">当</strong> 0 &lt; α &lt; ∞时，对于简单的线性回归，套索回归系数会在0到1之间。</p><p id="20cb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">它看起来与岭回归非常相似，但是让我们从不同的角度来看看这两种技术。</p><p id="d417" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">把岭回归想成解一个方程，其中权(系数)的平方和小于等于s，据此，考虑到给定问题中有2个参数，岭回归表示为<br/>w1+w2≤s</p><p id="c458" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这意味着岭回归系数对于位于上述方程给出的圆内的所有点具有最小的损失函数。</p><p id="2818" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">同样，把lasso回归想成解一个方程，其中权(系数)的模之和小于等于s，据此，考虑到给定问题中有2个参数，lasso回归表示为<br/>| w1 |+| w2 |≤s</p><p id="d3c4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这意味着岭回归系数对于位于上述等式给出的菱形内的所有点具有最小的损失函数。<br/>下图描述了上述等式:</p><p id="bffa" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在这幅图中我们可以看到，约束函数(蓝色区域)；左边的是套索，而右边的是山脊，以及损失函数(即RSS)的等高线(绿色月食)。</p><p id="4b47" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在上述情况下，对于两种回归技术，系数估计值由轮廓(日蚀)接触约束(圆形或菱形)区域的第一个点给出。</p><p id="fe85" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">岭回归系数估计将完全是非零的。为什么？因为岭回归有一个圆形约束，没有尖点，所以日蚀不会与轴上的约束相交。</p><p id="c4d3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">另一方面，套索约束，因为它的菱形，在每个轴上都有角，因此日蚀通常会在每个轴上相交。因此，至少有一个系数等于零。</p><p id="46cb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">上述情况表明，岭回归会将系数缩小到非常接近0，但永远不会使它们精确到0，这意味着最终模型将包括所有预测值。这是岭回归的一个缺点，叫做模型可解释性。</p><p id="cb26" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">然而，当α足够大时，lasso回归会将某些系数估计值缩小到恰好为0。这就是lasso提供稀疏解的原因。</p><p id="31fa" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这就是正规化及其技术，我希望现在你能更好地理解它。你可以利用这一点来提高你的机器学习模型的准确性。</p><p id="6d0c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，我们到了这篇“机器学习中的正则化”文章的结尾。希望这篇文章是有见地的！</p><p id="4d50" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你对这个话题有任何疑问，请在下面留下评论，我们会尽快回复你。如果你想查看更多关于Python、DevOps、Ethical Hacking等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="fd85" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">请留意本系列中解释数据科学各个方面的其他文章。</p><blockquote class="lg lh li"><p id="fd57" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 1。</em> <a class="ae lf" rel="noopener" href="/edureka/data-science-tutorial-484da1ff952b"> <em class="hh">数据科学教程</em> </a></p><p id="c151" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 2。</em> <a class="ae lf" rel="noopener" href="/edureka/math-and-statistics-for-data-science-1152e30cee73"> <em class="hh">数据科学的数学与统计</em> </a></p><p id="5a5d" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 3。</em><a class="ae lf" rel="noopener" href="/edureka/linear-regression-in-r-da3e42f16dd3"><em class="hh">R中的线性回归</em> </a></p><p id="c7bd" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 4。</em> <a class="ae lf" rel="noopener" href="/edureka/data-science-tutorial-484da1ff952b"> <em class="hh">数据科学教程</em> </a></p><p id="8997" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 5。</em><a class="ae lf" rel="noopener" href="/edureka/logistic-regression-in-r-2d08ac51cd4f"><em class="hh">R中的逻辑回归</em> </a></p><p id="7ef8" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 6。</em> <a class="ae lf" rel="noopener" href="/edureka/classification-algorithms-ba27044f28f1"> <em class="hh">分类算法</em> </a></p><p id="eb0c" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 7。</em> <a class="ae lf" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="e929" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 8。</em> <a class="ae lf" rel="noopener" href="/edureka/a-complete-guide-on-decision-tree-algorithm-3245e269ece"> <em class="hh">决策树中的R </em> </a></p><p id="8ddd" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 9。</em> <a class="ae lf" rel="noopener" href="/edureka/introduction-to-machine-learning-97973c43e776"> <em class="hh">机器学习入门</em> </a></p><p id="b405" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 10。</em> <a class="ae lf" rel="noopener" href="/edureka/naive-bayes-in-r-37ca73f3e85c"> <em class="hh">朴素贝叶斯在R </em> </a></p><p id="60cc" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 11。</em> <a class="ae lf" rel="noopener" href="/edureka/statistics-and-probability-cf736d703703"> <em class="hh">统计与概率</em> </a></p><p id="4268" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 12。</em> <a class="ae lf" rel="noopener" href="/edureka/decision-trees-b00348e0ac89"> <em class="hh">如何创建一个完美的决策树？</em>T73】</a></p><p id="6261" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 13。</em> <a class="ae lf" rel="noopener" href="/edureka/data-scientists-myths-14acade1f6f7"> <em class="hh">关于数据科学家角色的10大误区</em> </a></p><p id="274f" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 14。</em><a class="ae lf" rel="noopener" href="/edureka/machine-learning-algorithms-29eea8b69a54"><em class="hh">5大机器学习算法</em> </a></p><p id="44ac" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated">15。 <a class="ae lf" rel="noopener" href="/edureka/data-analyst-vs-data-engineer-vs-data-scientist-27aacdcaffa5"> <em class="hh">数据分析师vs数据工程师vs数据科学家</em> </a></p><p id="6735" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated">16。 <a class="ae lf" rel="noopener" href="/edureka/types-of-artificial-intelligence-4c40a35f784"> <em class="hh">人工智能的种类</em> </a></p><p id="a58b" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 17。</em><a class="ae lf" rel="noopener" href="/edureka/r-vs-python-48eb86b7b40f"><em class="hh">R vs Python</em></a></p><p id="0009" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 18。</em> <a class="ae lf" rel="noopener" href="/edureka/ai-vs-machine-learning-vs-deep-learning-1725e8b30b2e"> <em class="hh">人工智能vs机器学习vs深度学习</em> </a></p><p id="1b65" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated">19。 <a class="ae lf" rel="noopener" href="/edureka/machine-learning-projects-cb0130d0606f"> <em class="hh">机器学习项目</em> </a></p><p id="a39d" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated">20。 <a class="ae lf" rel="noopener" href="/edureka/data-analyst-interview-questions-867756f37e3d"> <em class="hh">数据分析师面试问答</em> </a></p><p id="d3ad" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 21。</em> <a class="ae lf" rel="noopener" href="/edureka/data-science-and-machine-learning-for-non-programmers-c9366f4ac3fb"> <em class="hh">面向非程序员的数据科学和机器学习工具</em> </a></p><p id="884f" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 22。</em> <a class="ae lf" rel="noopener" href="/edureka/top-10-machine-learning-frameworks-72459e902ebb"> <em class="hh">十大机器学习框架</em> </a></p><p id="220f" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 23。</em> <a class="ae lf" rel="noopener" href="/edureka/statistics-for-machine-learning-c8bc158bb3c8"> <em class="hh">用于机器学习的统计</em> </a></p><p id="e42c" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 24。</em> <a class="ae lf" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="293d" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 25。</em> <a class="ae lf" rel="noopener" href="/edureka/breadth-first-search-algorithm-17d2c72f0eaa"> <em class="hh">广度优先搜索算法</em> </a></p><p id="88c2" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 26。</em><a class="ae lf" rel="noopener" href="/edureka/linear-discriminant-analysis-88fa8ad59d0f"><em class="hh">R中的线性判别分析</em> </a></p><p id="1737" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 27。</em> <a class="ae lf" rel="noopener" href="/edureka/prerequisites-for-machine-learning-68430f467427"> <em class="hh">机器学习的先决条件</em> </a></p><p id="6b8f" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 28。</em> <a class="ae lf" rel="noopener" href="/edureka/r-shiny-tutorial-47b050927bd2"> <em class="hh">互动WebApps使用R闪亮</em> </a></p><p id="eff1" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 29。</em> <a class="ae lf" rel="noopener" href="/edureka/top-10-machine-learning-books-541f011d824e"> <em class="hh">机器学习十大书籍</em> </a></p><p id="e7a7" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh">三十。</em> <a class="ae lf" rel="noopener" href="/edureka/unsupervised-learning-40a82b0bac64"> <em class="hh">无监督学习</em> </a></p><p id="efa2" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated"><em class="hh"> 31.1 </em> <a class="ae lf" rel="noopener" href="/edureka/10-best-books-data-science-9161f8e82aca"> <em class="hh"> 0最佳数据科学书籍</em> </a></p><p id="2e09" class="il im lj in b io ip iq ir is it iu iv lk ix iy iz ll jb jc jd lm jf jg jh ji ha bi translated">32。 <a class="ae lf" rel="noopener" href="/edureka/supervised-learning-5a72987484d0"> <em class="hh">监督学习</em> </a></p></blockquote></div><div class="ab cl ln lo go lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ha hb hc hd he"><p id="9898" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="lj">原载于2019年8月7日</em><a class="ae lf" href="https://www.edureka.co/blog/regularization-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"><em class="lj">https://www.edureka.co</em></a><em class="lj">。</em></p></div></div>    
</body>
</html>