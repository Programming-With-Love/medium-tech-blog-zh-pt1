<html>
<head>
<title>Q Learning: All you need to know about Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">q学习:关于强化学习你需要知道的一切</h1>
<blockquote>原文：<a href="https://medium.com/edureka/q-learning-592524c3ecfc?source=collection_archive---------0-----------------------#2019-06-12">https://medium.com/edureka/q-learning-592524c3ecfc?source=collection_archive---------0-----------------------#2019-06-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/b1e684a1d712990d58e4065282080b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*oQUjOh71dQk9HeW_A3bnCA.png"/></div><figcaption class="il im et er es in io bd b be z dx">Q Learning — Edureka</figcaption></figure><p id="31d8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">人工智能和机器学习是行业中最热门的几个领域，这是有充分理由的。考虑到其主要目标是让机器模仿人类行为，人工智能将在2020年前创造230万个工作岗位。很奇怪，不是吗？因此，今天我们将按以下顺序讨论Q学习，即强化学习的构建模块:</p><ul class=""><li id="3c29" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">什么是强化学习？</li><li id="993e" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">q-学习过程</li><li id="b2c9" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">贝尔曼方程</li><li id="fc16" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">马尔可夫决策过程</li><li id="a393" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">演示:NumPy</li></ul><h1 id="7a99" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">什么是强化学习？</h1><p id="c435" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">让我们看看我们的日常生活。我们在环境中执行许多任务，有些任务会给我们带来回报，有些则不会。我们不断寻找不同的途径，试图找出哪条途径会带来回报，并根据我们的行动改进我们实现目标的策略。这是强化学习的一个最简单的类比。</p><p id="6326" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">主要关注领域:</p><ul class=""><li id="3321" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">环境</li><li id="9a6b" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">行动</li><li id="2739" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">报酬</li><li id="0215" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">状态</li></ul><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/03a0a5a28221b676f6228a644f21a96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*ISo6F6y97shaQ1sqK5_VEg.png"/></div></figure><p id="3f4e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">强化学习是机器学习的一个分支，它允许系统从自己的决策结果中学习。它解决了一种特殊的问题，即决策是连续的，目标是长期的。</p><h1 id="646e" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">q-学习过程</h1><p id="eed7" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">让我们用这里的问题陈述来理解什么是Q学习。它将帮助我们定义强化学习解决方案的主要组成部分，即代理、环境、行动、奖励和状态。</p><h2 id="bdfc" class="lj kc hh bd kd lk ll lm kh ln lo lp kl ja lq lr kp je ls lt kt ji lu lv kx lw bi translated"><strong class="ak">汽车工厂类比:</strong></h2><p id="9770" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们在一个装满机器人的汽车工厂。这些机器人帮助工厂工人运送组装汽车所需的必要零件。这些不同的零件位于工厂内9个工位的不同位置。零件包括底盘、车轮、仪表板、发动机等。工厂主将安装机箱的位置作为最高优先级。让我们看看这里的设置:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lx"><img src="../Images/14620f20ef3c2631ec32168687c8c503.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*hm5iWxXzvl9FsyRbHiOwhg.png"/></div></figure><p id="1a49" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">州:</strong></p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/e41ab8d66546129ac1b556fa029610c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*YaBdAy_sB7KDj3MMb0CpPQ.png"/></div></figure><p id="7796" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">机器人在特定情况下所处的位置称为其状态。因为对它进行编码比用名字来记忆更容易。让我们把位置映射到数字上。</p><p id="482a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">动作:</strong></p><p id="5a7b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">动作只不过是机器人移动到任何地方。假设一个机器人位于L2位置，它可以移动到的直接位置是L5、L1和L3。让我们更好地理解这一点，如果我们把它形象化:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lx"><img src="../Images/701b13f410727192be535c7a7bae8c7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*9miTAaEggVH8iRJBI_qrjg.png"/></div></figure><p id="9744" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">奖励:</strong></p><p id="e87a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">机器人直接从一个州到另一个州会得到奖励。例如，您可以从L2直接到达L5，反之亦然。因此，在任何一种情况下都将提供1的奖励。让我们来看看奖励表:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/db546b9a5a22d6a2512588fe4cd09672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*1B6aw75EhvEZ6gXnZnDjHA.png"/></div></figure><p id="e2a0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">还记得工厂主优先考虑底盘位置的时候吗？它是L7，所以我们将把这个事实纳入我们的奖励表。因此，我们将在(L7，L7)位置分配一个非常大的数字(本例中为999)。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/1cca5080b039530ec36541b94f13535f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*I2hLYAy8fJVjVGbx45flPw.png"/></div></figure><h1 id="d71d" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">贝尔曼方程</h1><p id="dd0e" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">现在假设一个机器人需要从A点到b点，它会选择一条能产生积极回报的路径。为此，假设我们为它提供一个足迹奖励。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/a1e9c7943ce925d56f77e05cafec9801.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*COAv4HcDa2Gfy6_Y7xR17A.png"/></div></figure><p id="437d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是，如果机器人从两者之间的某个地方开始，它可以看到两条或更多的路径。机器人因此不能做出决定，这主要是因为它没有<strong class="ir hi">记忆</strong>。这就是贝尔曼方程发挥作用的地方。</p><blockquote class="ly lz ma"><p id="ccec" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><strong class="ir hi"> V(s) = max(R(s，a)+𝜸v(s')</strong></p></blockquote><p id="4cc2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其中:</p><ul class=""><li id="7457" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">s =特定的州</li><li id="99e6" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">a =行动</li><li id="f813" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">s′=机器人从s进入的状态</li><li id="5266" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">𝜸 =贴现因子</li><li id="640a" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">R(s，a) =采用状态(s)和动作(a)并输出奖励值的奖励函数</li><li id="2ab9" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">V(s) =处于特定状态的值</li></ul><p id="ce45" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在目的地下面的方块会有1的奖励，这是最高的奖励，但是另一个方块呢？嗯，这就是折现系数的来源。让我们假设一个折扣系数为0.9，并逐一填充所有的块。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/d8035675fd83535f13592920858e0f46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*vaXvXIvdE2KZrKAa3qJrHQ.png"/></div></figure><h1 id="5ca0" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">马尔可夫决策过程</h1><p id="32f6" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">想象一个机器人在橙色方块上，需要到达目的地。但是，即使有轻微的功能障碍，机器人也会不知道该走哪条路，而不是向上走。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lx"><img src="../Images/64c042d72ed6d2efe0b0c89d24933980.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*-Ci2mcaG411BpFAzhXDPoQ.png"/></div></figure><p id="4ce0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以我们需要修改决策过程。它必须<strong class="ir hi">部分随机</strong>和<strong class="ir hi">部分受控于机器人</strong>。部分是随机的，因为我们不知道机器人什么时候会出故障，部分是可控的，因为这仍然是机器人的决定。这构成了马尔可夫决策过程的基础。</p><p id="422a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">马尔可夫决策过程(MDP)是一种离散时间随机控制过程。它提供了一个数学框架，用于在结果部分随机、部分受决策者控制的情况下对决策进行建模。</strong></p><p id="fe50" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以我们将使用我们最初的贝尔曼方程，并对其进行修改。我们不知道的是下一个状态。<strong class="ir hi">s’。</strong>我们所知道的是转弯的所有可能性，让我们改变等式。</p><p id="d8ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> V(s) = max(R(s，a)+𝜸v(s ')</strong></p><p id="9424" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> V(s) = max(R(s，a)+𝜸σs ' p(s，a，s ')v(s ')</strong></p><p id="200c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">σs ' P(s，a，s') V(s') : </strong>机器人的随机性期望</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lx"><img src="../Images/8d356b8a13c4ef8267494e7eb85c69c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*uRwY-mY0DbIkbQpPeJaKaQ.png"/></div></figure><p id="66cb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">V(s) = max(R(s，a) + 𝜸 ((0.8V(房间向上))+ (0.1V(房间向下)+ …。))</p><p id="25ae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们过渡到Q学习。Q-Learning提出了一种评估移动到一个状态的动作的质量的思想，而不是确定它被移动到的状态的可能值。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/1ef17ca93035815b698d8a7363128ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*0rgOhV5EG7rkR7U_lAkqTA.png"/></div></figure><p id="dd09" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是我们所得到的，如果我们把评估移动到某个状态s '的动作质量的想法结合进来。从更新后的贝尔曼方程中，如果我们去掉它们的<strong class="ir hi">最大</strong>分量，我们假设可能的动作只有一个足迹，除了动作的<strong class="ir hi">质量</strong>之外什么都没有。</p><p id="bb21" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> Q(s，a) = (R(s，a)+𝜸σs ' p(s，a，s ')v(s ')</strong></p><p id="9886" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这个量化行动质量的等式中，我们可以假设V(s)是Q(s，a)所有可能值的最大值。所以让我们用Q()的函数来代替v(s ')。</p><p id="e55b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> Q(s，a) = (R(s，a)+𝜸σs ' p(s，a，s') max Q(s '，a ')</strong></p><p id="56ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们离Q学习的最终方程式只有一步之遥。我们将引入一个<strong class="ir hi">时间差</strong>来计算环境随时间变化的Q值。但是我们如何观察Q的变化呢？</p><p id="0c51" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> TD(s，a) = (R(s，a)+𝜸σs ' p(s，a，s') max Q(s '，a ')—q(s，a) </strong></p><p id="0903" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们用同样的公式重新计算新的Q(s，a)，并从中减去以前已知的Q(s，a)。所以，上面的等式变成了:</p><p id="51c7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> Qt (s，a) = Qt-1 (s，a) + α TDt (s，a) </strong></p><p id="7eea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> Qt (s，a) = </strong>当前Q值</p><p id="9af5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> Qt-1 (s，a) = </strong>前一个Q值</p><p id="9ac9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> Qt (s，a) = Qt-1 (s，a) + α (R(s，a) + 𝜸最大Q(s '，a') — Qt-1(s，a)) </strong></p><h1 id="832c" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">演示:NumPy</h1><p id="04c2" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我将使用Python NumPy来演示Q学习是如何工作的。</p><p id="d85c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">步骤1:导入、参数、状态、动作和奖励</strong></p><pre class="lf lg lh li fd mf mg mh mi aw mj bi"><span id="5aad" class="lj kc hh mg b fi mk ml l mm mn">import numpy as np<br/> <br/>gamma = 0.75 # Discount factor<br/>alpha = 0.9 # Learning rate<br/> <br/>location_to_state = {<br/>    'L1' : 0,<br/>    'L2' : 1,<br/>    'L3' : 2,<br/>    'L4' : 3,<br/>    'L5' : 4,<br/>    'L6' : 5,<br/>    'L7' : 6,<br/>    'L8' : 7,<br/>    'L9' : 8<br/>}<br/> <br/>actions = [0,1,2,3,4,5,6,7,8]<br/> <br/>rewards = np.array([[0,1,0,0,0,0,0,0,0],<br/>              [1,0,1,0,0,0,0,0,0],<br/>              [0,1,0,0,0,1,0,0,0],<br/>              [0,0,0,0,0,0,1,0,0],<br/>              [0,1,0,0,0,0,0,1,0],<br/>              [0,0,1,0,0,0,0,0,0],<br/>              [0,0,0,1,0,0,0,1,0],<br/>              [0,0,0,0,1,0,1,0,1],<br/>              [0,0,0,0,0,0,0,1,0]])</span></pre><p id="8a6f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">步骤2:将索引映射到位置</strong></p><pre class="lf lg lh li fd mf mg mh mi aw mj bi"><span id="04dd" class="lj kc hh mg b fi mk ml l mm mn">state_to_location = dict((state,location) for location,state in location_to_state.items())</span></pre><p id="f3ac" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第三步:使用Q学习过程获得最佳路线</strong></p><pre class="lf lg lh li fd mf mg mh mi aw mj bi"><span id="1352" class="lj kc hh mg b fi mk ml l mm mn">def get_optimal_route(start_location,end_location):<br/>    rewards_new = np.copy(rewards)<br/>    ending_state = location_to_state[end_location]<br/>    rewards_new[ending_state,ending_state] = 999<br/> <br/>    Q = np.array(np.zeros([9,9]))<br/> <br/>    # Q-Learning process<br/>    for i in range(1000):<br/>        # Picking up a random state<br/>        current_state = np.random.randint(0,9) # Python excludes the upper bound<br/>        playable_actions = []<br/>        # Iterating through the new rewards matrix<br/>        for j in range(9):<br/>            if rewards_new[current_state,j] &gt; 0:<br/>                playable_actions.append(j)<br/>        # Pick a random action that will lead us to next state<br/>        next_state = np.random.choice(playable_actions)<br/>        # Computing Temporal Difference<br/>        TD = rewards_new[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]<br/>        # Updating the Q-Value using the Bellman equation<br/>        Q[current_state,next_state] += alpha * TD<br/> <br/>    # Initialize the optimal route with the starting location<br/>    route = [start_location]<br/>    #Initialize next_location with starting location<br/>    next_location = start_location<br/> <br/>    # We don't know about the exact number of iterations needed to reach to the final location hence while loop will be a good choice for iteratiing<br/>    while(next_location != end_location):<br/>        # Fetch the starting state<br/>        starting_state = location_to_state[start_location]<br/>        # Fetch the highest Q-value pertaining to starting state<br/>        next_state = np.argmax(Q[starting_state,])<br/>        # We got the index of the next state. But we need the corresponding letter.<br/>        next_location = state_to_location[next_state]<br/>        route.append(next_location)<br/>        # Update the starting location for the next iteration<br/>        start_location = next_location<br/> <br/>    return route</span></pre><p id="52e9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第四步:打印路线</strong></p><pre class="lf lg lh li fd mf mg mh mi aw mj bi"><span id="90cf" class="lj kc hh mg b fi mk ml l mm mn">print(get_optimal_route('L1', 'L9'))</span></pre><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mo"><img src="../Images/b8e2a92f8f4bbd32915b30132ae39963.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*lp28gO33nucKQ3z9mZbuJQ.png"/></div></figure><p id="688d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这样，我们就结束了Q-Learning。我希望你已经了解了Q学习的工作原理以及各种各样的依赖关系，比如时间差，贝尔曼方程等等。</p><p id="058d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="9c6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释深度学习的各个其他方面。</p><blockquote class="ly lz ma"><p id="f619" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">1.<a class="ae mp" rel="noopener" href="/edureka/tensorflow-tutorial-ba142ae96bca"> TensorFlow教程</a></p><p id="48b9" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">2.<a class="ae mp" rel="noopener" href="/edureka/pytorch-tutorial-9971d66f6893"> PyTorch教程</a></p><p id="33d2" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">3.<a class="ae mp" rel="noopener" href="/edureka/perceptron-learning-algorithm-d30e8b99b156">感知器学习算法</a></p><p id="95bf" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">4.<a class="ae mp" rel="noopener" href="/edureka/neural-network-tutorial-2a46b22394c9">神经网络教程</a></p><p id="5ca7" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">5.什么是反向传播？</p><p id="f372" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">6.<a class="ae mp" rel="noopener" href="/edureka/convolutional-neural-network-3f2c5b9c4778">卷积神经网络</a></p><p id="3ac7" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">7.<a class="ae mp" rel="noopener" href="/edureka/capsule-networks-d7acd437c9e">胶囊神经网络</a></p><p id="7532" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">8.<a class="ae mp" rel="noopener" href="/edureka/recurrent-neural-networks-df945afd7441">递归神经网络</a></p><p id="f507" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">9.<a class="ae mp" rel="noopener" href="/edureka/autoencoders-tutorial-cfdcebdefe37">自动编码器教程</a></p><p id="c366" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">10.<a class="ae mp" rel="noopener" href="/edureka/restricted-boltzmann-machine-tutorial-991ae688c154">受限玻尔兹曼机教程</a></p><p id="231c" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">11.<a class="ae mp" rel="noopener" href="/edureka/pytorch-vs-tensorflow-252fc6675dd7"> PyTorch vs TensorFlow </a></p><p id="394c" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">12.<a class="ae mp" rel="noopener" href="/edureka/deep-learning-with-python-2adbf6e9437d">用Python进行深度学习</a></p><p id="c256" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">13.<a class="ae mp" rel="noopener" href="/edureka/artificial-intelligence-tutorial-4257c66f5bb1">人工智能教程</a></p><p id="d310" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">14.<a class="ae mp" rel="noopener" href="/edureka/tensorflow-image-classification-19b63b7bfd95">张量流图像分类</a></p><p id="953d" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">15.<a class="ae mp" rel="noopener" href="/edureka/artificial-intelligence-applications-7b93b91150e3">人工智能应用</a></p><p id="5b0c" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">16.<a class="ae mp" rel="noopener" href="/edureka/become-artificial-intelligence-engineer-5ac2ede99907">如何成为一名人工智能工程师？</a></p><p id="dc81" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">17.<a class="ae mp" rel="noopener" href="/edureka/tensorflow-object-detection-tutorial-8d6942e73adc">tensor flow中的对象检测</a></p><p id="8ffb" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">18.<a class="ae mp" rel="noopener" href="/edureka/apriori-algorithm-d7cc648d4f1e"> Apriori算法</a></p><p id="f3ca" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">19.<a class="ae mp" rel="noopener" href="/edureka/introduction-to-markov-chains-c6cb4bcd5723">马尔可夫链与Python </a></p><p id="6d99" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">20.<a class="ae mp" rel="noopener" href="/edureka/artificial-intelligence-algorithms-fad283a0d8e2">人工智能算法</a></p><p id="3e4e" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">21.<a class="ae mp" rel="noopener" href="/edureka/best-laptop-for-machine-learning-a4a5f8ba5b">机器学习的最佳笔记本电脑</a></p><p id="89f8" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">22.<a class="ae mp" rel="noopener" href="/edureka/top-artificial-intelligence-tools-36418e47bf2a">12大人工智能工具</a></p><p id="c974" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">23.<a class="ae mp" rel="noopener" href="/edureka/artificial-intelligence-interview-questions-872d85387b19">人工智能(AI)面试问题</a></p><p id="de36" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">24.<a class="ae mp" rel="noopener" href="/edureka/theano-vs-tensorflow-15f30216b3bc"> Theano vs TensorFlow </a></p><p id="5f0e" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">25.<a class="ae mp" rel="noopener" href="/edureka/what-is-a-neural-network-56ae7338b92d">什么是神经网络？</a></p><p id="113e" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">26.<a class="ae mp" rel="noopener" href="/edureka/pattern-recognition-5e2d30ab68b9">模式识别</a></p><p id="b1cc" class="ip iq mb ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">27.<a class="ae mp" rel="noopener" href="/edureka/alpha-beta-pruning-in-ai-b47ee5500f9a">人工智能中的阿尔法贝塔剪枝</a></p></blockquote></div><div class="ab cl mq mr go ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ha hb hc hd he"><p id="449c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="mb">原载于2019年6月12日https://www.edureka.co</em><a class="ae mp" href="https://www.edureka.co/blog/q-learning/" rel="noopener ugc nofollow" target="_blank"><em class="mb"/></a><em class="mb">。</em></p></div></div>    
</body>
</html>