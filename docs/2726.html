<html>
<head>
<title>TensorFlow Tutorial — A Comprehensive Guide To Deep Learning Using TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow教程-使用TensorFlow进行深度学习的全面指南</h1>
<blockquote>原文：<a href="https://medium.com/edureka/tensorflow-tutorial-ba142ae96bca?source=collection_archive---------4-----------------------#2017-11-07">https://medium.com/edureka/tensorflow-tutorial-ba142ae96bca?source=collection_archive---------4-----------------------#2017-11-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/0683031c1963fe932559e1c721c0bf93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*fc5IA6pR1c9spvae9jA58Q.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">TensorFlow Tutorial — Edureka</figcaption></figure><p id="e661" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我为有兴趣应用深度学习算法使用TensorFlow解决各种问题的专业人士和爱好者设计了这篇TensorFlow教程。TensorFlow是一个开源的深度学习库，它基于数据流图的概念来构建模型。它允许您创建具有许多层的大规模神经网络。以下是这篇TensorFlow教程文章将讨论的主题:</p><ul class=""><li id="b9e6" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi">什么是张量流</strong></li><li id="ad64" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">张量流代码基础</strong></li><li id="b7ad" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi"> TensorFlow用例</strong></li></ul><h1 id="abd6" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">张量是什么？</h1><p id="654d" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">在这篇TensorFlow教程中，在讲TensorFlow之前，我们先来了解一下<em class="le">什么是张量</em>。张量只不过是深度学习中表示数据的事实。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/c5dc1906fd2788539c60cfd02a2cd478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*KE9U4oPoFxEZiBV_GTO8lA.png"/></div></figure><p id="369e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如上图所示，张量只是多维数组，它允许你表示更高维的数据。一般来说，深度学习处理高维数据集，其中维度指的是数据集中存在的不同特征。事实上,“张量流”这个名字来源于神经网络对张量执行的操作。它实际上是一个张量流。既然，你已经理解了什么是张量，让我们在这个张量流教程中继续前进，理解— <em class="le">什么是张量流？</em></p><h1 id="50c6" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">什么是张量流？</h1><p id="eb2d" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">TensorFlow是一个基于Python的库，为实现深度学习模型提供了不同类型的功能。如前所述，术语张量流由两个术语组成——张量和流:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/018474b445042c24b8d4e77fb9bb7fec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*OJl7FWyF10vsXtxUQMrhxg.png"/></div></figure><p id="b119" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在TensorFlow中，术语张量是指多维数组形式的数据表示，而术语流是指对张量执行的一系列操作，如上图所示。</p><p id="6cf3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们已经介绍了足够多的关于TensorFlow的背景知识。</p><p id="f639" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，在这个张量流教程中，我们将讨论张量流代码基础。</p><h1 id="3be2" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">代码基础</h1><p id="1f5d" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">基本上，编写TensorFlow程序的整个过程包括两个步骤:</p><ol class=""><li id="8b2a" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lk jt ju jv bi translated">构建计算图</li><li id="1e93" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lk jt ju jv bi translated">运行计算图表</li></ol><p id="1ad2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我来给你一一解释一下以上两个步骤:</p><h2 id="9c29" class="ll kc hh bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">构建计算图</h2><p id="abf4" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">那么，<em class="le">什么是计算图？</em>嗯，计算图是一系列排列成图中节点的张量流运算。每个节点取0个或多个张量作为输入，并产生一个张量作为输出。让我给你一个简单的计算图的例子，它由三个节点组成——a<strong class="ir hi"><em class="le">a</em></strong>，<strong class="ir hi"><em class="le">b</em></strong>&amp;<strong class="ir hi"><em class="le">c</em></strong>如下所示:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/5e842b152cff81376ce2ebc3395cfd9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*aEtCzlfgnskczEon1tZW0w.png"/></div></figure><h2 id="e2c2" class="ll kc hh bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">以上计算图的解释:</h2><ul class=""><li id="6950" class="jn jo hh ir b is kz iw la ja ma je mb ji mc jm js jt ju jv bi translated">常量节点用于存储常量值，因为它接受零输入，但产生存储的值作为输出。在上面的例子中，a和b分别是值为5和6的常量节点。</li><li id="d1ce" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">节点c表示常量节点a与b相乘的操作。因此，执行节点c将导致常量节点a与b相乘。</li></ul><p id="19e2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">基本上，人们可以将计算图视为一种替代方式，将张量流程序中发生的数学计算概念化。分配给计算图的不同节点的操作可以并行执行，因此在计算方面提供了更好的性能。</p><p id="8e2d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里我们只是描述计算，它不计算任何东西，它不保存任何值，它只是定义你的代码中指定的操作。</p><h2 id="4f44" class="ll kc hh bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">运行计算图表</h2><p id="1700" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">让我们以之前的计算图为例，了解如何执行它。以下是上一个示例中的代码:</p><p id="0367" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">例1: </strong></p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="8408" class="ll kc hh me b fi mi mj l mk ml">import tensorflow as tf<br/> <br/># Build a graph<br/>a = tf.constant(5.0)<br/>b = tf.constant(6.0)<br/>c = a * b</span></pre><p id="2611" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，为了获得节点c的输出，我们需要在一个<strong class="ir hi">会话</strong>中运行计算图。Session将图形操作放到设备上，如CPU或GPU，并提供执行它们的方法。</p><p id="0cec" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">会话封装了TensorFlow运行时的控制和状态，即它存储了有关所有操作执行顺序的信息，并将已计算操作的结果传递给管道中的下一个操作。让我向您展示如何在一个会话中运行上面的计算图(每行代码的解释已作为注释添加):</p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="eede" class="ll kc hh me b fi mi mj l mk ml"># Create the session object<br/>sess = tf.Session()<br/> <br/>#Run the graph within a session and store the output to a variable<br/>output_c = sess.run(c)<br/> <br/>#Print the output of node c<br/>print(output_c)<br/> <br/>#Close the session to free up some resources<br/>sess.close()</span></pre><p id="5c5b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出将是:</p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="2b01" class="ll kc hh me b fi mi mj l mk ml">30</span></pre><p id="af2f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，这都是关于会话和在会话中运行计算图。现在，让我们谈谈在使用TensorFlow构建深度学习模型时我们将广泛使用的变量和占位符。</p><h1 id="0084" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">常量、占位符和变量</h1><p id="04b4" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">在TensorFlow中，常量、占位符和变量用于表示深度学习模型的不同参数。因为我之前已经讨论过常量，所以我将从占位符开始。</p><h2 id="3b38" class="ll kc hh bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">占位符:</h2><p id="0ec3" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">TensorFlow常数允许您存储一个值，但是，如果您希望节点在运行时接受输入，该怎么办呢？对于这种功能，使用占位符允许您的图形将外部输入作为参数。基本上，占位符是对以后或运行时提供一个值的承诺。让我给你举个例子，让事情变得简单些:</p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="fb9f" class="ll kc hh me b fi mi mj l mk ml">import tensorflow as tf<br/> <br/># Creating placeholders<br/>a = tf. placeholder(tf.float32)<br/>b = tf. placeholder(tf.float32)<br/> <br/># Assigning multiplication operation w.r.t. a &amp;amp;amp; b to node mul<br/>mul = a*b<br/> <br/># Create session object<br/>sess = tf.Session()<br/> <br/># Executing mul by passing the values [1, 3] [2, 4] for a and b respectively<br/>output = sess.run(mul, {a: [1,3], b: [2, 4]})<br/>print('Multiplying a b:', output)</span></pre><p id="cc1f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出将是:</p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="5148" class="ll kc hh me b fi mi mj l mk ml"><strong class="me hi">Output:</strong><br/>[2. 12.]</span></pre><h2 id="32d3" class="ll kc hh bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">关于占位符需要记住的几点:</h2><ul class=""><li id="346d" class="jn jo hh ir b is kz iw la ja ma je mb ji mc jm js jt ju jv bi translated">占位符未初始化，不包含任何数据。</li><li id="bc92" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">人们必须向占位符提供在运行时要考虑的输入或反馈。</li><li id="741d" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">在没有输入的情况下执行占位符会产生错误。</li></ul><p id="7055" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们向前看，理解— <em class="le">什么是变量？</em></p><h2 id="c735" class="ll kc hh bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">变量</h2><p id="5ee5" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">在深度学习中，占位符用于在你的模型或图形中获取任意输入。除了接受输入之外，您还需要修改图形，以便它可以根据相同的输入产生新的输出。为此，您将使用变量。简而言之，变量允许您向图形中添加可训练的参数或节点，即值可以在一段时间内修改。变量通过提供初始值和类型来定义，如下所示:</p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="2d1f" class="ll kc hh me b fi mi mj l mk ml">var = tf.Variable( [0.4], dtype = tf.float32 )</span></pre><blockquote class="mm mn mo"><p id="f48c" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated"><strong class="ir hi">注:</strong></p><p id="38f2" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated"><em class="hh">如果您没有明确提供数据类型，TensorFlow将从初始化的值中推断出常量/变量的类型。</em></p><p id="8c12" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">TensorFlow有许多自己的数据类型，如<em class="hh"> tf.float32 </em>、<em class="hh"> tf.int32 </em>等。<em class="hh">这里的</em>  <em class="hh">你可以参考</em> <a class="ae ms" href="https://www.tensorflow.org/api_docs/python/tf/DType" rel="noopener ugc nofollow" target="_blank"> <em class="hh">以上的全部。</em></a></p></blockquote><p id="2356" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">常数在调用<em class="le"> tf.constant </em>时被初始化，它们的值永远不能改变。相反，当你调用<em class="le"> tf时，变量不会被初始化。变量</em>。要初始化TensorFlow程序中的所有变量，您<strong class="ir hi">必须</strong>显式调用如下所示的特殊操作:</p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="d919" class="ll kc hh me b fi mi mj l mk ml">init = tf.global_variables_initializer() <br/>sess.run(init)</span></pre><p id="db24" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">永远记住，在第一次使用图形之前，必须对变量进行初始化。</p><blockquote class="mm mn mo"><p id="6c24" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated"><strong class="ir hi">注:</strong> <em class="hh"> TensorFlow变量是包含张量的内存缓冲区，但与仅在图形运行时实例化并在运行后立即删除的普通张量不同，变量在图形的多次执行中仍然存在。</em></p></blockquote><p id="677b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们已经介绍了足够多的TensorFlow基础知识，让我们继续了解如何使用TensorFlow实现线性回归模型。</p><h1 id="a6a2" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">使用张量流的线性回归模型</h1><p id="dac0" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">线性回归模型用于使用如下所示的线性回归方程，从一个变量(因变量)的已知值预测另一个变量(自变量)的未知值:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/41950144799c4d41c5c565af13debef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*AAsgF51Ws2VKTD3_lYlVhw.png"/></div></figure><p id="8d8f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，要创建线性模型，您需要:</p><ol class=""><li id="7fd6" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lk jt ju jv bi translated">因变量或输出变量(Y)</li><li id="e348" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lk jt ju jv bi translated">斜率变量(w)</li><li id="6743" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lk jt ju jv bi translated">Y —截距或偏差(b)</li><li id="7d45" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lk jt ju jv bi translated">独立或输入变量(X)</li></ol><p id="82df" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，让我们开始使用TensorFlow构建线性模型:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es mt"><img src="../Images/2d00dd1d22c38997a40a9a7d651469de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*miEv0JyH8Nke8R5FkqYi6Q.png"/></div></div></figure><p id="e470" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">单击下面的按钮复制代码:</p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="8841" class="ll kc hh me b fi mi mj l mk ml"># Creating variable for parameter slope (W) with initial value as 0.4<br/>W = tf.Variable([.4], tf.float32)<br/> <br/>#Creating variable for parameter bias (b) with initial value as -0.4<br/>b = tf.Variable([-0.4], tf.float32)<br/> <br/># Creating placeholders for providing input or independent variable, denoted by x<br/>x = tf.placeholder(tf.float32)<br/> <br/># Equation of Linear Regression<br/>linear_model = W * x + b<br/> <br/># Initializing all the variables<br/>sess = tf.Session()<br/>init = tf.global_variables_initializer()<br/>sess.run(init)<br/> <br/># Running regression model to calculate the output w.r.t. to provided x values<br/>print(sess.run(linear_model {x: [1, 2, 3, 4]}))</span></pre><p id="feb9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="1681" class="ll kc hh me b fi mi mj l mk ml">[ 0.     0.40000001 0.80000007 1.20000005]</span></pre><p id="5acb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上述代码仅代表回归模型实现背后的基本思想，即如何遵循回归线方程，以获得与一组输入值相关的输出。但是，为了使这个模型成为一个完整的回归模型，还有两件事情需要添加:</p><ul class=""><li id="81af" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">首先，我们需要提供一种机制，通过这种机制，我们的模型可以根据给定的一组输入和相应的输出自动训练自己。</li><li id="66f1" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">我们需要做的第二件事是，根据给定的一组x值，通过将模型的输出与期望或目标输出进行比较，来验证我们训练好的模型。</li></ul><p id="c8ca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们了解如何将上述功能整合到我的回归模型代码中。</p><h1 id="e1e4" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">损失函数—模型验证</h1><p id="12cc" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">损失函数衡量模型的当前输出与期望输出或目标输出之间的差距。我将为我的线性回归模型使用一个最常用的损失函数，称为误差平方和或SSE。SSE计算出w.r.t .模型输出(由linear_model表示)和期望或目标输出(y)为:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es my"><img src="../Images/4696dc9f96320a4e92c77da60b1c06e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*B81EVU2QmsYDcz7fbnDEGA.png"/></div></figure><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es mz"><img src="../Images/ecebb193a66963e77dd15a5780b34164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJ44N2eux5vX7rvAvfHmnw.png"/></div></div></figure><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="233b" class="ll kc hh me b fi mi mj l mk ml">y = tf.placeholder(tf.float32)<br/>error = linear_model - y<br/>squared_errors = tf.square(error)<br/>loss = tf.reduce_sum(squared_errors)<br/>print(sess.run(loss, {x:[1,2,3,4], y:[2, 4, 6, 8]})</span></pre><p id="7b63" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出将是:</p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="75e7" class="ll kc hh me b fi mi mj l mk ml"><strong class="me hi">Output:</strong><br/>90.24</span></pre><p id="0241" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如你所见，我们的损失值很高。因此，我们需要调整我们的权重(W)和偏差(b ),以减少我们接收到的误差。</p><h1 id="d3d8" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">tf.train API —为模型定型</h1><p id="75bd" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">TensorFlow提供了<strong class="ir hi">优化器</strong>，它们缓慢地改变每个变量，以最小化损失函数或误差。最简单的优化器是<strong class="ir hi">梯度下降</strong>。它根据该变量的损失导数的大小来修改每个变量。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="er es mz"><img src="../Images/ecebb193a66963e77dd15a5780b34164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJ44N2eux5vX7rvAvfHmnw.png"/></div></div></figure><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="05c4" class="ll kc hh me b fi mi mj l mk ml">#Creating an instance of gradient descent optimizer<br/>optimizer = tf.train.GradientDescentOptimizer(0.01)<br/> <br/>train = optimizer.minimize(loss)<br/> <br/>for i in range(1000):<br/>     sess.run(train, {x:[1, 2, 3, 4], y:[2, 4, 6, 8]})<br/>print(sess.run([W, b]))</span></pre><p id="e2d6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出将是:</p><pre class="lg lh li lj fd md me mf mg aw mh bi"><span id="f700" class="ll kc hh me b fi mi mj l mk ml"><strong class="me hi">Output:<br/> </strong>[array([ 1.99999964], dtype=float32), array([ 9.86305167e-07], dtype=float32)]</span></pre><p id="ffb7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是如何使用TensorFlow创建一个线性模型，并训练它以获得所需的输出。</p><p id="faf2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">这就把我们带到了“张量流教程”这篇文章的结尾。我希望这篇文章对你有所帮助，并增加了你的知识价值。</em></p><p id="6864" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="85d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释深度学习的各个其他方面。</p><blockquote class="mm mn mo"><p id="3bf3" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">1.<a class="ae ms" rel="noopener" href="/edureka/tensorflow-tutorial-ba142ae96bca"/><a class="ae ms" rel="noopener" href="/edureka/tensorflow-object-detection-tutorial-8d6942e73adc">tensor flow中的物体检测</a></p><p id="860f" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">2.<a class="ae ms" rel="noopener" href="/edureka/pytorch-tutorial-9971d66f6893"> PyTorch教程</a></p><p id="5b92" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">3.<a class="ae ms" rel="noopener" href="/edureka/perceptron-learning-algorithm-d30e8b99b156">感知器学习算法</a></p><p id="9c9a" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">4.<a class="ae ms" rel="noopener" href="/edureka/neural-network-tutorial-2a46b22394c9">神经网络教程</a></p><p id="729d" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">5.<a class="ae ms" rel="noopener" href="/edureka/backpropagation-bd2cf8fdde81">什么是反向传播？</a></p><p id="53f5" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">6.<a class="ae ms" rel="noopener" href="/edureka/convolutional-neural-network-3f2c5b9c4778">卷积神经网络</a></p><p id="1fd6" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">7.<a class="ae ms" rel="noopener" href="/edureka/capsule-networks-d7acd437c9e">胶囊神经网络</a></p><p id="6024" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">8.<a class="ae ms" rel="noopener" href="/edureka/recurrent-neural-networks-df945afd7441">递归神经网络</a></p><p id="54d3" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">9.<a class="ae ms" rel="noopener" href="/edureka/autoencoders-tutorial-cfdcebdefe37">自动编码器教程</a></p><p id="40bf" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">10.<a class="ae ms" rel="noopener" href="/edureka/restricted-boltzmann-machine-tutorial-991ae688c154">受限玻尔兹曼机教程</a></p><p id="3408" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">11.<a class="ae ms" rel="noopener" href="/edureka/pytorch-vs-tensorflow-252fc6675dd7"> PyTorch vs TensorFlow </a></p><p id="8698" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">12.<a class="ae ms" rel="noopener" href="/edureka/deep-learning-with-python-2adbf6e9437d">用Python进行深度学习</a></p><p id="462f" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">13.<a class="ae ms" rel="noopener" href="/edureka/artificial-intelligence-tutorial-4257c66f5bb1">人工智能教程</a></p><p id="ef91" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">14.<a class="ae ms" rel="noopener" href="/edureka/tensorflow-image-classification-19b63b7bfd95">张量流图像分类</a></p><p id="4dea" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">15.<a class="ae ms" rel="noopener" href="/edureka/artificial-intelligence-applications-7b93b91150e3">人工智能应用</a></p><p id="291c" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">16.<a class="ae ms" rel="noopener" href="/edureka/become-artificial-intelligence-engineer-5ac2ede99907">如何成为一名人工智能工程师？</a></p><p id="9898" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">17.<a class="ae ms" rel="noopener" href="/edureka/q-learning-592524c3ecfc">问学习</a></p><p id="df15" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">18.<a class="ae ms" rel="noopener" href="/edureka/apriori-algorithm-d7cc648d4f1e"> Apriori算法</a></p><p id="141c" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">19.<a class="ae ms" rel="noopener" href="/edureka/introduction-to-markov-chains-c6cb4bcd5723">用Python实现马尔可夫链</a></p><p id="b19b" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">20.<a class="ae ms" rel="noopener" href="/edureka/artificial-intelligence-algorithms-fad283a0d8e2">人工智能算法</a></p><p id="df1a" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">21.<a class="ae ms" rel="noopener" href="/edureka/best-laptop-for-machine-learning-a4a5f8ba5b">机器学习的最佳笔记本电脑</a></p><p id="c941" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">22.<a class="ae ms" rel="noopener" href="/edureka/top-artificial-intelligence-tools-36418e47bf2a">12大人工智能工具</a></p><p id="5334" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">23.<a class="ae ms" rel="noopener" href="/edureka/artificial-intelligence-interview-questions-872d85387b19">人工智能(AI)面试问题</a></p><p id="ca97" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">24.<a class="ae ms" rel="noopener" href="/edureka/theano-vs-tensorflow-15f30216b3bc"> Theano vs TensorFlow </a></p><p id="f2bf" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">25.<a class="ae ms" rel="noopener" href="/edureka/what-is-a-neural-network-56ae7338b92d">什么是神经网络？</a></p><p id="1bb5" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">26.<a class="ae ms" rel="noopener" href="/edureka/pattern-recognition-5e2d30ab68b9">模式识别</a></p><p id="8f17" class="ip iq le ir b is it iu iv iw ix iy iz mp jb jc jd mq jf jg jh mr jj jk jl jm ha bi translated">27.<a class="ae ms" rel="noopener" href="/edureka/alpha-beta-pruning-in-ai-b47ee5500f9a">人工智能中的阿尔法贝塔剪枝</a></p></blockquote></div><div class="ab cl na nb go nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ha hb hc hd he"><p id="277a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">原载于2017年11月7日</em><a class="ae ms" href="https://www.edureka.co/blog/tensorflow-tutorial/" rel="noopener ugc nofollow" target="_blank"><em class="le">www.edureka.co</em></a><em class="le">。</em></p></div></div>    
</body>
</html>