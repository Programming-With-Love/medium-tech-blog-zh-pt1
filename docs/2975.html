<html>
<head>
<title>TensorFlow Image Classification - Build your own Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow图像分类-构建您自己的分类器</h1>
<blockquote>原文：<a href="https://medium.com/edureka/tensorflow-image-classification-19b63b7bfd95?source=collection_archive---------0-----------------------#2019-04-22">https://medium.com/edureka/tensorflow-image-classification-19b63b7bfd95?source=collection_archive---------0-----------------------#2019-04-22</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/96aae4e3429906395d2617422fa42dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*6DFclC9eASBqBDhDeFy2fA.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">TensorFlow Image Classification — Edureka</figcaption></figure><p id="f677" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">图像分类是一项即使是婴儿也可以在几秒钟内完成的任务，但对于机器来说，这一直是一项艰巨的任务，直到最近人工智能和深度学习的进步。自动驾驶汽车可以实时检测物体并采取所需的行动，而这一切之所以成为可能，是因为TensorFlow图像分类。在本文中，我将引导您了解以下主题:</p><ul class=""><li id="7920" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">什么是张量流？</li><li id="bf5c" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">什么是图像分类？</li><li id="26dc" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">TensorFlow图像分类:时尚MNIST</li><li id="a16f" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">美国有线电视新闻网</li></ul><h1 id="02db" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">什么是张量流？</h1><p id="60b4" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated"><a class="ae le" href="https://www.edureka.co/blog/tensorflow-tutorial?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=tensorflow-image-classification" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>是谷歌的开源机器学习框架，用于跨一系列任务的数据流编程。图中的节点表示数学运算，而图边表示它们之间通信的多维数据数组。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/966984eff0c36aec0829e4b4beed4283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*PNgBzP73kKPBplDJDvLYKA.png"/></div></figure><p id="7fc8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">张量只是多维数组，是二维表向更高维数据的扩展。Tensorflow有许多功能，这使它适合深度学习，它的核心开源库可以帮助您开发和训练ML模型。</p><h1 id="8d9a" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">什么是图像分类？</h1><p id="aaf4" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">图像分类的目的是将数字图像中的所有像素归类到几个<strong class="ir hi"/><strong class="ir hi">类别</strong>或<strong class="ir hi">主题</strong>中的一个。然后，这种分类数据可用于制作图像中出现的土地覆盖的<strong class="ir hi">专题地图</strong>。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/bf4b86138a666ff5d8237efe6ac4f00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*e2iV1jgxpzekWFzSH7pSuw.png"/></div></figure><p id="9b25" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，根据分类期间分析师和计算机之间的交互，有两种类型的分类:</p><ul class=""><li id="3753" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">监督&amp;</li><li id="05f2" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">无人监督的</li></ul><p id="a0b7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，不浪费任何时间，让我们进入张量流图像分类。我有两个例子:简单和困难。让我们从简单的开始。</p><h1 id="ec51" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">TensorFlow图像分类:时尚MNIST</h1><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es ll"><img src="../Images/391c61568819d82fe9b1002a60ed2f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*vIcknITZlyfCeTGnuEU8BQ.png"/></div></figure><p id="d963" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里我们将使用时尚MNIST数据集，它包含10个类别的70，000幅灰度图像。我们将把60000英镑用于培训，其余10000英镑用于测试。您可以直接从TensorFlow访问时尚MNIST，只需导入和加载数据。</p><h2 id="53af" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">让我们先导入库</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="d437" class="lm kc hh mb b fi mf mg l mh mi">from __future__ import absolute_import, division, print_function<br/># TensorFlow and tf.keras</span><span id="bba5" class="lm kc hh mb b fi mj mg l mh mi">import tensorflow as tf<br/>from tensorflow import keras</span><span id="0380" class="lm kc hh mb b fi mj mg l mh mi"># Helper libraries</span><span id="81be" class="lm kc hh mb b fi mj mg l mh mi">import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><h2 id="39d8" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">让我们加载数据</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="6686" class="lm kc hh mb b fi mf mg l mh mi">fashion_mnist = keras.datasets.fashion_mnist</span><span id="ed2e" class="lm kc hh mb b fi mj mg l mh mi">(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()</span></pre><h2 id="33b2" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">接下来，我们将图像映射到类别中</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="fc1d" class="lm kc hh mb b fi mf mg l mh mi">class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']</span></pre><h2 id="788e" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">探索数据</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="da81" class="lm kc hh mb b fi mf mg l mh mi">train_images.shape<br/>#Each Label is between 0-9</span><span id="08b9" class="lm kc hh mb b fi mj mg l mh mi">train_labels<br/>test_images.shape</span></pre><h2 id="d459" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">现在，该对数据进行预处理了。</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="df72" class="lm kc hh mb b fi mf mg l mh mi">plt.figure()<br/>plt.imshow(train_images[0])<br/>plt.colorbar()<br/>plt.grid(False)<br/>plt.show()<br/>#If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255.</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mk"><img src="../Images/2fea635fb918deb7254a562b49f58dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*cbovZwGEREIgARKm1ZiBNw.png"/></div></figure><h2 id="95a5" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">我们必须将图像从0到1进行缩放，以将其输入神经网络</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="ebfc" class="lm kc hh mb b fi mf mg l mh mi">train_images = train_images / 255.0</span><span id="53c7" class="lm kc hh mb b fi mj mg l mh mi">test_images = test_images / 255.0</span></pre><h2 id="49f7" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated">让我们展示一些图片。</h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="b68c" class="lm kc hh mb b fi mf mg l mh mi">plt.figure(figsize=(10,10))<br/>for i in range(25):<br/>    plt.subplot(5,5,i+1)<br/>    plt.xticks([])<br/>    plt.yticks([])<br/>    plt.grid(False)<br/>    plt.imshow(train_images[i], cmap=plt.cm.binary)<br/>    plt.xlabel(class_names[train_labels[i]])<br/>plt.show()</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es ml"><img src="../Images/f506978774e730f43d38c3e71331e908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*rYUVq9atI_gw2SxJKOw_fA.png"/></div></figure><h2 id="c9fe" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">设置图层</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="6a4e" class="lm kc hh mb b fi mf mg l mh mi">model = keras.Sequential([<br/>    keras.layers.Flatten(input_shape=(28, 28)),<br/>    keras.layers.Dense(128, activation=tf.nn.relu),<br/>    keras.layers.Dense(10, activation=tf.nn.softmax)<br/>])</span></pre><h2 id="3442" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">编译模型</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="a432" class="lm kc hh mb b fi mf mg l mh mi">model.compile(optimizer='adam',<br/>              loss='sparse_categorical_crossentropy',<br/>              metrics=['accuracy'])</span></pre><h2 id="6064" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">模特培训</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="269f" class="lm kc hh mb b fi mf mg l mh mi">model.fit(train_images, train_labels, epochs=10)</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es mm"><img src="../Images/f3cc7353967e4af394755662a07b0257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cbyA02zKE8N1nPP6d3T45A.png"/></div></div></figure><h2 id="f074" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">评估精度</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="e0c7" class="lm kc hh mb b fi mf mg l mh mi">test_loss, test_acc = model.evaluate(test_images, test_labels)</span><span id="43da" class="lm kc hh mb b fi mj mg l mh mi">print('Test accuracy:', test_acc)</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mr"><img src="../Images/92a5c7f3f05750fc4dac3ba4183f3317.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*8ezb-1NRds7VsmzNxvFMDg.png"/></div></figure><h2 id="3ecb" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">做出预测</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="46fb" class="lm kc hh mb b fi mf mg l mh mi">predictions = model.predict(test_images)</span><span id="9d0d" class="lm kc hh mb b fi mj mg l mh mi">predictions[0]</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/a2d6be485d2e4cf0a19fe27dcfb2b166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*jTbeD2kM0V5sapb0OX09Cg.png"/></div></figure><p id="8e87" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">预测是由10个数字组成的数组。这些描述了模型的“置信度”,即图像对应于10件不同衣服中的每一件。我们可以看到哪个标签的置信度值最高。</p><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="0c05" class="lm kc hh mb b fi mf mg l mh mi">np.argmax(predictions[0])<br/>#Model is most confident that it's an ankle boot. Let's see if it's correct</span></pre><p id="6681" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:9个</strong></p><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="3119" class="lm kc hh mb b fi mf mg l mh mi">test_labels[0]</span></pre><p id="57a3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:9 </strong></p><h2 id="b306" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">现在，是时候看看全套10个频道了</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="109c" class="lm kc hh mb b fi mf mg l mh mi">def plot_image(i, predictions_array, true_label, img):<br/>  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]<br/>  plt.grid(False)<br/>  plt.xticks([])<br/>  plt.yticks([])</span><span id="b702" class="lm kc hh mb b fi mj mg l mh mi">  plt.imshow(img, cmap=plt.cm.binary)</span><span id="a8fb" class="lm kc hh mb b fi mj mg l mh mi">  predicted_label = np.argmax(predictions_array)<br/>  if predicted_label == true_label:<br/>    color = 'green'<br/>  else:<br/>    color = 'red'</span><span id="6e41" class="lm kc hh mb b fi mj mg l mh mi">  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],<br/>                                100*np.max(predictions_array),<br/>                                class_names[true_label]),<br/>                                color=color)</span><span id="0e34" class="lm kc hh mb b fi mj mg l mh mi">def plot_value_array(i, predictions_array, true_label):<br/>  predictions_array, true_label = predictions_array[i], true_label[i]<br/>  plt.grid(False)<br/>  plt.xticks([])<br/>  plt.yticks([])<br/>  thisplot = plt.bar(range(10), predictions_array, color="#777777")<br/>  plt.ylim([0, 1])<br/>  predicted_label = np.argmax(predictions_array)</span><span id="b7a4" class="lm kc hh mb b fi mj mg l mh mi">  thisplot[predicted_label].set_color('red')<br/>  thisplot[true_label].set_color('green')</span></pre><h2 id="d013" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">先来看第0张和第10张图片</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="8078" class="lm kc hh mb b fi mf mg l mh mi">i = 0<br/>plt.figure(figsize=(6,3))<br/>plt.subplot(1,2,1)<br/>plot_image(i, predictions, test_labels, test_images)<br/>plt.subplot(1,2,2)<br/>plot_value_array(i, predictions,  test_labels)<br/>plt.show()</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/47657f2d884ef1363bf3543cf2bda917.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*C9Ma-XX_mvn5X6yh0_kykA.png"/></div></figure><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="0a4f" class="lm kc hh mb b fi mf mg l mh mi">i = 10<br/>plt.figure(figsize=(6,3))<br/>plt.subplot(1,2,1)<br/>plot_image(i, predictions, test_labels, test_images)<br/>plt.subplot(1,2,2)<br/>plot_value_array(i, predictions,  test_labels)<br/>plt.show()</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mu"><img src="../Images/fe2cdf34822ea768facf73cd21e9540c.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*egKHRsHnJGg8MI1aTc_Ciw.png"/></div></figure><h2 id="c51f" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">现在，让我们绘制几幅图像及其预测。正确的是绿色的，不正确的是红色的。</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="b24e" class="lm kc hh mb b fi mf mg l mh mi">num_rows = 5<br/>num_cols = 3<br/>num_images = num_rows*num_cols<br/>plt.figure(figsize=(2*2*num_cols, 2*num_rows))<br/>for i in range(num_images):<br/>  plt.subplot(num_rows, 2*num_cols, 2*i+1)<br/>  plot_image(i, predictions, test_labels, test_images)<br/>  plt.subplot(num_rows, 2*num_cols, 2*i+2)<br/>  plot_value_array(i, predictions, test_labels)<br/>plt.show()</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mv"><img src="../Images/0ff8835eb6adbee52c14194c0ccfff33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*T1xr33v4VpINdX5XP7ipSg.png"/></div></figure><h2 id="ac87" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">最后，我们将使用训练好的模型对单个图像进行预测。</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="e54f" class="lm kc hh mb b fi mf mg l mh mi"># Grab an image from the test dataset<br/>img = test_images[0]</span><span id="11da" class="lm kc hh mb b fi mj mg l mh mi">print(img.shape)</span><span id="ac31" class="lm kc hh mb b fi mj mg l mh mi"># Add the image to a batch where it's the only member.<br/>img = (np.expand_dims(img,0))</span><span id="dd12" class="lm kc hh mb b fi mj mg l mh mi">print(img.shape)</span><span id="27c8" class="lm kc hh mb b fi mj mg l mh mi">predictions_single = model.predict(img) <br/>print(predictions_single)</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mw"><img src="../Images/8e5e16a8897e3629b38dc040c8849b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*eIezZblk0o68azZDv3foSQ.png"/></div></figure><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="d0b1" class="lm kc hh mb b fi mf mg l mh mi">plot_value_array(0, predictions_single, test_labels)<br/>plt.xticks(range(10), class_names, rotation=45)<br/>plt.show()</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/ebb668ee8b23e667e8e084bd3f397e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*_dX_Vsrt2kr5OMMXPSaXAw.png"/></div></figure><h2 id="e3af" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">正如你所看到的，我们唯一的批量图像的预测。</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="d03f" class="lm kc hh mb b fi mf mg l mh mi">prediction_result = np.argmax(predictions_single[0])</span></pre><p id="a531" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:9 </strong></p><h1 id="a216" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">美国有线电视新闻网</h1><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es my"><img src="../Images/a6a3caadd30d5fc4f9c82d3c56b21ebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*7K5rlNlssYQeM-7EMI2dkw.png"/></div></figure><p id="05aa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">CIFAR-10数据集由飞机、狗、猫和其他对象组成。您将对图像进行预处理，然后对所有样本训练一个卷积神经网络。图像需要被标准化，标签需要被一次性编码。这个用例一定会消除你对TensorFlow图像分类的疑虑。</p><h2 id="62f8" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">下载数据</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="d2a8" class="lm kc hh mb b fi mf mg l mh mi">from urllib.request import urlretrieve<br/>from os.path import isfile, isdir<br/>from tqdm import tqdm <br/>import tarfile<br/><br/>cifar10_dataset_folder_path = 'cifar-10-batches-py'<br/><br/>class DownloadProgress(tqdm):<br/>    last_block = 0<br/><br/>    def hook(self, block_num=1, block_size=1, total_size=None):<br/>        self.total = total_size<br/>        self.update((block_num - self.last_block) * block_size)<br/>        self.last_block = block_num<br/><br/>""" <br/>    check if the data (zip) file is already downloaded<br/>    if not, download it from "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz" and save as cifar-10-python.tar.gz<br/>"""<br/>if not isfile('cifar-10-python.tar.gz'):<br/>    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:<br/>        urlretrieve(<br/>            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',<br/>            'cifar-10-python.tar.gz',<br/>            pbar.hook)<br/><br/>if not isdir(cifar10_dataset_folder_path):<br/>    with tarfile.open('cifar-10-python.tar.gz') as tar:<br/>        tar.extractall()<br/>        tar.close()</span></pre><h2 id="0fa6" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">导入必要的库</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="0613" class="lm kc hh mb b fi mf mg l mh mi">import pickle<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><h2 id="df0e" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">了解数据</strong></h2><p id="55dd" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">原始的一批数据是用numpy数组表示的10000×3072个张量，其中10000是样本数据的个数。图像是彩色的，大小为32×32。可以以(宽度x高度x数量_通道)或(数量_通道x宽度x高度)的格式进行馈送。让我们定义标签。</p><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="b509" class="lm kc hh mb b fi mf mg l mh mi">def load_label_names():<br/>    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']</span></pre><h2 id="f3d3" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">重塑数据</strong></h2><p id="6e75" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们将分两个阶段重塑数据</p><p id="144d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，将行向量(3072)分成3块。每一块对应一个通道。这导致了张量的(3×1024)维。然后将上一步得到的张量除以32。这里的32表示图像的宽度。这产生了(3x32x32)。</p><p id="bb9a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其次，我们必须将数据从(数量通道，宽度，高度)转置到(宽度，高度，数量通道)。为此，我们将使用转置函数。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/ddf1a3fb3e68ad3c016ad30a07c3012e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*cpONIZmO-tOkyfW4QmwCYA.png"/></div></figure><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="b63f" class="lm kc hh mb b fi mf mg l mh mi">def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):<br/>    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:<br/>        # note the encoding type is 'latin1'<br/>        batch = pickle.load(file, encoding='latin1')<br/>        <br/>    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)<br/>    labels = batch['labels']<br/>        <br/>    return features, label</span></pre><h2 id="87bb" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">探索数据</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="ed0e" class="lm kc hh mb b fi mf mg l mh mi">def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):<br/>    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)<br/>    <br/>    if not (0 &lt;= sample_id &lt; len(features)):<br/>        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))<br/>        return None<br/><br/>    print('\nStats of batch #{}:'.format(batch_id))<br/>    print('# of Samples: {}\n'.format(len(features)))<br/>    <br/>    label_names = load_label_names()<br/>    label_counts = dict(zip(*np.unique(labels, return_counts=True)))<br/>    for key, value in label_counts.items():<br/>        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))<br/>    <br/>    sample_image = features[sample_id]<br/>    sample_label = labels[sample_id]<br/>    <br/>    print('\nExample of Image {}:'.format(sample_id))<br/>    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))<br/>    print('Image - Shape: {}'.format(sample_image.shape))<br/>    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))<br/>    <br/>    plt.imshow(sample_image)</span></pre><h2 id="a528" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated">绘制统计数据</h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="328d" class="lm kc hh mb b fi mf mg l mh mi">%matplotlib inline<br/>%config InlineBackend.figure_format = 'retina'<br/><br/>import numpy as np<br/><br/># Explore the dataset<br/>batch_id = 3<br/>sample_id = 7000<br/>display_stats(cifar10_dataset_folder_path, batch_id, sample_id)</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/54275ccac4e7da34cc7dc75698cc1fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*HUsGwo97_Pekqp5GIud9vg.png"/></div></figure><h2 id="1c4b" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">实现预处理功能</strong></h2><p id="159b" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们将通过最小-最大归一化来归一化数据。这只是使所有x值的范围在0和1之间。y = (x-min) / (max-min)</p><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="b326" class="lm kc hh mb b fi mf mg l mh mi">def normalize(x):<br/>    """<br/>        argument<br/>            - x: input image data in numpy array [32, 32, 3]<br/>        return<br/>            - normalized x <br/>    """<br/>    min_val = np.min(x)<br/>    max_val = np.max(x)<br/>    x = (x-min_val) / (max_val-min_val)<br/>    return x</span></pre><h2 id="9a42" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">一键编码</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="b8cb" class="lm kc hh mb b fi mf mg l mh mi">def one_hot_encode(x):<br/>    """<br/>        argument<br/>            - x: a list of labels<br/>        return<br/>            - one hot encoding matrix (number of labels, number of class)<br/>    """<br/>    encoded = np.zeros((len(x), 10))<br/>    <br/>    for idx, val in enumerate(x):<br/>        encoded[idx][val] = 1<br/>    <br/>    return encoded</span></pre><h2 id="50ce" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">预处理并保存数据</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="333a" class="lm kc hh mb b fi mf mg l mh mi">def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):<br/>    features = normalize(features)<br/>    labels = one_hot_encode(labels)</span><span id="d5b9" class="lm kc hh mb b fi mj mg l mh mi">    pickle.dump((features, labels), open(filename, 'wb'))<br/></span><span id="8dc7" class="lm kc hh mb b fi mj mg l mh mi">def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):<br/>    n_batches = 5<br/>    valid_features = []<br/>    valid_labels = []</span><span id="af91" class="lm kc hh mb b fi mj mg l mh mi">    for batch_i in range(1, n_batches + 1):<br/>        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)<br/>        <br/>        # find index to be the point as validation data in the whole dataset of the batch (10%)<br/>        index_of_validation = int(len(features) * 0.1)</span><span id="a5d9" class="lm kc hh mb b fi mj mg l mh mi">        # preprocess the 90% of the whole dataset of the batch<br/>        # - normalize the features<br/>        # - one_hot_encode the lables<br/>        # - save in a new file named, "preprocess_batch_" + batch_number<br/>        # - each file for each batch<br/>        _preprocess_and_save(normalize, one_hot_encode,<br/>                             features[:-index_of_validation], labels[:-index_of_validation], <br/>                             'preprocess_batch_' + str(batch_i) + '.p')</span><span id="6958" class="lm kc hh mb b fi mj mg l mh mi">        # unlike the training dataset, validation dataset will be added through all batch dataset<br/>        # - take 10% of the whold dataset of the batch<br/>        # - add them into a list of<br/>        #   - valid_features<br/>        #   - valid_labels<br/>        valid_features.extend(features[-index_of_validation:])<br/>        valid_labels.extend(labels[-index_of_validation:])</span><span id="2a01" class="lm kc hh mb b fi mj mg l mh mi">    # preprocess the all stacked validation dataset<br/>    _preprocess_and_save(normalize, one_hot_encode,<br/>                         np.array(valid_features), np.array(valid_labels),<br/>                         'preprocess_validation.p')</span><span id="7b95" class="lm kc hh mb b fi mj mg l mh mi">    # load the test dataset<br/>    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:<br/>        batch = pickle.load(file, encoding='latin1')</span><span id="5930" class="lm kc hh mb b fi mj mg l mh mi">    # preprocess the testing data<br/>    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)<br/>    test_labels = batch['labels']</span><span id="b00d" class="lm kc hh mb b fi mj mg l mh mi">    # Preprocess and Save all testing data<br/>    _preprocess_and_save(normalize, one_hot_encode,<br/>                         np.array(test_features), np.array(test_labels),<br/>                         'preprocess_training.p')</span></pre><h2 id="152e" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">执行:</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="e3c0" class="lm kc hh mb b fi mf mg l mh mi">preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)</span></pre><h2 id="c48a" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">检查点</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="b200" class="lm kc hh mb b fi mf mg l mh mi">import pickle</span><span id="35c6" class="lm kc hh mb b fi mj mg l mh mi">valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))</span></pre><h2 id="874e" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">构建网络</strong></h2><p id="5b40" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">整个模型总共由14层组成。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es na"><img src="../Images/5509c7a8b312d5aeedd116c96cc8b16d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k9vqHlbdpn1z08fZh5miDw.png"/></div></div></figure><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="2db8" class="lm kc hh mb b fi mf mg l mh mi">import tensorflow as tf<br/><br/>def conv_net(x, keep_prob):<br/>    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))<br/>    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))<br/>    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))<br/>    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))<br/><br/>    # 1, 2<br/>    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')<br/>    conv1 = tf.nn.relu(conv1)<br/>    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')<br/>    conv1_bn = tf.layers.batch_normalization(conv1_pool)<br/><br/>    # 3, 4<br/>    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')<br/>    conv2 = tf.nn.relu(conv2)<br/>    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    <br/>    conv2_bn = tf.layers.batch_normalization(conv2_pool)<br/>  <br/>    # 5, 6<br/>    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')<br/>    conv3 = tf.nn.relu(conv3)<br/>    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  <br/>    conv3_bn = tf.layers.batch_normalization(conv3_pool)<br/>    <br/>    # 7, 8<br/>    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')<br/>    conv4 = tf.nn.relu(conv4)<br/>    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')<br/>    conv4_bn = tf.layers.batch_normalization(conv4_pool)<br/>    <br/>    # 9<br/>    flat = tf.contrib.layers.flatten(conv4_bn)  <br/><br/>    # 10<br/>    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)<br/>    full1 = tf.nn.dropout(full1, keep_prob)<br/>    full1 = tf.layers.batch_normalization(full1)<br/>    <br/>    # 11<br/>    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)<br/>    full2 = tf.nn.dropout(full2, keep_prob)<br/>    full2 = tf.layers.batch_normalization(full2)<br/>    <br/>    # 12<br/>    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)<br/>    full3 = tf.nn.dropout(full3, keep_prob)<br/>    full3 = tf.layers.batch_normalization(full3)    <br/>    <br/>    # 13<br/>    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)<br/>    full4 = tf.nn.dropout(full4, keep_prob)<br/>    full4 = tf.layers.batch_normalization(full4)        <br/>    <br/>    # 14<br/>    out = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)<br/>    return out</span></pre><h2 id="6be0" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">超参数</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="6cdb" class="lm kc hh mb b fi mf mg l mh mi">epochs = 10<br/>batch_size = 128<br/>keep_probability = 0.7<br/>learning_rate = 0.001</span><span id="d64d" class="lm kc hh mb b fi mj mg l mh mi">logits = conv_net(x, keep_prob)<br/>model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training</span><span id="a260" class="lm kc hh mb b fi mj mg l mh mi"># Loss and Optimizer<br/>cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))<br/>optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><span id="f669" class="lm kc hh mb b fi mj mg l mh mi"># Accuracy<br/>correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')</span></pre><h2 id="036f" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">训练神经网络</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="b1a7" class="lm kc hh mb b fi mf mg l mh mi">#Single Optimization<br/>def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):<br/>    session.run(optimizer, <br/>                feed_dict={<br/>                    x: feature_batch,<br/>                    y: label_batch,<br/>                    keep_prob: keep_probability<br/>                })</span><span id="3418" class="lm kc hh mb b fi mj mg l mh mi">#Showing Stats<br/>def print_stats(session, feature_batch, label_batch, cost, accuracy):<br/>    loss = sess.run(cost, <br/>                    feed_dict={<br/>                        x: feature_batch,<br/>                        y: label_batch,<br/>                        keep_prob: 1.<br/>                    })<br/>    valid_acc = sess.run(accuracy, <br/>                         feed_dict={<br/>                             x: valid_features,<br/>                             y: valid_labels,<br/>                             keep_prob: 1.<br/>                         })<br/>    <br/>    print('Loss: {:&gt;10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))</span></pre><h2 id="f8f6" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">充分训练并保存模型</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="1d6d" class="lm kc hh mb b fi mf mg l mh mi">def batch_features_labels(features, labels, batch_size):<br/>    """<br/>    Split features and labels into batches<br/>    """<br/>    for start in range(0, len(features), batch_size):<br/>        end = min(start + batch_size, len(features))<br/>        yield features[start:end], labels[start:end]</span><span id="c5c5" class="lm kc hh mb b fi mj mg l mh mi">def load_preprocess_training_batch(batch_id, batch_size):<br/>    """<br/>    Load the Preprocessed Training data and return them in batches of &lt;batch_size&gt; or less<br/>    """<br/>    filename = 'preprocess_batch_' + str(batch_id) + '.p'<br/>    features, labels = pickle.load(open(filename, mode='rb'))</span><span id="2313" class="lm kc hh mb b fi mj mg l mh mi">    # Return the training data in batches of size &lt;batch_size&gt; or less<br/>    return batch_features_labels(features, labels, batch_size)</span></pre><h2 id="43be" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">保存模型和路径</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="697b" class="lm kc hh mb b fi mf mg l mh mi">#Saving Model and Path<br/>save_model_path = './image_classification'<br/><br/>print('Training...')<br/>with tf.Session() as sess:<br/>    # Initializing the variables<br/>    sess.run(tf.global_variables_initializer())<br/>    <br/>    # Training cycle<br/>    for epoch in range(epochs):<br/>        # Loop over all batches<br/>        n_batches = 5<br/>        for batch_i in range(1, n_batches + 1):<br/>            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):<br/>                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)<br/>                <br/>            print('Epoch {:&gt;2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')<br/>            print_stats(sess, batch_features, batch_labels, cost, accuracy)<br/>            <br/>    # Save Model<br/>    saver = tf.train.Saver()<br/>    save_path = saver.save(sess, save_model_path)</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mv"><img src="../Images/d9eefe3ea8f3256c394ea34d6f4b1301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*3P6STirrNXyI25t1w76apg.png"/></div></figure><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/3cf865e319fdefe8221fe669aab557bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*7_n86D87F4NoxOFStruhrw.png"/></div></figure><p id="c13e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，张量流图像分类的重要部分已经完成。现在，是测试模型的时候了。</p><h2 id="e046" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">测试模型</strong></h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="3017" class="lm kc hh mb b fi mf mg l mh mi">import pickle<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import LabelBinarizer</span><span id="dc27" class="lm kc hh mb b fi mj mg l mh mi">def batch_features_labels(features, labels, batch_size):<br/>    """<br/>    Split features and labels into batches<br/>    """<br/>    for start in range(0, len(features), batch_size):<br/>        end = min(start + batch_size, len(features))<br/>        yield features[start:end], labels[start:end]</span><span id="5a45" class="lm kc hh mb b fi mj mg l mh mi">def display_image_predictions(features, labels, predictions, top_n_predictions):<br/>    n_classes = 10<br/>    label_names = load_label_names()<br/>    label_binarizer = LabelBinarizer()<br/>    label_binarizer.fit(range(n_classes))<br/>    label_ids = label_binarizer.inverse_transform(np.array(labels))</span><span id="4452" class="lm kc hh mb b fi mj mg l mh mi">    fig, axies = plt.subplots(nrows=top_n_predictions, ncols=2, figsize=(20, 10))<br/>    fig.tight_layout()<br/>    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)</span><span id="297c" class="lm kc hh mb b fi mj mg l mh mi">    n_predictions = 3<br/>    margin = 0.05<br/>    ind = np.arange(n_predictions)<br/>    width = (1. - 2. * margin) / n_predictions<br/>   <br/>    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):<br/>        if (image_i &lt; top_n_predictions):<br/>            pred_names = [label_names[pred_i] for pred_i in pred_indicies]<br/>            correct_name = label_names[label_id]<br/>            <br/>            axies[image_i][0].imshow((feature*255).astype(np.int32, copy=False))<br/>            axies[image_i][0].set_title(correct_name)<br/>            axies[image_i][0].set_axis_off()</span><span id="a8ae" class="lm kc hh mb b fi mj mg l mh mi">            axies[image_i][1].barh(ind + margin, pred_values[:3], width)<br/>            axies[image_i][1].set_yticks(ind + margin)<br/>            axies[image_i][1].set_yticklabels(pred_names[::-1])<br/>            axies[image_i][1].set_xticks([0, 0.5, 1.0])</span></pre><h2 id="26b3" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated">测试准确度</h2><pre class="lg lh li lj fd ma mb mc md aw me bi"><span id="feaa" class="lm kc hh mb b fi mf mg l mh mi">%matplotlib inline<br/>%config InlineBackend.figure_format = 'retina'<br/><br/>import tensorflow as tf<br/>import pickle<br/>import random<br/><br/>save_model_path = './image_classification'<br/>batch_size = 64<br/>n_samples = 10<br/>top_n_predictions = 5<br/><br/>def test_model():<br/>    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))<br/>    loaded_graph = tf.Graph()<br/><br/>    with tf.Session(graph=loaded_graph) as sess:<br/>        # Load model<br/>        loader = tf.train.import_meta_graph(save_model_path + '.meta')<br/>        loader.restore(sess, save_model_path)<br/><br/>        # Get Tensors from loaded model<br/>        loaded_x = loaded_graph.get_tensor_by_name('input_x:0')<br/>        loaded_y = loaded_graph.get_tensor_by_name('output_y:0')<br/>        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')<br/>        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')<br/>        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')<br/>        <br/>        # Get accuracy in batches for memory limitations<br/>        test_batch_acc_total = 0<br/>        test_batch_count = 0<br/>        <br/>        for train_feature_batch, train_label_batch in batch_features_labels(test_features, test_labels, batch_size):<br/>            test_batch_acc_total += sess.run(<br/>                loaded_acc,<br/>                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})<br/>            test_batch_count += 1<br/><br/>        print('Testing Accuracy: {}\n'.format(test_batch_acc_total/test_batch_count))<br/><br/>        # Print Random Samples<br/>        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))<br/>        random_test_predictions = sess.run(<br/>            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),<br/>            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})<br/>        display_image_predictions(random_test_features, random_test_labels, random_test_predictions, top_n_predictions)<br/><br/><br/>test_model()</span></pre><h2 id="097a" class="lm kc hh bd kd ln lo lp kh lq lr ls kl ja lt lu kp je lv lw kt ji lx ly kx lz bi translated"><strong class="ak">输出:</strong> <strong class="ak">测试精度:0.5882762738853503 </strong></h2><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es nb"><img src="../Images/a207dd2452a04c9c04ba7f202741e7fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h1ZSea5UFkbQqivfRtbRkA.png"/></div></div></figure><p id="8676" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，如果你为更多的时期训练你的神经网络或者改变激活函数，你可能会得到一个不同的结果，它可能有更好的准确性。</p><p id="45b8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="nc">就这样，我们结束了这篇TensorFlow图像分类的文章。我相信你现在可以用同样的方法对任何类型的图像进行分类，而且你也不是图像分类的初学者。</em></p><p id="58b3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你希望查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，那么你可以参考<a class="ae le" href="https://www.edureka.co/blog/?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=tensorflow-image-classification" rel="noopener ugc nofollow" target="_blank"> Edureka的官方网站。</a></p><p id="27e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释深度学习的各个其他方面。</p><blockquote class="nd ne nf"><p id="31e9" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">1.<a class="ae le" rel="noopener" href="/edureka/tensorflow-tutorial-ba142ae96bca">张量流教程</a></p><p id="b494" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">2.<a class="ae le" rel="noopener" href="/edureka/pytorch-tutorial-9971d66f6893"> PyTorch教程</a></p><p id="4bd0" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">3.<a class="ae le" rel="noopener" href="/edureka/perceptron-learning-algorithm-d30e8b99b156">感知器学习算法</a></p><p id="3544" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">4.<a class="ae le" rel="noopener" href="/edureka/neural-network-tutorial-2a46b22394c9">神经网络教程</a></p><p id="da2a" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">5.<a class="ae le" rel="noopener" href="/edureka/backpropagation-bd2cf8fdde81">什么是反向传播？</a></p><p id="cdfc" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">6.<a class="ae le" rel="noopener" href="/edureka/convolutional-neural-network-3f2c5b9c4778">卷积神经网络</a></p><p id="ee39" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">7.<a class="ae le" rel="noopener" href="/edureka/capsule-networks-d7acd437c9e">胶囊神经网络</a></p><p id="f238" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">8.<a class="ae le" rel="noopener" href="/edureka/recurrent-neural-networks-df945afd7441">递归神经网络</a></p><p id="bc18" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">9.<a class="ae le" rel="noopener" href="/edureka/autoencoders-tutorial-cfdcebdefe37">自动编码器教程</a></p><p id="ed3f" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">10.<a class="ae le" rel="noopener" href="/edureka/restricted-boltzmann-machine-tutorial-991ae688c154">受限玻尔兹曼机教程</a></p><p id="f8ef" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">11.<a class="ae le" rel="noopener" href="/edureka/pytorch-vs-tensorflow-252fc6675dd7"> PyTorch vs TensorFlow </a></p><p id="0aa7" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">12.<a class="ae le" rel="noopener" href="/edureka/deep-learning-with-python-2adbf6e9437d">用Python进行深度学习</a></p><p id="12b5" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">13.<a class="ae le" rel="noopener" href="/edureka/artificial-intelligence-tutorial-4257c66f5bb1">人工智能教程</a></p><p id="092d" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">14.<a class="ae le" rel="noopener" href="/edureka/tensorflow-object-detection-tutorial-8d6942e73adc">tensor flow中的对象检测</a></p><p id="fc63" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">15.<a class="ae le" rel="noopener" href="/edureka/artificial-intelligence-applications-7b93b91150e3">人工智能应用</a></p><p id="6e09" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">16.<a class="ae le" rel="noopener" href="/edureka/become-artificial-intelligence-engineer-5ac2ede99907">如何成为一名人工智能工程师？</a></p><p id="522a" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">17.<a class="ae le" rel="noopener" href="/edureka/q-learning-592524c3ecfc"> Q学习</a></p><p id="77e1" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">18.<a class="ae le" rel="noopener" href="/edureka/apriori-algorithm-d7cc648d4f1e"> Apriori算法</a></p><p id="0a21" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">19.<a class="ae le" rel="noopener" href="/edureka/introduction-to-markov-chains-c6cb4bcd5723">用Python实现马尔可夫链</a></p><p id="5a06" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">20.<a class="ae le" rel="noopener" href="/edureka/artificial-intelligence-algorithms-fad283a0d8e2">人工智能算法</a></p><p id="9414" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">21.<a class="ae le" rel="noopener" href="/edureka/best-laptop-for-machine-learning-a4a5f8ba5b">机器学习的最佳笔记本电脑</a></p><p id="a5e1" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">22.<a class="ae le" rel="noopener" href="/edureka/top-artificial-intelligence-tools-36418e47bf2a">12大人工智能工具</a></p><p id="2d7c" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">23.<a class="ae le" rel="noopener" href="/edureka/artificial-intelligence-interview-questions-872d85387b19">人工智能(AI)面试问题</a></p><p id="708b" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">24.<a class="ae le" rel="noopener" href="/edureka/theano-vs-tensorflow-15f30216b3bc"> Theano vs TensorFlow </a></p><p id="1dea" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">25.<a class="ae le" rel="noopener" href="/edureka/what-is-a-neural-network-56ae7338b92d">什么是神经网络？</a></p><p id="971b" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">26.<a class="ae le" rel="noopener" href="/edureka/pattern-recognition-5e2d30ab68b9">模式识别</a></p><p id="3f3f" class="ip iq nc ir b is it iu iv iw ix iy iz ng jb jc jd nh jf jg jh ni jj jk jl jm ha bi translated">27.<a class="ae le" rel="noopener" href="/edureka/alpha-beta-pruning-in-ai-b47ee5500f9a">人工智能中的阿尔法贝塔剪枝</a></p></blockquote></div><div class="ab cl nj nk go nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ha hb hc hd he"><p id="a110" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="nc">原载于2019年05月08日</em><a class="ae le" href="https://www.edureka.co/blog/tensorflow-image-classification" rel="noopener ugc nofollow" target="_blank"><em class="nc">https://www.edureka.co</em></a><em class="nc">。</em></p></div></div>    
</body>
</html>