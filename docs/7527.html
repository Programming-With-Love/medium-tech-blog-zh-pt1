<html>
<head>
<title>PySpark Basics — Create a Test Dataframe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark基础—创建测试数据框架</h1>
<blockquote>原文：<a href="https://medium.com/version-1/pyspark-basics-create-a-test-dataframe-7b39d3ba9c51?source=collection_archive---------0-----------------------#2022-03-24">https://medium.com/version-1/pyspark-basics-create-a-test-dataframe-7b39d3ba9c51?source=collection_archive---------0-----------------------#2022-03-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/9ba424435e998951f9aee9dd0cadfe9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/0*NXUFbvwGlQ3xDhcD.png"/></div></figure><p id="c0cf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">当我学习一项新技术时，我喜欢在将它们用于生产之前，在简单的测试数据上尝试新的特性和功能。</p><p id="93b8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">所以…</p><h1 id="d74e" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">我们如何在Spark中创建一个简单的测试数据框架？</h1><p id="e4b1" class="pw-post-body-paragraph il im hh in b io kh iq ir is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji ha bi translated">这里有一些快速简单的方法来创建少量的测试数据来测试一些PySpark函数。</p><p id="7abd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">注意</strong> —我在这些例子中使用了数据块，我还没有在其他平台上测试过。</p><h2 id="5931" class="km jk hh bd jl kn ko kp jp kq kr ks jt iw kt ku jx ja kv kw kb je kx ky kf kz bi translated"><strong class="ak">方法1 </strong> —来自Python列表的数据帧</h2><p id="e946" class="pw-post-body-paragraph il im hh in b io kh iq ir is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji ha bi translated"><em class="la"> spark.createDataFrame </em>允许我们从Python列表中创建数据帧。</p><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="9039" class="km jk hh lg b fi lk ll l lm ln">people = [<br/>  (10, "blue"),<br/>  (13, "red"),<br/>  (15, "blue"),<br/>  (99, "red"),<br/>  (67, "blue")<br/>]</span><span id="f2f0" class="km jk hh lg b fi lo ll l lm ln">peopleDf = spark.createDataFrame(people,["age","fave_colour"])<br/>peopleDf.show()</span><span id="8588" class="km jk hh lg b fi lo ll l lm ln">+---+------+<br/>|age|colour|<br/>+---+------+<br/>| 10|  blue|<br/>| 13|   red|<br/>| 15|  blue|<br/>| 99|   red|<br/>| 67|  blue|<br/>+---+------+</span></pre><p id="6395" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">注意——我们在createDataFrame调用中指定了列的名称，我们也可以执行其他设置工作，比如模式——参见Spark <a class="ae lp" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p></div><div class="ab cl lq lr go ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ha hb hc hd he"><h2 id="a267" class="km jk hh bd jl kn ko kp jp kq kr ks jt iw kt ku jx ja kv kw kb je kx ky kf kz bi translated"><strong class="ak">方法2 </strong> —使用范围</h2><p id="c6c5" class="pw-post-body-paragraph il im hh in b io kh iq ir is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji ha bi translated">这是创建大量数据的快捷方式，但所有行都是一样的，除非你有创意，我们在这里尽量保持简单…</p><p id="6fad" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> <em class="la"> spark.range </em> </strong>调用这里的键，并根据指定范围的大小创建dataframe，然后我们可以添加更多的列，使事情变得更令人兴奋！</p><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="63bf" class="km jk hh lg b fi lk ll l lm ln">from pyspark.sql import functions as F</span><span id="9e7b" class="km jk hh lg b fi lo ll l lm ln"># Create test frame<br/>dateDF = spark.range(3)\<br/>  .withColumn("today", F.current_date())\<br/>  .withColumn("timestamp",F.current_timestamp())\<br/>  .withColumn("num",7+F.col("id"))\</span><span id="f46c" class="km jk hh lg b fi lo ll l lm ln">dateDF.show(100,False)</span><span id="af4a" class="km jk hh lg b fi lo ll l lm ln">+---+----------+-----------------------+---+<br/>|id |today     |timestamp              |num|<br/>+---+----------+-----------------------+---+<br/>|0  |2022-02-12|2022-02-12 17:14:22.096|7  |<br/>|1  |2022-02-12|2022-02-12 17:14:22.096|8  |<br/>|2  |2022-02-12|2022-02-12 17:14:22.096|9  |<br/>+---+----------+-----------------------+---+</span></pre><p id="01e8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这里有几点—我们使用<strong class="in hi"> <em class="la"> pyspark.sql </em> </strong>库来使用日期函数—<strong class="in hi"><em class="la">current _ date()</em></strong>和<strong class="in hi"><em class="la">current _ timestamp()</em></strong></p><p id="736d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> <em class="la"> withColumn </em> </strong>用于在我们的基本数据框架中添加一个新的命名列，并为该列中的数据设置一个值。</p><p id="f554" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">此外，请注意<strong class="in hi"> <em class="la"> show() </em> </strong>方法的参数，在这种情况下，我们指定返回的最大行数为100，尽管dataframe只有3行，因此在这里没有影响。<strong class="in hi"> <em class="la"> False </em> </strong>参数停止show方法截断\切断输出。</p><p id="be31" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">同样，使用spark.range()创建数据帧时，更多选项参见<a class="ae lp" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.range.html" rel="noopener ugc nofollow" target="_blank">文档</a>，例如，控制id值。</p></div><div class="ab cl lq lr go ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ha hb hc hd he"><p id="2347" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">就是这样！我希望这是有用的，我将在以后的博客中更详细地讨论在数据帧中创建随机数据和探索PySpark日期函数。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es lx"><img src="../Images/0d5e1fa621cf70c90f08f4091e81000e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGElibzwzjvaO_5I60xg9g.jpeg"/></div></div></figure><p id="4ff3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">关于作者:</strong></p><p id="d2a6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">Mike Knee是第1版的Azure数据开发人员。</p></div></div>    
</body>
</html>