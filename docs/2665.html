<html>
<head>
<title>How Did NASA Use Hive To Solve Their Issues Of Storage &amp; Processing Data?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NASA如何使用Hive来解决他们的数据存储和处理问题？</h1>
<blockquote>原文：<a href="https://medium.com/edureka/hive-tutorial-b980dfaae765?source=collection_archive---------2-----------------------#2016-12-05">https://medium.com/edureka/hive-tutorial-b980dfaae765?source=collection_archive---------2-----------------------#2016-12-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/4721733a8391be06b4cebbfb248e95c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*yaYguWumAGk4ati1gTEcjQ.png"/></div><figcaption class="il im et er es in io bd b be z dx">Hive Tutorial - Edureka</figcaption></figure><h1 id="6d61" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">介绍</h1><p id="e330" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在这篇Hive教程博客中，我们将深入讨论Apache Hive。Apache Hive是<strong class="jp hi"> <em class="kl"> Hadoop生态</em> </strong>中的数据仓库工具，提供类似SQL的语言，用于查询和分析<strong class="jp hi"> <em class="kl">大数据</em> </strong>。开发Hive的动机是为SQL开发人员提供无摩擦的学习途径。Hive不仅是非编程背景的人的救星，它还减少了花很长时间写<strong class="jp hi"> <em class="kl"> MapReduce </em> </strong>程序的程序员的工作。在这篇Apache Hive教程博客中，我将讨论:</p><blockquote class="km kn ko"><p id="0bd5" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">1.什么是Hive？</p><p id="0905" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">2.阿帕奇蜂房的故事——从脸书到阿帕奇</p><p id="1966" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">3.Apache Hive的优势</p><p id="b0f6" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">4.阿帕奇蜂房——NASA案例研究</p><p id="c665" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">5.阿帕奇蜂房架构</p><p id="de86" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">6.Metastore配置</p><p id="08ea" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">7.Hive数据模型</p></blockquote><h1 id="a8cf" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">什么是Hive？</h1><p id="6a77" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">Apache Hive是建立在<strong class="jp hi"> <em class="kl"> Hadoop </em> </strong>之上的数据仓库系统，用于分析结构化和半结构化数据。Hive抽象了Hadoop MapReduce的复杂性。基本上，它提供了一种机制来将结构投射到数据上，并执行用HQL (Hive Query Language)编写的类似于SQL语句的查询。在内部，这些查询或HQL被Hive编译器转换为map reduce作业。因此，您不需要担心编写复杂的MapReduce程序来使用Hadoop处理您的数据。它面向熟悉SQL的用户。Apache Hive支持数据定义语言(DDL)、数据操作语言(DML)和用户定义函数(UDF)</p><blockquote class="km kn ko"><p id="59f3" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated"><strong class="jp hi">SQL+Hadoop MapReduce = hive QL</strong></p></blockquote><h1 id="aed2" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">蜂巢的故事——从脸书到阿帕奇</h1><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es kx"><img src="../Images/0cb9981beb1a4967b8d8642ec4571627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eyl6FZgCIycS44OyXw6j4Q.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lg">Facebook use case - Hive Tutorial</em></figcaption></figure><h2 id="71aa" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">脸书面临的挑战:数据呈指数级增长</h2><p id="7033" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">在2008年之前，脸书的所有数据处理基础设施都是围绕基于商业RDBMS的数据仓库而构建的。这些基础设施足以满足当时脸书的需求。但是，随着数据开始快速增长，管理和处理这个庞大的数据集成为一个巨大的挑战。根据脸书的一篇文章，数据从2007年的15 TB数据集扩展到2009年的2 PB数据。此外，许多脸书产品涉及数据分析，如观众洞察、脸书词典、脸书广告等。因此，他们需要一个可扩展且经济的解决方案来应对这个问题，因此开始使用Hadoop框架。</p><h1 id="aef2" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">Hadoop民主化— MapReduce</h1><p id="b705" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">但是，随着数据的增长，Map-Reduce代码的复杂性也成比例地增长。因此，培训没有编程背景的人编写MapReduce程序变得很困难。此外，为了执行简单的分析，必须编写一百行MapReduce代码。因为，SQL被包括脸书在内的工程师和分析师广泛使用，因此，将SQL放在Hadoop之上似乎是让具有SQL背景的用户访问Hadoop的一种逻辑方式。</p><p id="c4f5" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">因此，SQL满足大多数分析需求的能力和Hadoop的可扩展性催生了<strong class="jp hi"> Apache Hive </strong>，它允许对<strong class="jp hi"><em class="kl"/></strong>中的数据执行类似SQL的查询。后来，Hive项目在2008年8月由脸书开源，并作为今天的Apache Hive免费提供。</p><p id="5edd" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">现在，让我们来看看使Hive如此受欢迎的特性或优点。</p><h1 id="f75d" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">蜂巢的优势</h1><ul class=""><li id="d6c5" class="lv lw hh jp b jq jr ju jv jy lx kc ly kg lz kk ma mb mc md bi translated">对于没有编程背景的人很有用，因为它消除了编写复杂MapReduce程序的需要。</li><li id="bbb7" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi">可扩展</strong>和<strong class="jp hi">可扩展</strong>应对不断增长的数据量和种类，而不影响系统的性能。</li><li id="f873" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">它是一个高效的ETL(提取、转换、加载)工具。</li><li id="c166" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">Hive通过公开其<strong class="jp hi">节俭服务器</strong>，支持任何用Java、PHP、Python、C++或Ruby编写的客户端应用。(您可以使用这些嵌入SQL的客户端语言来访问DB2等数据库。).</li><li id="2c2b" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">由于Hive的元数据信息存储在RDBMS中，它大大减少了在查询执行期间执行语义检查的时间。</li></ul><h1 id="469b" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">在哪里使用Apache Hive？</h1><p id="3768" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">Apache Hive同时利用了SQL数据库系统和<strong class="jp hi"><em class="kl">Hadoop—MapReduce</em></strong>框架。因此，它被大量的公司使用。它主要用于数据仓库，您可以在其中执行不需要实时处理的分析和数据挖掘。可以使用Apache Hive的一些字段如下:</p><ul class=""><li id="6b1b" class="lv lw hh jp b jq kp ju kq jy mj kc mk kg ml kk ma mb mc md bi translated">数据库</li><li id="08bd" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">特别分析</li></ul><p id="65b1" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">正如人们所说，你不能只用一只手鼓掌，也就是说，你不能用一种工具解决所有问题。因此，您可以将Hive与其他工具结合使用，以便在许多其他领域使用它。例如，Tableau与Apache Hive一起可以用于数据可视化，Apache Tez与Hive的集成将为您提供实时处理能力等。<br/>在这篇Apache Hive教程博客中，让我们来看看NASA的一个案例研究，从中您将了解Hive如何解决NASA科学家在评估气候模型时面临的问题。</p><h1 id="3770" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">NASA案例研究</h1><p id="9aa8" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">气候模型是基于影响地球气候的各种因素的气候系统的数学表示。基本上，它描述了各种气候驱动因素的相互作用，如海洋、太阳、大气等。提供对气候系统动态的洞察。它通过基于影响气候的因素模拟气候变化来预测气候条件。美国航天局喷气推进实验室开发了区域气候模型评估系统(RCMES ),用于对照各种外部储存库中的遥感数据分析和评估气候输出模型。</p><p id="912a" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">RCMES(区域气候模型评估系统)有两个组成部分:</p><h2 id="1fb5" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">区域气候模式评估数据库:</h2><p id="19e1" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">这是一个可扩展的云数据库，使用Apache OODT提取器、Apache Tika等提取器加载与气候相关的遥感数据和再分析数据。最后，将数据转换为(纬度、经度、时间、数值、高度)形式的数据点模型，并存储到<strong class="jp hi"> <em class="kl"> MySQL </em> </strong>数据库中。客户端可以通过执行空间/时间查询来检索RCMED中的数据。这种查询的描述现在与我们无关。</p><h2 id="f8c1" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">区域气候模型评估工具包:</h2><p id="73cc" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">它使用户能够将RCMED中的参考数据与从其他来源获取的气候模型输出数据进行比较，以进行不同类型的分析和评估。你可以参考下图来理解RCMES的架构。</p><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mm"><img src="../Images/532f7e665ed4fb70191e50d310511913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*OsLapYhenr-BRERNd63VpQ.gif"/></div></div><figcaption class="il im et er es in io bd b be z dx">The architecture of RCMES - Hive Tutorial</figcaption></figure><p id="e952" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">根据气候模型评估所需的不同参数，RCMED中的参考数据来自卫星遥感。例如——AIRS(大气红外探测器)提供地面气温、温度和位势等参数，TRMM(热带降雨测量任务)提供月降雨量等。</p><h2 id="046b" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">NASA使用MySQL数据库系统面临的问题:</h2><ul class=""><li id="0e56" class="lv lw hh jp b jq jr ju jv jy lx kc ly kg lz kk ma mb mc md bi translated">在用60亿元组的形式(纬度、经度、时间、数据点值、高度)加载MySQL数据库后，系统崩溃了，如上图所示。</li><li id="6a08" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">即使将整个表分成更小的子集，系统在处理数据时也会产生巨大的开销。</li></ul><p id="dbcc" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">因此，他们需要一个可扩展的解决方案，能够使用类似SQL的查询功能存储和处理如此大量的数据。最后，他们决定使用Apache Hive来克服上述问题。</p><h1 id="9e48" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">Apache Hive如何解决问题？</h1><p id="1c7f" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">现在，让我们看看，是哪些特性说服了NASA的JPL团队将Apache Hive作为其解决方案策略的一个组成部分:</p><ul class=""><li id="e2ff" class="lv lw hh jp b jq kp ju kq jy mj kc mk kg ml kk ma mb mc md bi translated">因为Apache Hive运行在Hadoop之上，所以它是可伸缩的，并且可以以分布式和并行的方式处理数据。</li><li id="8d19" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">它提供了类似于SQL的Hive查询语言，因此很容易学习。</li></ul><h1 id="f603" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">配置单元的部署:</h1><p id="704b" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">下图说明了与Apache Hive集成的RCMES架构师:</p><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mn"><img src="../Images/39df339d914ccf07c9ebb0563f55d8ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AboQrEHlV7scQ43sFbldXw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lg">RCMES Architecture with Apache Hive - Hive Tutorial</em></figcaption></figure><p id="1a43" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">上图显示了RCMES中apache hive的部署。NASA团队在部署Apache Hive时采取了以下步骤:</p><ul class=""><li id="24f5" class="lv lw hh jp b jq kp ju kq jy mj kc mk kg ml kk ma mb mc md bi translated">他们使用Cloudera和Apache Hadoop安装了Hive，如上图所示。</li><li id="f512" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">他们使用<em class="kl"> Apache Sqoop </em>从MySQL数据库中获取数据。</li><li id="4793" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">实现Apache OODT包装器是为了在Hive上执行查询，并将数据检索回RCMET。</li></ul><h1 id="e1a7" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">Hive的初始基准测试观察:</h1><ul class=""><li id="17fa" class="lv lw hh jp b jq jr ju jv jy lx kc ly kg lz kk ma mb mc md bi translated">最初，他们将25亿个数据点加载到一个表中，并执行计数查询。例如，<em class="kl">Hive&gt;</em>select count(data point _ id)from data point。统计所有记录需要5-6分钟(68亿条记录需要15-17分钟)。</li><li id="ab9d" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">reduce阶段很快，但是map阶段占用了总处理时间的95%。他们使用六个(<strong class="jp hi"> 4x四核</strong>)系统和<strong class="jp hi"> 24 GB内存</strong>(大约。)在每个系统中。</li><li id="f78c" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">即使在添加了更多机器、更改了HDFS块大小(64 MB、128 MB、256 MB)以及更改了许多其他配置变量(io.sort.factor、io.sort.mb)之后，他们在减少完成计数的时间方面也没有取得多大成功。</li></ul><h1 id="877e" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">蜂巢社区成员的意见:</h1><p id="7a8b" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">最后，Hive社区的成员前来救援，并提供各种见解来解决他们当前Hive实现的问题:</p><ul class=""><li id="ab14" class="lv lw hh jp b jq kp ju kq jy mj kc mk kg ml kk ma mb mc md bi translated">他们提到，根据NameNode上的网络容量和工作负载，HDFS读取速度大约为<strong class="jp hi"> 60 MB/s </strong>，而本地磁盘的读取速度大约为<strong class="jp hi"> 1GB/s </strong>。</li><li id="e30b" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">成员们建议在他们当前的系统中需要<strong class="jp hi"> 16个映射器</strong>来匹配本地非Hadoop任务的I/O性能。</li><li id="2193" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">他们还建议减少每个映射器的<strong class="jp hi">分割大小</strong>，以增加映射器的数量，从而提供更多的并行性。</li><li id="a1bb" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">最后，社区成员告诉他们<strong class="jp hi">使用count (1) </strong>而不是引用<strong class="jp hi"> count (datapoint_id) </strong>。这是因为在count (1)的情况下，没有引用列，因此在执行计数时不会发生解压缩和反序列化。</li></ul><p id="feed" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">最后，通过考虑蜂箱社区成员提出的所有建议，美国宇航局能够按照他们的期望调整他们的蜂箱集群。因此，他们能够使用上述系统配置在15秒内查询数十亿行。</p><h1 id="6faa" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">Hive架构及其组件</h1><p id="f65a" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">下图描述了配置单元的体系结构以及将查询提交到配置单元并最终使用MapReduce框架进行处理的流程:</p><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mn"><img src="../Images/ba6b4bd8b2fcc507a54d825e52f0bd3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYYzqpaEbJl1V6VyvP6fDA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Hive Architecture - Hive Tutorial</figcaption></figure><p id="4465" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">如上图所示，配置单元体系结构可以分为以下几个部分:</p><ul class=""><li id="edf5" class="lv lw hh jp b jq kp ju kq jy mj kc mk kg ml kk ma mb mc md bi translated">Hive客户端: Hive支持用Java、C++、Python等多种语言编写的应用程序。使用JDBC，节俭和ODBC驱动程序。因此，人们总是可以用自己选择的语言编写hive客户端应用程序。</li><li id="80ab" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi"> Hive服务:</strong> Apache Hive提供各种服务，如CLI、Web界面等。执行查询。</li><li id="1cd2" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi">处理框架和资源管理:</strong>在内部，Hive使用Hadoop MapReduce框架作为事实上的引擎来执行查询。<strong class="jp hi"> <em class="kl"> Hadoop MapReduce框架</em> </strong>本身是一个独立的主题，因此，这里不做讨论。</li><li id="f13c" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi">分布式存储:</strong>由于Hive安装在Hadoop之上，它使用底层的HDFS作为分布式存储。</li></ul><p id="56e6" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">现在，让我们探索一下Hive架构中的前两个主要组件:</p><h2 id="147a" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">1.配置单元客户端:</h2><p id="149c" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">Apache Hive支持不同类型的客户端应用程序在Hive上执行查询。这些客户端可以分为三种类型:</p><ul class=""><li id="efce" class="lv lw hh jp b jq kp ju kq jy mj kc mk kg ml kk ma mb mc md bi translated"><strong class="jp hi"> <em class="kl"> Thrift客户端:</em> </strong>由于Hive server是基于Apache Thrift的，所以它可以服务于所有支持Thrift的编程语言的请求。</li><li id="e7af" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi"> <em class="kl"> JDBC客户端:</em> </strong> Hive允许Java应用程序使用org . Apache . Hadoop . Hive . JDBC . Hive driver类中定义的JDBC驱动程序连接到它</li><li id="4a5c" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi"> <em class="kl"> ODBC客户端:</em></strong>Hive ODBC驱动程序允许支持ODBC协议的应用程序连接到Hive。(与JDBC驱动程序一样，ODBC驱动程序使用Thrift与Hive服务器通信。)</li></ul><h2 id="20b1" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">2.配置单元服务:</h2><p id="bd70" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">Hive提供了许多服务，如上图所示。让我们来看看它们中的每一个:</p><ul class=""><li id="adb0" class="lv lw hh jp b jq kp ju kq jy mj kc mk kg ml kk ma mb mc md bi translated"><strong class="jp hi"> Hive CLI(命令行界面):</strong>这是Hive提供的默认shell，您可以在其中直接执行Hive查询和命令。</li><li id="bade" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi"> Apache Hive Web界面:</strong>除了命令行界面，Hive还提供了一个基于Web的GUI，用于执行Hive查询和命令。</li><li id="3705" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi"> Hive Server: </strong> Hive server建立在Apache Thrift之上，因此也被称为Thrift Server，它允许不同的客户端向Hive提交请求并检索最终结果。</li><li id="84da" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi"> Apache Hive驱动:</strong>负责接收客户端通过CLI、web UI、Thrift、ODBC或JDBC接口提交的查询。然后，驱动程序将查询传递给编译器，在metastore中的模式的帮助下，进行解析、类型检查和语义分析。在下一步中，以map-reduce任务和HDFS任务的DAG(有向无环图)的形式生成优化的逻辑计划。最后，执行引擎使用Hadoop按照依赖关系的顺序执行这些任务。</li><li id="7a74" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated"><strong class="jp hi"> Metastore: </strong>您可以将Metastore视为存储所有配置单元元数据信息的中央存储库。Hive元数据包括各种类型的信息，如表和分区的结构，以及列、列类型、序列化程序和反序列化程序，这是对HDFS中的数据进行读/写操作所必需的。<em class="kl">metastore由两个基本单元组成:</em></li></ul><blockquote class="km kn ko"><p id="74c7" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">提供对其他配置单元服务的metastore访问的服务。</p><p id="2b9d" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">元数据的磁盘存储，独立于HDFS存储。</p></blockquote><p id="0c72" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">现在，让我们在本Hive教程的下一节中理解实现Hive metastore的不同方式。</p><h1 id="9e25" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">Metastore配置</h1><p id="75de" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">Metastore使用RDBMS和称为Data Nucleus的开源ORM(对象关系模型)层存储元数据信息，该层将对象表示转换为关系模式，反之亦然。选择RDBMS而不是HDFS的原因是为了实现低延迟。我们可以在以下三种配置中实现metastore:</p><h2 id="9f92" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">1.嵌入式Metastore:</h2><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mm"><img src="../Images/4c68eeff3fe2b74f672e144ebea769a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T1l5RRAMnSeNctp3nVHXvA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Embedded Metastore - Hive Tutorial</figcaption></figure><p id="7b09" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">缺省情况下，metastore服务和Hive服务运行在同一个JVM中，使用嵌入式Derby数据库实例，元数据存储在本地磁盘中。这称为嵌入式metastore配置。在这种情况下，一次只有一个用户可以连接到metastore数据库。如果您启动配置单元驱动程序的第二个实例，您将得到一个错误。这对于单元测试来说是好的，但是对于实际的解决方案来说却不是。</p><h2 id="275a" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">2.本地Metastore:</h2><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mn"><img src="../Images/0c38e3fd4f702fe0660facfbce5a191d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Zt4OHXLnt8nK_dd9nl0kA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Local Metastore - Hive Tutorial</figcaption></figure><p id="83d4" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">此配置允许我们拥有多个配置单元会话，即多个用户可以同时使用metastore数据库。这可以通过使用任何JDBC兼容的数据库来实现，如MySQL，它运行在单独的JVM或不同的机器上，而不是运行在如上所示的相同JVM中的Hive服务和metastore服务的机器上。一般来说，最流行的选择是将MySQL服务器实现为metastore数据库。</p><h2 id="dcbd" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">3.远程Metastore:</h2><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mn"><img src="../Images/bd3d523250c58ed4aba7edb3f78a9afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eSVPpsYFn0TIfigZw0H1Uw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Remote Metastore</figcaption></figure><p id="0649" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">在远程metastore配置中，metastore服务运行在自己单独的JVM上，而不是在配置单元服务JVM中。其他进程使用节俭网络API与metastore服务器通信。在这种情况下，您可以拥有一个或多个metastore服务器来提供更高的可用性。使用远程metastore的主要优点是，您不需要与每个Hive用户共享JDBC登录凭据来访问metastore数据库。</p><h1 id="afc1" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">数据模型</h1><p id="892c" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">Hive中的数据可以在粒度级别上分为三种类型:</p><ul class=""><li id="0ebb" class="lv lw hh jp b jq kp ju kq jy mj kc mk kg ml kk ma mb mc md bi translated">桌子</li><li id="d0ed" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">划分</li><li id="b840" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">水桶</li></ul><h2 id="b1af" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">表格:</h2><p id="5f9e" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">Hive中的表与关系数据库中的表相同。您可以对它们执行筛选、投影、连接和联合操作。Hive中有两种类型的表:</p><h2 id="d7eb" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">1.托管表:</h2><p id="ac03" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><strong class="jp hi"> <em class="kl">命令:</em> </strong></p><p id="a801" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated"><em class="kl">创建表&lt;TABLE _ name&gt;(column 1 data _ type，column 2 data _ type)；</em></p><p id="76e0" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated"><em class="kl">将路径&lt;HDFS _文件_位置&gt;中的数据加载到表格托管_表格中；</em></p><p id="07b1" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">顾名思义(托管表)，Hive负责管理一个托管表的数据。换句话说，我所说的“Hive管理数据”的意思是，如果您将数据从位于HDFS的文件加载到Hive <em class="kl">管理的表</em>中，并对其发出DROP命令，该表及其元数据将被删除。因此，属于被删除的<em class="kl"> managed_table </em>的数据不再存在于HDFS的任何地方，您也无法以任何方式检索它。基本上，当您发出LOAD命令时，您正在将数据从HDFS文件位置移动到Hive warehouse目录。</p><blockquote class="km kn ko"><p id="f533" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated"><strong class="jp hi"> <em class="hh">注意:</em> </strong>仓库目录的默认路径设置为/user/hive/warehouse。配置单元表的数据驻留在仓库目录<strong class="jp hi"> / </strong>表名(HDFS)中。还可以在hive-site.xml中的hive.metastore.warehouse.dir配置参数中指定仓库目录的路径。</p></blockquote><h2 id="e937" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">2.外部表格:</h2><p id="5433" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><strong class="jp hi"> <em class="kl">命令:</em> </strong></p><p id="cbb0" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated"><em class="kl">创建外部表&lt;TABLE _ name&gt;(column 1 data _ type，column 2 data _ type)LOCATION '&lt;TABLE _ hive _ LOCATION&gt;'；</em></p><p id="f76c" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated"><em class="kl">将路径'&lt;HDFS _文件_位置&gt;中的数据加载到表&lt;表_名称&gt;中；</em></p><p id="2319" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">对于一个<em class="kl">外部表</em>，Hive不负责管理数据。在这种情况下，当您发出LOAD命令时，Hive将数据移动到它的仓库目录中。然后，Hive为外部表创建元数据信息。现在，如果您在<em class="kl">外部表</em>上发出DROP命令，只有关于外部表的元数据信息会被删除。因此，您仍然可以使用HDFS命令从仓库目录中检索外部表的数据。</p><h1 id="c4e2" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">分区:</h1><p id="1656" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><strong class="jp hi"> <em class="kl">命令:</em> </strong></p><p id="000d" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated"><em class="kl">创建由(partition1 data_type，partition2 data_type，…)分区的表table_name (column1 data_type，column2 data_type)。);</em></p><p id="911f" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">Hive将表组织成分区，以便根据列或分区键将相似类型的数据分组在一起。每个表可以有一个或多个分区键来标识特定的分区。这允许我们对数据切片进行更快的查询。</p><p id="30b5" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated"><strong class="jp hi"> <em class="kl">注意:</em> </strong>记住，创建分区时最常犯的错误是将现有的列名指定为分区列。在这样做的时候，您会收到一个错误——“语义分析中的错误:分区列中的列重复”。</p><p id="645f" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">让我们通过一个例子来理解分区，在这个例子中，我有一个表student_details，包含某个工程学院的学生信息，如student_id、姓名、院系、年份等。现在，如果我基于部门列执行分区，属于某个特定部门的所有学生的信息将一起存储在该分区中。从物理上讲，分区只不过是表目录中的一个子目录。</p><p id="9260" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">假设我们的student_details表中有三个部门的数据——CSE、ECE和Civil。因此，每个部门总共有三个分区，如下图所示。而且，对于每个部门，我们将把关于该部门的所有数据放在Hive表目录下的一个单独的子目录中。例如，所有关于CSE部门的学生数据都将存储在user/hive/warehouse/student _ details/dept . = CSE中。因此，关于CSE学生的查询只需查看CSE分区中的数据。这使得分区非常有用，因为它通过只扫描与<strong class="jp hi">相关的</strong>分区数据而不是整个数据集来减少查询延迟。事实上，在现实世界的实现中，您将处理数百TB的数据。因此，想象一下，为某个查询扫描如此大量的数据，而你扫描的<strong class="jp hi"> 95% </strong>的数据与你的查询不相关。</p><p id="7462" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">我建议你浏览一下关于<a class="ae mo" href="https://www.edureka.co/blog/hive-commands-with-examples?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=hive-tutorial" rel="noopener ugc nofollow" target="_blank"><strong class="jp hi"><em class="kl">Hive commands</em></strong></a>的博客，在那里你会找到实现分区的不同方法和例子。</p><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mm"><img src="../Images/189433e042213d974176348c38375cdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zIBd_85FQk8OLV6qNa3NQw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Hive Partitions Buckets - Hive Tutorial</figcaption></figure><h1 id="005e" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">桶:</h1><p id="fc52" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated"><strong class="jp hi"> <em class="kl">命令:</em> </strong></p><p id="9047" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated"><em class="kl">创建由(partition1 data_type，partition2 data_type，…)分区的表table_name。)按(column_name1，column_name2，…)分类，按(column_name [ASC|DESC]，…)]排序到num_buckets桶中；</em></p><p id="9776" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">现在，您可以根据表中某一列的散列函数，将每个分区或未分区表分成多个桶。实际上，每个桶只是分区目录或表目录(未分区表)中的一个文件。因此，如果您选择将分区划分为n个存储桶，那么您的每个分区目录中将有n个文件。例如，您可以看到上面的图像，其中我们将每个分区分成两个桶。因此，每个分区，比如说CSE，将有两个文件，每个文件将存储CSE学生的数据。</p><h2 id="6fe1" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">Hive如何将行分布到桶中？</h2><p id="5cbe" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">嗯，Hive通过使用公式来确定一行的桶号:<strong class="jp hi">hash _ function(bucket ing _ column)modulo(num _ of _ buckets)</strong>。这里，hash_function依赖于列数据类型。例如，如果您基于某个列(比如INT数据类型的user_id)对表进行分桶，那么hash_function将是—<strong class="jp hi">hash _ function(user _ id</strong>)<strong class="jp hi">= user _ id的整数值</strong>。并且，假设您已经创建了两个桶，那么Hive将通过计算:(user_id的值)modulo (2)来确定每个分区中进入桶1的行。因此，在这种情况下，user_id以偶数数字结尾的行将驻留在与每个分区对应的同一个存储桶中。其他数据类型的hash_function计算起来有点复杂，事实上，对于一个字符串，它甚至是人类无法识别的。</p><blockquote class="km kn ko"><p id="434e" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated"><strong class="jp hi">注意:</strong>如果你用的是Apache Hive 0.x或者1.x，你要发出命令—set Hive . enforce . bucket ing = true；在你的母舰终端上。这将允许您在使用cluster by子句对列进行分桶时拥有正确数量的reducer。如果您没有这样做，您可能会发现在您的表目录中生成的文件的数量不等于存储桶的数量。或者，您也可以使用set mapred . reduce . task = num _ bucket将缩减器的数量设置为桶的数量。</p></blockquote><h2 id="01e7" class="lh iq hh bd ir li lj lk iv ll lm ln iz jy lo lp jd kc lq lr jh kg ls lt jl lu bi translated">为什么我们需要水桶？</h2><p id="dfd9" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">对分区执行分桶有两个主要原因:</p><ul class=""><li id="4cd0" class="lv lw hh jp b jq kp ju kq jy mj kc mk kg ml kk ma mb mc md bi translated"><strong class="jp hi"> <em class="kl"> map side join </em> </strong>要求属于唯一连接键的数据出现在同一个分区中。但是那些分区键不同于join的情况呢？因此，在这些情况下，您可以通过使用连接键对表进行分桶来执行映射端连接。</li><li id="c56b" class="lv lw hh jp b jq me ju mf jy mg kc mh kg mi kk ma mb mc md bi translated">分桶使采样过程更有效，因此，允许我们减少查询时间。</li></ul><p id="8767" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">我想在这里结束这个蜂巢教程博客。我很确定在阅读完这篇Hive教程博客后，您会意识到Apache Hive的简单性。既然你们已经学习了所有的Hive基础知识，现在是时候来体验一下Apache Hive了。</p><p id="5537" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">如果你想查看更多关于人工智能、Python、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="70b3" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated">请留意本系列中解释大数据其他各方面的其他文章。</p><blockquote class="km kn ko"><p id="7ad9" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">1.<a class="ae mo" rel="noopener" href="/edureka/hadoop-tutorial-24c48fbf62f6"> Hadoop教程</a></p><p id="6af5" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">2.<a class="ae mo" rel="noopener" href="/edureka/big-data-tutorial-b664da0bb0c8">大数据教程</a></p><p id="106d" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">3.<a class="ae mo" rel="noopener" href="/edureka/pig-tutorial-2baab2f0a5b0">养猪教程</a></p><p id="cb5f" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">4.<a class="ae mo" rel="noopener" href="/edureka/mapreduce-tutorial-3d9535ddbe7c">地图缩小教程</a></p><p id="f4c7" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">5.<a class="ae mo" rel="noopener" href="/edureka/hbase-tutorial-bdc36ab32dc0">h基础教程</a></p><p id="0b75" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">6.<a class="ae mo" rel="noopener" href="/edureka/hdfs-tutorial-f8c4af1c8fde"> HDFS教程</a></p><p id="fac1" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">7.<a class="ae mo" rel="noopener" href="/edureka/hadoop-3-35e7fec607a"> Hadoop 3 </a></p><p id="68f9" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">8.<a class="ae mo" rel="noopener" href="/edureka/apache-sqoop-tutorial-431ed0af69ee"> Sqoop教程</a></p><p id="a8a5" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">9.<a class="ae mo" rel="noopener" href="/edureka/apache-flume-tutorial-6f7150210c76">水槽教程</a></p><p id="6cb9" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">10.<a class="ae mo" rel="noopener" href="/edureka/apache-oozie-tutorial-d8f7bbbe1591"> Oozie教程</a></p><p id="5196" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">11.<a class="ae mo" rel="noopener" href="/edureka/hadoop-ecosystem-2a5fb6740177"> Hadoop生态系统</a></p><p id="9338" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">12.<a class="ae mo" rel="noopener" href="/edureka/hive-commands-b70045a5693a">HQL顶级配置单元命令及示例</a></p><p id="b433" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">13.<a class="ae mo" rel="noopener" href="/edureka/create-hadoop-cluster-with-amazon-emr-f4ce8de30fd"> Hadoop集群搭配亚马逊EMR？</a></p><p id="6afa" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">14.<a class="ae mo" rel="noopener" href="/edureka/big-data-engineer-resume-7bc165fc8d9d">大数据工程师简历</a></p><p id="eda3" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">15.<a class="ae mo" rel="noopener" href="/edureka/hadoop-developer-cc3afc54962c"> Hadoop开发人员-工作趋势和薪水</a></p><p id="8bba" class="jn jo kl jp b jq kp js jt ju kq jw jx kr ks ka kb kt ku ke kf kv kw ki kj kk ha bi translated">16.<a class="ae mo" rel="noopener" href="/edureka/hadoop-interview-questions-55b8e547dd5c"> Hadoop面试问题</a></p></blockquote></div><div class="ab cl mp mq go mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ha hb hc hd he"><p id="1b1f" class="pw-post-body-paragraph jn jo hh jp b jq kp js jt ju kq jw jx jy ks ka kb kc ku ke kf kg kw ki kj kk ha bi translated"><em class="kl">原载于2016年12月5日</em><a class="ae mo" href="https://www.edureka.co/blog/hive-tutorial" rel="noopener ugc nofollow" target="_blank"><em class="kl">www.edureka.co</em></a><em class="kl">。</em></p></div></div>    
</body>
</html>