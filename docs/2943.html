<html>
<head>
<title>Explore How To Apply Decision Tree Algorithm With A Hands-On in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索如何在R中实际应用决策树算法</h1>
<blockquote>原文：<a href="https://medium.com/edureka/a-complete-guide-on-decision-tree-algorithm-3245e269ece?source=collection_archive---------1-----------------------#2019-03-15">https://medium.com/edureka/a-complete-guide-on-decision-tree-algorithm-3245e269ece?source=collection_archive---------1-----------------------#2019-03-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/ce6951a444a23efc735a090f08c6dc40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rwDyqW60ktwU6pg7JfLIGg.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">Decision Tree Algorithm - Edureka</figcaption></figure><p id="e1a3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随着解决行业级问题的机器学习算法的实现越来越多，对更复杂的迭代算法的需求已经成为一种需要。决策树算法就是这样一种用于解决回归和分类问题的算法。</p><p id="8114" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇关于决策树算法的文章中，您将学习决策树的工作原理，以及如何实现它来解决现实世界中的问题。本文将涵盖以下主题:</p><ol class=""><li id="07fb" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">为什么选择决策树？</li><li id="4bf6" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">什么是决策树？</li><li id="ed51" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">决策树算法是如何工作的？</li><li id="30f1" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">构建决策树</li><li id="755a" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">用R语言实现决策树算法</li></ol><p id="2113" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们都知道有n种机器学习算法可以用于分析，那么为什么要选择决策树呢？在下面的部分，我列出了几个原因。</p><h1 id="f8f5" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">为什么选择决策树算法？</h1><p id="182f" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">决策树被认为是最有用的机器学习算法之一，因为它可以用来解决各种问题。以下是您应该使用决策树的几个原因:</p><ol class=""><li id="ee4b" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">它被认为是最容易理解的机器学习算法，可以很容易地解释。</li><li id="059f" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">它可用于分类和回归问题。</li><li id="c707" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">与大多数机器学习算法不同，它可以有效地处理非线性数据。</li><li id="fdff" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">构建决策树是一个非常快速的过程，因为它只使用每个节点一个特征来分割数据。</li></ol><h1 id="501c" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">什么是决策树算法？</h1><p id="d6a9" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">决策树是一种有监督的机器学习算法，看起来像一棵倒置的树，其中每个节点代表一个<strong class="ir hi">预测变量</strong>(特征)，节点之间的链接代表一个<strong class="ir hi">决策</strong>，每个叶子节点代表一个<strong class="ir hi">结果</strong>(响应变量)。</p><p id="b5da" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了更好地理解决策树，让我们看一个例子:</p><p id="9e78" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设你举办了一个大型聚会，你想知道你的客人中有多少人是非素食者。为了解决这个问题，让我们创建一个简单的决策树。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es le"><img src="../Images/3ba0381122c1b675152e51cb5b92d021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1hfofE-ROPg2_fdxY13YNA.png"/></div></div></figure><p id="fada" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，我创建了一个决策树，将客人分为素食者和非素食者。每个节点代表一个预测变量，这将有助于断定一个客人是否是非素食者。当你沿着树向下遍历时，你必须在每个节点做出决定，直到你到达一个死胡同。</p><p id="a3c1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在你知道了决策树的逻辑，让我们定义一组与决策树相关的术语。</p><h1 id="d13e" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">决策树的结构</h1><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ln"><img src="../Images/7310862239220a063492fab2af39fab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n6CEhoqYYLVtLJDzs40ezg.png"/></div></div></figure><p id="8d7f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">决策树具有以下结构:</p><ul class=""><li id="6353" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lo jt ju jv bi translated"><strong class="ir hi">根节点:</strong>根节点是一棵树的起点。此时，将执行第一次拆分。</li><li id="3440" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lo jt ju jv bi translated"><strong class="ir hi">内部节点:</strong>每个内部节点代表一个最终导致预测结果的决策点(预测变量)。</li><li id="5317" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lo jt ju jv bi translated"><strong class="ir hi">叶/终端节点:</strong>叶节点代表结果的最后一类，因此它们也被称为终端节点。</li><li id="38b6" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lo jt ju jv bi translated"><strong class="ir hi">分支:</strong>分支是节点之间的连接，用箭头表示。每个分支代表一个响应，如“是”或“否”</li></ul><p id="4902" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是决策树的基本结构。现在让我们试着理解决策树的工作流程。</p><h1 id="01f0" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">决策树算法是如何工作的？</h1><p id="f005" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">决策树算法遵循以下步骤:</p><p id="8a46" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">步骤1: </strong>选择最能把数据集分类到所需类别的特征(预测变量)，并将该特征分配给根节点。<br/> <strong class="ir hi">步骤2: </strong>从根节点向下遍历，同时在每个内部节点做出相关决策，使得每个内部节点对数据进行最佳分类。<br/> <strong class="ir hi">第3步:</strong>返回第1步并重复，直到您为输入数据指定一个类别。</p><p id="1882" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上述步骤代表了用于分类目的的决策树的一般工作流程。</p><p id="bf2b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们试着理解决策树是如何创建的。</p><h1 id="637f" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">使用ID3算法构建决策树</h1><p id="1567" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">有许多方法可以建立一个决策树，在这篇文章中，我们将关注如何使用ID3算法来创建一个决策树。</p><h2 id="e8e9" class="lp kc hh bd kd lq lr ls kh lt lu lv kl ja lw lx kp je ly lz kt ji ma mb kx mc bi translated">ID3算法是什么？</h2><p id="e287" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">ID3或迭代二分法3算法是用于构建决策树的最有效的算法之一。它使用<em class="md">熵</em>和<em class="md">信息增益</em>的概念为给定的数据集生成决策树。</p><h2 id="8231" class="lp kc hh bd kd lq lr ls kh lt lu lv kl ja lw lx kp je ly lz kt ji ma mb kx mc bi translated"><strong class="ak"> ID3算法:</strong></h2><p id="20b6" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">ID3算法遵循以下工作流程来构建决策树:</p><ol class=""><li id="c72f" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">选择<strong class="ir hi">最佳属性</strong> (A)</li><li id="6579" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">指派一个作为根节点的决策变量。</li><li id="4c50" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">对于的每个值，生成该节点的后代。</li><li id="c649" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">将分类标签分配给叶节点。</li><li id="bf7e" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">如果数据分类正确:停止。</li><li id="f101" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">否则:遍历树。</li></ol><p id="15c3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该算法的第一步规定我们必须选择最佳属性。那是什么意思？</p><p id="5e11" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="md">最佳属性(预测变量)是最有效地将数据集分成不同类别的属性，或者是最佳分割数据集的特征。</em></p><p id="f6b4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在你脑海中的下一个问题一定是，“我如何决定哪个变量/特征最好地分割数据？”</p><p id="0ddb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">使用两种方法来确定最佳属性:</p><ol class=""><li id="f526" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">信息增益</li><li id="3452" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">熵</li></ol><h2 id="4b64" class="lp kc hh bd kd lq lr ls kh lt lu lv kl ja lw lx kp je ly lz kt ji ma mb kx mc bi translated">熵是什么？</h2><p id="6b02" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">熵测量数据中存在的杂质或不确定性。它用于决定决策树如何拆分数据。</p><h2 id="bdf2" class="lp kc hh bd kd lq lr ls kh lt lu lv kl ja lw lx kp je ly lz kt ji ma mb kx mc bi translated"><strong class="ak">熵的方程式:</strong></h2><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es me"><img src="../Images/35a3182323069cb06f5a284006aa9a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*97LdMvfKpzxWFqQ8vvg_Bw.png"/></div></figure><h2 id="d852" class="lp kc hh bd kd lq lr ls kh lt lu lv kl ja lw lx kp je ly lz kt ji ma mb kx mc bi translated">什么是信息增益？</h2><p id="57f9" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">信息增益(IG)是用于构建决策树的最重要的度量。它表明一个特定的特征/变量给了我们多少关于最终结果的“信息”。</p><p id="21d0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">信息增益很重要，因为它用来选择在决策树的每个节点上最好地分割数据的变量。具有最高IG的变量用于在根节点拆分数据。</p><h2 id="eeca" class="lp kc hh bd kd lq lr ls kh lt lu lv kl ja lw lx kp je ly lz kt ji ma mb kx mc bi translated"><strong class="ak">信息增益(IG)方程:</strong></h2><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mf"><img src="../Images/e70a2d6735e33521e567fdf289a509b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mBqKF0V9UWoCdTSZe5SEww.png"/></div></div></figure><p id="a480" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了更好地理解如何使用信息增益和熵来创建决策树，让我们看一个例子。下面的数据集代表了基于某些参数的汽车速度。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/0247d0a645a7e50d53f7f859ae7399c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*8CQ3u3LW6CtPg0QNKbSqMA.png"/></div></figure><p id="b6a9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你的问题陈述是研究这个数据集并创建一个决策树，根据以下预测变量将汽车速度(响应变量)分为慢或快:</p><ul class=""><li id="16f9" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lo jt ju jv bi translated">道路类型</li><li id="6765" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lo jt ju jv bi translated">障碍</li><li id="1b62" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lo jt ju jv bi translated">速度限制</li></ul><p id="459f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将使用这些变量构建一个决策树，以预测汽车的速度。正如我前面提到的，我们必须首先决定一个能最好地分割数据集的变量，并将该变量分配给根节点，并对其他节点重复同样的操作。</p><p id="3ea5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">此时，您可能想知道如何知道哪个变量最好地分隔了数据？答案是，具有最高信息增益的变量最好地将数据划分到所需的输出类中。</p><p id="5660" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，让我们从计算每个预测变量的熵和信息增益(IG)开始，从“道路类型”开始。</p><p id="6241" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们的数据集中，“道路类型”列中有四个观察值，对应于“汽车速度”列中的四个标签。我们将从计算父节点的熵(汽车的速度)开始。</p><p id="6cb7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第一步是找出父节点中存在的两个类的比例。我们知道父节点中总共有四个值，其中两个样本属于“慢”类，另外两个属于“快”类，因此:</p><ul class=""><li id="2a46" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lo jt ju jv bi translated">p(慢)-&gt;父节点中“慢”结果的分数</li><li id="4a14" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lo jt ju jv bi translated">p(快速)-&gt;父节点中“快速”结果的分数</li></ul><p id="801a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计算P(慢)的公式为:</p><p id="be4d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="md"> p(慢)=父节点中“慢”结果的数量/结果总数</em></p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mh"><img src="../Images/4e5e26e86d12278d83a729766226edf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*bDsJxjUAY7R5NFHVMuuwIA.png"/></div></div></figure><p id="513a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">同样，计算P(fast)的公式为:</p><p id="fc55" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="md"> p(快速)=父节点中“快速”结果的数量/结果总数</em></p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mh"><img src="../Images/e559acd0e3f727f1dfb9f75de9e951c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*BEco5ZpNAlGJqRtXYTx-Zw.png"/></div></div></figure><p id="04af" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，父节点的熵为:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/af401ec10d317b538a03cca1e91bbc9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*JEh48lG_xq6d52nwXoJlfQ.png"/></div></figure><p id="fd0b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="md">熵(父)=—{ 0.5 log2(0.5)+0.5 log2(0.5)} =—{-0.5+(-0.5)} = 1</em></p><p id="4668" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们知道父节点的熵是1，让我们看看如何计算“道路类型”变量的信息增益。请记住，如果“道路类型”变量的信息增益大于所有其他预测变量的信息增益，则只能使用“道路类型”变量来分割根结点。</p><p id="f5bf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了计算“道路类型”变量的信息增益，我们首先需要通过“道路类型”变量来拆分根节点。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mi"><img src="../Images/a6342d8c7d12bd6c35205f78ce87ba36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FriePpjQfUtF4UiLr2_C8w.png"/></div></div></figure><p id="70e1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，我们使用“道路类型”变量分割了父节点，子节点表示数据集中显示的相应响应。现在，我们需要测量子节点的熵。</p><p id="5fbe" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">右侧子节点(fast)的熵是0，因为该节点中的所有结果都属于一个类(fast)。以类似的方式，我们必须找到左手边节点的熵(慢，慢，快)。</p><p id="eee6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这个节点中，有两种类型的结果(快速和慢速)，因此，我们首先需要计算这个特定节点的慢速和快速结果的比例。</p><p id="1613" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="md"> P(慢)= 2/3 = 0.667</em>T2<em class="md">P(快)= 1/3 = 0.334 </em></p><p id="d888" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，熵是:</p><p id="db41" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="md">熵(左子节点)=-{ 0.667 log2(0.667)+0.334 log2(0.334)} =-{-0.38+(-0.52)}</em><br/><em class="md">= 0.9</em></p><p id="31fa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的下一步是用加权平均计算熵(子代):</p><ul class=""><li id="edb4" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm lo jt ju jv bi translated">父节点中的结果总数:4</li><li id="f173" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lo jt ju jv bi translated">左侧子节点中的结果总数:3</li><li id="7015" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm lo jt ju jv bi translated">右子节点中的结果总数:1</li></ul><p id="84b6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="md">用加权平均值求熵(子代)的公式。:</em> </strong></p><blockquote class="mj mk ml"><p id="4d3a" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated">[加权平均]熵(子节点)=(左侧子节点的结果数)/(父节点的结果总数)*(左侧节点的熵)+(右侧子节点的结果数)/(父节点的结果总数)*(右侧节点的熵)</p></blockquote><p id="c7d8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过使用上面的公式你会发现，熵(儿童)与加权平均。is = 0.675</p><p id="e563" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的最后一步是将上述加权平均值代入IG公式，以计算“道路类型”变量的最终IG:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mf"><img src="../Images/812076e10a881230e842d514cbc88cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nptEgoKlNbsh57PX55K-eg.png"/></div></div></figure><p id="368d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，</p><p id="b544" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="md">信息增益(道路类型)= 1–0.675 = 0.325</em></p><p id="767d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="md">道路类型特征的信息增益为</em> 0.325。</p><p id="d42e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">就像我前面提到的，决策树算法选择信息增益最高的变量来拆分决策树。因此，通过使用上述方法，您需要计算所有预测变量的信息增益，以检查哪个变量具有最高的IG。</p><p id="4829" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，通过使用上述方法，您必须获得每个预测变量的以下值:</p><ol class=""><li id="20b1" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><em class="md">信息增益(道路类型)= 1–0.675 = 0.325</em></li><li id="503f" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><em class="md">信息增益(阻碍)= 1–1 = 0</em></li><li id="b7e8" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><em class="md">信息增益(限速)= 1–0 = 1</em></li></ol><p id="1096" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，这里我们可以看到“速度限制”变量具有最高的信息增益。因此，该数据集的最终决策树是使用“速度限制”变量构建的。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mi"><img src="../Images/89efe6f487cc1f4a0352e897081ba749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VtqIOQ3M0csudpIr05HD_A.png"/></div></div></figure><p id="d193" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在您已经知道了决策树是如何创建的，让我们运行一个简短的演示，通过实现决策树来解决一个现实世界中的问题。</p><h1 id="bc8d" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">R-决策树算法实例中决策树的实现</h1><h2 id="cab4" class="lp kc hh bd kd lq lr ls kh lt lu lv kl ja lw lx kp je ly lz kt ji ma mb kx mc bi translated"><strong class="ak">问题陈述:</strong></h2><p id="8a5b" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">研究蘑菇数据集，以预测给定的蘑菇对人类是可食用的还是有毒的。</p><h2 id="6c18" class="lp kc hh bd kd lq lr ls kh lt lu lv kl ja lw lx kp je ly lz kt ji ma mb kx mc bi translated"><strong class="ak">数据集描述:</strong></h2><p id="c3d2" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">给定的数据集包含总共8124个不同种类蘑菇的观察结果及其属性，如气味、栖息地、种群等。下面的演示展示了数据集的更深入的结构。</p><h2 id="dbd1" class="lp kc hh bd kd lq lr ls kh lt lu lv kl ja lw lx kp je ly lz kt ji ma mb kx mc bi translated"><strong class="ak">逻辑:</strong></h2><p id="8def" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">建立决策树模型，通过研究蘑菇样品的气味、根、栖息地等属性，将蘑菇样品分类为有毒或可食用。</p><p id="9121" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在你知道了这个演示的目的，让我们开动脑筋，开始编码吧。对于这个演示，我将使用R语言来构建模型。</p><p id="06bc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">步骤1: </strong>安装并加载库</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="120f" class="lp kc hh mq b fi mu mv l mw mx">#Installing libraries<br/>install.packages('rpart')<br/>install.packages('caret')<br/>install.packages('rpart.plot')<br/>install.packages('rattle')<br/> <br/>#Loading libraries<br/>library(rpart,quietly = TRUE)<br/>library(caret,quietly = TRUE)<br/>library(rpart.plot,quietly = TRUE)<br/>library(rattle)</span></pre><p id="c6c5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第二步:</strong>导入数据集</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="3934" class="lp kc hh mq b fi mu mv l mw mx">#Reading the data set as a dataframe<br/>mushrooms &lt;- read.csv ("/Users/zulaikha/Desktop/decision_tree/mushrooms.csv")</span></pre><p id="d4fb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，要显示数据集的结构，可以使用名为str()的R函数:</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="6827" class="lp kc hh mq b fi mu mv l mw mx"># structure of the data<br/>&gt; str(mushrooms)<br/>'data.frame': 8124 obs. of 22 variables:<br/>$ class : Factor w/ 2 levels "e","p": 2 1 1 2 1 1 1 1 2 1 ...<br/>$ cap.shape : Factor w/ 6 levels "b","c","f","k",..: 6 6 1 6 6 6 1 1 6 1 ...<br/>$ cap.surface : Factor w/ 4 levels "f","g","s","y": 3 3 3 4 3 4 3 4 4 3 ...<br/>$ cap.color : Factor w/ 10 levels "b","c","e","g",..: 5 10 9 9 4 10 9 9 9 10 ...<br/>$ bruises : Factor w/ 2 levels "f","t": 2 2 2 2 1 2 2 2 2 2 ...<br/>$ odor : Factor w/ 9 levels "a","c","f","l",..: 7 1 4 7 6 1 1 4 7 1 ...<br/>$ gill.attachment : Factor w/ 2 levels "a","f": 2 2 2 2 2 2 2 2 2 2 ...<br/>$ gill.spacing : Factor w/ 2 levels "c","w": 1 1 1 1 2 1 1 1 1 1 ...<br/>$ gill.size : Factor w/ 2 levels "b","n": 2 1 1 2 1 1 1 1 2 1 ...<br/>$ gill.color : Factor w/ 12 levels "b","e","g","h",..: 5 5 6 6 5 6 3 6 8 3 ...<br/>$ stalk.shape : Factor w/ 2 levels "e","t": 1 1 1 1 2 1 1 1 1 1 ...<br/>$ stalk.root : Factor w/ 5 levels "?","b","c","e",..: 4 3 3 4 4 3 3 3 4 3 ...<br/>$ stalk.surface.above.ring: Factor w/ 4 levels "f","k","s","y": 3 3 3 3 3 3 3 3 3 3 ...<br/>$ stalk.surface.below.ring: Factor w/ 4 levels "f","k","s","y": 3 3 3 3 3 3 3 3 3 3 ...<br/>$ stalk.color.above.ring : Factor w/ 9 levels "b","c","e","g",..: 8 8 8 8 8 8 8 8 8 8 ...<br/>$ stalk.color.below.ring : Factor w/ 9 levels "b","c","e","g",..: 8 8 8 8 8 8 8 8 8 8 ...<br/>$ veil.color : Factor w/ 4 levels "n","o","w","y": 3 3 3 3 3 3 3 3 3 3 ...<br/>$ ring.number : Factor w/ 3 levels "n","o","t": 2 2 2 2 2 2 2 2 2 2 ...<br/>$ ring.type : Factor w/ 5 levels "e","f","l","n",..: 5 5 5 5 1 5 5 5 5 5 ...<br/>$ spore.print.color : Factor w/ 9 levels "b","h","k","n",..: 3 4 4 3 4 3 3 4 3 3 ...<br/>$ population : Factor w/ 6 levels "a","c","n","s",..: 4 3 3 4 1 3 3 4 5 4 ...<br/>$ habitat : Factor w/ 7 levels "d","g","l","m",..: 6 2 4 6 2 2 4 4 2 4 ...</span></pre><p id="0e94" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出显示了许多用于预测蘑菇输出类别(有毒或可食用)的预测变量。</p><p id="96ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第三步:</strong>数据清洗</p><p id="37ec" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这个阶段，我们必须寻找任何空值或缺失值以及不必要的变量，以便我们的预测尽可能准确。在下面的代码片段中，我删除了“veil.type”变量，因为它对结果没有影响。这种不一致和冗余数据必须在这一步得到解决。</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="6aa8" class="lp kc hh mq b fi mu mv l mw mx"># number of rows with missing values<br/>nrow(mushrooms) - sum(complete.cases(mushrooms))<br/> <br/># deleting redundant variable `veil.type`<br/>mushrooms$veil.type &lt;- NULL</span></pre><p id="6c60" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第四步:</strong>数据探索与分析</p><p id="6fca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了更好地理解21个预测变量，我为每个预测变量与类别类型(响应/结果变量)创建了一个表，以了解特定的预测变量对于检测输出是否重要。</p><p id="bdfb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我只显示了“气味”变量的表格，您可以按照下面的代码片段为每个变量创建一个表格:</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="f165" class="lp kc hh mq b fi mu mv l mw mx"># analyzing the odor variable<br/>&gt; table(mushrooms$class,mushrooms$odor)<br/>a      c      f       l       m       n       p        s      y<br/>e   400     0      0     400     0    3408    0       0      0<br/>p   0      192   2160  0      36     120    256  576  576</span></pre><p id="210f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的片段中，“e”代表可食用类，“p”代表蘑菇的有毒类。</p><p id="ab55" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上面的输出显示气味值为' c '，' f '，' m '，' p '，' s '和' y '的蘑菇明显有毒。并且具有杏仁气味(400)的蘑菇是可食用的。这样的观察将帮助我们更准确地预测输出类。</p><p id="2616" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们在数据探索阶段的下一步是预测哪个变量是分裂决策树的最佳变量。出于这个原因，我绘制了一个图表，表示21个变量中每个变量的分割，输出如下所示:</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="f750" class="lp kc hh mq b fi mu mv l mw mx">number.perfect.splits &lt;- apply(X=mushrooms[-1], MARGIN = 2, FUN = function(col){<br/>t &lt;- table(mushrooms$class,col)<br/>sum(t == 0)<br/>})<br/> <br/># Descending order of perfect splits<br/>order &lt;- order(number.perfect.splits,decreasing = TRUE)<br/>number.perfect.splits &lt;- number.perfect.splits[order]<br/> <br/># Plot graph<br/>par(mar=c(10,2,2,2))<br/>barplot(number.perfect.splits,<br/>main="Number of perfect splits vs feature",<br/>xlab="",ylab="Feature",las=2,col="wheat")</span></pre><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es my"><img src="../Images/85117be2e3eec04f69058aba86a0f4bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkdMQVnoRYLVapCDaftSIw.png"/></div></div></figure><p id="93d7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出结果表明,“气味”变量在预测蘑菇的输出类别中起着重要作用。</p><p id="f447" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第五步:</strong>数据拼接</p><p id="51ae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据拼接是将数据拆分为训练集和测试集的过程。训练集用于构建决策树模型，测试集用于验证模型的效率。拆分是在下面的代码片段中执行的:</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="dbbc" class="lp kc hh mq b fi mu mv l mw mx">#data splicing<br/>set.seed(12345)<br/>train &lt;- sample(1:nrow(mushrooms),size = ceiling(0.80*nrow(mushrooms)),replace = FALSE)<br/># training set<br/>mushrooms_train &lt;- mushrooms[train,]<br/># test set<br/>mushrooms_test &lt;- mushrooms[-train,]</span></pre><p id="c4db" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了使这个演示更有趣，并尽量减少误分类为可食用的毒蘑菇的数量，我们将指定一个比由于显而易见的原因将可食用蘑菇分类为有毒的惩罚大10倍的惩罚。</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="1153" class="lp kc hh mq b fi mu mv l mw mx"># penalty matrix<br/>penalty.matrix &lt;- matrix(c(0,1,10,0), byrow=TRUE, nrow=2)</span></pre><p id="3dc3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第六步:</strong>建立模型</p><p id="8ea7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在此阶段，我们将使用rpart(递归分区和回归树)算法构建一个决策树:</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="7275" class="lp kc hh mq b fi mu mv l mw mx"># building the classification tree with rpart<br/>tree &lt;- rpart(class~.,<br/>data=mushrooms_train,<br/>parms = list(loss = penalty.matrix),<br/>method = "class")</span></pre><p id="dfec" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第七步:</strong>观想树</p><p id="aaaa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这一步中，我们将使用rpart.plot库来绘制最终的决策树:</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="0946" class="lp kc hh mq b fi mu mv l mw mx"># Visualize the decision tree with rpart.plot<br/>rpart.plot(tree, nn=TRUE)</span></pre><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mz"><img src="../Images/2f27c08b388b8ffc947b9fee88b00497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_b-Im88ekxBbzLlugjnrkw.png"/></div></div></figure><p id="f1f1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第八步:</strong>测试模型</p><p id="85f0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，为了测试我们的决策树模型，我们将对我们的模型应用测试数据集，如下所示:</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="ec9b" class="lp kc hh mq b fi mu mv l mw mx">#Testing the model<br/>pred &lt;- predict(object=tree,mushrooms_test[-1],type="class")</span></pre><p id="4fd9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第九步:</strong>计算精度</p><p id="26c8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将使用混淆矩阵来计算模型的准确性。代码如下:</p><pre class="lf lg lh li fd mp mq mr ms aw mt bi"><span id="e39b" class="lp kc hh mq b fi mu mv l mw mx">#Calculating accuracy<br/>t &lt;- table(mushrooms_test$class,pred) &gt; confusionMatrix(t)<br/>Confusion Matrix and Statistics<br/> <br/>pred<br/>e      p<br/>e  839    0<br/>p  0     785<br/> <br/>Accuracy : 1<br/>95% CI : (0.9977, 1)<br/>No Information Rate : 0.5166<br/>P-Value [Acc &gt; NIR] : &lt; 2.2e-16<br/> <br/>Kappa : 1<br/>Mcnemar's Test P-Value : NA<br/> <br/>Sensitivity : 1.0000<br/>Specificity : 1.0000<br/>Pos Pred Value : 1.0000<br/>Neg Pred Value : 1.0000<br/>Prevalence : 0.5166<br/>Detection Rate : 0.5166<br/>Detection Prevalence : 0.5166<br/>Balanced Accuracy : 1.0000<br/> <br/>'Positive' Class : e</span></pre><p id="3e27" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出显示测试数据集中的所有样本都已被正确分类，我们在测试数据集上获得了100%的准确率，置信区间为95%(0.9977，1)。因此，我们可以使用决策树模型正确地将蘑菇分类为有毒或可食用。</p><p id="1761" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，就这样，我们到了这篇文章的结尾。我希望你们都觉得这篇文章内容丰富。</p><p id="b137" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于Python、DevOps、Ethical Hacking等市场最热门技术的文章，那么你可以参考<a class="ae na" href="https://www.edureka.co/blog/?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=decision-tree-algorithm" rel="noopener ugc nofollow" target="_blank"> Edureka的官方网站。</a></p><p id="27e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释数据科学的各个方面。</p><blockquote class="mj mk ml"><p id="7a5f" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 1。</em> <a class="ae na" rel="noopener" href="/edureka/data-science-tutorial-484da1ff952b"> <em class="hh">数据科学教程</em> </a></p><p id="915a" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 2。</em> <a class="ae na" rel="noopener" href="/edureka/math-and-statistics-for-data-science-1152e30cee73"> <em class="hh">数据科学的数学与统计</em> </a></p><p id="fae8" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 3。</em><a class="ae na" rel="noopener" href="/edureka/linear-regression-in-r-da3e42f16dd3"><em class="hh">R中的线性回归</em> </a></p><p id="3e2f" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 4。</em> <a class="ae na" rel="noopener" href="/edureka/data-science-tutorial-484da1ff952b"> <em class="hh">数据科学教程</em> </a></p><p id="ba97" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 5。</em><a class="ae na" rel="noopener" href="/edureka/logistic-regression-in-r-2d08ac51cd4f"><em class="hh">R中的逻辑回归</em> </a></p><p id="4a0c" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 6。</em> <a class="ae na" rel="noopener" href="/edureka/classification-algorithms-ba27044f28f1"> <em class="hh">分类算法</em> </a></p><p id="2c49" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 7。</em> <a class="ae na" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="9f8f" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 8。</em><a class="ae na" rel="noopener" href="/edureka/machine-learning-algorithms-29eea8b69a54"><em class="hh">5大机器学习算法</em> </a></p><p id="e71d" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 9。</em> <a class="ae na" rel="noopener" href="/edureka/introduction-to-machine-learning-97973c43e776"> <em class="hh">机器学习入门</em> </a></p><p id="989d" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated">10。 <a class="ae na" rel="noopener" href="/edureka/naive-bayes-in-r-37ca73f3e85c"> <em class="hh">朴素贝叶斯在R </em> </a></p><p id="c6ec" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 11。</em> <a class="ae na" rel="noopener" href="/edureka/statistics-and-probability-cf736d703703"> <em class="hh">统计与概率</em> </a></p><p id="a065" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 12。</em> <a class="ae na" rel="noopener" href="/edureka/decision-trees-b00348e0ac89"> <em class="hh">如何创建一个完美的决策树？</em> </a></p><p id="6ae4" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated">13。 <a class="ae na" rel="noopener" href="/edureka/data-scientists-myths-14acade1f6f7"> <em class="hh">关于数据科学家角色的10大误区</em> </a></p><p id="3b2f" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 14。</em> <a class="ae na" rel="noopener" href="/edureka/data-science-projects-b32f1328eed8"> <em class="hh">顶级数据科学项目</em> </a></p><p id="272b" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated">15。 <a class="ae na" rel="noopener" href="/edureka/data-analyst-vs-data-engineer-vs-data-scientist-27aacdcaffa5"> <em class="hh">数据分析师vs数据工程师vs数据科学家</em> </a></p><p id="6db5" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 16。</em> <a class="ae na" rel="noopener" href="/edureka/types-of-artificial-intelligence-4c40a35f784"> <em class="hh">人工智能的种类</em> </a></p><p id="1944" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 17。</em><a class="ae na" rel="noopener" href="/edureka/r-vs-python-48eb86b7b40f"><em class="hh">R vs Python</em></a></p><p id="41a7" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 18。</em> <a class="ae na" rel="noopener" href="/edureka/ai-vs-machine-learning-vs-deep-learning-1725e8b30b2e"> <em class="hh">人工智能vs机器学习vs深度学习</em> </a></p><p id="aba0" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 19。</em> <a class="ae na" rel="noopener" href="/edureka/machine-learning-projects-cb0130d0606f"> <em class="hh">机器学习项目</em> </a></p><p id="4891" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 20。</em> <a class="ae na" rel="noopener" href="/edureka/data-analyst-interview-questions-867756f37e3d"> <em class="hh">数据分析师面试问答</em> </a></p><p id="55aa" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 21。</em> <a class="ae na" rel="noopener" href="/edureka/data-science-and-machine-learning-for-non-programmers-c9366f4ac3fb"> <em class="hh">面向非程序员的数据科学和机器学习工具</em> </a></p><p id="1b81" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 22。</em> <a class="ae na" rel="noopener" href="/edureka/top-10-machine-learning-frameworks-72459e902ebb"> <em class="hh">十大机器学习框架</em> </a></p><p id="7e1b" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 23。</em> <a class="ae na" rel="noopener" href="/edureka/statistics-for-machine-learning-c8bc158bb3c8"> <em class="hh">用于机器学习的统计</em> </a></p><p id="61af" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 24。</em> <a class="ae na" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="5bfe" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 25。</em> <a class="ae na" rel="noopener" href="/edureka/breadth-first-search-algorithm-17d2c72f0eaa"> <em class="hh">广度优先搜索算法</em> </a></p><p id="f06f" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 26。</em><a class="ae na" rel="noopener" href="/edureka/linear-discriminant-analysis-88fa8ad59d0f"><em class="hh">R中的线性判别分析</em> </a></p><p id="78ec" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated">27。 <a class="ae na" rel="noopener" href="/edureka/prerequisites-for-machine-learning-68430f467427"> <em class="hh">机器学习的先决条件</em> </a></p><p id="cff9" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated">28。 <a class="ae na" rel="noopener" href="/edureka/r-shiny-tutorial-47b050927bd2"> <em class="hh">互动WebApps使用R闪亮</em> </a></p><p id="cf0e" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 29。</em> <a class="ae na" rel="noopener" href="/edureka/top-10-machine-learning-books-541f011d824e"> <em class="hh">十大机器学习书籍</em> </a></p><p id="9c08" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated">三十岁。 <a class="ae na" rel="noopener" href="/edureka/unsupervised-learning-40a82b0bac64"> <em class="hh">无监督学习</em> </a></p><p id="00d8" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated"><em class="hh"> 31.1 </em> <a class="ae na" rel="noopener" href="/edureka/10-best-books-data-science-9161f8e82aca"> <em class="hh"> 0最佳数据科学书籍</em> </a></p><p id="b674" class="ip iq md ir b is it iu iv iw ix iy iz mm jb jc jd mn jf jg jh mo jj jk jl jm ha bi translated">32。 <a class="ae na" rel="noopener" href="/edureka/supervised-learning-5a72987484d0"> <em class="hh">监督学习</em> </a></p></blockquote></div><div class="ab cl nb nc go nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ha hb hc hd he"><p id="6969" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="md">原载于2019年3月15日www.edureka.co</em><a class="ae na" href="https://www.edureka.co/blog/decision-tree-algorithm/" rel="noopener ugc nofollow" target="_blank"><em class="md"/></a><em class="md">。</em></p></div></div>    
</body>
</html>