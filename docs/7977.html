<html>
<head>
<title>Text Classification: How BERT boost the performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类:BERT如何提高性能</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/text-classification-how-bert-boost-the-performance-e65d1d678afb?source=collection_archive---------0-----------------------#2021-02-18">https://medium.com/walmartglobaltech/text-classification-how-bert-boost-the-performance-e65d1d678afb?source=collection_archive---------0-----------------------#2021-02-18</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5d2d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用BERT进行自然语言处理的小指南</p><p id="cea6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="jc jd ge" href="https://medium.com/u/630a6397128e?source=post_page-----e65d1d678afb--------------------------------" rel="noopener" target="_blank">朱</a>撰写</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/83ca0d4bdd803b2f1f99535a72235a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VkuwkSFE7pCgZWdzBKE6BQ.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Photo credit: Pixabay</figcaption></figure><h1 id="4de2" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">伯特</h1><p id="2f5f" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">这是《T2》<em class="ky">芝麻街</em>  <em class="ky">里的伯特吗？？？</em>今天我不会谈论这个黄色木偶卡通人物，而是一种自然语言处理的机器学习技术(简称NLP)<em class="ky">。</em> BERT是变压器双向编码器表示的简称。它由谷歌开发，由雅各布·德夫林(Jacob Devlin)等在2018年出版。</p></div><div class="ab cl kz la go lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ha hb hc hd he"><h1 id="609e" class="ju jv hh bd jw jx lg jz ka kb lh kd ke kf li kh ki kj lj kl km kn lk kp kq kr bi translated">议程</h1><ul class=""><li id="2abd" class="ll lm hh ig b ih ks il kt ip ln it lo ix lp jb lq lr ls lt bi translated">什么是文本分类？</li><li id="5de0" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">数据预处理</li><li id="fa67" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">使用CNN和LSTM进行实验</li><li id="b08c" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">使用BERT进行实验</li><li id="ab68" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">CNN vs LSTM vs伯特</li><li id="940f" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">结论</li></ul></div><div class="ab cl kz la go lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ha hb hc hd he"><h1 id="eed6" class="ju jv hh bd jw jx lg jz ka kb lh kd ke kf li kh ki kj lj kl km kn lk kp kq kr bi translated"><strong class="ak">什么是文本分类？</strong></h1><p id="d645" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">由于文本是不像结构化表格那样的非结构化格式，自然语言处理在数据挖掘中起着非常重要的作用。它也是监督学习的一部分，这意味着它对训练数据有预先标记的响应。在训练该模型之后，我们可以使用该模型对未来的输入文本进行分类。例如，电子邮件可以被分类为垃圾邮件或不是垃圾邮件，来自客户的文本反馈可以被分类为正面或负面，社交媒体文本可以被分类为对待或不对待等。</p><p id="8cb0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">还有一个更实际的例子，我们有很多来自Twitter、脸书等社交网络的顾客评论数据。我们想理解这些文本的意思，比如积极或消极。对于高层次，我们将致力于以下过程。</p><ol class=""><li id="41cd" class="ll lm hh ig b ih ii il im ip lz it ma ix mb jb mc lr ls lt bi translated">根据五个类别标记这些文本输入:好的、部分好的、中性的、部分坏的、坏的。</li><li id="c210" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb mc lr ls lt bi translated">我们可以使用这个训练数据集来训练我们的NLP模型。</li><li id="ad65" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb mc lr ls lt bi translated">我们可以将新的文本输入到NLP模型中得到预测。</li></ol><p id="647b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">输入下面的自由文本，也许我们会得到<strong class="ig hi"> <em class="ky">部分坏的</em> </strong>输出，然后产品经理可以从UI端或其他地方深入到这个文本中去做未来的改进，比如在UI端做一些改变。</p><blockquote class="md me mf"><p id="6b1d" class="ie if ky ig b ih ii ij ik il im in io mg iq ir is mh iu iv iw mi iy iz ja jb ha bi translated">“我喜欢使用这个新功能，但有时它让我很困惑，因为它的用户界面”——伯特·斯诺</p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mj"><img src="../Images/80ba4fb1bdae91701308a2902d616a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NYZu11EC4UZ87pI5L09sdA.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Example: The workflow for one text classification</figcaption></figure><p id="55df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">包括上面的例子，通常我们可以将文本分类分为以下几类:</p><blockquote class="md me mf"><p id="852c" class="ie if ky ig b ih ii ij ik il im in io mg iq ir is mh iu iv iw mi iy iz ja jb ha bi translated"><strong class="ig hi">情感分析:</strong>理解给定文本是正面还是负面谈论给定主题的过程(例如，用于品牌监控目的)。(参考文献1)</p><p id="0ff2" class="ie if ky ig b ih ii ij ik il im in io mg iq ir is mh iu iv iw mi iy iz ja jb ha bi translated"><strong class="ig hi">话题检测:</strong>识别一段文字的主题或话题的任务(例如，在分析客户反馈时，了解产品评论是关于<em class="hh">易用性</em>、<em class="hh">客户支持</em>还是<em class="hh">定价</em>)。(参考文献1)</p><p id="e81c" class="ie if ky ig b ih ii ij ik il im in io mg iq ir is mh iu iv iw mi iy iz ja jb ha bi translated"><strong class="ig hi">语言检测:</strong>检测给定文本的语言的过程(例如，知道传入的支持票是用英语还是西班牙语写的，以便自动将票发送给适当的团队)。(参考文献1)</p></blockquote></div><div class="ab cl kz la go lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ha hb hc hd he"><h1 id="731b" class="ju jv hh bd jw jx lg jz ka kb lh kd ke kf li kh ki kj lj kl km kn lk kp kq kr bi translated">二元文本分类模型</h1><p id="7ec3" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">我们将使用CNN、LSTM和伯特提出三个二进制文本分类模型。</p><h2 id="ab17" class="mk jv hh bd jw ml mm mn ka mo mp mq ke ip mr ms ki it mt mu km ix mv mw kq mx bi translated">数据预处理</h2><p id="81e3" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">因为我们从Twitter或脸书等社交网络获取数据，所以原始数据集中有大量无用或有噪声的数据。在将数据输入NLP模型进行训练之前，我们首先需要清理我们的文本数据。我在下面列出了一些我们遵循的步骤，你可以在这里为清理方面修改任何规则。</p><ul class=""><li id="62fe" class="ll lm hh ig b ih ii il im ip lz it ma ix mb jb lq lr ls lt bi translated"><em class="ky">删除空规则</em></li><li id="39a0" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">转发规则</em></li><li id="6b8e" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">标签规则</em></li><li id="6f95" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">标记规则</em></li><li id="7f95" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky"> url规则</em></li><li id="bad9" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">电子邮件规则</em></li><li id="8f20" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">编号规则</em></li><li id="df57" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">去掉标点符号</em></li><li id="9cde" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">移除不可打印的内容</em></li><li id="75c6" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">删除ascii子集</em></li><li id="95bd" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">引理化规则</em></li><li id="7174" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">标记化规则</li><li id="83de" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated">矢量化规则</li><li id="44de" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb lq lr ls lt bi translated"><em class="ky">等</em></li></ul><h2 id="39bb" class="mk jv hh bd jw ml mm mn ka mo mp mq ke ip mr ms ki it mt mu km ix mv mw kq mx bi translated">CNN和LSTM实验</h2><p id="ce61" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">对于CNN模型(卷积神经网络的简称),它通常用于图像部分，但我们只是将其用作基线，看看最坏的情况是什么。我们可以在下面找到模型定义。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="my mz l"/></div><figcaption class="jq jr et er es js jt bd b be z dx">nlp cnn model</figcaption></figure><p id="05df" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于长短期记忆的LSTM模型，它具有从递归神经网络(RNN)到反馈连接的一切。对于NLP，使用LSTM或RNN更有意义，因为在一个句子中，一些后面的单词会影响前面的单词。我们可以在下面找到LSTM模型的定义。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="my mz l"/></div><figcaption class="jq jr et er es js jt bd b be z dx">nlp lstm model</figcaption></figure><h2 id="3905" class="mk jv hh bd jw ml mm mn ka mo mp mq ke ip mr ms ki it mt mu km ix mv mw kq mx bi translated">使用BERT进行实验</h2><p id="5c2c" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">迁移学习是机器学习领域中非常重要的一部分。</p><blockquote class="md me mf"><p id="1f0e" class="ie if ky ig b ih ii ij ik il im in io mg iq ir is mh iu iv iw mi iy iz ja jb ha bi translated">它专注于存储在解决一个问题时获得的知识，并将其应用于另一个不同但相关的问题。(参考文献2)</p></blockquote><p id="e818" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">BERT是用于语言理解的深度双向转换器的预训练。我们可以在训练前BERT模型的基础上增加几层。例如，在我们的例子中，基于sequence_output层，我们添加了4个更密集的层，并带有dropout和regularizer。在最后一层，因为我们做了二进制分类，num_classes是2。下面我们可以填充整个BERT模型。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="my mz l"/></div><figcaption class="jq jr et er es js jt bd b be z dx">nlp bert mode</figcaption></figure><h2 id="490a" class="mk jv hh bd jw ml mm mn ka mo mp mq ke ip mr ms ki it mt mu km ix mv mw kq mx bi translated">CNN vs LSTM vs伯特</h2><p id="d114" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">基于这三个模型，我们计算了一些性能指标，如精确度、召回率、AUC和准确度。此外，我们使用15个纪元来训练我们的模型。</p><p id="466f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以发现，BERT的参数比其他算法多167倍，训练时间更长，性能更好。我们正在使用的BERT是BERT _ en _ un cased _ L-24 _ H-1024 _ A-16。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="my mz l"/></div><figcaption class="jq jr et er es js jt bd b be z dx">performance metric</figcaption></figure></div><div class="ab cl kz la go lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="ha hb hc hd he"><h1 id="ecf2" class="ju jv hh bd jw jx lg jz ka kb lh kd ke kf li kh ki kj lj kl km kn lk kp kq kr bi translated">结论</h1><p id="cb5d" class="pw-post-body-paragraph ie if hh ig b ih ks ij ik il kt in io ip ku ir is it kv iv iw ix kw iz ja jb ha bi translated">在我们的例子中，BERT确实提高了NLP的性能，但是它很耗时。基于模型再训练的频率，我们可以选择使用哪个版本。例如，我们需要每周重新训练模型，我们肯定可以使用BERT来获得高精度，但是如果我们需要每天重新训练模型，我们可以使用其他方法来减少训练时间。然而，让我们欢迎伯特来到我们的世界。如果你有任何问题，请在这里评论，我们可以讨论更多。</p><h1 id="dd9c" class="ju jv hh bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">参考</h1><ol class=""><li id="c832" class="ll lm hh ig b ih ks il kt ip ln it lo ix lp jb mc lr ls lt bi translated"><a class="ae kx" href="https://monkeylearn.com/what-is-text-classification/" rel="noopener ugc nofollow" target="_blank">https://monkeylearn.com/what-is-text-classification/</a></li><li id="d33e" class="ll lm hh ig b ih lu il lv ip lw it lx ix ly jb mc lr ls lt bi translated">https://en.wikipedia.org/wiki/Transfer_learning<a class="ae kx" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank"/></li></ol></div></div>    
</body>
</html>