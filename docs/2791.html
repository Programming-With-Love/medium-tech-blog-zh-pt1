<html>
<head>
<title>PySpark Tutorial - Learn Apache Spark Using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark教程——使用Python学习Apache Spark</h1>
<blockquote>原文：<a href="https://medium.com/edureka/pyspark-tutorial-87d41dab9657?source=collection_archive---------0-----------------------#2018-06-28">https://medium.com/edureka/pyspark-tutorial-87d41dab9657?source=collection_archive---------0-----------------------#2018-06-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/ab27630caabcd3049c72cee5f0abffe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CfeqJDg8gBVBhhhTINmeFw.png"/></div><figcaption class="il im et er es in io bd b be z dx">PySpark Tutorial — Edureka</figcaption></figure><p id="bc52" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在一个数据以如此惊人的速度产生的世界里，在正确的时间对数据进行正确的分析是非常有用的。实时处理大数据并执行分析的最令人惊叹的框架之一是<a class="ae jn" href="https://www.edureka.co/blog/spark-tutorial?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=pyspark-tutorial" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>。总之，<strong class="ir hi"> </strong> <em class="jo"> Python for Spark </em>或PySpark是最受欢迎的认证课程之一，让Scala for Spark望尘莫及。因此，在本文中，我将讨论以下主题:</p><ul class=""><li id="d42f" class="jp jq hh ir b is it iw ix ja jr je js ji jt jm ju jv jw jx bi translated">PySpark是什么？</li><li id="e35f" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated">业内的PySpark</li><li id="6faf" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated">为什么选择Python？</li><li id="12ed" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated">火花RDDs</li><li id="0efe" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated">使用PySpark进行机器学习</li></ul><h1 id="93ef" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">PySpark是什么？</h1><p id="b294" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">Apache Spark是一个快速集群计算框架，用于处理、查询和分析大数据。基于内存计算，它比其他几个大数据框架有优势。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lg"><img src="../Images/920dc3ef58ad5e05e75af811a0fbc1df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SPaFcajWvoZxSbLIzAcexw.png"/></div></div></figure><p id="faa7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最初用Scala编程语言编写，开源社区开发了一个惊人的工具，为Apache Spark支持Python。PySpark通过它的库<strong class="ir hi"> Py4j帮助数据科学家与Apache Spark和Python中的rdd接口。【PySpark有许多特性使其成为比其他框架更好的框架:</strong></p><ul class=""><li id="8f55" class="jp jq hh ir b is it iw ix ja jr je js ji jt jm ju jv jw jx bi translated"><strong class="ir hi">速度:</strong>比传统大规模数据处理框架快100倍</li><li id="0011" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated"><strong class="ir hi">强大的缓存:</strong>简单的编程层提供了强大的缓存和磁盘持久性功能</li><li id="4290" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated"><strong class="ir hi">部署:</strong>可以通过Mesos、Hadoop via Yarn或者Spark自己的集群管理器进行部署</li><li id="41c8" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated"><strong class="ir hi">实时:</strong>实时计算&amp;由于内存计算，延迟较低</li><li id="95e2" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated"><strong class="ir hi"> Polyglot: </strong>支持Scala、Java、Python和R编程</li></ul><p id="0032" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们继续我们的文章，看看Spark在行业中的应用。</p><h1 id="42ba" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">业内的PySpark</h1><p id="b3bf" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">每个<a class="ae jn" href="https://www.edureka.co/blog/big-data-applications-revolutionizing-various-domains?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=pyspark-tutorial" rel="noopener ugc nofollow" target="_blank">行业都围绕着大数据</a>，哪里有大数据，哪里就有分析。所以让我们来看看Apache Spark被应用的各个行业。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/e6eea4d809f1abcf6bc1df076910cc55.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*wMmkz_YYevsCizlDkgJrNA.jpeg"/></div></figure><p id="7f60" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">媒体是向在线流媒体发展的最大行业之一。<strong class="ir hi">网飞</strong>使用Apache Spark进行实时流处理，为其客户提供个性化的在线推荐。它每天处理4500亿个流向服务器端应用程序的事件。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/e6a29bee769e5318e0856a76511fe4c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*ynWyP1lI4OWSMUlbGhXahQ.jpeg"/></div></figure><p id="dcfc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">金融</strong>是Apache Spark的实时处理发挥重要作用的另一个领域。银行正在使用Spark访问和分析社交媒体档案，以获得洞察力，帮助他们为<strong class="ir hi">信用风险评估</strong>、定向广告和客户细分做出正确的商业决策。<strong class="ir hi">使用Spark后，客户流失</strong>也有所减少。<strong class="ir hi">欺诈检测</strong>是Spark参与的机器学习中应用最广泛的领域之一。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/8ef1278c8139d6d1fbcc0e4f4223c9ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*GfEGj-uOr7sLrEFEen2oUQ.jpeg"/></div></figure><p id="58e6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">医疗保健</strong>提供商正在使用Apache Spark<strong class="ir hi">分析患者记录</strong>以及过去的临床数据，以确定哪些患者在出院后可能面临健康问题。Apache Spark用于<strong class="ir hi">基因组测序</strong>以减少处理基因组数据所需的时间。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/f5a6ba2f84ef735d59ccb34711aa2b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*ua4yh-YDhr8JNS95prsJyg.jpeg"/></div></figure><p id="20a5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">零售和电子商务</strong>如果不使用分析和有针对性的广告，人们很难想象这个行业会如何发展。当今最大的电子商务平台之一<strong class="ir hi">阿里巴巴</strong>运行着一些世界上最大的Spark作业，以分析数Pb的数据。阿里巴巴在图像数据中执行<strong class="ir hi">特征提取</strong>。<strong class="ir hi">易贝</strong>利用Apache Spark提供<strong class="ir hi">针对性优惠</strong>，提升客户体验，优化整体性能。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/2ec6a3f81a25195f4b7ae3b1522d5692.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*ZQ8jXZWeGIhAcHH7hulhSw.jpeg"/></div></figure><p id="a0bd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">旅游</strong>行业也用阿帕奇Spark。帮助用户计划完美旅行的领先旅游网站猫途鹰正在使用Apache Spark加速其个性化客户推荐。猫途鹰使用apache spark向数百万旅行者提供建议，方法是<strong class="ir hi">比较数百个网站</strong>为其客户找到最佳酒店价格。</p><p id="a0e2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">本PySpark教程的一个重要方面是理解我们为什么需要使用Python？为什么不是Java，Scala或者R？</p><h1 id="b0d0" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">为什么选择Python？</h1><p id="69bb" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">对于程序员来说，Python相对来说更容易学习，因为它的语法和标准库。此外，它是一种动态类型语言，这意味着rdd可以保存多种类型的对象。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/2b0a74416a51fe2ee6beba38cf28afa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*0-pD1Aukai3-_61hoH21bg.png"/></div></figure><p id="67da" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">大量的库:</strong> Scala没有足够的数据科学工具和像Python这样的库来进行机器学习和自然语言处理。此外，Scala缺乏良好的可视化和本地数据转换。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/47638f63c5892c54e2f530673fefef1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*_AcK_HFsVCABtannoTv-8w.png"/></div></figure><p id="391d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">巨大的社区支持: Python有一个全球社区，有数百万开发者，他们在数千个虚拟和物理位置进行在线和离线交互。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/38df66e43835f87945725f258dffe574.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*wuvxLiTtEAgrDecwWAFOYw.png"/></div></figure><p id="2def" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">本PySpark教程中最重要的主题之一是rdd的使用。让我们了解一下什么是rdd</p><h1 id="e607" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">火花RDDs</h1><p id="7548" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">当谈到迭代分布式计算时，即在计算中通过多个作业处理数据时，我们需要在多个作业之间重用或共享数据。像Hadoop这样的早期框架在处理多种操作/作业时存在问题，比如</p><ul class=""><li id="f69e" class="jp jq hh ir b is it iw ix ja jr je js ji jt jm ju jv jw jx bi translated">将数据存储在中间存储器中，如HDFS</li><li id="0f16" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated">多个I/O作业会降低计算速度</li><li id="e7a3" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated">复制和序列化，这反过来又会使过程变得更慢</li></ul><p id="146d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">rdd试图通过支持容错分布式内存计算来解决所有问题。RDD是弹性分布式数据集的缩写。 RDD是一种分布式内存抽象，允许程序员以容错方式在大型集群上执行内存计算。它们是在一组机器上分区的<strong class="ir hi">只读对象集合</strong>，如果一个分区丢失，可以重建这些对象。在rdd上执行多种操作:</p><ul class=""><li id="74bc" class="jp jq hh ir b is it iw ix ja jr je js ji jt jm ju jv jw jx bi translated"><strong class="ir hi">转换:</strong>转换从现有的数据集创建一个新的数据集。懒惰评估</li><li id="72c5" class="jp jq hh ir b is jy iw jz ja ka je kb ji kc jm ju jv jw jx bi translated"><strong class="ir hi">动作:</strong> Spark仅在RDDs上调用动作时强制执行计算</li></ul><p id="5e6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们了解一些转换、动作和功能</p><h2 id="2c2a" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">读取文件并显示前n个元素:</strong></h2><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="34dc" class="lt ke hh mi b fi mm mn l mo mp">rdd = sc.textFile("file:///home/edureka/Desktop/Sample") <br/>rdd.take(n)</span></pre><p id="e053" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="52ce" class="lt ke hh mi b fi mm mn l mo mp">[u'Deforestation is arising as the main environmental and social issue which has now taken the form of more than a powerful demon. ',<br/> u'We must know about the causes, effects and ways to solve the problems arisen because of the deforestation. ',<br/> u'We have provided many paragraphs, long and short essay on deforestation in order to help your kids and children to get aware about the problem as well as get participated in the essay writing competition in the school or outside the school. ',<br/> u'You can select any deforestation essay given below according to the class standard. ',<br/> u'Deforestation is arising as the major global problem to the society and environment.']</span></pre><h2 id="56c6" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">转换为小写并拆分:(小写并拆分)</strong></h2><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="9a31" class="lt ke hh mi b fi mm mn l mo mp">def Func(lines):<br/>lines = lines.lower()<br/>lines = lines.split()<br/>return lines<br/>rdd1 = rdd.map(Func)<br/> <br/>rdd1.take(5)</span></pre><p id="0100" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="158f" class="lt ke hh mi b fi mm mn l mo mp">[[u'deforestation',<br/>  u'is',<br/>  u'arising',<br/>  u'as',<br/>  u'the',<br/>  u'main',<br/>  u'environmental',<br/>  u'and',<br/>  u'social',<br/>  u'issue',<br/>  u'which',<br/>  u'has',<br/>  u'now',<br/>  u'taken',<br/>.....<br/>.<br/>.<br/>.<br/>]</span></pre><h2 id="5fbd" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">删除停用词:(过滤器)</strong></h2><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="7426" class="lt ke hh mi b fi mm mn l mo mp">stop_words = ['a','all','the','as','is','am','an','and','be','been','from','had','I','I’d','why','with']<br/>rdd2 = rdd1.filter(lambda z: z not in stop_words)<br/>rdd2.take(10)</span></pre><p id="f99a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="4593" class="lt ke hh mi b fi mm mn l mo mp">[u'deforestation',<br/> u'arising',<br/> u'main',<br/> u'environmental',<br/> u'social',<br/> u'issue',<br/> u'which',<br/> u'has',<br/> u'now',<br/> u'taken']</span></pre><h2 id="93bc" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">从1到500的数字之和:(减少)</strong></h2><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="3574" class="lt ke hh mi b fi mm mn l mo mp">sum_rdd = sc.parallelize(range(1,500))<br/>sum_rdd.reduce(lambda x,y: x+y)</span></pre><p id="6480" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="1fee" class="lt ke hh mi b fi mm mn l mo mp">124750</span></pre><h1 id="9830" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">使用PySpark进行机器学习</h1><p id="af0e" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">继续我们的文章，我们来分析一些篮球数据，做一些未来预测。所以，这里我们要用的是自<strong class="ir hi"> 1980 </strong>【三分球引入年】以来NBA所有球员的篮球数据。</p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/08b64d9cc2355a5c500998299cfe1986.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*C9yydLywrMRA2v7oZ8xE3A.png"/></div></figure><h2 id="ce2a" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">数据加载:</strong></h2><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="3194" class="lt ke hh mi b fi mm mn l mo mp">df = spark.read.option('header','true')\<br/>.option('inferSchema','true')<br/>.csv("file:///home/edureka/Downloads/season_totals.csv")</span></pre><h2 id="781b" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">打印栏目:</strong></h2><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="f567" class="lt ke hh mi b fi mm mn l mo mp">print(df.columns)</span></pre><p id="b107" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="2ef1" class="lt ke hh mi b fi mm mn l mo mp">['_c0', 'player', 'pos', 'age', 'team_id', 'g', 'gs', 'mp', 'fg', 'fga', 'fg_pct', 'fg3', 'fg3a', 'fg3_pct', 'fg2', 'fg2a', 'fg2_pct', 'efg_pct', 'ft', 'fta', 'ft_pct', 'orb', 'drb', 'trb', 'ast', 'stl', 'blk', 'tov', 'pf', 'pts', 'yr']</span></pre><h2 id="6b07" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">球员排序(OrderBy)和托班达斯:</strong></h2><p id="5dba" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">在这里，我们根据一个赛季的得分对球员进行分类。</p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="44be" class="lt ke hh mi b fi mm mn l mo mp">df.orderBy('pts',ascending = False).limit(10).toPandas()[['yr','player','age','pts','fg3']]</span></pre><p id="ab27" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es mq"><img src="../Images/fab5c916aff60e8d62ac81a804f5092e.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*OV-yfjvd1p6k0EfFx5_4GQ.png"/></div></figure><h2 id="1345" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">使用DSL和matplotlib: </strong></h2><p id="945c" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">这里我们分析的是在<strong class="ir hi"> 36分钟</strong>的时限内<strong class="ir hi">每个赛季的平均3分出手次数</strong>【这个区间对应于一场休息充分的近似完整的NBA比赛】。我们使用三分球投篮次数(fg3a)和上场时间(mp)来计算这一指标，然后使用<strong class="ir hi"> matlplotlib </strong>来绘制结果。</p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="ab11" class="lt ke hh mi b fi mm mn l mo mp">from pyspark.sql.functions import col<br/>fga_py = df.groupBy('yr')\<br/>.agg({'mp' : 'sum', 'fg3a' : 'sum'})<br/>.select(col('yr'), (36*col('sum(fg3a)')/col('sum(mp)')).alias('fg3a_p36m'))\<br/>.orderBy('yr')<br/> <br/>from matplotlib import pyplot as plt<br/>import seaborn as sns<br/>plt.style.use('fivethirtyeight')<br/> <br/> <br/>_df = fga_py.toPandas()<br/>plt.plot(_df.yr,_df.fg3a_p36m, color = '#CD5C5C')<br/>plt.xlabel('Year')<br/>_=plt.title('Player average 3-point attempts (per 36 minutes)')<br/>plt.annotate('3 pointer introduced', xy=(1980, .5), xytext=(1981, 1.1), fontsize = 9,<br/>arrowprops=dict(facecolor='grey', shrink=0, linewidth = 2))<br/>plt.annotate('NBA moved in 3-point line', xy=(1996, 2.4), xytext=(1991.5, 2.7), fontsize = 9,<br/>arrowprops=dict(facecolor='grey', shrink=0, linewidth = 2))<br/>plt.annotate('NBA moved back\n3-point line', xy=(1998, 2.), xytext=(1998.5, 2.4), fontsize = 9, arrowprops=dict(facecolor='grey', shrink=0, linewidth = 2))</span></pre><p id="9650" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es mr"><img src="../Images/f3870469c5f287a63dbe46ff840e58ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*694g6lGcBx8Zs4STE2Q2Zg.png"/></div></figure><h2 id="e082" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">线性回归和向量汇编:</strong></h2><p id="afea" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">我们可以用一个线性回归模型来拟合这条曲线，以模拟未来5年的投篮次数。我们必须使用VectorAssembler函数将数据转换为单个列。这是MLlib中线性回归API的一个<strong class="ir hi">需求</strong>。</p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="8bfd" class="lt ke hh mi b fi mm mn l mo mp">from pyspark.ml.feature import VectorAssembler<br/>t = VectorAssembler(inputCols=['yr'], outputCol = 'features')<br/>training = t.transform(fga_py)\<br/>.withColumn('yr',fga_py.yr)\<br/>.withColumn('label',fga_py.fg3a_p36m)<br/>training.toPandas().head()</span></pre><p id="c8ba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/a249344d8b52dbe3b02939950a891a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*tCopuHMAmD0NFpAhqY3LjQ.png"/></div></figure><h2 id="1f63" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">建筑模型:</strong></h2><p id="2e3c" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">然后，我们使用转换后的数据构建线性回归模型对象。</p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="d64e" class="lt ke hh mi b fi mm mn l mo mp">from pyspark.ml.regression import LinearRegression <br/>lr = LinearRegression(maxIter=10)<br/>model = lr.fit(training)</span></pre><h2 id="0ec3" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">将训练好的模型应用到数据集:</strong></h2><p id="4d7c" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">我们将经过训练的模型对象模型应用于原始训练集以及未来5年的数据</p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="8df7" class="lt ke hh mi b fi mm mn l mo mp">from pyspark.sql.types import Row<br/> <br/># apply model for the 1979-80 season thru 2020-21 season<br/>training_yrs = training.select('yr').rdd.map(lambda x: x[0]).collect()<br/>training_y = training.select('fg3a_p36m').rdd.map(lambda x: x[0]).collect()<br/>prediction_yrs = [2017, 2018, 2019, 2020, 2021]<br/>all_yrs = training_yrs + prediction_yrs<br/> <br/># built testing DataFrame<br/>test_rdd = sc.parallelize(all_yrs)<br/>row = Row('yr')&amp;amp;amp;lt<br/>all_years_features = t.transform(test_rdd.map(row).toDF())<br/> <br/># apply linear regression model<br/>df_results = model.transform(all_years_features).toPandas()</span></pre><h2 id="c45f" class="lt ke hh bd kf lu lv lw kj lx ly lz kn ja ma mb kr je mc md kv ji me mf kz mg bi translated"><strong class="ak">绘制最终预测:</strong></h2><p id="f053" class="pw-post-body-paragraph ip iq hh ir b is lb iu iv iw lc iy iz ja ld jc jd je le jg jh ji lf jk jl jm ha bi translated">然后，我们可以绘制我们的结果，并将图形保存在指定的位置。</p><pre class="lh li lj lk fd mh mi mj mk aw ml bi"><span id="a47f" class="lt ke hh mi b fi mm mn l mo mp">plt.plot(df_results.yr,df_results.prediction, linewidth = 2, linestyle = '--',color = '#224df7', label = 'L2 Fit')<br/>plt.plot(training_yrs, training_y, color = '#f08080', label = None)<br/>plt.xlabel('Year')<br/>plt.ylabel('Number of attempts')<br/>plt.legend(loc = 4)<br/>_=plt.title('Player average 3-point attempts (per 36 minutes)')<br/>plt.tight_layout()<br/>plt.savefig("/home/edureka/Downloads/Images/REGRESSION.png")</span></pre><p id="71ff" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><figure class="lh li lj lk fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/6a5552a30b42a8ee84059a7772c5cb38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*VtWwZut0kjgsSJTmxFR5sQ.png"/></div></figure><p id="0fca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有了这个图表，我们就到了这篇文章的结尾。</p><p id="c153" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">就这样了，伙计们！</p><p id="9582" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我希望你们在这篇文章中了解了什么是Pyspark，为什么Python最适合于Spark，RDDs和PySpark的机器学习。恭喜你，你不再是PySpark的新手了。</p><p id="c414" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，那么你可以参考<a class="ae jn" href="https://www.edureka.co/blog/?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=pyspark-tutorial" rel="noopener ugc nofollow" target="_blank"> Edureka的官方网站</a>。</p><p id="27e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释PySpark的各个方面。</p><blockquote class="mu mv mw"><p id="60fe" class="ip iq jo ir b is it iu iv iw ix iy iz mx jb jc jd my jf jg jh mz jj jk jl jm ha bi translated">1.<a class="ae jn" rel="noopener" href="/edureka/pyspark-dataframe-tutorial-9335f3d09b4"> PySpark数据帧教程</a></p><p id="b279" class="ip iq jo ir b is it iu iv iw ix iy iz mx jb jc jd my jf jg jh mz jj jk jl jm ha bi translated">2.<a class="ae jn" rel="noopener" href="/edureka/pyspark-rdd-ef9edd060a25">py spark中的rdd</a></p><p id="7e12" class="ip iq jo ir b is it iu iv iw ix iy iz mx jb jc jd my jf jg jh mz jj jk jl jm ha bi translated">3.<a class="ae jn" rel="noopener" href="/edureka/pyspark-mllib-tutorial-759391dbb08a"> PySpark MLlib教程</a></p><p id="6411" class="ip iq jo ir b is it iu iv iw ix iy iz mx jb jc jd my jf jg jh mz jj jk jl jm ha bi translated">4.<a class="ae jn" rel="noopener" href="/edureka/pyspark-programming-e007e68fbccb"> PySpark编程</a></p></blockquote></div><div class="ab cl na nb go nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ha hb hc hd he"><p id="eb24" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jo">原载于2018年6月28日</em><a class="ae jn" href="https://www.edureka.co/blog/pyspark-tutorial/" rel="noopener ugc nofollow" target="_blank"><em class="jo">www.edureka.co</em></a><em class="jo">。</em></p></div></div>    
</body>
</html>