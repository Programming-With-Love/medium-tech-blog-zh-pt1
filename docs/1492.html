<html>
<head>
<title>Dask + Numba for Efficient In-Memory Model Scoring</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Dask + Numba用于高效的内存模型评分</h1>
<blockquote>原文：<a href="https://medium.com/capital-one-tech/dask-numba-for-efficient-in-memory-model-scoring-dfc9b68ba6ce?source=collection_archive---------2-----------------------#2017-11-28">https://medium.com/capital-one-tech/dask-numba-for-efficient-in-memory-model-scoring-dfc9b68ba6ce?source=collection_archive---------2-----------------------#2017-11-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/95b2cb91b24bf4c141c0360f76b2fea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y1IPwDujhQkhDsq_7yAndw.jpeg"/></div></div></figure><p id="ebbd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我非常感谢Anaconda，Inc .的迈克尔·格兰特和吉姆·克里斯特贡献了这篇文章的许多核心观点，以及许多有益而有见地的讨论。</p><h1 id="0475" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">简介dask和numba是什么？</strong></h1><p id="0036" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">数据科学家经常处理令人不安的大数据集，例如，对这些数据进行简单的操作会占用大量的内存资源。在这些环境中，我们可能会尝试使用某种形式的分布式计算；然而，过早地迁移到分布式环境会带来巨大的成本，有时甚至会<a class="ae kr" href="https://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html" rel="noopener ugc nofollow" target="_blank">降低性能</a>与实现良好的单机解决方案相比。在本帖中，我们将回顾两个基于python的工具dask和numba，它们用于在有许多可能的模型输出和资源可能紧张的情况下实现单机模型评分管道。</p><p id="ff6a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">dask和numba都是用于优化计算的python库。</p><ul class=""><li id="7f34" class="ks kt hh ir b is it iw ix ja ku je kv ji kw jm kx ky kz la bi translated">numba 允许运行时编译函数来优化单机代码。这意味着如果你打算多次调用一个函数，你可以通过在第一次调用时编译这个函数来显著减少计算时间。这样，numba对于加速单个任务是有用的。</li><li id="57ca" class="ks kt hh ir b is lb iw lc ja ld je le ji lf jm kx ky kz la bi translated"><a class="ae kr" href="http://dask.pydata.org/en/latest/" rel="noopener ugc nofollow" target="_blank"> dask </a>被宣传为大规模(通常是核外或分布式)计算的“并行计算库”。dask的核心是一系列任务调度器——用于确定何时以及如何运行各种用户定义的计算“任务”的算法；因此，dask可以自动识别哪些任务可以并行运行，或者根本不运行。使用dask的调度程序使我们能够<em class="jn">扩展到一个包含许多相关任务的网络中，并且只高效地计算那些我们需要的输出，即使是在一台机器上。</em></li></ul><p id="cbc2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将会看到，将dask和numba合并到我们的python代码中可以像将function<a class="ae kr" href="https://www.python.org/dev/peps/pep-0318/" rel="noopener ugc nofollow" target="_blank">decorator</a>应用到我们预先存在的函数中一样简单。因此，这些库可以以强大的方式组合起来，以产生高效的数据科学工作流，该工作流与通用的<a class="ae kr" href="https://www.scipy.org/stackspec.html" rel="noopener ugc nofollow" target="_blank"> SciPy堆栈</a>集成，并且不需要额外的工具或架构。</p><p id="fdcb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">特别是，这里我们将重点关注单机模型评分(而不是模型拟合)，这是在给定一组适当输入的情况下计算模型预测的过程。例如，在金融服务中，可能有大量的模型构建在彼此之上，大量的时变输出与每个模型相关联，以及大量可以打开/关闭的可能“覆盖”。编写既在单个模型级别有效又在许多模型需要同时运行时有效的代码可能很棘手，但是使用dask和numba，认知开销是最小的。</p><p id="c4b5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将从简单介绍numba的<code class="du lg lh li lj b">guvectorize</code>开始，我们使用它来加速单个模型的模型评分。对于本文的讨论，熟悉python中的函数修饰符和代码编译的概念是很有用的。接下来，我们在一个更大的环境中想象我们的单个模型，其中有可能需要并行运行的多个模型和特征创建步骤。为此，我们引入<code class="du lg lh li lj b">dask.delayed</code>并演示如何使用dask调度程序来提高计算效率。这里的所有示例都是玩具示例，所提供的计时实验旨在强调各种可能性——显然，性能提升是高度特定于应用的。</p><p id="26be" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所有代码都可以复制/粘贴到IPython会话中。如果您对重新运行任何代码感兴趣，我们使用的包版本是:</p><pre class="lk ll lm ln fd lo lj lp lq aw lr bi"><span id="6872" class="ls jp hh lj b fi lt lu l lv lw">dask version: 0.14.1<br/>numba version: 0.34.0<br/>numpy version: 1.13.1</span></pre><p id="b46f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">数字巴</strong></p><p id="7519" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们从一个简单的例子开始:一个接收4个输入并返回15个输出的模型。对于上下文，假设我们预测一个随时间变化的量，这个函数产生未来15个月的预测。</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="c419" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在第一次调用该函数时，numba将<em class="jn">将该函数编译</em>成机器码(<strong class="ir hi">j</strong>ust<strong class="ir hi">I</strong>n<strong class="ir hi">t</strong>ime”)，以便后续调用<em class="jn">显著</em>更高效。演示<code class="du lg lh li lj b">jit</code>的简单用法(完整文档可在<a class="ae kr" href="http://numba.pydata.org/numba-doc/0.34.0/user/jit.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到):</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="0171" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们看到，在这种情况下,<code class="du lg lh li lj b">jit</code>为我们提供了1.3倍的加速，可以获得25，000个观察值，而没有以任何实质性的方式改变我们的代码。注意，为了正确编译，numba需要推断输入的类型。在这种情况下，<code class="du lg lh li lj b">jit</code>会在运行时推断出类型并相应地编译；或者，我们可以提供类型签名，我们很快就会看到。</p><p id="e70e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">观察力敏锐的读者可能会进一步追问:我们正在对输入的numpy数组的每个元素进行相同的计算，是否有某种方法可以告诉numba利用这一点，类似于<code class="du lg lh li lj b">.apply()</code>调用？事实上是有的；numba的<code class="du lg lh li lj b">guvectorize</code> decorator允许我们编写逐行操作的代码，它将适当地实现计算。让我们看看在我们的例子中应用<code class="du lg lh li lj b">guvectorize</code>装饰器是什么样子的:</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="214f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们应该强调一下与<code class="du lg lh li lj b">guvectorize</code>的一些不同之处；首先要观察的是<code class="du lg lh li lj b">guvectorize</code>装饰器需要两个参数:一个类型签名和一个形状签名。在这种情况下，我们的类型签名包括64位整数、64位浮点、64位浮点数组和一个布尔值(分别为<code class="du lg lh li lj b">i8</code>、<code class="du lg lh li lj b">f8</code>、<code class="du lg lh li lj b">f8[:]</code>和<code class="du lg lh li lj b">b1</code>)。如果我们用不可强制的值调用函数，就会抛出一个<code class="du lg lh li lj b">TypeError</code>。</p><p id="8bfd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">形状签名告诉numba输入的相对形状，其中<code class="du lg lh li lj b">() </code>表示标量。这里有一些技术标注:</p><p id="4ab8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> </strong>每一个额外的输出维度必须有<em class="jn">至少一个与期望输出形状相同的</em>输入；例如，在上面的例子中，我们实际上没有15列的输入，但是我们的输出需要有15列。所以我们需要在调用<code class="du lg lh li lj b">fast_predict_over_time</code>之前预分配输出形状，并将预分配的数组作为我们的参数之一(对应上面的下划线)。</p><p id="2316" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> </strong>对于用<code class="du lg lh li lj b">guvectorize</code>修饰的函数，<em class="jn">输出</em>需要作为函数的最终<em class="jn">自变量</em>，但在实际调用中并不需要</p><p id="2954" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> </strong>此外，注意函数没有返回语句。这是由于numba如何编译代码，超出了本文的范围。让我们看看这在实践中是如何工作的:</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="fa35" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这种情况下，使用<code class="du lg lh li lj b">guvectorize</code>可以使我们的速度比<code class="du lg lh li lj b">jit</code>快3.5倍，比标准实现快5倍，而这一切都不需要使用python！此外，请注意，我们的代码是用“观察级语义”编写的，因为我们编写函数时，就好像它接收来自单个观察的输入。因此，对我们的代码进行推理感觉很自然。</p><h1 id="f5a8" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">+= <code class="du lg lh li lj b">dask</code></h1><p id="4b6f" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">现在我们想象我们上面的模型被集成到一个更大的建模管道中；特性<code class="du lg lh li lj b">x, y</code>和<code class="du lg lh li lj b">z</code>是从其他(可能是复杂的)任务中创建的，我们还希望对其他模型进行评分。为了使讨论更加具体，假设我们有以下工作流:</p><figure class="lk ll lm ln fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lz"><img src="../Images/f38d6454a5f21ab484b5ddd2356c40be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7x4-4gE4KOEIPF3AVD-Kg.png"/></div></div></figure><p id="1500" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该任务图中的每个方块代表一个具体的输入或具体的输出；椭圆形代表函数。小写字母代表用户输入。我们可以看到两个特征创建步骤(<code class="du lg lh li lj b">feature_a</code>和<code class="du lg lh li lj b">complicated_feature_b</code>)，它们以两种不同的方式与其他变量一起输入到我们的<code class="du lg lh li lj b">fast_predict_over_time</code>模型中。还有另外一款<code class="du lg lh li lj b">predict_another_thing</code>是评分产生<code class="du lg lh li lj b">Output 1</code>。</p><p id="9f05" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">dask允许我们首先建立这个任务图<em class="jn">而不评估任何代码</em>(所有计算将<em class="jn">延迟</em>)。每当我们需要这些输出中的一个(或全部)时，我们告诉dask去计算它，它将决定最有效的处理方式。这种计算策略通常被称为“懒惰评估”。使用dask进行惰性评估的一个主要优点是，我们将只计算我们所请求的输出所必需的内容。例如，如果我们请求上图中<code class="du lg lh li lj b">predict_another_thing</code>的输出，那么<code class="du lg lh li lj b">fast_predict</code> <em class="jn">将永远不会被调用或计算</em>。对于越来越大的任务图，这可以节省大量内存和时间。</p><p id="8fe2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们通过展示如何使用dask延迟<code class="du lg lh li lj b">fast_predict_over_time</code>的计算并继续设置其他模型来使讨论更加具体:</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="38d4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">观察到运行时间也略有增加；这是由dask实际调度任务所需的少量开销造成的。不同的dask调度器有不同的开销，默认为多线程<code class="du lg lh li lj b">dask.threaded.get</code>。</p><p id="0142" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">转到其他模型，我们可以使用<code class="du lg lh li lj b">delayed</code>装饰器将它们声明为<code class="du lg lh li lj b">delayed</code>计算对象；然后我们通过调用实际的<em class="jn">任务</em>来调度它们，就像调用其他函数一样。唯一的区别是，在我们额外调用<code class="du lg lh li lj b">.compute()</code>之前，这些不会被计算:</p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="f2b4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，在这一步绝对没有执行<em class="jn">计算。每当我们需要来自模型评分引擎的输出时，我们可以简单地键入<code class="du lg lh li lj b">results</code>并调用<code class="du lg lh li lj b">.compute()</code>(如果我们想要多个输出，我们可以使用顶级的<code class="du lg lh li lj b">dask.compute(*args)</code>，它将返回一组请求的输出)。例如，让我们查询任务图中的<code class="du lg lh li lj b">w_overlay</code>和<code class="du lg lh li lj b">predict_another_thing</code>，看看计时会发生什么:</em></p><figure class="lk ll lm ln fd ii"><div class="bz dy l di"><div class="lx ly l"/></div></figure><p id="5805" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请注意，我们的计算时间远低于计算<code class="du lg lh li lj b">complicated_feature_b</code>所需的1分钟，因为dask知道我们从来不需要计算它！如果我们在这里编写过程化风格的代码，那么要避免<code class="du lg lh li lj b">complicated_feature_b</code>的1分钟瓶颈会困难得多。这也降低了我们的<em class="jn">内存</em>需求，因为我们从来不需要在内存中保存所有的输出。</p><h1 id="89da" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">最后的想法</h1><p id="a34f" class="pw-post-body-paragraph ip iq hh ir b is km iu iv iw kn iy iz ja ko jc jd je kp jg jh ji kq jk jl jm ha bi translated">dask和numba允许我们显著加快函数执行速度，并创建内存高效的流水线。此外，这是在不脱离python生态系统和不改变我们的架构需求的情况下实现的。当然，这些并不是简单的解决方案，它们可以加速一切现成的东西——明智的应用对于实际编写高性能代码至关重要。</p><p id="2e5f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">披露声明:这些观点是作者的观点。除非本帖中另有说明，否则Capital One不属于所提及的任何公司，也不被其认可。使用或展示的所有商标和其他知识产权都是其各自所有者的所有权。本文为2017首都一。</em></p></div></div>    
</body>
</html>