# 在 Pinterest 上使用 Apache Spark SQL 进行交互式查询

> 原文：<https://medium.com/pinterest-engineering/interactive-querying-with-apache-spark-sql-at-pinterest-2a3eaf60ac1b?source=collection_archive---------2----------------------->

Sanchay Javeria |软件工程师，大数据查询平台，数据工程

Ashish Singh |数据工程大数据查询平台技术主管

为了实现通过我们的视觉发现引擎为每个人带来灵感的使命，Pinterest 在很大程度上依赖于做出数据驱动的决策，以改善超过 4.75 亿月活跃用户的 Pinner 体验。可靠、快速且可扩展的交互式查询对于实现这些数据驱动的决策至关重要。过去，我们在 Pinterest 上发表过 [Presto 如何服务于这个功能。在这里，我们将分享如何使用 Apache Spark SQL 构建一个可伸缩、可靠、高效的交互式查询平台，每天处理数百 Pb 的数据。通过对各种架构选择、挑战以及我们应对这些挑战的解决方案的详细讨论，我们分享了我们如何通过 Spark SQL 成功实现交互式查询。](/pinterest-engineering/presto-at-pinterest-a8bda7515e52)

# 计划查询与交互式查询

查询是用户从 Pinterest 的数据中获取理解的最受欢迎的方式。这种分析的应用存在于所有商业/工程功能中，如机器学习、广告、搜索、家庭反馈推荐、信任和安全等。提交这些查询主要有两种方式:预定的和交互的。

1.  **预定查询**是按预定义的频率运行的查询。这些查询通常有严格的服务级别目标(SLO)。
2.  **交互式查询**是在需要时执行的查询，通常不会按照预先定义的节奏重复。与预定查询不同，用户等待交互式查询完成，并不知道可能导致查询失败的潜在问题。这些特征使得交互式查询平台的需求不同于预定的查询平台。

在接下来的章节中，我们将深入探讨如何在 Pinterest 上使用 Spark SQL 扩展交互式查询。我们首先讨论如何在 Pinterest 使用 Spark SQL，以及使用 Spark SQL 进行交互式查询所面临的挑战。接下来，我们将介绍该架构，并讨论我们如何应对一路走来所面临的挑战。

# Spark SQL 交互式查询

我们支持 Hive、Presto 和 Spark SQL 来查询数据。然而，我们正在贬低 Hive 而支持 Spark SQL，给我们留下了两个主要的查询引擎(即 Presto 和 Spark SQL)。Presto 用于快速交互查询，本[帖子](/pinterest-engineering/presto-at-pinterest-a8bda7515e52)对此进行了介绍。Spark SQL 用于所有计划的查询(在 Hive 弃用完成后不久)和大型数据集上的交互式查询。下面是我们在从 Hive 迁移到 Spark SQL 时考虑的支持使用 Spark SQL 进行交互查询的各种方法。

## Apache Spark 的节俭 JDBC/ODBC 服务器

Apache Spark 的节俭 JDBC/ODBC 服务器(STS)类似于 HiveServer2，允许客户端通过 JDBC/ODBC 协议执行 Spark SQL 查询。JDBC/ODBC 协议是各种客户端提交查询的最流行方式之一。使用 STS 将允许现有的 JDBC/ODBC 协议支持工具与 Spark SQL 无缝协作。然而，这种方法不能在提交给同一节俭服务器的查询之间提供适当的隔离。

单个查询的问题可能会影响在同一节约服务器上运行的所有其他查询。在过去使用 Hiveserver2 进行交互查询时，我们看到了几个问题，其中一个错误的查询导致整个服务器崩溃，导致所有并发运行的查询终止/失败。在大多数情况下，这是由于单个查询以本地模式运行，而查询优化占用了太多内存，或者是由于加载本机 jar 的查询导致服务器上的内核死机。根据我们的经验，我们决定不选择这种方法。

## Spark SQL 查询作为 Apache 纱线上的 shell 命令应用程序

另一个运行 Spark SQL 查询的通用机制是通过 spark-sql 命令行界面(CLI)。但是，对于交互式应用程序而言，CLI 方法不能很好地工作，也不能提供最佳的用户体验。

可以构建一个服务，从各种客户端启动 spark-sql CLI，作为我们的纱线集群上的 shell 命令应用程序。但是，这将导致等待在纱线集群上分配容器，然后为每个查询启动火花会话的前期成本。此过程可能需要几分钟，具体取决于群集上的资源可用性。

例如，这种方法会导致交互查询的用户体验较差，因为用户可能需要等待几分钟才能找到语法问题。此外，这种方法很难检索结果、提供语句级进度更新，或者在出现故障时很难从驱动程序日志中获取异常堆栈跟踪。这些是我们对于出色的交互式查询体验的一些要求。

## 具有批处理会话的 Apache Livy

[Apache Livy](https://livy.apache.org/) 是一项通过 RESTful 界面与 Spark cluster 进行交互的服务。使用 Livy，我们可以轻松地将 Spark SQL 查询提交给我们的纱线集群，并通过简单的 REST 调用来管理 Spark 上下文。这是对我们复杂的 Spark 基础设施的理想抽象，并且将允许与面向用户的客户端直接集成。

Livy 提供两个作业提交选项:批处理和交互。批处理模式类似于 spark-submit，用于提交批处理申请。在批处理模式下，查询的所有语句都一起提交以供执行。这使得我们为交互式查询设想的一些可用性特性变得很困难，例如:对在哪里运行基于语句的查询做出不同的选择，支持用 SQL 语句更改 spark 会话的功能，以及创建可重用的用户会话/缓存。我们将在本文后面详细讨论这些功能。

## 具有交互会话的 Apache Livy

与 Apache Livy 的批处理会话不同，交互会话使我们能够启动一个会话，将查询和/或语句作为单独的请求提交，并在完成后显式结束该会话。

此外，Livy 还提供了多租户、通过会话恢复实现的高可用性以及故障隔离，这些都是我们最优先考虑的体系结构。这帮助我们选择 Livy 作为在 Pinterest 进行交互式 Spark SQL 查询的理想解决方案。

# 体系结构

下面的图 1 描述了 Spark SQL 的查询执行体系结构和交互查询用例的请求流的概述。

![](img/8cc56ab0e1c0fe596dd58cfebe4543ac.png)

*Figure 1: Request flow for scheduled and interactive querying with Spark SQL at Pinterest*

图中提出的一个显而易见的问题是*为什么*我们需要分别处理 DDL 和 DML 查询。我们将在后面的文章中讨论这个体系结构的这个和其他有趣的方面，同时讨论我们在成功使用 Spark SQL 进行交互查询时所面临的挑战以及我们如何解决这些挑战。图 1 中的控制流程在下面详细说明，用于交互式 DML 和 DDL 查询。

## 交互式 DML 查询

1.  像 [Querybook](http://querybook.org/) 和 Jupyter 这样的客户向 Livy 提交交互式 DML 查询。
2.  Livy 从纱线资源管理器(RM)请求一个容器来运行远程火花上下文(RSC)客户端。
3.  RM 分配一个容器，RSC 客户端在其中启动。该 RSC 客户端随后启动 RSC 驱动程序程序。
4.  Livy 通过与 RSC Client 通信来跟踪查询进度，RSC Client 运行驱动程序。
5.  Spark SQL 驱动程序从 Hive Metastore 服务(HMS)获取查询规划所需的表元数据。
6.  基于资源需求，驱动程序要求 RM 提供容器以启动执行程序。
7.  Spark SQL 驱动程序分配任务并协调执行程序之间的工作，直到完成用户查询的所有 Spark 作业。

## 交互式 DDL 查询

1.  客户端向 Livy 提交交互式 DDL 查询。
2.  Livy 从本地会话池中获取一个 Spark 会话(细节将在后面的章节中讨论),并作为当前请求用户正确地更新用户凭证。
3.  本地 Spark SQL 驱动程序从 HMS 获取查询计划的表元数据，并根据需要执行 DDL 操作。

# 挑战和我们的解决方案

本节讨论我们必须解决的各种挑战，以便在 Pinterest 上成功地使用 Spark SQL 进行交互式查询。

## 无缝查询提交

虽然 Livy 提供了一个可靠的解决方案来提交查询作为 Spark 作业，但是我们需要用户使用一个标准接口从任何客户端提交查询，该接口可以用作一个插件依赖项，以便轻松地与 Livy 通信。

我们在 Livy 上构建了一个通用的符合 DB-API 的 Python 客户机，名为 BigPy，多个查询客户机使用它来提交查询。在 BigPy 中，我们提供了一个接口来实现以下功能:

*   状态轮询:它监视 Livy 会话的状态，并向客户机报告应用程序是成功、失败还是当前正在运行。此外，我们报告了 spark 应用程序的完成百分比。
*   跟踪链接:它返回所有跟踪链接来监控 Spark 应用程序的状态，包括到 Spark UI、驱动程序日志和 Dr. Elephant 的链接，后者用于监控性能和调优 Spark 应用程序。
*   结果检索:它提供了以分页方式从 AWS S3 这样的对象存储中检索查询结果的能力。
*   异常检索:Spark 驱动程序和执行器日志经常会很嘈杂，查找查询失败的原因会很麻烦。BigPy 返回异常，它的堆栈直接跟踪到客户端，以获得更轻松的调试体验。

BigPy 支持跨多个不同系统与 Livy 交互的模块化方式，提供了与客户端代码的清晰分离。

## 快速元数据查询

spark-shell 实用程序在集群模式下向 RM 发送一个纱线应用请求。RM 启动应用程序主机(AM ),然后应用程序主机启动驱动程序。驱动程序进一步向 RM 请求更多用于启动执行器的容器。我们发现，这种资源分配过程可能需要几分钟的时间来开始处理每个查询，这大大增加了数据定义语言(DDL)/仅元数据查询的延迟，这些查询通常是低延迟的元存储操作。

DDL 查询在驱动程序上执行，不需要额外的执行器或与 DML 查询相同的隔离。为了减轻容器分配对 YARN 集群和 Spark 会话启动时间的冗余延迟，我们在 Apache Livy 中实现了一个本地会话池，它维护一个以本地模式运行的 Spark 会话池。

这个问题有两个部分:1)将查询识别为 DDL 语句，2)实现 Spark 应用程序的缓存池来处理这些查询。我们利用“SparkSqlParser”来获得用户查询的逻辑计划，以识别 DDL 查询。由于这个逻辑计划只是一个从“TreeNode”类继承的逻辑操作符树，我们可以很容易地遍历这个树，并根据一组 DDL 执行命令检查每个节点的类。如果逻辑计划的所有节点都与 DDL 命令匹配，我们将该查询标识为 DDL。实际上，它看起来像这样:

![](img/90d88afde5a7af45d3a31cf859252a1f.png)

一旦我们知道查询是一个 DDL 语句，我们就将它路由到一个缓存的 Spark 应用程序。我们在 Livy 中构建了这个缓存应用程序池，由一个本地运行的 Spark 驱动程序池表示。它设计为完全自力更生，具有以下特点:

*   陈旧应用程序的自动垃圾收集和启动新应用程序
*   一个守护线程，监视池的运行状况，并将查询路由到下一个可用的应用程序
*   以可配置的步调重新启动应用程序，以确保它获取最新的资源(例如模式 jar ),从而确保数据的新鲜
*   在启动时异步启动轻量级元数据操作，以初始化“SparkContext ”,并建立到 metastore 的实时连接，以实现更快的后续操作

通过这种设计，我们将查询延迟从 70 秒减少到平均 10 秒(大约 6.3 倍的改进)。

![](img/a3b9ab0df9d32704cea1fb06b0cfc11d.png)

*Figure 2: Wall clock time comparison for DDL queries run in local vs. cluster mode*

## 快速失败:更快的语法检查

以集群模式运行每个查询的另一个缺点是，在最坏的情况下，语法检查至少需要启动应用程序所需的时间。在临时环境中，用户通常希望语法问题早点出现，而等待几分钟才报告一个语法问题会带来令人沮丧的体验。我们通过使用“SparkSqlParser”来改进这一点，并在启动 YARN 应用程序之前获取查询的逻辑计划。如果查询包含语法错误，解析器将在生成逻辑计划时抛出“ParseException ”,并方便地返回行号和列号，我们将这些信息报告给客户端。通过这种方式，我们将整体语法检查延迟从几分钟减少到了不到两秒钟(提高了 30 倍以上)。

## 错误处理建议

在特定环境中，查询失败是不言而喻的。然而，修复这些故障通常是一个令人生畏的循环:浏览驱动程序日志，通过自我诊断或寻求外部帮助找到解决方案，然后重试查询。为了简化这一过程，我们提供了一些常见问题的自动故障排除信息，这些问题乍一看很难解决。该解决方案包括四个部分:

**I .根据最后一个查询的执行状态，使 YARN 应用失败**

集群模式下 Livy 交互会话的一个问题是，它们总是向 YARN AM 报告“成功”状态。这是因为 Livy 提交给“SparkLauncher”的远程驱动程序启动了一个 Spark 上下文，在该上下文中运行一些查询，然后关闭该上下文。不管查询运行的状态如何，报告的最终状态将始终是 SparkContext 是否能够成功关闭。这是对用户和平台主的误导。为了减轻这个问题，我们在单个交互式会话中跟踪最终查询运行的状态，如果查询失败，则在远程驱动程序中抛出运行时异常。这有助于正确地向 AM 报告状态，并用故障原因(如果有的话)填充纱线诊断。

**二。识别用户查询中的常见错误**

一旦我们用查询的失败原因正确地填充了 YARN 诊断，我们就可以利用添加到 YARN 集群中的额外日志来方便地在 SparkSQL 表中跟踪遇到的错误。然后，我们查看了失败堆栈跟踪的历史，并使用正则表达式对它们进行了分类。根据频率，我们获得了前 n 名错误的列表。

我们利用[大象博士](https://github.com/linkedin/dr-elephant)来跟踪 Spark 应用启发和度量，并添加了一个错误分类机制，该机制查看应用的纱线诊断信息，并基于正则表达式引擎对其进行分类。使用上面的正则表达式，我们将通过 REST API 暴露的常见错误的故障排除信息添加到 Dr. Elephant web UI 和其他外部客户端，如 Querybook。

**三世。Livy 中的大象博士集成**

我们在 Livy 中为每个启动的 Spark 应用程序集成了上面提到的 Dr. Elephant API。该端点在每次查询运行时返回给客户端，便于查看故障排除信息。

**四。客户端集成**

在从 Livy 获取 Dr. Elephant 故障排除分析端点之后，客户机从 API 中提取该信息，并将其显示在查询日志中。这样，当我们看到查询失败时，我们可以提供常见错误的故障排除信息，帮助用户更快地诊断问题。

## 资源利用率可见性

查看我们的 ad-hoc 集群上的历史内存消耗指标，我们注意到应用程序经常过度分配执行器和驱动程序内存，导致不必要的资源浪费。另一方面，对于内存不足(OOM)的应用程序，我们的用户经常要求我们让他们更容易地抢先捕捉这些问题，以便更快地返回他们的查询。

为了解决这个问题，我们直接在客户机上显示实时内存消耗信息，包括所有执行器使用的最大、最小和平均内存。我们还标记消费不足和过度消费，并提示用户根据启发采取行动。

我们通过使用 [Spark 指标库](https://spark.apache.org/docs/latest/monitoring.html#metrics)的自定义指标接收器来收集每个 Spark 应用程序的实时内存消耗信息。然后，我们在 BigPy 中使用这些指标，并检查它们是否违反了任何资源阈值，以 UI 友好的减价表格式将信息返回给客户端。这种方法的一个例子可以在下面的 GIF 中的 Querybook 上看到:

![](img/ffa7ddc74739559612facb476adb3066.png)

*Figure 3: Realtime driver/executor(s) memory consumption information with various aggregations*

## 大型结果处理和状态跟踪

默认情况下，Livy 对查询结果集的限制是 1，000 行。增加这个限制并不理想，因为结果集存储在内存中，增加这个限制可能会在像我们这样的内存受限环境中导致大规模问题。为了解决这个问题，我们为每个查询的最终结果实现了 AWS S3 重定向。这样，大型结果集可以以多部分的方式上传到 S3，而不会影响服务的整体性能。在客户端，我们稍后检索 REST 响应中返回的最终 S3 输出路径，并以分页的方式从 S3 获取结果。这使得在列出路径对象时检索速度更快，而没有 S3 超时的风险。这种重定向也可以在查询级别进行配置，因此如果用户希望查询返回少于 1，000 行的数据，可以直接从 REST 端点进行检索，而不需要对文件存储进行额外的调用。

我们还提供实时进度更新，这是通过对 Spark SQL 查询的已完成和活动任务数与任务总数进行平均而获得的。在上面图 3 的 GIF 中可以看到一个预览。

## Livy 操作改进

我们平均每天看到大约 1，500 个临时 SparkSQL 查询，为了支持这一负载，我们的系统必须为我们的用户保持健康和可靠。我们对可靠性和稳定性进行了大量改进，使我们能够为 Livy 维持 99.5%的正常运行时间 SLO。一些重要亮点:

**有效负载均衡**

根据设计，Livy 是一个有状态的 web 服务。它在内存中存储会话的状态，如查询运行、每个查询的状态、最终结果等等。因为我们的客户端遵循 HTTP 轮询机制来获取这些属性，所以很难在上面添加经典/应用负载平衡器。为了解决这个问题，我们在应用程序级别实现了负载平衡算法，以循环方式将每个查询路由到最不繁忙的 Livy 实例。在这里,“繁忙”是由在特定 Livy 实例上运行的“活动”会话的数量来定义的。这种简单而有效的机制使我们能够在整个车队中更均匀地分配负载。

**指标&日志记录改进**

我们向 Livy 添加了事件监听器支持，其中事件被定义为任何 Livy 活动，包括会话创建和向会话提交语句。我们使用这些侦听器将 JSON 对象记录到本地磁盘，跟踪各种事件。无论何时出现问题，这都可以实现更快的调试和使用监控。

**指标**

我们还使用 [Scalatra 指标](https://scalatra.org/guides/2.4/monitoring/metrics.html)来跟踪关键的服务水平指标，例如健康检查、MAU、用户/查询的 DAU 计数、缓存的会话命中率、查询成功率等等。这些顶级指标对于跟踪跨集群的整体临时活动非常重要。

# 摘要

为了支持使用 SQL 分析和处理数百 Pb 的数据，我们正在 Pinterest 上整合 Spark SQL 和 Presto。虽然 Presto 仍然是资源需求有限的快速交互式查询的最流行的查询引擎选择，但我们使用 Spark SQL 来支持所有规模的查询。交互式查询用例与预定查询有不同的需求。这些特性包括无缝查询提交、快速元数据查询、快速语法检查以及更好的调试和调优支持。基于我们对交互式查询的需求和可用开源解决方案提供的功能，我们决定用 Apache Livy 构建 Spark SQL 交互式查询平台。然而，Livy 并没有满足我们开箱即用的要求，我们添加了各种功能来弥合这一差距。在这篇文章中，我们讨论了我们的架构选择和改进，以使交互式查询在 Pinterest 上取得成功。我们计划将这些变化的大部分回馈给开源社区。

# 感谢

让 Spark SQL 交互式查询获得成功需要 Pinterest 许多团队的努力。特别感谢大数据查询平台团队的 Zaheen Aziz、TPM 团队的 Hannah Chen、数据隐私团队的 Keith Regier、SRE 团队的 Rakesh Kalidindi and、Ashim Shrestha 以及批处理平台团队的 Zirui Li、Daniel Dai and、Soam Acharya。这是一项巨大的努力，如果没有管理层的帮助，这是不可能的。感谢金柱成、王春燕和戴夫·伯吉斯给予的坚定支持和指导。

*要在 Pinterest 了解更多关于工程的知识，请查看我们的* [*工程博客*](https://medium.com/pinterest-engineering) *，并访问我们的*[*Pinterest Labs*](https://labs.pinterest.com/?utm_source=medium&utm_medium=blog-article&utm_campaign=singh-javeria-july-12-2021)*网站。要查看和申请空缺职位，请访问我们的* [*职业*](https://www.pinterestcareers.com/?utm_source=medium&utm_medium=blog-article&utm_campaign=singh-javeria-july-12-2021) *页面。*