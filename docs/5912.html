<html>
<head>
<title>Benchmarking PyTorch on OCI &amp; EfficientNet’s Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">根据OCI和效率网的模型对PyTorch进行基准测试</h1>
<blockquote>原文：<a href="https://medium.com/oracledevs/benchmarking-pytorch-on-oci-and-efficientnet-models-1d729b45d503?source=collection_archive---------0-----------------------#2022-05-23">https://medium.com/oracledevs/benchmarking-pytorch-on-oci-and-efficientnet-models-1d729b45d503?source=collection_archive---------0-----------------------#2022-05-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a488" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">欢迎阅读本系列的第四篇文章，我们将深入探讨神经网络。</p><p id="632f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将讨论PyTorch在不同参数下的效率和性能，以及这些参数如何影响模型的训练时间。</p><p id="5b7f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从受欢迎程度的角度来看，我从Google Trends中提取了这些信息来分析PyTorch和Tensorflow的受欢迎程度:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/4c6b2809d8d7eb3a1637b5261f70b8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gfTbL2vW18rO8qqh"/></div></div></figure><p id="15c4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如我们所见，张量流现在统治着世界。让我们看看性能是否符合预期。</p><h1 id="ea56" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">机器规格</h1><p id="2e0a" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">为了进行这些测试，我们需要确保我们测试的硬件具有与下一篇文章中相同的规格，在下一篇文章中我们将分析TensorFlow的性能。</p><p id="6934" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，建议在私有/专用基础设施中执行测试，这意味着我们理想情况下需要:</p><ul class=""><li id="372f" class="kr ks hh ig b ih ii il im ip kt it ku ix kv jb kw kx ky kz bi translated"><strong class="ig hi">专用基础设施，而不是使用共享基础设施— </strong>这意味着最好是只给你一个OCI计算实例，而不给其他人。例如，这会阻止CPU与其他OCI用户虚拟共享资源。这反过来会减少I/O中断，从而使我们的基准测试更加准确。</li><li id="ea70" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated"><strong class="ig hi">一个共享笔记本会话— </strong>这样操作系统就不会通过CPU调度程序意外地给予一个Jupyter / Zeppelin项目比另一个更高的优先级。</li></ul><p id="215f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该测试的机器规格如下:</p><ul class=""><li id="6202" class="kr ks hh ig b ih ii il im ip kt it ku ix kv jb kw kx ky kz bi translated"><strong class="ig hi"> OCI计算形状:</strong> VM.Standard.E3.Flex这是一个很棒的形状，可能是我最喜欢的形状之一，因为它允许我们选择可变数量的CPU/DRAM(最多100个OCPUs)。</li><li id="45a0" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated">OCPUs数量: 16</li><li id="6a98" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated"><strong class="ig hi">内存容量(GB): </strong> 32</li><li id="a4a8" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated"><strong class="ig hi">块存储大小(GB): </strong> 100。我们不需要那么多，但这是我们在OCI能选择的最低数量。当我们使用OCI数据科学时，我们可以创建高达10，240 GB (10TB)的笔记本会话，如果您的使用案例需要这么大容量的话。</li></ul><p id="5205" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我选择使用CPU版本运行，不像以前的文章那样使用GPU实例，只是为了节省一些钱。如果您有兴趣找出大幅提高GPU吞吐量的细节，请告诉我，我会确保在未来的文章中写一些关于它的内容。</p><h1 id="3bdc" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">使用有效网络模型</h1><p id="17ab" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">为了提高任何模型的基准性能，我们还需要考虑我们的竞争对手是谁。当然，用几个层创建我们自己的模型并看到我们的工作在一些数据集的压力下进行是很棒的。但我相信数据科学的魅力，也是最困难的部分，是知道在哪里寻找更好的东西，也许是由更有经验的人来做。让我们面对现实:在我们尝试的任何事情上，总会有比我们更好的人，所以为什么不利用这个事实呢？互联网让我们不断寻找更好的东西。</p><p id="2470" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了衡量PyTorch的性能，我们将使用<a class="ae lg" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank">一种叫做EfficientNet </a>的深度神经网络，它已经在几个图像处理数据集上进行了测试。这是一个预训练的卷积神经网络，它试图系统地改变人们如何设计和构建他们自己的模型。在这种情况下，EfficientNet专注于使用称为c的系数在图像的所有维度(深度、宽度、分辨率)上应用自己的特定缩放。使用该系数已被证明效果非常好(其变体之一称为EfficientNet 7，在ImageNet数据集上具有最高的准确性)，比顶级列表中的其他模型小约8倍(快约6倍)。除了能够根据ImageNet数据集准确预测结果(这本身可以被认为是一项极其困难的任务)，它在其他众所周知的数据集上表现非常好，如<a class="ae lg" href="https://paperswithcode.com/dataset/cifar-100" rel="noopener ugc nofollow" target="_blank"> CIFAR100，一个包含动物图像的数据集</a>，Flowers datase，t和其他三个参数比该模型的竞争对手少得多的数据集。</p><p id="f4e7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">将EfficientNet模型视为NNs的设计和架构部分的自动化。这种自动化非常有帮助，不仅对于我们的特定用例，而且对于您在作为数据科学家、数据分析师或类似角色的职业生涯中会发现的一些图像处理问题也是如此。</p><p id="c116" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">EfficientNet使用数百万个参数，因此可以被视为深度神经网络。我想提一下漏失层(我们在上一篇文章中应用的)对显著提高神经网络精度的影响。在这里，我们可以通过观察哪些神经元被激活，哪些神经元被去激活，以及这种隐藏层实现如何试图减少由不准确的预测产生的噪声，从而提高模型的训练/测试准确性，来图形化地观察当实现时丢弃层的效果。<a class="ae lg" href="https://www.researchgate.net/publication/343232588_Implementation_of_Dropout_Neuronal_Units_Based_on_Stochastic_Memristive_Devices_in_Neural_Networks_with_High_Classification_Accuracy/figures?lo=1" rel="noopener ugc nofollow" target="_blank">样本图像</a>使用MNIST数据集进行测量:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lh"><img src="../Images/4ea460c258891d9d53e8fc8f5f5f1a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/0*wewP1vB0XK1Lkyxk"/></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es li"><img src="../Images/20cb1db4134acb95b40a7793ff18db95.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*lEyagtttNF28HAwO"/></div></figure><p id="a5fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">人们可能会认为EfficientNet是一个“简单”的模型，但在现实中，我们需要考虑到，由于训练是由计算机自动完成的，每秒钟执行数百万次操作，这导致像这样的模型有数百万个参数。我从<a class="ae lg" href="https://github.com/tensorflow/tpu" rel="noopener ugc nofollow" target="_blank">最初的TensorFlow资源库</a>(其中有<a class="ae lg" href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet" rel="noopener ugc nofollow" target="_blank">一些关于EfficientNet </a>的文档)中提取了一些可视化效果，并在这里展示给你，这样你就可以对像这样的模型的“规模”有一个感觉。希望这也能让你看到EfficientNet的新系数C与其他模型相比有多好。</p><p id="a8af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与其他模型相比，第一幅图考虑了数百万个参数。第二张图比较了模型进行预测时所执行的浮点运算的数量。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lj"><img src="../Images/702061dfee38f14ce07e97e228e8222a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*alVwYSz9pk0WSJFN"/></div></div></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lj"><img src="../Images/a66ab493d824caa6df3ceea758658bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w3KbYZkTqxnH7EqO"/></div></div></figure><h1 id="fe93" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">可用的基准工具</h1><p id="2c75" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">执行基准测试有几个选项。当然，我们总是可以使用Python提供的标准库来帮助我们，或者选择像<a class="ae lg" href="https://github.com/tensorflow/benchmarks/tree/master/perfzero" rel="noopener ugc nofollow" target="_blank"> PerfZero </a>这样更高级的方法。在这种情况下，我们将避免复杂的库，因为学习如何以正确的方式执行基准测试比学习如何使用特定的库/工具更重要。随着技术的变化，我总是说最重要的事情是在思想上回到我们头脑中的基本概念和基本想法，然后深入探索和尝试我们用例中需要的细节。</p><p id="d702" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我测试了一个非常适合初学者的包，它是我从PyPi找到的，名为<a class="ae lg" href="https://pypi.org/project/pytorch-benchmark/" rel="noopener ugc nofollow" target="_blank"> pytorch-benchmark </a>，使用标准的时间度量。给<a class="ae lg" href="https://github.com/LukasHedegaard/" rel="noopener ugc nofollow" target="_blank">卢卡斯·赫泽高</a>的大礼包的道具。</p><p id="b72c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，让我们安装库:</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="beb6" class="lp jp hh ll b fi lq lr l ls lt">pip install pytorch-benchmark</span></pre><p id="1200" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">接下来，让我们将必要的模块加载到代码中:</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="86cc" class="lp jp hh ll b fi lq lr l ls lt">import numpy as np<br/>import torch<br/>import torch.nn as nn<br/>import datetime<br/>from torchvision.models import efficientnet_b0, efficientnet_b1 # b0...b7<br/>from pytorch_benchmark import benchmark # benchmarking library</span></pre><p id="ac44" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们创建将执行基准测试的函数:</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="ea28" class="lp jp hh ll b fi lq lr l ls lt">import yaml</span><span id="8fc8" class="lp jp hh ll b fi lu lr l ls lt">def benchmark_efficientnet():</span><span id="e53e" class="lp jp hh ll b fi lu lr l ls lt">    model = efficientnet_b0() # change to whichever model you want to benchmark its performance<br/>    # also, I discovered it's possible to perform benchmarking to your own custom models.<br/>    # check this URL out from Lukas Hedegaard: https://github.com/LukasHedegaard/pytorch-benchmark/blob/main/tests/test_custom_class.py</span><span id="b3de" class="lp jp hh ll b fi lu lr l ls lt">    if torch.cuda.is_available():<br/>        model = model.cuda()</span><span id="b43d" class="lp jp hh ll b fi lu lr l ls lt">    sample = torch.randn(2, 3, 224, 224)  # (B, C, H, W)</span><span id="863d" class="lp jp hh ll b fi lu lr l ls lt">    results = benchmark(<br/>        model=model,<br/>        sample=sample,<br/>        num_runs=1000,<br/>        batch_size=8,<br/>        print_details=True<br/>    )</span><span id="407a" class="lp jp hh ll b fi lu lr l ls lt">    for prop in {"device", "flops", "params", "timing"}:<br/>        assert prop in results</span><span id="1ac3" class="lp jp hh ll b fi lu lr l ls lt">    return yaml.dump(results)</span><span id="3edb" class="lp jp hh ll b fi lu lr l ls lt">Warming up with batch_size=1: 100%|██████████| 1/1 [00:00&lt;00:00, 21.34it/s]<br/>Warming up with batch_size=1: 100%|██████████| 10/10 [00:00&lt;00:00, 30.06it/s]<br/>Measuring inference for batch_size=1: 100%|██████████| 100/100 [00:03&lt;00:00, 32.58it/s]<br/>Warming up with batch_size=8: 100%|██████████| 10/10 [00:01&lt;00:00,  8.05it/s]<br/>Measuring inference for batch_size=8: 100%|██████████| 100/100 [00:11&lt;00:00,  8.50it/s]</span></pre><p id="fdce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们用这个库来帮助我们衡量表现。</p><pre class="jd je jf jg fd lk ll lm ln aw lo bi"><span id="804c" class="lp jp hh ll b fi lq lr l ls lt">result = benchmark_efficientnet()<br/>print(result) # beautify it</span></pre><p id="9b09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在执行函数之后，我们看到<a class="ae lg" href="https://github.com/jasperan/pytorch-tensorflow/blob/main/benchmark_out.txt" rel="noopener ugc nofollow" target="_blank">输出(在我们的例子中非常长)</a>。基准测试流程如下(图片取自Lukas Hedegaard的GitHub ):</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lv"><img src="../Images/09643a033fe8f2547fb87d25c7562df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/0*Ql97OJ5WqC6Sdar5"/></div></figure><p id="dae2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的高效网络模型有529万个参数，我们可以看到不同的步骤。</p><p id="b641" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像所有现代卷积神经网络(CNN)一样，高效网络模型的结构(它们是如何在内部构建的)如下:</p><ul class=""><li id="d4ef" class="kr ks hh ig b ih ii il im ip kt it ku ix kv jb kw kx ky kz bi translated">卷积(基本上，将图像分成更小的部分，以便神经网络可以分别尝试和学习图像的每个部分)</li><li id="e0c1" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated">批处理规范化(BatchNorm2D)</li><li id="0478" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated">一个激活函数，在这种情况下是路斯(我们之前见过ReLU，区别可以在下图中看到)</li></ul><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es lw"><img src="../Images/7a072686f197a60cebd5da1952f335e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/0*et0ahGxdGXMPvSA7"/></div></figure><p id="dabb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该过程通常重复几次，并与其他模块混合(如<a class="ae lg" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">挤压和激励(SE)模块</a>)。听到这些术语不要慌！它们只是改进模型内部性能的奇特方法。如果你想的话，你可以在SE blocks上免费查看这篇论文。</p><p id="433b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个过程中要做很多次，每次我们都会看到这个模型是如何在CNN的每一层中考虑更多/更少的参数的。</p><h1 id="8358" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">衡量绩效</h1><p id="79c3" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">此外，在文件的结尾，我们有许多设备信息。请注意，我们无法提取已分配的DRAM数量，因为只有在支持和使用CUDA架构的机器上执行基准测试时，这些信息才可用。同样，我们无法收集能耗统计数据，因为官方仅支持NVIDIA Jetson设备。</p><p id="a612" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里有一些关于执行的信息:</p><ul class=""><li id="bdf0" class="kr ks hh ig b ih ii il im ip kt it ku ix kv jb kw kx ky kz bi translated">MFLOPS(每秒百万次浮点运算):~401</li><li id="93d2" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated">已用DRAM: 3.56 GB</li><li id="b109" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated">[27.01，42.20]中每秒的平均批数(批大小= 1)</li><li id="7abe" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated">[18.06，27.73]中每秒的平均批数(批大小= 8)</li><li id="5ced" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated">[23.697，37.018]毫秒内的批处理延迟(batch_size = 1)</li><li id="4004" class="kr ks hh ig b ih la il lb ip lc it ld ix le jb kw kx ky kz bi translated">[36.067，55.365]毫秒内的批处理延迟(batch_size = 8)</li></ul><p id="89ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如预期的那样，以1为一批进行计算时的性能高于以8为一批进行计算时的性能。然而，我们可以清楚地看到它并没有大8倍，这意味着使用批处理实际上是有益的。</p><p id="2183" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，批处理延迟代表我们最感兴趣的指标，即<strong class="ig hi">一行数据/样本在同一时间</strong>遍历神经网络需要多长时间。</p><h1 id="6566" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="1e0a" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">正如我们所看到的，使用PyTorch测试EfficientNet模型的性能并不困难。</p><p id="b816" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">考虑到我们的机器规格，使用16个英特尔至强白金8167M @ 2.00GHz OCPUs，我们可以产生0.28毫秒/ OCPU /行数据。</p><p id="41af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，在以下时间跨度内计算了401个MFLOPS:批量大小= 1: 26秒，批量大小= 8: 42秒</p><p id="4cae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这意味着大约15.4 MFLOPS /秒，也就是大约0.96 MFLOPS /秒/ CPU。</p><p id="3aa5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在下一篇文章中，我们将使用TensorFlow进行类似的练习，希望尝试比较这两个库，并测试我们在以前的文章中提出的假设，即TensorFlow由于在TensorFlow本身的基础上使用Keras生态系统而有点“慢”。</p><p id="67bf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我已经附上了<a class="ae lg" href="https://github.com/jasperan/pytorch-tensorflow/blob/main/article4.ipynb" rel="noopener ugc nofollow" target="_blank">这篇文章中使用的代码的笔记本</a>，以防你有兴趣下载并亲自尝试。</p><p id="096b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">敬请期待！</p><h1 id="ecd1" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">我如何开始学习OCI？</h1><p id="8d93" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">请记住，你可以随时免费注册OCI！您的Oracle Cloud帐户提供多项始终免费的服务和300美元免费积分的免费试用，可用于所有符合条件的OCI服务，最长30天。这些永远免费的服务在<strong class="ig hi">无限期</strong>内有效。免费试用服务可能会一直使用到您的300美元免费点数用完或30天到期，以先到者为准。你可以<a class="ae lg" href="https://signup.cloud.oracle.com/?language=en&amp;sourceType=:ow:de:te::::&amp;intcmp=:ow:de:te::::" rel="noopener ugc nofollow" target="_blank">在这里免费注册</a>。</p><h1 id="839b" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">加入对话！</h1><p id="6029" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">如果你对Oracle开发人员在他们的自然环境中发生的事情感到好奇，请加入我们的公共休闲频道！我们不介意成为你的鱼缸🐠</p><h1 id="c2bf" class="jo jp hh bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">许可证</h1><p id="d62a" class="pw-post-body-paragraph ie if hh ig b ih km ij ik il kn in io ip ko ir is it kp iv iw ix kq iz ja jb ha bi translated">由<a class="ae lg" href="https://www.linkedin.com/in/ignacio-g-martinez/" rel="noopener ugc nofollow" target="_blank">伊格纳西奥·吉尔勒莫·马丁内兹</a><a class="ae lg" href="https://github.com/jasperan" rel="noopener ugc nofollow" target="_blank">@贾斯珀兰</a>撰写，由<a class="ae lg" href="https://www.linkedin.com/in/dawsontech/" rel="noopener ugc nofollow" target="_blank">艾琳·道森</a>编辑</p><p id="4cb8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">版权所有202 Oracle和/或其附属公司。</p><p id="2912" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据通用许可许可证(UPL)1.0版进行许可。</p><p id="a901" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">详见<a class="ae lg" href="https://github.com/oracle-devrel/leagueoflegends-optimizer/blob/main/LICENSE" rel="noopener ugc nofollow" target="_blank">许可证</a>。</p></div></div>    
</body>
</html>