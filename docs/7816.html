<html>
<head>
<title>Neural Networks Part 2: Activation Functions And Differentiation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络第2部分:激活函数和微分</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/neural-networks-part-2-activation-functions-and-differentiation-cd16b7eb2aa1?source=collection_archive---------4-----------------------#2019-10-04">https://medium.com/walmartglobaltech/neural-networks-part-2-activation-functions-and-differentiation-cd16b7eb2aa1?source=collection_archive---------4-----------------------#2019-10-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="a393" class="hf hg hh bd b fp hi hj hk hl hm hn dx ho translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><h1 id="befd" class="in io hh bd ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk bi translated">激活功能</h1><p id="c825" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">神经网络是以特定方式相互连接的人工神经元的网络。神经网络的工作是从给定的数据中学习。神经网络必须学习的预测函数可能是高度非线性的。选择人工神经元的激活函数来捕捉潜在的非线性。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kj"><img src="../Images/16d63a40eb1659cafdb9fed79f9a8943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*FMTxLEs99WTZ6Ctq7cM8xg.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Linear prediction (Source: Tensorflow playground <a class="ae kv" href="http://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">link</a>)</figcaption></figure><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/1a847b5799cbb319d131fcadef541bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*m2oF-ZzbXhVFq6ss9R4S_Q.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Non-linear prediction (Source: Tensorflow playground <a class="ae kv" href="http://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">link</a>)</figcaption></figure><p id="b459" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated">激活函数(一般)的函数形式为<strong class="jn hr"><em class="lc">f(u)= f(wᵀx+b)</em></strong>其中<strong class="jn hr"> <em class="lc"> w </em> </strong>为权重向量，<strong class="jn hr"> <em class="lc"> x </em> </strong>为单个训练数据向量</p><p id="f308" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated">这可以被视为输入的线性组合，随后是非线性变换。有许多选项可用于选择非线性变换。一些突出的例子如下。</p><h2 id="e5d4" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">1.Sigmoid激活函数</h2><p id="967f" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">一个sigmoid函数，<strong class="jn hr"><em class="lc">f(u)= 1/(1+e⁻ᵘ)</em></strong>。它取一个实数值，并将其“挤压”到介于<strong class="jn hr"> <em class="lc"> 0 </em> </strong>和<strong class="jn hr"> <em class="lc"> 1 </em> </strong>之间的范围内。大负数变成≈ <strong class="jn hr"> <em class="lc"> 0 </em> </strong>大正数变成≈ <strong class="jn hr"> <em class="lc"> 1 </em> </strong>。</p><p id="fb7f" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated"><strong class="jn hr">优点:</strong></p><p id="ed92" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated">对于二元分类问题，它被用作神经网络输出层的激活。</p><p id="b907" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated"><strong class="jn hr">缺点</strong>:</p><ol class=""><li id="2032" class="lq lr hh jn b jo kx js ky jw ls ka lt ke lu ki lv lw lx ly bi translated"><strong class="jn hr">能饱和并杀死梯度:</strong>当神经元的激活在<strong class="jn hr"> <em class="lc"> 1 </em> </strong>或<strong class="jn hr"> <em class="lc"> 0 </em> </strong>处饱和时，梯度几乎为零。这就造成了学习上的困难。</li><li id="34bd" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated"><strong class="jn hr">输出不以零为中心</strong>:由于输出在<strong class="jn hr"> <em class="lc"> 0 </em> </strong>到<strong class="jn hr"> <em class="lc"> 1 </em> </strong>范围内，下一层的神经元将接收不以零为中心的数据。因此，在反向传播期间，权重的梯度<strong class="jn hr"> <em class="lc"> w </em> </strong>将全部是正的或者全部是负的，这可能在权重的梯度更新中导致不期望的曲折动态。当考虑在一批所有训练数据上添加梯度时，与“饱和并消除梯度”相比，这个问题不会太严重</li></ol><h2 id="6c82" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">2.Tanh激活函数</h2><p id="8f80" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">一个<strong class="jn hr"> tanh </strong>函数，<strong class="jn hr">T63】f(u)= sinh(u)/cosh(u)</strong>。它接受一个实数值，并将其“压缩”到-1和1之间的范围内。大负数变为≈1，大正数变为≈1。</p><p id="91e6" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated"><strong class="jn hr">优点</strong>:</p><p id="4c66" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated">它优于sigmoid，因为其输出以零为中心</p><p id="074f" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated"><strong class="jn hr">缺点</strong>:</p><p id="0a74" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated"><strong class="jn hr">能饱和并杀死梯度:</strong>当神经元的激活在1或-1饱和时，梯度几乎变为零。这就造成了学习上的困难。</p><h2 id="243b" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">3.ReLU激活功能</h2><p id="b0d8" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">整流后的线性单位，<strong class="jn hr"> ReLU </strong>为<strong class="jn hr">T5】f(u)= max(0，u)  </strong></p><p id="7828" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated"><strong class="jn hr">优点:</strong></p><ol class=""><li id="42af" class="lq lr hh jn b jo kx js ky jw ls ka lt ke lu ki lv lw lx ly bi translated">与tanh和sigmoid相比，大大提高了训练速度</li><li id="0652" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated">与<strong class="jn hr"> <em class="lc"> </em> </strong> tanh和sigmoid相比，计算成本更低</li><li id="fe2a" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated">降低渐变消失的可能性。由于当<strong class="jn hr"> <em class="lc"> u &gt; 0 </em> </strong>时，梯度具有恒定值。</li><li id="f22f" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated"><strong class="jn hr">稀疏度:</strong>当更多的u &lt; =0时，<strong class="jn hr"><em class="lc">【f(u)</em></strong>可以更稀疏</li></ol><p id="aff8" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated"><strong class="jn hr">缺点:</strong></p><ol class=""><li id="541e" class="lq lr hh jn b jo kx js ky jw ls ka lt ke lu ki lv lw lx ly bi translated">倾向于炸掉激活(没有机制约束神经元的输出，因为<strong class="jn hr"> <em class="lc"> u </em> </strong>本身就是输出)。</li><li id="d07b" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated"><strong class="jn hr"> Closed ReLU或Dead ReLU </strong>:如果输入倾向于使<strong class="jn hr"> <em class="lc"> u &lt; =0 </em> </strong>，那么大多数神经元将总是具有0梯度更新，因此关闭或死亡。</li></ol><h2 id="8925" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">4.泄漏ReLU:</h2><p id="d1b6" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">它解决了死ReLU问题。<strong class="jn hr"> <em class="lc"> 0.01 </em> </strong>是泄漏系数。泄漏的ReLU如下:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es me"><img src="../Images/a3f11207854108dc7827933c8d0afa48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*OITu-G0Q3BpxSN7z8mTqOw.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Leaky ReLU</figcaption></figure><h2 id="b3a8" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">5.参数化ReLU或PReLU:</h2><p id="8dce" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">将泄漏ReLU中的泄漏系数αα参数化。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mf"><img src="../Images/3e89800c9e5c92a3af562f64391ae467.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*xfn1_pNEpXNBW2zdzKJlvA.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Parameterized ReLU</figcaption></figure><h2 id="62ff" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">6.最大输出</h2><p id="0420" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">ReLU，Leaky ReLU和PReLU的推广。它没有<strong class="jn hr"><em class="lc">f(u)= f(wᵀx+b)</em></strong>的函数形式，而是计算函数<strong class="jn hr"><em class="lc">【max(w′ᵀx+b′,wᵀx+b】</em></strong></p><p id="706f" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated"><strong class="jn hr">优点:</strong></p><p id="3587" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated">Maxout有ReLU的优点，但没有死的ReLU问题</p><p id="11a4" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated"><strong class="jn hr">缺点:</strong></p><p id="2c24" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated">它具有两倍数量的权重参数来学习<strong class="jn hr"><em class="lc">w′</em></strong>和<strong class="jn hr"> <em class="lc"> w </em> </strong></p><h2 id="59d7" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">7.Softmax</h2><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mg"><img src="../Images/a3203396a29932540d317c11c59485f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_IDMoFnoJT916hhUREiFAQ.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Udacity Deep Learning Slide on Softmax</figcaption></figure><p id="1e00" class="pw-post-body-paragraph jl jm hh jn b jo kx jq jr js ky ju jv jw kz jy jz ka la kc kd ke lb kg kh ki ha bi translated">sofmax函数是sigmoid函数的推广。Sigmoid用于2类(二元)分类，而Softmax用于多类分类。如上图所示，Softmax函数将logit[2.0，1.0，0.1]转换为概率[0.7，0.2，0.1]</p><h1 id="0ba2" class="in io hh bd ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk bi translated">我应该使用什么激活功能？</h1><ol class=""><li id="d4c8" class="lq lr hh jn b jo jp js jt jw ml ka mm ke mn ki lv lw lx ly bi translated">对于输出图层，在分类任务中使用sigmoid或softmax</li><li id="b1c2" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated">对于输出层，在回归任务中使用无激活或纯林函数<strong class="jn hr"> <em class="lc"> f(u)=u </em> </strong></li><li id="a472" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated">如果您仔细设置学习率并监控网络中“无效ReLU”的比例，请使用ReLU非线性。</li><li id="3c8f" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated">否则请尝试Leaky ReLU或Maxout。</li><li id="0e22" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated">或者试试tanh，虽然可能比ReLU差</li><li id="9c36" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated">避免乙状结肠</li></ol><h1 id="1612" class="in io hh bd ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk bi translated">差异化:</h1><h2 id="07a5" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">基本公式:</h2><p id="7533" class="pw-post-body-paragraph jl jm hh jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki ha bi translated">给定f(x)f(x)和g(x)g(x)是可微函数(导数存在)，cc和nn是任意实数:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mo"><img src="../Images/9c50e4c248b53acac9265447352cce2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SdIbnPSMn-FCdFg43uzKgQ.png"/></div></div></figure><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mp"><img src="../Images/440b0dbbbccc9ffe207d6d2cab473966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gIDcQZtnSOMMkt-fmGdEAA.png"/></div></div></figure><h2 id="8a31" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">Sigmoid函数:</h2><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mq"><img src="../Images/e505d6ed3431fd82f96ad3eb89fa6efa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E9uA79QeTABDPTlPnWts9Q.png"/></div></div></figure><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mr"><img src="../Images/1828d0a1cb40f3b818319076b4087aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M3ytrJulGtD9PpC7y081Zg.png"/></div></div></figure><h2 id="1545" class="ld io hh bd ip le lf lg it lh li lj ix jw lk ll jb ka lm ln jf ke lo lp jj hn bi translated">双曲正切函数:</h2><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es ms"><img src="../Images/9fe26b2bb607e7de8cf8ea6e9afc784e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X42mbOMPAAc9s5Bt_mdALQ.png"/></div></div></figure><h1 id="1d0b" class="in io hh bd ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk bi translated">参考资料:</h1><ol class=""><li id="5980" class="lq lr hh jn b jo jp js jt jw ml ka mm ke mn ki lv lw lx ly bi translated">Tensorflow游乐场<a class="ae kv" href="http://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="40d5" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated"><a class="ae kv" href="http://cs231n.github.io" rel="noopener ugc nofollow" target="_blank"> http://cs231n.github.io </a></li><li id="3067" class="lq lr hh jn b jo lz js ma jw mb ka mc ke md ki lv lw lx ly bi translated">Softmax上的Udacity深度学习幻灯片</li></ol></div></div>    
</body>
</html>