<html>
<head>
<title>Deep learning for fraud detection in retail transactions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于零售交易中欺诈检测的深度学习</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/deep-learning-for-fraud-detection-in-retail-transactions-564d31e5d1a3?source=collection_archive---------0-----------------------#2020-11-11">https://medium.com/walmartglobaltech/deep-learning-for-fraud-detection-in-retail-transactions-564d31e5d1a3?source=collection_archive---------0-----------------------#2020-11-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/c9dadcdccda9e46e4bd0578471c4d7ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MAz4M_E2J9Nsbdx2gnRqUQ.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="426b" class="pw-subtitle-paragraph ip hr hs bd b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg dx translated">挑战和基于深度学习的解决方案概述</h2></div><p id="dc78" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">随着<strong class="jj ht"> <em class="kd">【网上购物】</em></strong><a class="ae ke" href="https://en.wikipedia.org/wiki/E-commerce" rel="noopener ugc nofollow" target="_blank"><strong class="jj ht"><em class="kd">电子商务</em> </strong> </a>和<strong class="jj ht"> <em class="kd">电子零售</em> </strong>的不断发展，前所未有的新增网络用户和网购者的新渠道也为欺诈和滥用创造了更多的机会。为了理解这个问题的范围和复杂性，这里有一个例子。考虑到<em class="kd">身份被盗</em>、<em class="kd">账户被接管</em>或信用卡被盗的可能性，我们如何确定新老客户购物或退货并获得退款(在线或店内)是否真正合法？</p><p id="19a8" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">欺诈检测是<a class="ae ke" href="https://en.wikipedia.org/wiki/Anomaly_detection" rel="noopener ugc nofollow" target="_blank"> <strong class="jj ht"> <em class="kd">异常检测</em> </strong> </a>的一个例子，它是<a class="ae ke" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="jj ht">机器学习</strong></a><a class="ae ke" href="https://en.wikipedia.org/wiki/Artificial_intelligence" rel="noopener ugc nofollow" target="_blank"><strong class="jj ht">【人工智能】</strong> </a> (AI)中更广泛的主题<strong class="jj ht"> <em class="kd"> </em> </strong>，并且遭受在定义异常(或离群值)方面的不确定性以及在结果验证和性能监控方面的困难。大多数异常检测方法可以被认为是无监督或半监督学习问题，但是如果我们的数据库中有足够的标记或验证数据来学习，那么监督学习可以用于建立检测模型。</p><p id="3a73" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">除了金融交易中巨大的数据量和复杂性之外，欺诈检测还有一些挑战需要克服，下面将进行总结。在这篇文章中，我们将重点关注基于<strong class="jj ht">人工神经网络</strong> (ANN)或<a class="ae ke" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="jj ht">深度学习</strong> </a>的零售交易欺诈检测解决方案，并探讨该领域的几个主题。正如在许多其他机器学习问题中一样，深度学习(DL)方法有望表现得更好。DL的好处包括对特征工程的需求大大降低，随着我们获得越来越多的数据，学习和性能会更好。</p></div><div class="ab cl kf kg go kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ha hb hc hd he"><h1 id="597e" class="km kn hs bd ko kp kq kr ks kt ku kv kw iy kx iz ky jb kz jc la je lb jf lc ld bi translated"><strong class="ak">检测欺诈有哪些挑战？</strong></h1><p id="5a8c" class="pw-post-body-paragraph jh ji hs jj b jk le it jm jn lf iw jp jq lg js jt ju lh jw jx jy li ka kb kc ha bi translated">从讨论的示例中可以看出，解决大多数欺诈和滥用检测问题需要访问大量关于客户和交易的当前和历史信息，包括客户的购物概况、进行购买的客户与其他客户之间的关系，以及他们过去在哪里以及如何进行购买或退货。接下来的问题是如何以及何时确定购买是欺诈性的。这里要讨论的另一个问题是，传统的欺诈检测解决方案需要大量的数据预处理和工程步骤。具体来说，</p><ul class=""><li id="cf23" class="lj lk hs jj b jk jl jn jo jq ll ju lm jy ln kc lo lp lq lr bi translated">建立欺诈检测模型是一项机器学习任务，它处理由拥有<strong class="jj ht">高维特征空间</strong>引起的难题。在预处理、量化和特征嵌入之后，我们最终得到大量不同种类的输入属性或特征。我们从当前交易中获取这些特征，并结合从关于客户的历史数据以及所有相关的过去交易和相关事件中生成的属性。</li><li id="2c7a" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">金融欺诈检测的主要挑战之一是，只有非常小比例(远低于1%)的交易是欺诈:不平衡的机器学习问题。因此，很难了解如何以高准确度识别欺诈案件，同时限制<strong class="jj ht">假阳性率(FPR) </strong>。仅仅专注于提高欺诈检测(或真阳性)率会导致更高的FPR。如果一个正常的交易被错误地预测为欺诈(即误报)，这将导致销售损失，但更重要的是，这将导致客户不满，随后<em class="kd">客服</em>(即更高的人力成本)将进一步介入调查该案件，并帮助感到不安和受到侮辱的客户。因此，具有较高FPR的检测模型将转化为较高的<strong class="jj ht">客户侮辱</strong>率，并可能导致失去一些合法客户。并不是所有的客户侮辱都会联系客服。</li><li id="411b" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">建立或开发欺诈检测模型的另一个挑战是<strong class="jj ht">数据标记</strong>问题(监督或半监督学习所需)，即确定交易是否实际上是欺诈。</li><li id="e67e" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">关于数据标签的另一个问题是，欺诈有各种类型、复杂程度和形式。此外，欺诈者不断使用新方法实施欺诈。因此，我们的历史数据库可能缺少这种欺诈或滥用模式。在这种情况下，无监督或半监督的异常检测模型可能会有所帮助。</li><li id="4bdf" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">当执行欺诈检测时，给定交易的检测模型的输出可能是连续的'<strong class="jj ht">欺诈得分</strong>或欺诈概率，我们通常需要选择一些阈值，以便将其转换为最终决策。</li><li id="0683" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">当前部署或工作的金融欺诈检测系统包括若干基于规则的决策的组合、若干数据预处理步骤，包括从历史数据中提取有用的属性和网络信息，以及最后在将数据馈送到基于机器学习的检测模型之前的大量特征工程，(流行的机器学习方法包括无监督聚类方法，如<em class="kd">'</em><a class="ae ke" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"><em class="kd">k-means</em></a><em class="kd">'、</em> <a class="ae ke" href="https://en.wikipedia.org/wiki/Isolation_forest" rel="noopener ugc nofollow" target="_blank"> <em class="kd">隔离森林</em></a><em class="kd"/>和<em class="kd"> ' </em>)流行的监督方法有<em class="kd"> ' </em> <a class="ae ke" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank"> <em class="kd"> logistic回归</em> </a> <em class="kd">'，</em> <a class="ae ke" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank"> <em class="kd">随机森林</em> </a> <em class="kd">'，</em> <a class="ae ke" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank"> <em class="kd">渐变助推机</em> </a> <em class="kd">'，</em>和<em class="kd">'</em><a class="ae ke" href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf" rel="noopener ugc nofollow" target="_blank"><em class="kd">XGboost</em></a><em class="kd">')。</em>一般来说，这意味着很高的维护成本。</li><li id="47a0" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">即使基于深度学习的解决方案仍然需要数据预处理和特征嵌入，但其对特征工程的需求低于经典(非深度学习)解决方案。深度学习检测模型的问题在于训练或构建它们的难度，通常在生成预测时会有更高的<strong class="jj ht">延迟</strong>，这使得在运营或生产模式下更难扩展以服务于每秒钟的大量交易。</li></ul><h1 id="ff1b" class="km kn hs bd ko kp lx kr ks kt ly kv kw iy lz iz ky jb ma jc la je mb jf lc ld bi translated">基于深度学习的解决方案</h1><p id="8f81" class="pw-post-body-paragraph jh ji hs jj b jk le it jm jn lf iw jp jq lg js jt ju lh jw jx jy li ka kb kc ha bi translated">我们将在本博客的剩余部分更详细地回顾几种欺诈检测方法，包括编码器-解码器结构、生成式对抗网络、其他半监督方法、监督方法和迁移学习。</p><h2 id="a1e5" class="mc kn hs bd ko md me mf ks mg mh mi kw jq mj mk ky ju ml mm la jy mn mo lc mp bi translated">编码器-解码器结构或自动编码器</h2><p id="6f6c" class="pw-post-body-paragraph jh ji hs jj b jk le it jm jn lf iw jp jq lg js jt ju lh jw jx jy li ka kb kc ha bi translated">由于大量的未标记数据以及标记数据的困难和不确定性(欺诈与正常)，将欺诈检测视为无监督或自我监督的异常检测问题并不罕见，而<a class="ae ke" href="https://en.wikipedia.org/wiki/Autoencoder" rel="noopener ugc nofollow" target="_blank">自动编码器</a> (AE)可能是解决这一问题的好办法。</p><ul class=""><li id="e19b" class="lj lk hs jj b jk jl jn jo jq ll ju lm jy ln kc lo lp lq lr bi translated">自动编码器的基本目标是学习数据的压缩表示或学习“正常”数据的生成模型。然后，我们可以通过检查重构误差是否超过阈值来检测“异常”输入。AE具有<strong class="jj ht"> <em class="kd">编码器</em> </strong>部分，其将输入特征压缩成瓶颈或潜在空间向量，其后是<strong class="jj ht"> <em class="kd">解码器</em> </strong>部分，其基本上基于压缩的潜在向量创建或生成类似于输入的输出。</li><li id="13d5" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">由于AE从大量正常数据中学习紧凑的生成模型，当异常数据作为输入时，输出将很可能不同于输入。为了我们的目的，重构误差(即输入和输出之间的差异)可以用作<strong class="jj ht">欺诈分数</strong>。</li></ul><p id="4fd3" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">有许多自动编码器方法，其中最有前途的是<strong class="jj ht">变型自动编码器</strong> (VAE) [ <a class="ae ke" href="https://arxiv.org/pdf/1906.02691.pdf" rel="noopener ugc nofollow" target="_blank">金玛和韦林</a>。同样参见<a class="ae ke" href="https://arxiv.org/pdf/1606.05908.pdf" rel="noopener ugc nofollow" target="_blank">这个</a></p><figure class="mr ms mt mu fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mq"><img src="../Images/d3ccd04580dff5d4b538f250f69027f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8Wu1rMEiWME8Phhu4DVfQ.jpeg"/></div></div><figcaption class="mv mw et er es mx my bd b be z dx">Fig. 1. Basic structure of a variational auto-encoder (VAE). It includes probabilistic encoder and decoder structures. The model learns to reconstruct the input, and the output is controlled by the random latent vector Z, which is a low-dimensional representation of input.</figcaption></figure><p id="bbc0" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">VAE是初始AE结构的概率生成扩展。VAE编码器估计的压缩向量有两个:均值和标准差，代表数据在压缩低维空间的概率分布。潜在向量<em class="kd"> Z </em>从分布中采样以生成输出，当数据正常时，该输出与输入<em class="kd"> X </em>非常“相似”,并且与VAE训练期间看到的大多数数据相似。</p><ul class=""><li id="2b87" class="lj lk hs jj b jk jl jn jo jq ll ju lm jy ln kc lo lp lq lr bi translated">在几个数据集中，被用于异常检测和欺诈检测[参见，例如，由<a class="ae ke" href="http://dm.snu.ac.kr/static/docs/TR/SNUDM-TR-2015-03.pdf" rel="noopener ugc nofollow" target="_blank"> Jinwon </a>、<a class="ae ke" href="https://link.springer.com/chapter/10.1007/978-3-030-44584-3_2" rel="noopener ugc nofollow" target="_blank"> Alazizi </a>、<a class="ae ke" href="https://arxiv.org/pdf/1802.03903.pdf" rel="noopener ugc nofollow" target="_blank"> Xu </a>和他们的共同作者的工作]。注意，根据输入数据的类型和设计者的选择，编码器和解码器部分中的每一个都可以是<a class="ae ke" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank"><strong class="jj ht"/></a><strong class="jj ht">【conv net，或CNN】</strong><a class="ae ke" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank"><strong class="jj ht">递归神经网络</strong></a><strong class="jj ht">【RNN】</strong>或<a class="ae ke" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="jj ht">全连接网络</strong> </a>(也称为前馈网络或多层感知器)结构或它们的组合。</li><li id="9821" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">另一种用于异常检测的AE结构是所谓的<strong class="jj ht">‘鲁棒深度自动编码器’</strong><a class="ae ke" href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-with-robust-deep-auto-encoders" rel="noopener ugc nofollow" target="_blank">【周等，2017】</a>，其承诺处理不完全正常的训练数据，即，它可能包含异常或异常样本。该方法基于“<a class="ae ke" href="https://arxiv.org/pdf/0912.3599.pdf" rel="noopener ugc nofollow" target="_blank">鲁棒PCA </a>”，并通过将输入数据X分成两部分X = L + S来工作，其中L可以由自动编码器有效地重构，S模拟难以重构的噪声和异常分量，然后最小化具有两项的正则化目标(或损失)函数。</li></ul><p id="22dc" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">尽管使用自动编码器存在一些挑战，但它们并不总是像预期的那样工作。局限性在于，当输入数据噪声很大或包含大量不相关或大部分随机的特征或属性时，它们可能无法很好地工作。这是因为自动编码器将很难学会重建这些属性。此外，解码或数据生成过程是有损耗的，并不完美，主要是由于瓶颈处使用的压缩，这可能会删除一些有用的信息。自动编码器可能会导致问题的次优解决方案，因为自动编码器的训练通常独立于我们报告异常检测的最终决策部分(无论是应用简单的阈值，还是在重建误差的最终阶段应用另一个机器学习模型以得出最终的欺诈检测)。</p><h2 id="0e9d" class="mc kn hs bd ko md me mf ks mg mh mi kw jq mj mk ky ju ml mm la jy mn mo lc mp bi translated"><strong class="ak">生成对抗网络</strong>(甘)</h2><p id="5a7a" class="pw-post-body-paragraph jh ji hs jj b jk le it jm jn lf iw jp jq lg js jt ju lh jw jx jy li ka kb kc ha bi translated">gan是VAE的扩展，但不需要明确的概率密度估计。gan由生成器部分和鉴别器部分组合而成，相互竞争，其中<strong class="jj ht">生成器</strong>从随机且紧凑的潜在空间向量中创建合成或虚假数据，并试图使其尽可能接近真实数据，<strong class="jj ht">鉴别器</strong>学习识别输入数据的真假，挑战生成器。生成器的潜在空间向量决定了生成数据的分布或属性，[ <a class="ae ke" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="noopener ugc nofollow" target="_blank"> Goodfellow </a>。见<a class="ae ke" href="https://arxiv.org/pdf/1710.07035.pdf" rel="noopener ugc nofollow" target="_blank">这</a>也一样】。</p><ul class=""><li id="f4b1" class="lj lk hs jj b jk jl jn jo jq ll ju lm jy ln kc lo lp lq lr bi translated">GAN的训练过程通常包括多个目标的同时优化(例如，以多个加权损失函数的最小化的形式)。例如，训练可能包括损失函数的最小化，该损失函数是<em class="kd">生成误差</em>和<em class="kd">辨别误差</em>的加权或正则化组合，但是研究人员开发了几个GAN版本，这些版本在如何进行这种优化方面有所不同，并且可能包括其他误差或损失项。</li></ul><p id="d78f" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">如果我们有一些标记的数据，我们可以使用它们来更好地训练鉴别器模型。就在训练过程中使用数据标签而言，存在无监督、半监督和监督版本的GAN。在监督版本中，GAN的鉴别器部分也学习输出输入数据的类别标签，或者在训练期间使用标记的数据作为附加的(监督的)损失项。</p><figure class="mr ms mt mu fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es mz"><img src="../Images/1a5c4ba0acd45fd2fc1e9ee713c3abec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JsBap5tFBGWlF41A-8PxHw.jpeg"/></div></div><figcaption class="mv mw et er es mx my bd b be z dx">Fig. 2. A semi-supervised GAN-based model for anomaly detection. The generator and discriminator networks are learned using a training dataset by optimizing a loss function which includes a combination of an unsupervised and a supervised loss terms, [<a class="ae ke" href="https://www.mdpi.com/2072-4292/9/10/1042/htm" rel="noopener ugc nofollow" target="_blank">He, et al., 2017</a>].</figcaption></figure><p id="4f4d" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">对于GAN中的数据发生器，如前所述，通常使用VAE结构的解码器部分。请注意，VAE是一个用于数据生成的无监督学习任务，GAN添加了一个监督学习组件，即鉴别器，以便通过同时学习生成器和鉴别器部分来挑战VAE并帮助提高其性能。</p><figure class="mr ms mt mu fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es na"><img src="../Images/9a7312041fdddfe52f2936aa11e58ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5QqqrqlNDXU2PKi8XwDQeA.jpeg"/></div></div><figcaption class="mv mw et er es mx my bd b be z dx">Fig. 3. Structure of a VAE-GAN model: The encoder decoder parts make a variational auto-encoder which are trained in unsupervised way using all normal data. The discriminator can be trained using normal data as well as any available labeled abnormal (fraud) data, [<a class="ae ke" href="https://openaccess.thecvf.com/content_WACV_2020/papers/Kimura_Adversarial_Discriminative_Attention_for_Robust_Anomaly_Detection_WACV_2020_paper.pdf" rel="noopener ugc nofollow" target="_blank">Kimura, et al., 2020</a>].</figcaption></figure><p id="2ad5" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">在使用GAN进行异常检测时，在另一种称为<a class="ae ke" href="https://arxiv.org/pdf/1703.05921.pdf" rel="noopener ugc nofollow" target="_blank"> AnoGAN </a>的方法中，对于输入数据样本X，在潜在空间中搜索样本Z，使得对应生成的合成数据和X相似。训练后，由于潜在空间覆盖了法线的分布，当输入异常时，真实数据和生成的伪数据会有很大的差异。</p><figure class="mr ms mt mu fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es nb"><img src="../Images/4ee642703c37db28f753d3837d8b3978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwitOSEVWWla8qNprpbJkg.jpeg"/></div></div><figcaption class="mv mw et er es mx my bd b be z dx">Fig. 4. <a class="ae ke" href="https://arxiv.org/pdf/1805.06725.pdf" rel="noopener ugc nofollow" target="_blank">GANomaly</a> structure: G, E, Disc and A denote generator, encoder, discriminator, and ‘anomaly score’ respectively. The ‘anomaly score’ in this method is the encoder loss in the ‘latent space’, i.e., the difference between <em class="nc">Z</em> and its reconstructed version, E(G(<em class="nc">X</em>)). The loss function used to optimize or train this structure is a combination of several parts, including the construction loss, encoder loss and adversarial loss, [<a class="ae ke" href="https://arxiv.org/pdf/1805.06725.pdf" rel="noopener ugc nofollow" target="_blank">Akcay et al., 2018</a>]. This structure is similar to bidirectional GAN (<a class="ae ke" href="https://arxiv.org/pdf/1605.09782.pdf" rel="noopener ugc nofollow" target="_blank">BiGAN</a>).</figcaption></figure><p id="c7de" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">还有其他版本的GAN可能对这里有用。<a class="ae ke" href="https://arxiv.org/pdf/1411.1784.pdf" rel="noopener ugc nofollow" target="_blank">条件GAN </a> (CGAN)模型学习生成以额外输入为条件的样本，如数据标签。在CGAN中，数据标签向量作为第二个输入提供给生成器和鉴别器。经过训练后，CGAN使我们能够生成我们想要的特定类型的数据。</p><p id="85de" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated"><a class="ae ke" href="https://arxiv.org/pdf/1701.07875.pdf" rel="noopener ugc nofollow" target="_blank"> Wasserstein GAN </a> (WGAN)是GANs的扩展，目标是可解释性和使训练更加稳定，(另见<a class="ae ke" href="https://arxiv.org/pdf/1807.01202.pdf" rel="noopener ugc nofollow" target="_blank"> this </a>)。WGAN使用<a class="ae ke" href="https://en.wikipedia.org/wiki/Wasserstein_metric" rel="noopener ugc nofollow" target="_blank"> Wasserstein距离</a>(或推土机距离)来测量分布之间的距离。推土机的距离被解释为运输一个分布的概率质量直到它与另一个匹配的成本，并且两个分布不需要有重叠。WGAN的一个特性是它能够生成分类数据，这在这里是一个有用的特性，因为欺诈检测的一些输入特征或属性是分类数据。</p><p id="893c" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">除了使用GAN中的“生成器”来学习数据生成模型和底层紧凑潜在空间之外，一旦被训练，它还可以用于创建额外的合成数据来扩充用于某些目的的训练集。请注意，总的来说，真实数据比GAN创建的合成数据更有价值，但是GAN可以在某些情况下帮助扩充数据。</p><p id="80f9" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">一般来说，与自动编码器相比，GAN在训练过程中有更多的损耗分量需要优化，可以生成看起来更真实的合成数据，最终可以用于检测复杂的异常情况。然而，培训GANs更加困难，培训可能无法达成可接受的解决方案。最后，由于训练基本版本的GANs的主要目的是从紧凑的潜在空间生成数据，而不是异常检测，因此对于我们这里的目的来说，结果可能不是最理想的。</p><h2 id="31e9" class="mc kn hs bd ko md me mf ks mg mh mi kw jq mj mk ky ju ml mm la jy mn mo lc mp bi translated">其他半监督方法</h2><p id="3660" class="pw-post-body-paragraph jh ji hs jj b jk le it jm jn lf iw jp jq lg js jt ju lh jw jx jy li ka kb kc ha bi translated">还有其他几种方法可用于构建半监督和弱监督异常检测模型:</p><ul class=""><li id="b4b3" class="lj lk hs jj b jk jl jn jo jq ll ju lm jy ln kc lo lp lq lr bi translated"><a class="ae ke" href="https://arxiv.org/pdf/1911.08623.pdf" rel="noopener ugc nofollow" target="_blank">偏差网络</a>【Pang，et al .，2019】使用大量未标记的数据以及少量标记的异常和基于异常分数的高斯先验来训练异常检测模型。其思想是使用正态数据的先验概率信息，以确保对于异常数据，我们的异常得分与位于分布尾部的正态数据的异常得分有显著的偏差。</li><li id="07f2" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">由<a class="ae ke" href="https://arxiv.org/pdf/1906.02694.pdf" rel="noopener ugc nofollow" target="_blank"> Ruff及其同事【2020】</a>提出的半监督异常检测方法基于集成或组合损失函数的最小化，包括学习未标记的正常数据表示以及学习稀疏或有限数量的标记异常样本的异常分数，确保异常远离正常。它使用的思想是，与异常数据相比，正常数据的潜在分布熵应该更低，因此他们的方法通过最小化正常数据的潜在分布熵，同时最大化异常数据的潜在分布熵来工作。他们得出结论，当一些标记数据可用时，采用这些数据标签的半监督方法比建立完全无监督的异常检测模型(并忽略所有标签)更好。</li></ul><h2 id="6e4d" class="mc kn hs bd ko md me mf ks mg mh mi kw jq mj mk ky ju ml mm la jy mn mo lc mp bi translated">使用监督方法进行欺诈检测</h2><p id="db75" class="pw-post-body-paragraph jh ji hs jj b jk le it jm jn lf iw jp jq lg js jt ju lh jw jx jy li ka kb kc ha bi translated">如果我们有足够大的一组历史标记数据集，我们就可以非常有把握地知道哪些数据样本以及何时发生了<a class="ae ke" href="https://en.wikipedia.org/wiki/Chargeback" rel="noopener ugc nofollow" target="_blank"><strong class="jj ht"/></a>退款，哪些交易是欺诈性的，何时发生了客户侮辱事件，以及何时发生了退货滥用案例等。基于监督的深度学习的欺诈检测方法可以提供较高的准确性和性能。</p><p id="949c" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">由于存在各种类型的输入特征或属性(包括点数据以及事件和序列数据)，因此在特征提取过程中可以采用两种方法:</p><ol class=""><li id="15e8" class="lj lk hs jj b jk jl jn jo jq ll ju lm jy ln kc nd lp lq lr bi translated">我们可以直接将序列数据和事件时间序列提供给康文网络或RNN深度学习模型，其中特征提取任务将由深度学习模型自动完成；这是一个所谓的<strong class="jj ht">端到端</strong>机器学习设计的例子。</li><li id="0143" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc nd lp lq lr bi translated">除了关于当前交易的信息之外，我们还可以从连续的历史信息以及过去的网络或交互信息中导出或提取各种相关属性或特征。然后，我们可以将所有这些提取的定量特征输入到深度学习模型中。与传统的机器学习方法相比，深度学习模型在高维数据上表现更好。</li></ol><p id="4ec5" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">为了解决不平衡训练数据集的问题，欺诈样本的过采样和正常样本的欠采样非常有用。</p><h2 id="c9c6" class="mc kn hs bd ko md me mf ks mg mh mi kw jq mj mk ky ju ml mm la jy mn mo lc mp bi translated">迁移学习</h2><p id="3c05" class="pw-post-body-paragraph jh ji hs jj b jk le it jm jn lf iw jp jq lg js jt ju lh jw jx jy li ka kb kc ha bi translated">如果我们有一个基于DL的模型，最初是为欺诈检测问题或具有大型数据集的应用程序训练的，那么我们可以通过微调其参数将该模型用于另一个类似的问题或应用程序，即使新问题的数据集很小。这叫做迁移学习，<strong class="jj ht">学习的迁移</strong>，或者“<strong class="jj ht">领域适应</strong>”。换句话说，假设我们的另一个新的欺诈检测问题没有足够的数据来从头训练一个新的检测模型，但它使用相同的输入数据属性。然后，如果我们有一个良好且训练有素的欺诈检测模型，可以很好地处理不同但相似的问题，我们可以使用预训练的模型作为初始网络，使用我们在新问题中拥有的小数据集执行有限的额外训练或<strong class="jj ht">微调</strong>。通过这样做，我们实际上是在推广经验，并将从解决一个问题(有时称为“源领域”)中学到的知识<strong class="jj ht">转移到另一个训练数据不足的问题(称为“目标领域”)中，从而提高性能，否则这是不可能的。[例如，参见<a class="ae ke" href="https://arxiv.org/pdf/1911.02685.pdf" rel="noopener ugc nofollow" target="_blank">庄</a>和<a class="ae ke" href="https://www.researchgate.net/publication/332180999_Deep-Learning_Domain_Adaptation_Techniques_for_Credit_Cards_Fraud_Detection" rel="noopener ugc nofollow" target="_blank">乐比丘</a> ]。</strong></p><ul class=""><li id="2289" class="lj lk hs jj b jk jl jn jo jq ll ju lm jy ln kc lo lp lq lr bi translated">我们可以通过各种方式使用迁移学习，从无监督的方法到有监督的方法。例如，在受监督的方法中，由于DL模型的初始层执行特征提取任务，一种方式是我们保持原始预训练DL模型的初始层完整无损，并且仅使用我们新的但较小的数据集来微调或调整顶层(即，接近输出的层)。另一种方式是，我们允许调整原始DL模型的所有参数或网络层，但是通过降低学习率、对每一层使用不同的学习率(其中靠近输入的层获得非常小的学习率)、或者通过限制训练迭代的次数来限制调整量。</li><li id="1d87" class="lj lk hs jj b jk ls jn lt jq lu ju lv jy lw kc lo lp lq lr bi translated">迁移学习可能有帮助的另一个领域是当我们做<strong class="jj ht">数据扩充</strong>和使用合成数据时，特别是对于时间序列或顺序数据。请注意，异常事件在现实生活中非常稀少，因此生成合成异常可能有助于训练过程。例如，【Wen】和Keyes 在一大组合成序列数据上预先训练了用于异常检测的ConvNet模型，然后在一小组真实数据上微调了模型参数。</li></ul><h1 id="df1f" class="km kn hs bd ko kp lx kr ks kt ly kv kw iy lz iz ky jb ma jc la je mb jf lc ld bi translated">结束语</h1><p id="885f" class="pw-post-body-paragraph jh ji hs jj b jk le it jm jn lf iw jp jq lg js jt ju lh jw jx jy li ka kb kc ha bi translated">与传统的机器学习方法相比，深度学习模型可以帮助降低假阳性率(FPR)(这意味着更少的客户侮辱)，并减少退款和欺诈。请注意，当我们处理大规模且竞争激烈的电子商务时，即使FPR和退款率的小幅降低(如5%至10%的降低)也可能转化为业务利润或绩效的显著增加。</p><p id="b1e3" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated">基于深度学习的模型的一个关键优势是它们的<strong class="jj ht">学习能力</strong>，这意味着随着我们数据库规模的增长，它们能够自动提取更复杂的特征，学习更复杂的检测模型，并通过更多或更大的训练数据来提高性能。另一个主要好处是它们能够处理各种各样的输入特征或属性，而不必太担心属性的统计特性、它们的依赖性或相关性以及特征工程工作。深度学习方法的许多应用是针对非常复杂问题的<strong class="jj ht">端到端</strong>解决方案，这意味着在特征提取或维度的<strong class="jj ht">诅咒</strong>方面的挑战更小。然而，鉴于欺诈检测中固有的复杂性和挑战，正如我们在本帖中所讨论的，仍有改进的空间和研究的需要:需要设计一种适合异常检测目的的深度学习方法，以优化探索未标记数据中的丰富信息，改进各种输入数据和属性的处理方式，然后输入到深度学习模型中，同时降低计算复杂性，以便它可以在生产中扩展，以每秒处理大量交易，并具有合理的检测延迟。</p><p id="49b8" class="pw-post-body-paragraph jh ji hs jj b jk jl it jm jn jo iw jp jq jr js jt ju jv jw jx jy jz ka kb kc ha bi translated"><em class="kd">我要感谢我在沃尔玛的同事James Tang、Henry Chen、Julia Albath、Camilo Rivera、Jan Johnson和夏静，他们阅读了草稿，并提供了他们的宝贵意见，帮助改进了这篇文章。</em></p></div></div>    
</body>
</html>