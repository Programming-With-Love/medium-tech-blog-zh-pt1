<html>
<head>
<title>Shallow Mirror</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">浅镜</h1>
<blockquote>原文：<a href="https://medium.com/pinterest-engineering/shallow-mirror-f543b14bb25?source=collection_archive---------2-----------------------#2021-05-10">https://medium.com/pinterest-engineering/shallow-mirror-f543b14bb25?source=collection_archive---------2-----------------------#2021-05-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="a57d" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Kafka MirrorMaker的增强功能可降低CPU/内存压力</h1><p id="fb59" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Henry Cai |软件工程师，数据工程</p><p id="34cc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Pinterest使用<a class="ae kf" href="https://www.confluent.io/blog/running-kafka-at-scale-at-pinterest/" rel="noopener ugc nofollow" target="_blank"> Kafka </a>作为数据传输的主干。作为我们基础设施的一部分，<a class="ae kf" href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330" rel="noopener ugc nofollow" target="_blank"> Kafka MirrorMaker </a> v1用于复制分布在多个地区的不同Kafka集群之间的流量。Kafka MirrorMaker是在不同Kafka集群之间复制/聚合数据的强大工具。图1显示了使用MirrorMaker在两个或多个区域之间复制流量的典型设置。图2显示了一个MirrorMaker v1集群如何在多节点设置中工作。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/81bdb9df896fab55855660ba8e3dd20b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUmAYk3An_R059bGP9GMzQ.png"/></div></div></figure><p id="d3a5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">虽然Kafka Mirrormaker满足了我们最初的需求，但我们开始看到几个可伸缩性问题。</p><h2 id="5246" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated"><strong class="ak">因交通量增加导致OOM</strong></h2><p id="031b" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在流量高峰期，我们经常看到CPU峰值和内存不足(OOM)频繁发生。进一步的诊断显示，大部分CPU时间花在消息解压缩和重新压缩上；内存使用量通常是我们通过网络获取的字节数的2-10倍。这也解释了OOMs的部分根本原因。</p><p id="bc9b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在对Kafka MirrorMaker的内部进行进一步调查后，我们确认，除了解压缩之外，Kafka MirrorMaker内部多个位置的数据(缓冲区)复制也导致了内存使用量的增加。图3显示了Kafka MirrorMaker过程的内部结构，该过程由复制数据的各个阶段组成。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lg"><img src="../Images/22867522401caf34fed0537ed9fc9f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rXubtLJHYUltzgEAPpeYTg.png"/></div></div></figure><p id="6b35" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">消息处理过程中有这么多阶段的原因是因为Kafka MirrorMaker的设计允许消息在发送之前由自定义插件进行检查/转换，因此消息需要解压缩并从网络原始字节转换为Java ConsumerRecord对象。但是在Pinterest部署中，我们仅仅使用MirrorMaker作为复制引擎在Kafka集群之间传输字节。源集群和目的集群通常具有相似的设置，并且源集群中的消息已经被适当地压缩和批处理。</p><p id="555b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们首先想到的是将原始字节从接收者的套接字缓冲区直接复制到发送者的套接字缓冲区，因为我们只关心消息复制。然而，我们很快意识到，为了使有效载荷可读，响应包需要重新组装。图5是网络上的典型Kafka FetchResponse消息(消费者在MirrorMaker节点上收到的消息):</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lh"><img src="../Images/a51fa26f5ca56f06085fb8789d3b5854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O6qsdogvUjyEgXeiRgXQ0A.png"/></div></div></figure><p id="76d5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">图5显示了FetchResponse的结构。如果来自不同主题和分区的数据驻留在相同的源节点上，则这些数据通常被一起打包在相同的网络消息中。类似地，从MirrorMaker发送到目标Kafka集群的Kafka ProduceRequest消息也将多个主题/分区的数据打包在一起，如果这些分区位于同一目标节点上的话。不能保证源群集和目标群集之间的分区到节点映射是相同的。因此，我们知道我们需要分解字节来重新打包。但是我们能不能跳过解压缩/重新压缩，这是最占用CPU/内存的操作？</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es li"><img src="../Images/f69bae961f00acd4942929e602521917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/0*A7GmNbr_jRaBRY4u"/></div><figcaption class="lj lk et er es ll lm bd b be z dx">Figure-6: MemoryRecords Data Structure</figcaption></figure><p id="e985" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">从每个分区的MemoryRecords数据结构(图6)中，我们可以看到分区内的消息是按照记录批次的顺序组织的，每个批次内的记录都被压缩了，但是批次头没有被压缩。因此，如果我们可以按原样传递RecordBatch，而不深入批内部，我们就可以跳过压缩阶段。</p><p id="cf18" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">这让我们设计了一个改进的Kafka MirrorMaker，称为Shallow Mirror，逻辑如下(图7):</p><ol class=""><li id="1bbd" class="ln lo hh je b jf ka jj kb jn lp jr lq jv lr jz ls lt lu lv bi translated">我们将<strong class="je hi">浅层次地</strong>迭代MemoryRecords结构中的记录批，而不是深度迭代记录批中的记录</li><li id="c82d" class="ln lo hh je b jf lw jj lx jn ly jr lz jv ma jz ls lt lu lv bi translated">我们将<strong class="je hi">浅显地</strong>为消息复制(共享)ByteBuffer中的指针，而不是将字节深度复制和反序列化到对象中</li></ol><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mb"><img src="../Images/0c5dcf8680ab18e39c4361afbdb460d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AFZumlO5REZJZs01o5GdVw.png"/></div></div></figure><p id="411d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们没有编写新的镜像产品，而是决定深入Kafka生产者/消费者库，引入原始字节模式。当原始字节模式打开时，使用者将把与RecordBatch相对应的原始字节返回给应用程序客户端，应用程序客户端将把RecordBatch原始字节传递给生产者以发送到目的地，否则它将按照旧的方式工作。我们选择增强消费者/生产者API，以便其他用例可以采用该特性(例如MirrorMaker v2)。我们还在<a class="ae kf" href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-712%3A+Shallow+Mirroring" rel="noopener ugc nofollow" target="_blank">卡夫卡KIP-712 </a>中提出了我们的浅层镜面增强。</p><p id="911e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在2020年末，我们在生产中部署了浅镜像，由于节省了CPU/内存/GC，我们看到了显著的扩展和性能改进。图8和图9显示了启用浅层镜像前后CPU/内存使用的变化。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mc"><img src="../Images/8dd57078e5068e0527704fb701d9d915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Abwexrmg9nVxckynFMRLMQ.png"/></div></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es md"><img src="../Images/501aed336ee48a85eb365858e1d39ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a_wjdgPc0b6tP5TjLYCOqA.png"/></div></div></figure><p id="a9a2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">虽然这个策略原则上听起来不错，但我们在实施过程中遇到了几个障碍。下一节将介绍我们在浅层镜像实现过程中学到的经验。</p><h2 id="6a0e" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated"><strong class="ak">字节缓冲区陷阱</strong></h2><p id="af52" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们在接收方和发送方之间传递字节缓冲区指针以避免深度复制，但我们很快意识到我们需要修改字节缓冲区。图10显示了RecordBatch字节缓冲区的头字段。猜测哪些内容需要更改？</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es me"><img src="../Images/c7f0f6c0aa9344cb6886a6fd81194658.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/0*xeAKhYOpkTMvFVC2"/></div><figcaption class="lj lk et er es ll lm bd b be z dx">Figure-10: RecordBatch data structure</figcaption></figure><p id="48fb" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">是，BaseOffset字段。传入消息中的BaseOffset表示源卡夫卡集群中的卡夫卡消息偏移，但它们在目标集群中不会具有相同的偏移。一旦我们开始更改字节缓冲区，我们就必须注意缓冲区的读/写模式以及偏移、位置、限制和标记。</p><h2 id="05e1" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated"><strong class="ak">第一批</strong></h2><p id="d306" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在产品部署之后，有时我们注意到我们得到的消息比我们要求的更多。看看下面的例子:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mf"><img src="../Images/0188bb2513082348068d0d399f62a4d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eWYsjHIMdVMQcW1COxWhXg.png"/></div></div></figure><p id="98bc" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">当源代理只发送消息A、B和C时，后端的使用者会在A、B和C之前收到更多的消息。这种情况时有发生。我们将此问题追溯到消息是如何存储在源代理端的。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mg"><img src="../Images/b272f83d6753b6ace23e928422caed4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N9FKk2w3xl9tPMgXkL3xqQ.png"/></div></div></figure><p id="101f" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在源代理端，消息由RecordBatch存储。在此示例中，消息1、2和3存储在第一批中，而4、5和6存储在第二批中。当消费者从offset 3请求消息时，代理实际上分两批发回所有6条消息(因为Kafka代理利用sendFile API消除了到用户空间的缓冲区拷贝)，并依赖消费者来进行过滤。基于这种理解，我们知道我们必须在将第一批返回给客户之前对其进行裁剪。</p><h2 id="7c60" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated"><strong class="ak">小批量</strong></h2><p id="2963" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">最初的原型适用于第一个主题。然而，当我们转到第二个卡夫卡主题时，我们注意到MirrorMaker和目标代理之间的出站消息吞吐量开始下降。第二个卡夫卡主题有什么特别之处？当我们分析该主题的原始字节时，我们注意到该主题的批处理大小非常小(平均几百个字节)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mh"><img src="../Images/ec772af8954b88afdd2e63d4a1b8f78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/0*-3O63_IH8W0zA5O2"/></div><figcaption class="lj lk et er es ll lm bd b be z dx">Figure 13: Batch size for the small batch messages</figcaption></figure><p id="b7b7" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">当批处理大小小时，我们会损失大量网络效率(请记住，网络缓冲区和数据包通常以千字节大小组织)。我们尝试通过增加Kafka的max . in fly . requests配置来增加出站流并行性，并尝试使用多个TCP连接，但这没有帮助，因为目标代理正在顺序处理回复消息以保持消息排序。</p><p id="6cce" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">但是，我们注意到网络性能只是出站流量的问题，而不是入站流量的问题。通过查看入站FetchResponse消息和出站ProduceRequest消息，我们注意到一个奇偶校验异常。入站和出站消息都使用MemoryRecords数据结构来捕获一个主题/分区中的数据(参见图6中的MemoryRecords数据结构)。但是入站MemoryRecords包含多个记录批，而出站memory records只包含一个记录批。我们修改了生产者库代码来放松这个约束，这解决了那个用例的吞吐量问题。</p><h2 id="7311" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated"><strong class="ak">消息再次转换…在代理中</strong></h2><p id="fc97" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">一切看起来都很美好，直到我们的SRE工程师注意到代理上的消息转换:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es li"><img src="../Images/7f68b4500a60a2c930646173bb0beee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/0*yDf5gB6qXKeWfFwB"/></div><figcaption class="lj lk et er es ll lm bd b be z dx">Figure-14: Message conversion stats chart</figcaption></figure><p id="d021" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">图14显示了消息转换发生在代理内部。简单介绍一下消息转换，这是Kafka代理需要将消息从一种格式转换为另一种格式的过程，通常发生在生产者和代理使用不同的消息格式或不同的消息压缩算法时。消息转换是Kafka上高CPU使用率和低吞吐量的关键根源之一。</p><p id="60b8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如果到目前为止您还在使用我们的产品，您会发现我们已经花费了大量精力来减少MirrorMaker节点上的CPU/内存使用，以跳过不必要的消息转换，现在问题已经转移到了水管的后半部分(代理)。我们检查了代理代码，并意识到它在多个RecordBatches输入上重建了整个批处理，因此我们修复了这个问题。</p><h2 id="9510" class="ks if hh bd ig kt ku kv ik kw kx ky io jn kz la is jr lb lc iw jv ld le ja lf bi translated"><strong class="ak">未来工作</strong></h2><p id="f0f0" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">我们希望通过提交Kafka KIP提案将这项工作回馈给Kafka社区:<a class="ae kf" href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-712%3A+Shallow+Mirroring" rel="noopener ugc nofollow" target="_blank"> KIP-712 </a>。请随意在此参与讨论<a class="ae kf" href="https://www.mail-archive.com/dev@kafka.apache.org/msg116025.html" rel="noopener ugc nofollow" target="_blank">或扩展这项工作以支持更多用例，如Kafka MirrorMaker v2。</a></p><p id="04f6" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><strong class="je hi">确认</strong></p><p id="7d8d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated"><em class="mi">我们要感谢Pinterest日志团队(安布德·夏尔马、瓦希德·哈什米安</em> <em class="mi">等人)和Apache社区成员的集思广益、深入审查和指导。</em></p></div></div>    
</body>
</html>