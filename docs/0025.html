<html>
<head>
<title>Unboxing the Random Forest Classifier: The Threshold Distributions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">拆箱随机森林分类器:阈值分布</h1>
<blockquote>原文：<a href="https://medium.com/airbnb-engineering/unboxing-the-random-forest-classifier-the-threshold-distributions-22ea2bb58ea6?source=collection_archive---------2-----------------------#2015-10-01">https://medium.com/airbnb-engineering/unboxing-the-random-forest-classifier-the-threshold-distributions-22ea2bb58ea6?source=collection_archive---------2-----------------------#2015-10-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="19c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者:郑世豪</p><p id="78bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在Airbnb的信任和安全团队中，我们在许多风险缓解模型中使用随机森林分类器。尽管我们在这方面取得了成功，但是树的集合以及在每个节点上随机选择的特征使得简洁地描述特征是如何被分割的变得困难。在这篇文章中，我们提出了一种通过生成加权阈值分布来聚合和汇总这些分割值的方法。</p><h2 id="cf83" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">动机</h2><p id="c57c" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">尽管随机森林分类器具有多功能性和开箱即用的性能，但它通常被称为黑盒模型。很容易理解为什么有些人会倾向于这样想。首先，每个节点处的最优决策分裂仅从特征集的随机子集中抽取。更令人费解的是，该模型使用训练集的引导样本生成了一个系综树。所有这些仅仅意味着一个特征可能在同一棵树的不同节点上分裂，并且可能具有不同的分裂值，并且这可能在多个树中重复。</p><p id="3976" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管所有这些随机性都被抛到了脑后，但确定性依然存在。在训练了一个随机森林之后，我们确切地知道了森林的每一个细节。对于每个节点，我们知道用什么特征来分割，用什么阈值，用什么效率。所有的细节都在那里，挑战是知道如何将它们拼凑在一起，以建立一个准确和翔实的所谓黑匣子的描述。</p><p id="6aa0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据特征描述训练的随机森林的一种常见方法是根据它们的分裂效率，按照它们的重要性对它们进行排序。尽管这种方法设法量化了每个特征的杂质减少的贡献，但是它没有阐明模型如何根据它们做出决定。在这篇文章中，我们提出了一种简洁地描述森林内部决策的方法，即逐节点挖掘整个森林，并给出每个特征的阈值的加权分布(通过分割效率和样本大小)。</p><h2 id="153d" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">方法学</h2><p id="0605" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">为了说明这种方法，我们求助于来自<a class="ae kc" href="http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>的公开可用的在线新闻流行度数据集。该数据集包含39797条对<a class="ae kc" href="http://mashable.com/" rel="noopener ugc nofollow" target="_blank"> Mashable </a>文章的观察，每条文章有58个特征。该数据集的正标签被定义为特定文章的份额数是否大于或等于1，400。这些特征都是数字的，范围从简单的统计，如内容中的字数，到更复杂的统计，如与特定LDA衍生主题的接近程度。</p><p id="3ade" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在训练随机森林之后，我们遍历整个森林，并从每个非终端(或非叶)节点提取以下信息:</p><ol class=""><li id="5ca5" class="kd ke hh ig b ih ii il im ip kf it kg ix kh jb ki kj kk kl bi translated">功能名称</li><li id="79da" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">分割阈值-节点分割的值</li><li id="125a" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">样本大小-训练时通过节点的观察数量</li><li id="45c1" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">大于阈值。—大多数正面观察的方向</li><li id="c1b3" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">基尼系数变化——分割后杂质的减少</li><li id="6c64" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb ki kj kk kl bi translated">树索引—树的标识符</li></ol><p id="279d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">可以收集到如下表格中:</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/215f87df663d2f599388a25701e0d092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/0*H3bzQXPuPXfRHD3w.png"/></div></figure><p id="7de0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像上面这样的公共表很可能在不同的树中多次包含相同的特性，甚至在同一个树中。在这一点上，收集特定特性的所有阈值并将它们堆积在一个直方图中可能很有诱惑力。然而，这是不公平的，因为从少数观察中找到最佳特征阈值元组的节点不应该具有与从数千个观察中找到的节点相同的权重。因此，对于分裂节点[latex]i[/latex]，我们将样本大小[latex]N_i[/latex]定义为在随机森林的训练阶段到达分裂节点I的观察值的数量。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kz"><img src="../Images/7ac012ea16362b4cf271ec2c9d384810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*sG4VaDHLFTqvuY5i45uAIw.png"/></div></figure><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es la"><img src="../Images/ab3e9a7f250967485e3b7c2f06f66e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/0*yyWkKkW1wd2gO-oK.png"/></div></figure><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es lb"><img src="../Images/a523d343a87e4f6077ba97b085ddec12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*QchaiON0AOuelaEMj0tVbQ.png"/></div></figure><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es lc"><img src="../Images/fafdd2b7a9fb67acb14fff1ab70adc20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/0*FzIy22gzKcyW8LlN.png"/></div></figure><p id="aab1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中实心圆是正标记的观察值，而空心圆是负标记的观察值。在本例中，尽管两个节点在同一要素上分割，但它们这样做的原因却大相径庭。在左边的分裂节点中，大部分正观察结果在小于或等于分支上结束，而在右边的分裂节点中，大部分正观察结果在大于分支上结束。换句话说，左分裂节点认为正标记的观察值更可能具有较小的特征X值，而右分裂节点认为正标记的观察值更可能具有较大的特征X值。这种差异使用is_greater_than_threshold(或手性)标志来计算，其中1为真(或大于)，0为假(或小于或等于)。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kz"><img src="../Images/69d99be1ae33e7dd07ea8d39e3ef2f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*HmbRQ-SPeTmEASgwX_gaRg.png"/></div></figure><h1 id="55d9" class="ld jd hh bd je le lf lg ji lh li lj jm lk ll lm jp ln lo lp js lq lr ls jv lt bi translated">例子</h1><p id="61e8" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">在训练分类器模型之后，我们遍历整个森林并收集上一节中指定的所有信息。这些信息使我们能够描述哪些阈值控制了分割，比如num_hrefs(文章中的链接数):</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es lu"><img src="../Images/da8274515f9e718bdc7b5c1fed6b5314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*CNYTSwthMUF-dzL1.png"/></div></figure><p id="1857" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在上图中，我们看到有两种分布。橙色对应于手性大于的节点，红色对应于手性小于等于的节点。num_hrefs的这些加权分布表明，每当使用特征num_hrefs来决定一篇文章是否是受欢迎的文章(+1，400次分享)时，主要描述是<strong class="ig hi">大于</strong> ~15个链接，这由15附近的大于分布的峰值箱来说明。这种方法的另一个有趣的说明是关于global_rate_positive_words和global_rate_negative_words，它们分别被定义为文章内容中正面和负面单词的比例。前一种描述如下:</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es lv"><img src="../Images/ff184586f2246258d6c0ef870f6b70e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/0*duArAHn3fjTy4o38.png"/></div></figure><p id="685e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中，就模型而言，流行文章倾向于由较大的global_rate_positive_words支配，其中截止值由0.03支配。然而，一个更有趣的分布是，对于global_rate_negative_words:</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es lw"><img src="../Images/49b3bda3debef815de830ae3dfe2a930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/0*T5kCb2oQd_NSXoMU.png"/></div></figure><p id="f411" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">其中分布表明每个内容大小使用大于约0.01个词的负面词增加了健康的流行度，而过多的负面词，比如说大于约0.02，将使模型预测较低的流行度。这是从大约0.01处的大于分布峰值而大约0.02处的小于等于分布峰值推断出来的。</p><h2 id="f067" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">下一步是什么</h2><p id="d4bd" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">在TnS，我们渴望让我们的模型更加透明。这篇文章只涉及一种非常特殊的方法来检查一个训练过的模型的内部工作。其他可能的方法可以通过询问以下问题:</p><ul class=""><li id="e9fd" class="kd ke hh ig b ih ii il im ip kf it kg ix kh jb lx kj kk kl bi translated">模型将哪些观察结果视为聚类？</li><li id="a7f1" class="kd ke hh ig b ih km il kn ip ko it kp ix kq jb lx kj kk kl bi translated">在随机森林中，要素是如何交互的？</li></ul><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es ly"><img src="../Images/3913f6470a7657e02386189e67b4eb30.png" data-original-src="https://miro.medium.com/v2/resize:fit:108/format:webp/1*YsUOrWx3mRxZZljtc9xZyw.png"/></div></figure><h2 id="888d" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">在<a class="ae kc" href="http://airbnb.io" rel="noopener ugc nofollow" target="_blank"> airbnb.io </a>查看我们所有的开源项目，并在Twitter上关注我们:<a class="ae kc" href="https://twitter.com/AirbnbEng" rel="noopener ugc nofollow" target="_blank">@ Airbnb eng</a>+<a class="ae kc" href="https://twitter.com/AirbnbData" rel="noopener ugc nofollow" target="_blank">@ Airbnb data</a></h2></div><div class="ab cl lz ma go mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ha hb hc hd he"><p id="fedc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="mg">原载于2015年10月1日nerds.airbnb.com</em><em class="mg">的</em> <a class="ae kc" href="http://nerds.airbnb.com/large-scale-payments-systems-ruby-rails/" rel="noopener ugc nofollow" target="_blank"> <em class="mg">。</em></a></p></div></div>    
</body>
</html>