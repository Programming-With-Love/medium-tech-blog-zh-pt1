<html>
<head>
<title>How To Perform Data Compression Using Autoencoders?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用自动编码器执行数据压缩？</h1>
<blockquote>原文：<a href="https://medium.com/edureka/autoencoders-tutorial-cfdcebdefe37?source=collection_archive---------0-----------------------#2018-10-12">https://medium.com/edureka/autoencoders-tutorial-cfdcebdefe37?source=collection_archive---------0-----------------------#2018-10-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/0a7f0cfb0f13a5a5ae877e413e3f48fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*K09JzKPGa5ur7IwUoDOc9Q.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">Autoencoders Tutorial — Edureka</figcaption></figure><p id="79b5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">人工智能涵盖了广泛的技术和技巧，使计算机系统能够解决数据压缩等问题，这些问题用于计算机视觉、计算机网络、计算机体系结构和许多其他领域。<strong class="ir hi">自动编码器</strong>是<em class="jn">无监督的神经网络</em>，它使用机器学习来为我们进行压缩。本<strong class="ir hi">自动编码器教程</strong>将按以下顺序为您提供对自动编码器的全面了解:</p><ul class=""><li id="0376" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">什么是自动编码器？</li><li id="f805" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">对自动编码器的需求</li><li id="b95d" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">自动编码器的应用</li><li id="0636" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">自动编码器的体系结构</li><li id="6a09" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">属性和超参数</li><li id="85af" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">自动编码器的类型</li><li id="475f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">使用自动编码器进行数据压缩(演示)</li></ul><p id="baa3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们从最基本的问题开始，什么是自动编码器？</p><h1 id="ba57" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">什么是自动编码器？</h1><p id="87fe" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">自动编码器神经网络是一种<strong class="ir hi">无监督机器学习</strong>算法，它应用反向传播，将目标值设置为等于输入。自动编码器用于将我们的输入缩减为更小的表示形式。如果有人需要原始数据，他们可以从压缩数据中重建它。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/08d6279c0a189e15a74850998be2dedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Fm7RPNckepfCgLKtkKfskQ.png"/></div></figure><p id="1ab6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们有类似的机器学习算法ie。完成相同任务的PCA。所以你可能会想为什么我们需要自动编码器呢？让我们继续这个自动编码器教程，并找出使用自动编码器背后的原因。</p><h1 id="721f" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">自动编码器的需求</h1><p id="4947" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">自动编码器优于PCA，因为:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lk"><img src="../Images/95556f2f207cfa6d4004ddc488839974.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*8ZpTks-VFfu1O2gifH64pQ.png"/></div></figure><ul class=""><li id="3cb3" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">自动编码器可以利用<strong class="ir hi">非线性激活函数</strong>和多个层来学习<strong class="ir hi">非线性</strong> <strong class="ir hi">变换</strong>。</li><li id="fbfa" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">它不需要学习密集的层。它可以使用<strong class="ir hi">卷积层</strong>来学习视频、图像和系列数据哪个更好。</li><li id="641a" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">用自动编码器学习几个层比用PCA学习一个巨大的变换更有效。</li><li id="f050" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">自动编码器提供每一层的表示作为输出。</li><li id="48fe" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">它可以利用来自另一个模型的<strong class="ir hi">预训练层</strong>来应用迁移学习以增强编码器/解码器。</li></ul><p id="a522" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们来看看自动编码器的一些工业应用。</p><h1 id="83c3" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">自动编码器的应用</h1><h2 id="e4f2" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated">图像着色</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/e3c1dc1ef3e161c058c01bcded2e7f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*kFbxIk0I2aObIFGsrBU-Sg.png"/></div></figure><p id="69b9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">自动编码器用于将任何黑白图片转换成彩色图像。根据图片中的内容，可以判断出应该是什么颜色。</p><h2 id="46c7" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated">特征变化</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/9394e87e175382fe2f14d1f94825db45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Mmw_sDoQyTlzOACxfnUchw.png"/></div></figure><p id="991b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它只提取图像所需的特征，并通过消除任何噪声或不必要的干扰来生成输出。</p><h2 id="aac3" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated">降维</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/71630723c77a93485ff3e54249e0bb15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*owmgM9TIf3uOqi1D-I7APw.png"/></div></figure><p id="005f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">重建的图像与我们的输入相同，但是维数减少了。它有助于提供像素值减少的相似图像。</p><h2 id="2b41" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated">图像去噪</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/9e02bc4605324462516255c655be8eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*UGFC8BIXEWAqqoxAObJENQ.png"/></div></figure><p id="6082" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">自动编码器看到的输入不是原始输入，而是随机损坏的版本。去噪自动编码器因此被训练来从有噪声的版本中重建原始输入。</p><h2 id="df85" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated">水印去除</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/e0369efc76b1d7954348b0e9ff13d91f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*kRbsnz_R-FT0Sw55H81tcw.png"/></div></figure><p id="2ac6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它还用于在拍摄视频或电影时去除图像中的水印或任何物体。</p><p id="8e18" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在你已经对自动编码器的不同工业应用有了一个概念，让我们继续我们的文章，理解自动编码器的复杂架构。</p><h1 id="42ac" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">自动编码器的体系结构</h1><p id="cd02" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">自动编码器由三层组成:</p><ol class=""><li id="ccf3" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lz ju jv jw bi translated"><strong class="ir hi">编码器</strong></li><li id="0b6d" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lz ju jv jw bi translated"><strong class="ir hi">代号</strong></li><li id="3e88" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lz ju jv jw bi translated"><strong class="ir hi">解码器</strong></li></ol><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/17e6c5a3df9a6568b603045ad76e0d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*FGbATgGye0fTU9SMaCo16A.png"/></div></figure><ul class=""><li id="b316" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi">编码器:</strong>这部分网络将输入压缩成一个<strong class="ir hi">潜在空间表示</strong>。编码器层<strong class="ir hi">将输入图像编码</strong>为缩减维度的压缩表示。压缩图像是原始图像的失真版本。</li><li id="aacb" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">代码:</strong>网络的这一部分代表输入解码器的压缩输入。</li><li id="a988" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">解码器:</strong>该层<strong class="ir hi">将</strong>编码后的图像解码回原始尺寸。解码图像是原始图像的有损重建，并且是从潜在空间表示中重建的。</li></ul><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es ma"><img src="../Images/bbaa645cc69c764e8fbbd40ca033c861.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*RANboGCGBucWUwiK7KZqJQ.png"/></div></figure><p id="6879" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">编码器和解码器之间的层，即。该代码也被称为<strong class="ir hi">瓶颈</strong>。这是一个设计良好的方法，用于确定观察数据的哪些方面是相关信息，哪些方面可以丢弃。它通过平衡两个标准来做到这一点:</p><ul class=""><li id="2867" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">表示的紧密度，以可压缩性来衡量。</li><li id="5323" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">它从输入中保留了一些行为相关的变量。</li></ul><p id="6dd3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在您已经对自动编码器的架构有了一个概念。让我们继续我们的文章，理解在训练自动编码器时涉及的不同属性和超参数。</p><h1 id="c3a5" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">属性和超参数</h1><h2 id="9273" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated"><strong class="ak">自动编码器的属性:</strong></h2><ul class=""><li id="8c7d" class="jo jp hh ir b is la iw lb ja mb je mc ji md jm jt ju jv jw bi translated"><strong class="ir hi">特定于数据的</strong>:自动编码器只能压缩类似于它们被训练过的数据。</li><li id="45b6" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">有损:</strong>与原始输入相比，解压缩后的输出质量会下降。</li><li id="0ce2" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">从实例中自动学习:</strong>很容易训练算法的专门实例，这些实例将在特定类型的输入上表现良好。</li></ul><h2 id="dd9b" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated"><strong class="ak">自动编码器的超参数:</strong></h2><p id="d466" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在训练自动编码器之前，我们需要设置<strong class="ir hi"> 4 </strong>个超参数:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es me"><img src="../Images/c517a4fc3912bd3d37e4326ee614f723.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*BcsKLkrO40Pub5oTy_r_nQ.png"/></div></figure><ul class=""><li id="4120" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi">码长</strong>:表示中间层的节点数。尺寸越小，压缩越大。</li><li id="c5cf" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">层数</strong>:自动编码器可以包含我们想要的层数。</li><li id="b630" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">每层的节点数量</strong>:每层的节点数量随着编码器的每个后续层而减少，并在解码器中增加。就层结构而言，解码器与编码器是对称的。</li><li id="8a46" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">损失函数:</strong>我们要么用均方差，要么用二进制交叉熵。如果输入值在范围[0，1]内，那么我们通常使用交叉熵，否则，我们使用均方误差。</li></ul><p id="c290" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然您已经知道了自动编码器训练中涉及的属性和超参数。让我们继续我们的文章，了解不同类型的自动编码器以及它们之间的区别。</p><h1 id="364c" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">自动编码器的类型</h1><h2 id="aefb" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated"><strong class="ak">卷积自动编码器</strong></h2><p id="d2ec" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">传统公式中的自动编码器没有考虑到一个信号可以被视为其他信号的总和的事实。卷积自动编码器使用卷积运算符来利用这种观察。他们学习将输入编码成一组简单的信号，然后尝试从这些信号中重建输入，修改图像的几何形状或反射率。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/5082a99afa121cdd544328724f48f8e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*9VvSUQfttJ5S0KRH2vnHIg.png"/></div></figure><p id="9dc2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">CAE用例:</strong></p><ul class=""><li id="7325" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">图像重建</li><li id="e9e5" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">图像彩色化</li><li id="0b14" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">潜在空间聚类</li><li id="4b9f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">生成更高分辨率的图像</li></ul><h2 id="ee23" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated"><strong class="ak">稀疏自动编码器</strong></h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/66c2891786d6659969d9da8416d98f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*jBS2Pb6rg326otn9Kk1tZg.png"/></div></figure><p id="b548" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">稀疏自动编码器为我们提供了引入信息瓶颈<strong class="ir hi">的替代方法，而不需要减少隐藏层的节点数量</strong>。相反，我们将构建我们的损失函数，以便我们惩罚层内的激活。</p><h2 id="fea6" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated"><strong class="ak">深度自动编码器</strong></h2><p id="03f4" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">简单自动编码器的扩展是<strong class="ir hi">深度自动编码器</strong>。深度自动编码器的第一层用于<strong class="ir hi">原始输入</strong>中的一阶特征。第二层用于一阶特征外观中对应于<strong class="ir hi">图案</strong>的二阶特征。深度自动编码器的更深层倾向于学习甚至更高阶的特征。</p><p id="08ba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">深度自动编码器由两个对称的深度信任网络组成</p><ol class=""><li id="2cee" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lz ju jv jw bi translated">前四或五个浅层代表网络的编码部分。</li><li id="700e" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lz ju jv jw bi translated">构成解码一半的第二组四或五层。</li></ol><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/30297548973c80383e96f2594a0f0040.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*owoY8gYmHmgzBihSyyRoyg.png"/></div></figure><p id="07d1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">深度自动编码器的使用案例</strong></p><ul class=""><li id="fd49" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">图像搜索</li><li id="47e4" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">数据压缩</li><li id="07c2" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">主题建模与信息检索</li></ul><h2 id="2e93" class="ll kd hh bd ke lm ln lo ki lp lq lr km ja ls lt kq je lu lv ku ji lw lx ky ly bi translated"><strong class="ak">收缩自动编码器</strong></h2><p id="6a15" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated"><strong class="ir hi">收缩自动编码器</strong>是一种无监督的深度学习技术，可以帮助神经网络对未标记的训练数据进行编码。这是通过构建一个<strong class="ir hi">损失项</strong>来实现的，该损失项惩罚我们的隐藏层激活相对于输入训练示例的大导数，<strong class="ir hi">实质上惩罚</strong>输入的小变化导致编码空间的大变化的情况。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/a4398464b99566733166d01a746ef0e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*wLJgldBiS9EKbvpzA__cKQ.png"/></div></figure><p id="30c5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在你对什么是自动编码器有了一个概念，它有不同的类型和属性。让我们继续我们的文章，理解在Python中使用TensorFlow的简单实现。</p><h1 id="2347" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">使用自动编码器进行数据压缩(演示)</h1><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/104e22687cf31b211893a1bb303f0d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*ovazB8StPw5AiPE2XupfYQ.png"/></div></figure><p id="5bbc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">让我们导入所需的库</strong></p><pre class="lg lh li lj fd mh mi mj mk aw ml bi"><span id="621c" class="ll kd hh mi b fi mm mn l mo mp">import numpy as np<br/>from keras.layers import Input, Dense<br/>from keras.models import Model<br/>from keras.datasets import mnist<br/>import matplotlib.pyplot as plt</span></pre><p id="9260" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">隐藏层和变量的声明</strong></p><pre class="lg lh li lj fd mh mi mj mk aw ml bi"><span id="86d6" class="ll kd hh mi b fi mm mn l mo mp"># this is the size of our encoded representations<br/>encoding_dim = 32 # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats<br/> <br/># this is our input placeholder<br/>input_img = Input(shape=(784,))<br/># "encoded" is the encoded representation of the input<br/>encoded = Dense(encoding_dim, activation='relu')(input_img)<br/># "decoded" is the lossy reconstruction of the input<br/>decoded = Dense(784, activation='sigmoid')(encoded)<br/># this model maps an input to its reconstruction<br/>autoencoder = Model(input_img, decoded)<br/># this model maps an input to its encoded representation<br/>encoder = Model(input_img, encoded)<br/># create a placeholder for an encoded (32-dimensional) input<br/>encoded_input = Input(shape=(encoding_dim,))<br/># retrieve the last layer of the autoencoder model<br/>decoder_layer = autoencoder.layers[-1]<br/># create the decoder model<br/>decoder = Model(encoded_input, decoder_layer(encoded_input))<br/># configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:<br/>autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')</span></pre><p id="9e9c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">准备输入数据(MNIST数据集)</strong></p><pre class="lg lh li lj fd mh mi mj mk aw ml bi"><span id="d8eb" class="ll kd hh mi b fi mm mn l mo mp">(x_train, _), (x_test, _) = mnist.load_data()<br/># normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.<br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/>x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))<br/>x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))<br/>print x_train.shape<br/>print x_test.shape</span></pre><p id="9963" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">为50个时期训练自动编码器</strong></p><pre class="lg lh li lj fd mh mi mj mk aw ml bi"><span id="a632" class="ll kd hh mi b fi mm mn l mo mp">autoencoder.fit(x_train, x_train,<br/>epochs=50,<br/>batch_size=256,<br/>shuffle=True,<br/>validation_data=(x_test, x_test))<br/># encode and decode some digits<br/># note that we take them from the *test* set<br/>encoded_imgs = encoder.predict(x_test)<br/>decoded_imgs = decoder.predict(encoded_imgs)</span></pre><p id="705b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">使用Matplotlib可视化重构输入和编码表示</strong></p><pre class="lg lh li lj fd mh mi mj mk aw ml bi"><span id="bcbf" class="ll kd hh mi b fi mm mn l mo mp">n = 20 # how many digits we will display<br/>plt.figure(figsize=(20, 4))<br/>for i in range(n):<br/># display original<br/>ax = plt.subplot(2, n, i + 1)<br/>plt.imshow(x_test[i].reshape(28, 28))<br/>plt.gray()<br/>ax.get_xaxis().set_visible(False)<br/>ax.get_yaxis().set_visible(False)<br/> <br/> <br/> <br/># display reconstruction<br/>ax = plt.subplot(2, n, i + 1 + n)<br/>plt.imshow(decoded_imgs[i].reshape(28, 28))<br/>plt.gray()<br/>ax.get_xaxis().set_visible(False)<br/>ax.get_yaxis().set_visible(False)<br/>plt.show()</span></pre><p id="5135" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输入</strong>在第一行，<strong class="ir hi">输出</strong>在第二行。参考下文。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es lf"><img src="../Images/80e9733318c52270a6d681634bea2924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*bOi1yrZApqqQ82khPf8zjg.png"/></div></figure><p id="088a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在到此，我们结束这篇文章。我希望你们喜欢这篇文章，并理解Tensorflow的力量，以及解压图像是多么容易。所以，如果你读过这篇文章，你就不再是自动编码器的新手了。尝试这些例子，如果您在部署代码时遇到任何挑战，请告诉我。</p><p id="6864" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="27e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释深度学习的各个其他方面。</p><blockquote class="mr ms mt"><p id="1554" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">1.<a class="ae mq" rel="noopener" href="/edureka/tensorflow-tutorial-ba142ae96bca">张量流教程</a></p><p id="2c80" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">2.<a class="ae mq" rel="noopener" href="/edureka/pytorch-tutorial-9971d66f6893"> PyTorch教程</a></p><p id="14db" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">3.<a class="ae mq" rel="noopener" href="/edureka/perceptron-learning-algorithm-d30e8b99b156">感知器学习算法</a></p><p id="5603" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">4.<a class="ae mq" rel="noopener" href="/edureka/neural-network-tutorial-2a46b22394c9">神经网络教程</a></p><p id="ff34" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">5.什么是反向传播？</p><p id="0e14" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">6.<a class="ae mq" rel="noopener" href="/edureka/convolutional-neural-network-3f2c5b9c4778">卷积神经网络</a></p><p id="145e" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">7.<a class="ae mq" rel="noopener" href="/edureka/capsule-networks-d7acd437c9e">胶囊神经网络</a></p><p id="0fd9" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">8.<a class="ae mq" rel="noopener" href="/edureka/recurrent-neural-networks-df945afd7441">递归神经网络</a></p><p id="7597" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">9.<a class="ae mq" rel="noopener" href="/edureka/tensorflow-object-detection-tutorial-8d6942e73adc">tensor flow中的物体检测</a></p><p id="7f54" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">10.<a class="ae mq" rel="noopener" href="/edureka/restricted-boltzmann-machine-tutorial-991ae688c154">受限玻尔兹曼机教程</a></p><p id="ce33" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">11.<a class="ae mq" rel="noopener" href="/edureka/pytorch-vs-tensorflow-252fc6675dd7"> PyTorch vs TensorFlow </a></p><p id="8c51" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">12.<a class="ae mq" rel="noopener" href="/edureka/deep-learning-with-python-2adbf6e9437d">用Python进行深度学习</a></p><p id="a92b" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">13.<a class="ae mq" rel="noopener" href="/edureka/artificial-intelligence-tutorial-4257c66f5bb1">人工智能教程</a></p><p id="fe3b" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">14.<a class="ae mq" rel="noopener" href="/edureka/tensorflow-image-classification-19b63b7bfd95">张量流图像分类</a></p><p id="a0e0" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">15.<a class="ae mq" rel="noopener" href="/edureka/artificial-intelligence-applications-7b93b91150e3">人工智能应用</a></p><p id="6e11" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">16.<a class="ae mq" rel="noopener" href="/edureka/become-artificial-intelligence-engineer-5ac2ede99907">如何成为一名人工智能工程师？</a></p><p id="7711" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">17.<a class="ae mq" rel="noopener" href="/edureka/q-learning-592524c3ecfc">问学习</a></p><p id="2479" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">18.<a class="ae mq" rel="noopener" href="/edureka/apriori-algorithm-d7cc648d4f1e"> Apriori算法</a></p><p id="7695" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">19.<a class="ae mq" rel="noopener" href="/edureka/introduction-to-markov-chains-c6cb4bcd5723">用Python实现马尔可夫链</a></p><p id="cd4e" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">20.<a class="ae mq" rel="noopener" href="/edureka/artificial-intelligence-algorithms-fad283a0d8e2">人工智能算法</a></p><p id="d2a7" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">21.<a class="ae mq" rel="noopener" href="/edureka/best-laptop-for-machine-learning-a4a5f8ba5b">机器学习的最佳笔记本电脑</a></p><p id="6f9c" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">22.<a class="ae mq" rel="noopener" href="/edureka/top-artificial-intelligence-tools-36418e47bf2a">12大人工智能工具</a></p><p id="0440" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">23.<a class="ae mq" rel="noopener" href="/edureka/artificial-intelligence-interview-questions-872d85387b19">人工智能(AI)面试问题</a></p><p id="0eed" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">24.<a class="ae mq" rel="noopener" href="/edureka/theano-vs-tensorflow-15f30216b3bc"> Theano vs TensorFlow </a></p><p id="f368" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">25.<a class="ae mq" rel="noopener" href="/edureka/what-is-a-neural-network-56ae7338b92d">什么是神经网络？</a></p><p id="3de4" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">26.<a class="ae mq" rel="noopener" href="/edureka/pattern-recognition-5e2d30ab68b9">模式识别</a></p><p id="8f17" class="ip iq jn ir b is it iu iv iw ix iy iz mu jb jc jd mv jf jg jh mw jj jk jl jm ha bi translated">27.<a class="ae mq" rel="noopener" href="/edureka/alpha-beta-pruning-in-ai-b47ee5500f9a">人工智能中的阿尔法贝塔剪枝</a></p></blockquote></div><div class="ab cl mx my go mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="ha hb hc hd he"><p id="6b94" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">原载于2018年10月12日</em><a class="ae mq" href="https://www.edureka.co/blog/autoencoders-tutorial/" rel="noopener ugc nofollow" target="_blank"><em class="jn">www.edureka.co</em></a><em class="jn">。</em></p></div></div>    
</body>
</html>