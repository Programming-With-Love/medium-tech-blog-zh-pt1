<html>
<head>
<title>Earthquake Detection System Using Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Spark的地震检测系统</h1>
<blockquote>原文：<a href="https://medium.com/edureka/spark-tutorial-2a036075a572?source=collection_archive---------0-----------------------#2017-05-04">https://medium.com/edureka/spark-tutorial-2a036075a572?source=collection_archive---------0-----------------------#2017-05-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/bb218add2b0e17f4d6d3da4604f38dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*kez5lV-AhJYkpw7tJFhg4g.png"/></div><figcaption class="il im et er es in io bd b be z dx">Spark Tutorial — Edureka</figcaption></figure><p id="4791" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Apache Spark是一个用于<em class="jn">实时处理</em>的<em class="jn">开源集群计算框架</em>。它是Apache软件基金会中最成功的项目之一。Spark显然已经发展成为大数据处理的市场领导者。今天，Spark正在被亚马逊、易贝和雅虎等主要公司采用。许多组织在具有数千个节点的集群上运行Spark，在您的职业生涯中，成为Spark认证专家是一个巨大的机会。本文是即将推出的Apache Spark系列的第一篇文章，该系列将包括Spark流、Spark面试问题、Spark MLlib等。</p><p id="d51d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">谈到实时数据分析，Spark是所有其他解决方案的首选工具。通过这篇文章，我将向您介绍Apache Spark这个令人兴奋的新领域，我们将浏览一个完整的用例，使用Spark的<em class="jn">地震检测</em>。</p><p id="0211" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以下是这篇Spark教程博客中涉及的主题:</p><ol class=""><li id="35d2" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">实时分析</li><li id="493b" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Hadoop已经存在，为什么还要Spark？</li><li id="a519" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">什么是阿帕奇火花？</li><li id="1bd4" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">火花特征</li><li id="d116" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Spark入门</li><li id="b92d" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">在Hadoop中使用Spark</li><li id="e7b7" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">火花部件</li><li id="8b3a" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">用例:使用Spark进行地震检测</li></ol><h1 id="675e" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">实时分析</h1><p id="de5a" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在我们开始之前，让我们看看社交媒体领导者每分钟产生的数据量。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/b9ab047f27863705a868267f538090d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*orxRRANQE59URLOWfRzQlQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lo">Data Generated — Spark Tutorial</em></figcaption></figure><p id="9d0e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如我们所见，互联网世界需要在几秒钟内处理大量数据。我们将经历企业中处理大数据的所有阶段，并发现对名为<strong class="ir hi"> <em class="jn"> Apache Spark </em> </strong>的<em class="jn">实时处理框架</em>的需求。</p><p id="65e7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，让我向您介绍一下当今世界使用实时分析的几个重要领域。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/79e8680f3ed9fcbd21623d0914ce4fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lsRU1bm-cCq0w1SQfJBBQQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lo">Examples of Real-Time Analytics — Spark Tutorial</em></figcaption></figure><p id="2b17" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们可以看到，大数据的实时处理已经深入到我们生活的方方面面。从银行业的欺诈检测到政府的实时监控系统，从医疗保健领域的自动机器到股票市场的实时预测系统，我们周围的一切都围绕着近乎实时地处理大数据。</p><p id="d570" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们来看看<strong class="ir hi"> <em class="jn">实时分析</em> </strong>的一些使用案例:</p><blockquote class="lp lq lr"><p id="47ba" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><strong class="ir hi">医疗保健</strong>:医疗保健领域使用实时分析持续检查危重患者的医疗状态。寻求血液和器官移植的医院需要在紧急情况下保持实时联系。及时就医对病人来说是生死攸关的问题。</p><p id="fe4e" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><strong class="ir hi">政府</strong>:政府机构主要在国家安全领域执行实时分析。各国需要持续跟踪所有军事和警察机构，以了解有关安全威胁的最新情况。</p><p id="981a" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><strong class="ir hi">电信</strong>:围绕电话、视频聊天和流媒体等服务的公司使用实时分析来减少客户流失，并在竞争中保持领先地位。他们还提取移动网络中抖动和延迟的测量值，以改善客户体验。</p><p id="5150" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><strong class="ir hi">银行业</strong>:银行业处理着世界上几乎所有的货币。确保整个系统的容错事务变得非常重要。通过银行业的实时分析，欺诈检测成为可能。</p><p id="7f27" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><strong class="ir hi">股票市场</strong>:股票经纪人利用实时分析来预测股票组合的变动。公司在使用实时分析来分析市场对其品牌的需求后，会重新思考他们的商业模式。</p></blockquote><h1 id="f680" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Hadoop已经存在，为什么还要Spark？</h1><p id="d94f" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">当谈到Spark时，每个人都会问的第一个问题是，“<strong class="ir hi"> <em class="jn">我们已经有了Hadoop，为什么还要Spark？</em> </strong>”。</p><p id="de29" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">要回答这个问题，我们必须了解批处理和实时处理的概念。<strong class="ir hi"> <em class="jn"> Hadoop </em> </strong>基于<em class="jn">批处理</em>的概念，对已经存储了一段时间的数据块进行处理。当时，Hadoop在2005年用革命性的MapReduce框架打破了所有的预期。<strong class="ir hi"><em class="jn">Hadoop MapReduce</em></strong>是批量处理数据的最佳框架。</p><p id="bd0c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这种情况一直持续到2014年，直到Spark超越Hadoop。Spark的USP是它可以<em class="jn">实时<em class="jn">处理数据</em></em>并且在批量处理大型数据集时比Hadoop MapReduce快100倍左右。</p><p id="f5d3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下图详细解释了Spark和Hadoop中的处理差异。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lv"><img src="../Images/a870e4aff267612ed6a7b77c5f1ad9bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ds4hnZZnpcqshJWQqc13-w.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lo">Differences between Hadoop and Spark — Hadoop Tutorial</em></figcaption></figure><p id="707f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，我们可以引出Hadoop和Spark之间的一个关键区别。Hadoop是基于大数据的批量处理。这意味着数据会存储一段时间，然后使用Hadoop进行处理。而在Spark中，处理可以实时进行。Spark中的这种实时处理能力帮助我们解决了我们在上一节中看到的实时分析用例。此外，Spark的批处理速度比Hadoop MapReduce(Apache Hadoop中的处理框架)快100倍。<strong class="ir hi"> <em class="jn">因此，Apache Spark是业内大数据处理的必备工具。</em> </strong></p><h1 id="c90a" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">什么是阿帕奇火花？</h1><p id="c701" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Apache Spark是用于<em class="jn">实时处理的<em class="jn">开源集群计算框架</em>。它有一个繁荣的开源社区，是目前最活跃的Apache项目。Spark提供了一个接口，用于通过隐式数据并行和容错对整个集群进行编程。</em></p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lw"><img src="../Images/f559100f3b1b5a832ed815de4bfd1cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c67SciQltoCwxc5asuXgHw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lo">Real-Time Processing in Apache Spark — Spark Tutorial</em></figcaption></figure><p id="b1b9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它建立在Hadoop MapReduce之上，并扩展了MapReduce模型以有效地使用更多类型的计算。</p><h1 id="2c14" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Apache Spark的特性</h1><p id="6c8b" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark有以下特点:</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/6db0067cd2797e4228546c63687a1288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e1dmzDmLPtaYMCo6WywLog.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Features of Spark — Spark Tutorial</figcaption></figure><p id="6862" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们详细看看这些功能:</p><h2 id="685f" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">多语言</strong>:</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es ml"><img src="../Images/db5f6cf1ed2d82f5a2d2f8638a698181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rhO3Ap_g0KMbpvD0q2A9tg.png"/></div></div></figure><p id="f378" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark提供了Java、Scala、Python和r的高级API，Spark代码可以用这四种语言中的任何一种编写。它用Scala和Python提供了一个shell。Scala shell可以通过<strong class="ir hi">访问。/bin/spark-shell </strong>和Python shell通过<strong class="ir hi">。安装目录中的/bin/pyspark </strong>。</p><h2 id="e7c6" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">速度</strong>:</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/dd05a11fbc4c5ba84e9774c58dad9363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*mJe0Dwnce6OD2QjCMWajoA.png"/></div></figure><p id="3d17" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于大规模数据处理，Spark的运行速度比Hadoop MapReduce快100倍。Spark能够通过受控分区实现这一速度。它使用分区管理数据，有助于以最小的网络流量并行处理分布式数据。</p><h2 id="640a" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">多种格式</strong>:</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/86bfc46004cc8c4fc488610363086b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*ElrhFTQrf5yxJktPj9nmUg.png"/></div></figure><p id="d64e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark支持多种数据源，如Parquet、JSON、Hive和Cassandra，此外还有文本文件、CSV和RDBMS表等常见格式。数据源API提供了通过Spark SQL访问结构化数据的可插拔机制。数据源不仅仅是简单的转换数据并将其导入Spark的管道。</p><h2 id="0126" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">懒评</strong>:</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/ef2b7950c4a4b2cca375f7b65bc9de72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*xVk_KxwyuZ0V64jX2yml9A.png"/></div></figure><p id="ddaa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Apache Spark延迟其评估，直到绝对必要时。这是促成其速度的关键因素之一。对于转换，Spark将它们添加到DAG(有向无环图)计算中，只有当驱动程序请求一些数据时，DAG才会真正执行。</p><h2 id="c9cf" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">实时计算</strong>:</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/26d739cff98512db818fa57852d7b6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*sFbyePLSojtsy1BaKIqo0g.png"/></div></figure><p id="cc5d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark的计算是实时的，并且由于其内存计算而具有低延迟。Spark旨在实现巨大的可扩展性，Spark团队记录了运行具有数千个节点的生产集群的系统用户，并支持多种计算模型。</p><h2 id="13f9" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak"> Hadoop集成</strong>:</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/5a712d8b9c6cf0bbf15b93cb917520fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*N2O5ze37UzWYBN7-PdzeKw.png"/></div></figure><p id="31be" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Apache Spark提供了与Hadoop的平滑兼容性。这对所有以Hadoop开始职业生涯的大数据工程师来说都是福音。Spark是Hadoop MapReduce功能的潜在替代品，而Spark能够在现有Hadoop集群之上运行，使用YARN进行资源调度。</p><h2 id="8265" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">机器学习</strong>:</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/7e4179e48426cb9ec70a93d21c66151e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*0DKiPbfFMigT_ft2qJ_faQ.png"/></div></figure><p id="592b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark的MLlib是机器学习组件，在大数据处理方面非常方便。它消除了使用多个工具的需要，一个用于处理，一个用于机器学习。Spark为数据工程师和数据科学家提供了一个强大的统一引擎，既快速又易于使用。</p><h1 id="e9c4" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">【Spark入门</h1><p id="56e2" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark入门的第一步是安装。让我们在Linux系统上安装Apache Spark 2.1.0(我使用的是Ubuntu)。</p><h2 id="747d" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">安装:</strong></h2><ol class=""><li id="2ef7" class="jo jp hh ir b is la iw lb ja mn je mo ji mp jm jt ju jv jw bi translated">安装Spark的先决条件是已经安装了Java和Scala。</li><li id="9fd6" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">如果没有安装Java，请使用以下命令下载。</li></ol><pre class="lg lh li lj fd mq mr ms mt aw mu bi"><span id="230d" class="lx kd hh mr b fi mv mw l mx my">sudo apt-get install python-software-properties<br/>sudo apt-add-repository ppa:webupd8team/java<br/>sudo apt-get update<br/>sudo apt-get install oracle-java8-installer</span></pre><p id="bf26" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">3.从<a class="ae mz" href="http://www.scala-lang.org/" rel="noopener ugc nofollow" target="_blank"> Scala Lang官方</a>页面下载最新的Scala版本。安装完成后，在<code class="du na nb nc mr b">~/.bashrc </code>文件中设置scala路径，如下所示。</p><pre class="lg lh li lj fd mq mr ms mt aw mu bi"><span id="6f17" class="lx kd hh mr b fi mv mw l mx my">export SCALA_HOME=Path_Where_Scala_File_Is_Located<br/>export PATH=$SCALA_HOME/bin:PATH</span></pre><p id="9d3e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">4.从Apache Spark下载页面下载Spark 2.1.0。您也可以选择下载以前的版本。</p><p id="dfb2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">5.使用下面的命令提取火花焦油。</p><pre class="lg lh li lj fd mq mr ms mt aw mu bi"><span id="e0df" class="lx kd hh mr b fi mv mw l mx my">tar -xvf spark-2.1.0-bin-hadoop2.7.tgz</span></pre><p id="7fcf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">6.在<code class="du na nb nc mr b">~/.bashrc </code>文件中设置Spark_Path。</p><pre class="lg lh li lj fd mq mr ms mt aw mu bi"><span id="6d8d" class="lx kd hh mr b fi mv mw l mx my">export SPARK_HOME=Path_Where_Spark_Is_Installed<br/>export PATH=$PATH:$SPARK_HOME/bin</span></pre><p id="60b4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们继续之前，让我们在我们的系统上启动Apache Spark，并习惯Spark的主要概念，如Spark会话、数据源、rdd、数据帧和其他库。</p><h2 id="0bb2" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">火花壳:</strong></h2><p id="b658" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark的shell提供了一种简单的学习API的方法，以及一个强大的交互分析数据的工具。</p><h2 id="adf6" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">火花时段:</strong></h2><p id="688b" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在Spark的早期版本中，Spark上下文是Spark的入口点。对于其他API，我们需要使用不同的上下文。对于流，我们需要StreamingContext、SQL sqlContext和HiveContext。为了解决这个问题，SparkSession应运而生。它本质上是SQLContext、HiveContext和future StreamingContext的组合。</p><h2 id="87e3" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">数据来源:</strong></h2><p id="970a" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">数据源API提供了通过Spark SQL访问结构化数据的可插拔机制。数据源API用于将结构化和半结构化数据读取和存储到Spark SQL中。数据源不仅仅是简单的转换数据并将其导入Spark的管道。</p><h2 id="4ffa" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak"> RDD: </strong></h2><p id="6d5c" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">弹性分布式数据集(RDD)是Spark的基本数据结构。它是一个不可变的分布式对象集合。RDD中的每个数据集都被划分为逻辑分区，这些分区可以在集群的不同节点上进行计算。rdd可以包含任何类型的Python、Java或Scala对象，包括用户定义的类。</p><h2 id="209a" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">数据集:</strong></h2><p id="25cd" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">数据集是数据的分布式集合。数据集可以由JVM对象构建，然后使用函数转换(map、flatMap、filter等)进行操作。).数据集API有Scala和Java两种版本。</p><h2 id="f2cf" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">数据帧:</strong></h2><p id="4371" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">DataFrame是组织成命名列的数据集。它在概念上相当于关系数据库中的一个表或R/Python中的一个数据框，但是在底层有更丰富的优化。数据帧可以从各种来源构建，如结构化数据文件、Hive中的表、外部数据库或现有rdd。</p><h1 id="c46a" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">在Hadoop中使用Spark</h1><p id="2551" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark最好的部分是它与Hadoop的兼容性。因此，这是一个非常强大的技术组合。在这里，我们将了解Spark如何从Hadoop的优势中获益。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/882307fe8230dad2ea99421b60b3ad1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OM6sxNHfKJA46sTbc-9hdQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lo">Spark Features — Spark Tutorial</em></figcaption></figure><p id="f20d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Hadoop组件可以通过以下方式与Spark一起使用:</p><blockquote class="lp lq lr"><p id="b6e4" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><strong class="ir hi"> HDFS </strong> : Spark可以运行在HDFS之上，以利用分布式复制存储。</p><p id="a8ab" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><strong class="ir hi"> MapReduce </strong> : Spark可以在同一个Hadoop集群中与MapReduce一起使用，也可以单独作为一个处理框架使用。</p><p id="395c" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><strong class="ir hi"> YARN </strong> : Spark应用可以在YARN (Hadoop NextGen)上运行。</p><p id="b3d2" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><strong class="ir hi">批处理&amp;实时处理</strong> : MapReduce和Spark一起使用，MapReduce用于批处理，Spark用于实时处理。</p></blockquote><h1 id="90bd" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">火花部件</h1><p id="2b9f" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark组件使得Apache Spark快速可靠。很多Spark组件都是为了解决使用Hadoop MapReduce时出现的问题而构建的。Apache Spark具有以下组件:</p><ol class=""><li id="1fa2" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi">火花芯</strong></li><li id="629f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">火花流</strong></li><li id="b32c" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi"> Spark SQL </strong></li><li id="8f6d" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi"> GraphX </strong></li><li id="5448" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi"> MLlib(机器学习)</strong></li></ol><h1 id="562e" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">火花核心</h1><p id="2ad7" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated"><em class="jn"> Spark Core </em>是大规模并行和分布式数据处理的基础引擎。核心是分布式执行引擎，Java、Scala和Python APIs为分布式ETL应用程序开发提供了一个平台。此外，构建在核心之上的额外库允许流、SQL和机器学习的不同工作负载。它负责:</p><ol class=""><li id="7d6c" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">内存管理和故障恢复</li></ol><p id="c64c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">2.在群集上调度、分发和监控作业</p><p id="f809" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">3.与存储系统交互</p><h1 id="a928" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">火花流</h1><p id="22eb" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated"><em class="jn"> Spark流</em>是Spark的组件，用于处理实时流数据。因此，它是对核心Spark API的有益补充。它支持实时数据流的高吞吐量和容错流处理。基本流单元是数据流，它基本上是一系列用于处理实时数据的弹性分布式数据集。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/c105637cbaf4f7d3166b7d30cc81e680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YJRthnBHIWB-Z3h-BA-9PQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lo">Spark Streaming — Spark Tutorial</em></figcaption></figure><h1 id="ef55" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Spark SQL</h1><p id="4365" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated"><em class="jn"> Spark SQL </em>是Spark中的一个新模块，它将关系处理与Spark的函数式编程API集成在一起。它支持通过SQL或Hive查询语言查询数据。对于那些熟悉RDBMS的人来说，Spark SQL将是从早期工具的简单过渡，在早期工具中，您可以扩展传统关系数据处理的边界。</p><p id="64f9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark SQL将关系处理与Spark的函数式编程集成在一起。此外，它提供了对各种数据源的支持，并使将SQL查询与代码转换结合起来成为可能，从而产生了一个非常强大的工具。</p><p id="8258" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="jn">以下是Spark SQL的四个库。</em> </strong></p><ol class=""><li id="b2f5" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">数据源API</li><li id="9957" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">数据框架API</li><li id="f529" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">解释器和优化器</li><li id="ee28" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">SQL服务</li></ol><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/86e9e83b785a2c78cf1deeb0236a2d19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjAydjTS2U5lrpXVAtEUGg.png"/></div></div></figure><h1 id="4df1" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">GraphX</h1><p id="6377" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">GraphX是用于图形和图形并行计算的Spark API。因此，它用弹性分布式属性图扩展了火花RDD。</p><p id="8829" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">属性图是一个有向多重图，它可以有多条平行边。每个边和顶点都有用户定义的相关属性。这里，平行边允许相同顶点之间的多种关系。在高层次上，GraphX通过引入弹性分布式属性图扩展了Spark RDD抽象:一个每个顶点和边都有属性的有向多图。</p><p id="330f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了支持图形计算，GraphX公开了一组基本操作符(例如，子图、joinVertices和mapReduceTriplets)以及Pregel API的优化变体。此外，GraphX还包含了越来越多的图形算法和构建器来简化图形分析任务。</p><h1 id="0de7" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">机器学习</h1><p id="58af" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated"><em class="jn"> MLlib </em>代表机器学习库。Spark MLlib用于在Apache Spark中执行机器学习。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es nd"><img src="../Images/0401146513324d9a2d0b3839676ec1c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GeVIjfQWqe1064KfALa6g.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Machine Learning Flow Diagram — Spark Tools</figcaption></figure><h1 id="20c2" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">使用火花的地震探测</h1><p id="f362" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">现在我们已经理解了Spark的核心概念，让我们使用Apache Spark解决一个现实生活中的问题。这将有助于我们有信心在未来从事任何Spark项目。</p><h2 id="cfea" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">问题陈述</strong>:</h2><p id="41bc" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated"><em class="jn">设计一个实时地震检测模型来发送救生警报，这应该可以改善其机器学习，以提供接近实时的计算结果。</em></p><h2 id="4f75" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">要求</strong>:</h2><ol class=""><li id="45d8" class="jo jp hh ir b is la iw lb ja mn je mo ji mp jm jt ju jv jw bi translated">实时处理数据</li><li id="8e26" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">处理来自多个来源的输入</li><li id="538f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">易于使用的系统</li><li id="625b" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">警报的批量传输</li></ol><p id="5f91" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将使用Apache Spark，它是满足我们需求的完美工具。</p><h2 id="37c7" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">数据集</strong>:</h2><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es ne"><img src="../Images/4948cb655ffede7cd3546db5b39b8bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8kv3VkULdrtjwLwnpK3lA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lo">Earthquake Dataset — Spark Tutorial</em></figcaption></figure><p id="4c6f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在继续之前，有一个概念我们必须了解，我们将在我们的地震探测系统中使用，它被称为接收器工作特性(ROC)。ROC曲线是图示二元分类器系统在其辨别阈值变化时的性能的图表。我们将使用数据集，通过Apache Spark中的机器学习来获得ROC值。</p><h2 id="df08" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">流程图</strong>:</h2><p id="62b4" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">下图清楚地解释了我们的<em class="jn">地震探测系统</em>中涉及的所有步骤。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/84226f07e5f4138607e51873241ec6a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8q0K-QVQuG0HF-Gq4Es9g.png"/></div></div><figcaption class="il im et er es in io bd b be z dx"><em class="lo">Flow diagram of Earthquake Detection using Apache Spark — Spark Tutorial</em></figcaption></figure><h2 id="7286" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">火花实现</strong>:</h2><p id="dc40" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">接下来，让我们使用Eclipse IDE for Spark来实现我们的项目。</p><p id="35c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">找到下面的伪代码:</p><pre class="lg lh li lj fd mq mr ms mt aw mu bi"><span id="8896" class="lx kd hh mr b fi mv mw l mx my">//Importing the necessary classes<br/>import org.apache.spark._<br/>...<br/>//Creating an Object earthquake<br/>object earthquake {<br/> def main(args: Array[String]) {<br/>  <br/>//Creating a Spark Configuration and Spark Context<br/>val sparkConf = new SparkConf().setAppName("earthquake").setMaster("local[2]")<br/>val sc = new SparkContext(sparkConf)<br/>  <br/>//Loading the Earthquake ROC Dataset file as a LibSVM file<br/>val data = MLUtils.loadLibSVMFile(sc, *Path to the Earthquake File* )<br/>  <br/>//Training the data for Machine Learning<br/>val splits = data.randomSplit( *Splitting 60% to 40%* , seed = 11L)<br/>val training = splits(0).cache()<br/>val test = splits(1)<br/> <br/>//Creating a model of the trained data<br/>val numIterations = 100<br/>val model = *Creating SVM Model with SGD* (  *Training Data* , *Number of Iterations* )<br/>  <br/>//Using map transformation of model RDD<br/>val scoreAndLabels = *Map the model to predict features* <br/>  <br/>//Using Binary Classification Metrics on scoreAndLabels<br/>val metrics = * Use Binary Classification Metrics on scoreAndLabels *(scoreAndLabels)<br/>val auROC = metrics. *Get the area under the ROC Curve*()<br/>  <br/>//Displaying the area under Receiver Operating Characteristic<br/>println("Area under ROC = " + auROC)<br/> }<br/>}</span></pre><p id="e1cc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">从我们的Spark程序中，我们得到ROC值为0.088137。我们将转换该值，以获得ROC曲线下的面积。</p><h2 id="25aa" class="lx kd hh bd ke ly lz ma ki mb mc md km ja me mf kq je mg mh ku ji mi mj ky mk bi translated"><strong class="ak">可视化结果</strong>:</h2><p id="49d9" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">我们将绘制ROC曲线，并与具体的地震点进行比较。当地震点超过ROC曲线时，这些点被视为大地震。根据我们计算ROC曲线下面积的算法，我们可以假设这些大地震都在里氏6.0级以上。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es nf"><img src="../Images/19ac1d6278b47b3490e4b8283fd0b6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NaKz6BmtU4ClYJ60flKs2Q.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Earthquake ROC Curve — Spark Tutorial</figcaption></figure><p id="cd3a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上图以橙色显示了地震线。蓝色区域是我们从Spark项目中获得的ROC曲线。让我们把曲线放大，以便获得更好的图像。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/4d3b06522cc85f4914de144a36e2b30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j7saC2rt_QRvHVTQJubprg.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Visualizing Earthquake Points — Spark Tutorial</figcaption></figure><p id="eca5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们绘制了地震曲线和ROC曲线。在橙色曲线位于蓝色区域上方的点处，我们预测地震为大地震，即震级大于6.0。因此，有了这些知识，我们可以使用Spark SQL并查询现有的Hive表来检索电子邮件地址，并向人们发送个性化的警告电子邮件。因此，我们再一次利用科技来拯救人类生命，让每个人的生活变得更好。</p><p id="28dc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，Apache Spark文章到此结束。我希望你喜欢阅读它，并发现它的信息。到目前为止，您一定已经很好地理解了Apache Spark是什么。实际操作的例子将为您在Apache Spark中遇到的任何未来项目提供所需的信心。</p><p id="f741" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">原来就是这样！我希望这篇博客能给你提供信息，增加你的知识。如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="367a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释Spark的各个方面。</p><blockquote class="lp lq lr"><p id="8996" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><em class="hh"> 1。</em> <a class="ae mz" rel="noopener" href="/edureka/spark-architecture-4f06dcf27387"> <em class="hh">阿帕奇星火架构</em> </a></p><p id="14b1" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><em class="hh"> 2。</em> <a class="ae mz" rel="noopener" href="/edureka/spark-streaming-92bdcb1d94c4"> <em class="hh">火花串流教程</em> </a></p><p id="9867" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><em class="hh"> 3。</em><a class="ae mz" rel="noopener" href="/edureka/spark-mllib-e87546ac268"><em class="hh">Spark ml lib</em></a></p><p id="eef3" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><em class="hh"> 4。</em> <a class="ae mz" rel="noopener" href="/edureka/spark-sql-tutorial-6de1e241bf76"> <em class="hh"> Spark SQL教程</em> </a></p><p id="84a3" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><em class="hh"> 5。</em> <a class="ae mz" rel="noopener" href="/edureka/spark-graphx-f9bd805ac429"> <em class="hh"> Spark GraphX教程</em> </a></p><p id="f0e6" class="ip iq jn ir b is it iu iv iw ix iy iz ls jb jc jd lt jf jg jh lu jj jk jl jm ha bi translated"><em class="hh"> 6。</em> <a class="ae mz" rel="noopener" href="/edureka/spark-java-tutorial-cb2f54991c2b"> <em class="hh"> Spark Java教程</em> </a></p></blockquote></div><div class="ab cl ng nh go ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ha hb hc hd he"><p id="1577" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">原载于2017年5月4日www.edureka.co</em><em class="jn"/><a class="ae mz" href="https://www.edureka.co/blog/spark-tutorial/" rel="noopener ugc nofollow" target="_blank"><em class="jn">。</em></a></p></div></div>    
</body>
</html>