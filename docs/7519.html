<html>
<head>
<title>Connecting to Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">连接到Spark</h1>
<blockquote>原文：<a href="https://medium.com/version-1/motivation-2d8e6bf61c3c?source=collection_archive---------3-----------------------#2022-02-24">https://medium.com/version-1/motivation-2d8e6bf61c3c?source=collection_archive---------3-----------------------#2022-02-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="3ac5" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">动机</h1><p id="b459" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Spark是数据工程、数据科学和机器学习任务的伟大引擎。Spark提供了完成工作的能力，这在普通工作站上是不可能的，它通过提供一个可以执行代码和提取结果的分布式环境来管理。这很好，但是，如何从Docker容器中运行的应用程序中利用Spark功能呢？</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ka"><img src="../Images/92d4c826ac74bae365dd8e7c8128c32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4uWBJ-9ZHOouSvq8D99flw.png"/></div></div></figure><h1 id="1f99" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">选择</h1><p id="2e68" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">对于手头的任务，我们有什么选择？有几个选项可供选择:</p><p id="70ac" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">Apache Livy声称允许我们与Spark容器进行交互，运行作业或进行交互会话。</p><p id="e5aa" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated"><strong class="je hi"> Databricks Connect </strong>，声称允许我们将定制应用程序直接连接到Databriks集群。</p><p id="18a0" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">这两种方法都有缺点和优点，下面列出了其中一些。</p><h1 id="6156" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">阿帕奇·李维</h1><p id="411c" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Apache Livy是一个通过REST接口与Spark集群轻松交互的服务。这个应用程序需要一些设置，并且需要访问Spark安装目录。</p><p id="add6" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">然而，该服务提供了REST访问，这种访问与构建消费应用程序的语言无关。此外，Livy还提供了一些其他功能，如安全性、容错和共享缓存数据。该服务的用例是一个无法使用云服务而必须使用现场Spark实例的场景。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kr"><img src="../Images/c8d3c34970c8bb615ff376172b59e01b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZNJUPSu_sT1ViMrd0X5Ifw.png"/></div></div></figure><p id="74d0" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">来自https://livy.apache.org<a class="ae ks" href="https://livy.apache.org" rel="noopener ugc nofollow" target="_blank">的</a></p><h1 id="bae8" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">数据块连接</h1><p id="3eb8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Databricks connect允许您将IDE或自定义应用程序连接到Databricks集群，并根据需要运行代码。这是一个非常简单的设置，不需要许多配置步骤或安装。</p><p id="6704" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">这种方法的一个明显缺点是依赖于数据块，因为这种方法不能用于plain Spark。如果由于这样或那样的原因，您不能使用云，而必须使用现场Spark，那么这种方法是不合适的。然而，如果可以使用云和数据库，那么这种方法似乎相对简单易用。</p><h1 id="03a7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">履行</h1><p id="2d33" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在本次评估中，我们选择了Databricks Connect来连接Spark，因为我们没有异地Spark，并且我们可以使用云来存储数据和应用程序。</p><h1 id="9b80" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">配置</h1><p id="7865" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">这里的<a class="ae ks" href="https://docs.databricks.com/dev-tools/databricks-connect.html" rel="noopener ugc nofollow" target="_blank">介绍了安装和配置，不会重复，因此您需要运行配置步骤。然而，确保所有现有的<strong class="je hi"> pyspark </strong>库都从包含连接器的Python虚拟环境中移除是很重要的。</a></p><p id="9046" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">该配置创建了<strong class="je hi">。用户主目录中的databricks-connect </strong>文件。配置期间该文件的内容看起来有点像这样:</p><p id="19da" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi">{</p><p id="6353" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">《主持人》:"https://adb-453245345.4.azuredatabricks.net/？o=45645645645645# "，</p><p id="a9a1" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">" token ":" ewrtdfg 56456 dhgdfgh 5 dgjjkljkldfgd "，</p><p id="de57" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">" org_id": "45645645645645 "，</p><p id="ba20" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">“端口”:“15001”，</p><p id="85ef" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">" cluster _ id ":" 34534–345345-ices 56567 "</p><p id="5a5e" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi">}</p><p id="3ac7" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">所有的值都可以在数据库门户中获得；创建用户令牌的位置。成功配置后，复制<strong class="je hi">。将</strong>文件连接到项目目录中。</p><h1 id="176c" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">码头工人</h1><p id="95c6" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">您还需要更新您的Docker文件以包含一行代码，它复制了<strong class="je hi">。将</strong>文件连接到主目录。</p><p id="c736" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">收到。数据块-连接/根</p><p id="ab7c" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">根据您的设置，如果您还没有JDK，您可能还需要在配置中包含它。</p><h1 id="696e" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">应用</h1><p id="f735" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">应用程序部分是这里的最后一步。这种方法不是说明性的，它描述了POC是如何完成的。</p><p id="c5b7" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">该应用程序由一个Flask应用程序组成，它通过公开REST端点来充当服务模块。这并没有什么特别的原因，除了我们需要一个按钮，这样我们就可以看到这个过程在工作。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ka"><img src="../Images/5949570c546749d8527f78a70351c010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Arx2eX8yAijt9-SHslVj1w.png"/></div></div></figure><p id="c95c" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">服务模块从代理包中创建一个代理实例。代理的实例是创建和封装Spark会话的实例。</p><pre class="kb kc kd ke fd kt ku kv kw aw kx bi"><span id="5da7" class="ky if hh ku b fi kz la l lb lc">self.spark = SparkSession.builder.getOrCreate()</span></pre><p id="ad8e" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">代理的同一个实例公开了与Spark会话交互的函数。在创建Spark会话的过程中，将连接到在<strong class="je hi">中定义的集群。数据块-用适当的凭证连接</strong>。现在，您可以根据需要使用这个Spark会话。</p><h1 id="2ed9" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">结论</h1><p id="d3e3" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">有几种方法可以从本地环境连接到Spark集群，并通过REST接口访问Spark会话或运行代码。使用主要取决于外部因素(比如使用云的可能性)，次要取决于个人偏好。</p><p id="b26f" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">Apache Livy方式似乎更适合于现场环境，它为不同的消费者提供了一个公共接口，并且适合于某些产品。</p><p id="77d1" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">上面使用和描述的数据块连接提供了对Spark会话的访问，并提供了对Spark集群的快速和直接的访问。</p><p id="1912" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">Apache Livy和Databricks Connect构建了不同的策略来处理对Spark集群的访问。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ld"><img src="../Images/803ada61d6cb95be17caecb300951627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXQ6cu5L4XvMZ9rmPAMdAg.jpeg"/></div></div></figure><p id="505b" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">亚历山大·苏沃罗夫是第一版的顾问。</p></div></div>    
</body>
</html>