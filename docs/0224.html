<html>
<head>
<title>Mussel — Airbnb’s Key-Value Store for Derived Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">muscle——Airbnb派生数据的键值存储</h1>
<blockquote>原文：<a href="https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296?source=collection_archive---------1-----------------------#2022-10-10">https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296?source=collection_archive---------1-----------------------#2022-10-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d387" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">Airbnb如何构建持久、高可用性和低延迟的键-值存储引擎，以访问来自离线和流媒体活动的派生数据。</strong></p><p id="478a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> By: </strong> <a class="ae jc" href="http://linkedin.com/in/chandramoulir" rel="noopener ugc nofollow" target="_blank">钱德拉穆里</a>，<a class="ae jc" href="http://linkedin.com/in/shouyan-guo" rel="noopener ugc nofollow" target="_blank">寿言郭</a>，<a class="ae jc" href="http://linkedin.com/in/yuxijin" rel="noopener ugc nofollow" target="_blank">俞希金</a></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/3b1e5843cfa524a2314eb8a47614f892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DviEp2cHiuC4NoH5"/></div></div></figure><h1 id="c844" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">介绍</h1><p id="c399" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">在Airbnb中，许多在线服务需要访问派生数据，这些数据是通过Spark等大规模数据处理引擎或Kafka等流媒体事件计算的数据，并离线存储。这些服务需要高质量的衍生数据存储系统，具有强大的可靠性、可用性、可扩展性和延迟保证，以服务在线流量。例如，user profiler服务存储和访问Airbnb上的实时和历史用户活动，以提供更个性化的体验。</p><p id="0b14" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我们将讨论我们如何利用大量开源技术，包括<a class="ae jc" href="https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/regionserver/HRegion.html" rel="noopener ugc nofollow" target="_blank"> HRegion </a>、<a class="ae jc" href="https://helix.apache.org/" rel="noopener ugc nofollow" target="_blank"> Helix </a>、<a class="ae jc" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Spark </a>、<a class="ae jc" href="https://zookeeper.apache.org/" rel="noopener ugc nofollow" target="_blank"> Zookeeper </a>和<a class="ae jc" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank"> Kafka </a>来为数百个Airbnb产品和平台用例构建可扩展的低延迟键值商店。</p><h1 id="b81a" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">Airbnb的衍生数据</h1><p id="001b" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">在过去的几年里，Airbnb发展并增强了我们对衍生数据服务的支持，从推出定制解决方案的团队转移到名为muscle的多租户存储平台。这一演变可以概括为三个阶段:</p><p id="6cd8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第一阶段(2015年1月):统一只读键值存储(HFileService) </strong></p><p id="8021" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在2015年之前，Airbnb内部没有满足四个关键要求的统一键值存储解决方案:</p><ol class=""><li id="5b1a" class="ks kt hh ig b ih ii il im ip ku it kv ix kw jb kx ky kz la bi translated">扩展到Pb级数据</li><li id="124d" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">高效批量加载(批量生成和上传)</li><li id="be2c" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">低延迟读取(&lt; 50毫秒p99)</li><li id="ec9e" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">可供多个客户使用的多租户存储服务</li></ol><p id="af1f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，现有的解决方案都不能满足这些要求。<a class="ae jc" href="https://www.mysql.com/" rel="noopener ugc nofollow" target="_blank"> MySQL </a>不支持批量加载，<a class="ae jc" href="https://hbase.apache.org/" rel="noopener ugc nofollow" target="_blank"> Hbase </a>的大规模批量加载(distcp)不是最佳和可靠的，RocksDB没有内置的水平分片，我们没有足够的C++专业知识来构建支持RocksDB文件格式的批量加载管道。</p><p id="ce91" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">于是我们构建了HFileService，内部使用了<a class="ae jc" href="http://devdoc.net/bigdata/hbase-0.98.7-hadoop1/book/hfilev2.html#:~:text=HFile%20is%20a%20low%2Dlevel,to%20write%20those%20inline%20blocks." rel="noopener ugc nofollow" target="_blank">HFile</a>(Hadoop h base的构建块，基于Google的SSTable):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lg"><img src="../Images/cf1078bfff795023653962af3a98e612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*38vsUsNg4FnouSzp"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx"><em class="ll">Fig. 1: HFileService Architecture</em></figcaption></figure><ol class=""><li id="7b58" class="ks kt hh ig b ih ii il im ip ku it kv ix kw jb kx ky kz la bi translated">服务器经过分片和复制，以解决可扩展性和可靠性问题</li><li id="9a07" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">碎片的数量是固定的(相当于批量加载作业中Hadoop reducers的数量),服务器到碎片的映射存储在Zookeeper中。我们通过手动更改Zookeeper中的映射来配置映射到特定shard的服务器数量</li><li id="b7f0" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">一个日常的Hadoop任务将离线数据转换为HFile格式，并上传到S3。每台服务器都将自己分区的数据下载到本地磁盘，并删除旧版本的数据</li><li id="ff47" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">不同的数据源由主键划分。客户端通过计算主键的散列值并以碎片总数为模来确定其请求应该到达的正确碎片。然后查询Zookeeper以获得拥有这些碎片的服务器列表，并向其中一个服务器发送请求</li></ol><p id="72e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第二阶段(2015年10月):存储实时数据和衍生数据(Nebula) </strong></p><p id="aef3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然我们构建了一个支持高效批量装载和低延迟读取的多租户键值存储，但它也有缺点。例如，它不支持点、低延迟写入，并且对存储数据的任何更新都必须通过每日批量加载作业。随着Airbnb的发展，对低延迟访问实时数据的需求越来越大。</p><p id="9fee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，Nebula被构建为在单个系统中支持批量更新和实时数据。它内部使用DynamoDB存储实时数据，使用S3/HFile存储批量更新数据。Nebula引入了基于时间戳的版本控制机制作为版本控制机制。对于读取请求，将从HFileService中的动态表列表和静态快照中读取数据，并基于时间戳合并结果。</p><p id="50c7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了最大限度地减少在线合并操作，Nebula还安排了每天运行的spark作业，并将DynamoDB数据的快照与HFileService的静态快照合并。Zookeeper用于协调动态表的写可用性、被标记为ready的快照以及过时表的丢弃。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lm"><img src="../Images/0c753033e951fd8b40af02f1020623af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EGFSNgoYuewt2NptXXIdxA.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">Fig. 2: Nebula Architecture</figcaption></figure><p id="564c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">第3阶段(2018年):可扩展和低延迟的键值存储引擎(muslet)</strong></p><p id="0032" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在第3阶段，我们构建了一个支持实时读写和批量更新数据的系统，并提供了基于时间戳的冲突解决方案。然而，仍有改进的机会:</p><ol class=""><li id="aae4" class="ks kt hh ig b ih ii il im ip ku it kv ix kw jb kx ky kz la bi translated">向外扩展的挑战:随着数据增长，在Zookeeper中手动编辑分区映射很麻烦，或者通过添加额外的节点来横向扩展系统以增加流量也很麻烦</li><li id="c079" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">在峰值写入流量下提高读取性能</li><li id="b2e7" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">高维护开销:我们需要同时维护HFileService和DynamoDB</li><li id="f86e" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">低效的合并过程:随着我们的总数据变得越来越大，合并DynamoDB和HFileService每日增量更新的过程变得非常缓慢。DynamoDB中的每日更新数据仅为HFileService中基线数据的1–2%。但是，我们每天将完整的快照(总数据大小的102%)重新发布回HFileService</li></ol><p id="f4a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解决这些缺点，我们想出了一个新的键值存储系统，叫做<strong class="ig hi">musle</strong>。</p><ol class=""><li id="87ad" class="ks kt hh ig b ih ii il im ip ku it kv ix kw jb kx ky kz la bi translated">我们引入了Helix来管理集群中的分区映射</li><li id="fff8" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">我们利用Kafka作为复制日志，将写入复制到所有副本，而不是直接写入muscle存储</li><li id="525d" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">我们使用HRegion作为muscle存储节点中唯一的存储引擎</li><li id="7b97" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">我们构建了一个Spark管道，将数据从数据仓库直接加载到存储节点</li></ol><p id="675e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们在下面的段落中更详细地讨论一下。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ln"><img src="../Images/d5694405e16d9d96f5e55eb7d9de2b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p_mMDfHNFVebMhGA3MXBvg.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">Fig. 3: Mussel Architecture</figcaption></figure><p id="dde9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">用Helix管理分区</strong></p><p id="5824" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在muscle中，为了使我们的集群更具可伸缩性，我们将碎片的数量从HFileService中的8个增加到了1024个。在muscle中，数据通过主键的散列划分到这些碎片中，所以我们引入了Apache Helix来管理这些逻辑碎片。Helix自动管理逻辑碎片到物理存储节点的映射。每个muscle存储节点可以保存多个逻辑碎片。每个逻辑碎片都在多个muscle存储节点上复制。</p><p id="08b1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">卡夫卡式的无领导复制</strong></p><p id="ded9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于muscle是一个读取量很大的商店，所以我们采用了无领导架构。读取请求可以由具有相同逻辑碎片的任何Mussel存储节点提供服务，这增加了读取的可伸缩性。在写入路径中，我们需要考虑以下几点:</p><ol class=""><li id="d984" class="ks kt hh ig b ih ii il im ip ku it kv ix kw jb kx ky kz la bi translated">我们希望平滑写入流量，以避免对读取路径的影响</li><li id="ff02" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">因为我们在每个分片中没有领导节点，所以我们需要一种方法来确保每个Mussel存储节点以相同的顺序应用写请求，以便不同节点之间的数据是一致的</li></ol><p id="fea7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了解决这些问题，我们在这里引入了Kafka作为预写日志。对于写请求，它将首先异步写入Kafka，而不是直接写入muscle存储节点。Kafka主题有1024个分区，每个分区属于Mussel中的一个逻辑片。每个贻贝存储节点将轮询来自Kafka的事件，并将更改应用到其本地存储。由于碎片之间没有主从关系，这种配置允许分区内正确的写入顺序，从而确保一致的更新。这里的缺点是它只能提供最终的一致性。但是，考虑到派生的数据用例，为了确保可用性和分区容差，在一致性上做出妥协是可以接受的。</p><p id="331d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">在一个存储引擎中支持读取、写入和压缩</strong></p><p id="6328" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了降低管理DynamoDB的硬件成本和操作负载，我们决定将其移除，并扩展HFileService作为唯一的存储引擎来同时服务于实时和离线数据。为了更好地支持读写操作，我们使用了<a class="ae jc" href="https://hbase.apache.org/1.1/apidocs/org/apache/hadoop/hbase/regionserver/HRegion.html" rel="noopener ugc nofollow" target="_blank"> HRegion </a>而不是<a class="ae jc" href="https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/HFile.html" rel="noopener ugc nofollow" target="_blank"> Hfile </a>。HRegion是一个全功能的键值存储，具有MemStore和BlockCache。在内部，它使用日志结构的合并(LSM)树来存储数据，并支持读写操作。</p><p id="5035" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">HRegion表包含列族，列族是列的逻辑和物理分组。列族中有列限定符，它们是列。柱族包含带有时间戳版本的柱。列只有在被插入时才存在，这使得HRegion成为一个稀疏数据库。我们将客户数据映射到HRegion，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lo"><img src="../Images/6150b87526248dae61e96eb848899d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1wtQhbeeNTrCLsChR4BDLQ.png"/></div></div></figure><p id="478e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过这种映射，对于读取查询，我们能够支持:</p><ol class=""><li id="8e12" class="ks kt hh ig b ih ii il im ip ku it kv ix kw jb kx ky kz la bi translated">用主键查找数据的点查询</li><li id="2132" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">通过扫描辅助关键字上的数据进行前缀/范围查询</li><li id="fb3e" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">查询最新数据或特定时间范围内的数据，因为写入Mussel的实时和离线数据都有时间戳</li></ol><p id="113b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因为我们在Mussel中有超过4000个客户端表，所以每个用户表都映射到HRegion中的一个列族，而不是它自己的表，以减少元数据管理层的可伸缩性挑战。此外，由于HRegion是一个基于列的存储引擎，每个列族都存储在一个单独的文件中，因此可以独立地读/写它们。</p><p id="3522" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于写请求，它使用来自Kafka的写请求，并调用HRegion put API直接写数据。对于每个表，它还可以支持定制最大版本和TTL(生存时间)。</p><p id="79fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们用HRegion处理写请求时，另一件要考虑的事情是压缩。需要运行压缩来清理已删除或已达到最大版本或最大TTL的数据。同样，当HRegion中的MemStore达到一定大小时，它会被刷新到磁盘的StoreFile中。压缩会将这些文件合并在一起，以减少磁盘寻道并提高读取性能。然而，另一方面，当压缩运行时，它会导致更高的cpu和内存使用率，并阻止写入以防止JVM (Java虚拟机)堆耗尽，这会影响集群的读写性能。</p><p id="1628" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这里，我们使用Helix将每个逻辑碎片的Mussel存储节点标记为两种类型的资源:在线节点和批处理节点。例如，如果一个逻辑片有9个Mussel存储节点，其中6个是在线节点，3个是批处理节点。在线和批量的关系是:</p><ol class=""><li id="70fe" class="ks kt hh ig b ih ii il im ip ku it kv ix kw jb kx ky kz la bi translated">它们都服务于写请求</li><li id="cab9" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">只有在线节点为读取请求提供服务，我们对在线节点上的压缩进行了速率限制，以获得良好的读取性能</li><li id="3bdc" class="ks kt hh ig b ih lb il lc ip ld it le ix lf jb kx ky kz la bi translated">Helix安排在线节点和批处理节点之间的每日轮换。在上面的示例中，它将3个在线节点移动到批处理，并将3个批处理节点移动到在线，这样这3个新的批处理节点就可以执行全速主压缩来清理旧数据</li></ol><p id="d03a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有了这一改变，现在我们能够用一个存储引擎同时支持读取和写入。</p><p id="9e25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">支持从数据仓库进行批量加载</strong></p><p id="7d4d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们支持两种类型的通过<a class="ae jc" href="https://airflow.apache.org/" rel="noopener ugc nofollow" target="_blank">气流</a>从数据仓库到muscle的批量装载管道:合并类型和替换类型。合并类型意味着合并来自数据仓库的数据和来自先前写操作的带有旧时间戳的数据。替换意味着从数据仓库导入数据，并删除所有带有以前时间戳的数据。</p><p id="d82a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们利用Spark将数据仓库中的数据转换成HFile格式，并上传到S3。每个muscle存储节点下载文件，并使用HRegion bulkLoadHFiles API将这些HFiles加载到列族中。</p><p id="bf6e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有了这个批量加载管道，我们可以每天只将增量数据加载到集群中，而不是完整的数据快照。在迁移之前，用户配置文件服务每天需要将大约4TB的数据加载到集群中。之后，它只需要加载大约40–80GB，大大降低了成本并提高了群集的性能。</p><h1 id="48a2" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论和下一步措施</h1><p id="b011" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">在过去的几年里，Airbnb在为我们的工程师提供高质量的衍生数据存储方面取得了很大进展。最新的键值存储muscle在Airbnb中被广泛使用，并已成为任何基于键值的应用程序的基础构建块，具有强大的可靠性、可用性、可伸缩性和性能保证。自从引入以来，在muscle中创建了大约4000个表，在我们的生产集群中存储了大约130TB的数据，而没有复制。muscle一直在可靠地为大量读取、写入和批量加载请求提供服务:例如，我们最大的集群muscle-general已经实现了&gt; 99.9%的可用性，平均读取QPS &gt; 800k，平均写入QPS &gt; 35k，平均P95读取延迟小于8毫秒。</p><p id="ed09" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管muscle可以很好地服务于我们当前的用例，但仍有许多改进的机会。例如，我们期待为我们的客户提供写后读一致性。我们还希望基于集群中的流量实现自动扩展和重新分区。我们期待着尽快分享更多的细节。</p><h1 id="9529" class="jp jq hh bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">感谢</h1><p id="5279" class="pw-post-body-paragraph ie if hh ig b ih kn ij ik il ko in io ip kp ir is it kq iv iw ix kr iz ja jb ha bi translated">muscle是Airbnb仓储团队的合作成果，团队成员包括:<a class="ae jc" href="http://linkedin.com/in/calvinzou" rel="noopener ugc nofollow" target="_blank"> Calvin Zou </a>、<a class="ae jc" href="linkedin.com/in/dionitas" rel="noopener ugc nofollow" target="_blank"> Dionitas Santos </a>、<a class="ae jc" href="http://linkedin.com/in/ruan-maia-367281161" rel="noopener ugc nofollow" target="_blank"> Ruan Maia </a>、<a class="ae jc" href="http://linkedin.com/in/wonheec" rel="noopener ugc nofollow" target="_blank"> Wonhee Cho </a>、<a class="ae jc" href="linkedin.com/in/xiaomou-wang-5880b537" rel="noopener ugc nofollow" target="_blank">王小谋</a>、<a class="ae jc" href="linkedin.com/in/yanhan-zhang-724088a4" rel="noopener ugc nofollow" target="_blank">张艳菡</a>。</p><p id="5780" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有兴趣在Airbnb仓储团队工作吗？看看这个角色:<a class="ae jc" href="https://careers.airbnb.com/positions/3029584/" rel="noopener ugc nofollow" target="_blank">员工软件工程师，分布式存储</a></p></div></div>    
</body>
</html>