<html>
<head>
<title>Introduction to Naive Bayes Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类器简介</h1>
<blockquote>原文：<a href="https://medium.com/edureka/naive-bayes-tutorial-80939835d5cb?source=collection_archive---------0-----------------------#2018-08-07">https://medium.com/edureka/naive-bayes-tutorial-80939835d5cb?source=collection_archive---------0-----------------------#2018-08-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/cf443daf322d22918b795baddfed7aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*P8aPyn1SzSpJ0Z2HolsdWw.jpeg"/></div></figure><p id="b00b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在一个充满机器学习和人工智能的世界，围绕着我们周围的几乎一切，分类和预测是机器学习最重要的方面之一，而朴素贝叶斯是一种简单但惊人强大的预测建模算法，据<strong class="in hi"> <em class="jj">机器学习行业专家</em> </strong>。所以，伙计们，在这个朴素贝叶斯教程中，我将涵盖以下主题:</p><ul class=""><li id="c80d" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">什么是朴素贝叶斯？</strong></li><li id="641b" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">什么是贝叶斯定理？</strong></li><li id="b578" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">利用贝叶斯定理的游戏预测</strong></li><li id="fe40" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">业内的朴素贝叶斯</strong></li><li id="57d0" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">逐步实现朴素贝叶斯</strong></li><li id="f53b" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">带SKLEARN的朴素贝叶斯</strong></li></ul><h1 id="c658" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">什么是朴素贝叶斯？</h1><p id="ab13" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">朴素贝叶斯是用于<strong class="in hi">分类</strong>的最简单和最强大的算法之一，它基于贝叶斯定理，假设预测器之间是独立的。朴素贝叶斯模型易于构建，对于非常大的数据集尤其有用。这个算法有两个部分:</p><ul class=""><li id="30e3" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">天真</strong></li><li id="3ec4" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">贝叶斯</strong></li></ul><p id="e4ee" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">朴素贝叶斯分类器假设类中某个要素的存在与任何其他要素无关。即使这些特征相互依赖或依赖于其他特征的存在，所有这些属性独立地促成了特定水果是苹果、桔子或香蕉的概率，这就是为什么它被称为<strong class="in hi">“天真”。</strong></p><p id="cab9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们继续我们的朴素贝叶斯教程博客，理解贝叶斯定理。</p><h1 id="b825" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">什么是贝叶斯定理？</h1><p id="12c0" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">在统计学和概率论中，贝叶斯定理描述了一个事件发生的概率，它基于可能与该事件相关的条件的先验知识。这是一种计算条件概率的方法。</p><p id="503e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">给定一个假设<strong class="in hi"> H </strong>和证据<strong class="in hi"> E，</strong>贝叶斯定理说明，得到证据<strong class="in hi"> P(H) </strong>前假设的概率和得到证据<strong class="in hi"> P(H|E) </strong>后假设的概率之间的关系为:</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/648ea8090a61b8dac6430b31560605d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/0*MmZKCbCykZj_S14k.png"/></div></figure><p id="bc66" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这就把得到证据前的假设概率<strong class="in hi"> P(H) </strong>，和得到证据后的假设概率<strong class="in hi"> P(H|E) </strong>联系起来了。为此，将<strong class="in hi">称为先验概率</strong>，而将<strong class="in hi"> P(H|E) </strong>称为<strong class="in hi">后验概率</strong>。将两者联系起来的因子<strong class="in hi"> P(H|E) / P(E) </strong>称为<strong class="in hi">似然比</strong>。使用这些术语，贝叶斯定理可以重新表述为:</p><p id="527f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">"后验概率等于先验概率乘以似然比。"</p><p id="91d6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">走的有点迷茫？别担心。<br/>让我们继续我们的朴素贝叶斯教程博客，用一个简单的概念来理解这个概念。</p><h2 id="257b" class="lg jz hh bd ka lh li lj ke lk ll lm ki iw ln lo km ja lp lq kq je lr ls ku lt bi translated">贝叶斯定理示例</h2><p id="c547" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">假设我们有一副牌，我们希望找出“<strong class="in hi">我们随机挑选的牌成为王的概率，假设它是一张脸牌</strong>”。所以，根据贝叶斯定理，我们可以解决这个问题。首先，我们需要找出概率</p><ul class=""><li id="29ed" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">P(k)</strong>即<strong class="in hi"> 4/52 </strong>，因为一副牌中有4张k。</li><li id="bd0c" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi"> P(脸|王)</strong>等于<strong class="in hi"> 1 </strong>因为所有的王都是脸牌。</li><li id="e105" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi"> P(面)</strong>等于<strong class="in hi"> 12/52 </strong>因为一套13张牌中有3张面牌，总共有4套。</li></ul><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/09982cd8192fd9ede173fbeea8513115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*RZwu1Tt_IXkZBx-eU67DlQ.png"/></div></div></figure><p id="89fd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，把所有的值放入贝叶斯方程，我们得到的结果是<strong class="in hi"> 1/3 </strong></p><h1 id="a845" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">利用贝叶斯定理进行游戏预测</h1><p id="7937" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">让我们继续我们的朴素贝叶斯教程博客，并预测我们拥有的天气数据的未来。</p><p id="08e1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这是我们的数据，包括天气、天气、湿度、风况和我们必须预测的最后一栏。</p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/38d93f290c47fc40aba51e7bd51b53da.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*w4-G6DH7GbxUqoHTMXUG0w.png"/></div></figure><ul class=""><li id="83db" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated">首先，我们将使用数据集的每个属性创建一个<strong class="in hi">频率</strong>表。</li></ul><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es ma"><img src="../Images/fd34bcf9be83ad7ed0105c8405281692.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*XQODl_8-iUQepZFNNXZ9uw.png"/></div></figure><ul class=""><li id="ac04" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated">对于每个频率表，我们将生成一个<strong class="in hi">可能性</strong>表。</li></ul><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/69ebb257a2dc77eb775986ae90c8ac14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*HQKLncKWR01Jiq0sO-hoWg.png"/></div></figure><ul class=""><li id="bea3" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated">给定'<strong class="in hi">晴</strong>'的'<strong class="in hi">是</strong>的可能性为:</li></ul><p id="962e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> P(c|x) = P(是|晴)= P(晴|是)* P(是)/ P(晴)= (0.3 x 0.71) /0.36 = 0.591 </strong></p><ul class=""><li id="5932" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated">类似地，假设'<strong class="in hi">晴</strong>'，则'<strong class="in hi">无</strong>'的可能性为:</li></ul><p id="7f10" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">P(c | x)= P(No | Sunny)= P(Sunny | No)* P(No)/P(Sunny)=(0.4 x 0.36)/0.36 = 0.40</strong></p><ul class=""><li id="09fc" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated">现在，以同样的方式，我们还需要为其他属性创建可能性表。</li></ul><figure class="lc ld le lf fd ii er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mb"><img src="../Images/76c3230130646cb774a5c6a020f1926e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q8GCk_lne0Oloe1cxhcKHg.png"/></div></div></figure><p id="54ad" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">假设我们有一个<strong class="in hi">日</strong>，其值如下:</p><ul class=""><li id="a490" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">展望=下雨</strong></li><li id="db7b" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">湿度=高</strong></li><li id="dcd6" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">风=弱</strong></li><li id="f125" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">玩=？</strong></li><li id="a37f" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated">所以，有了数据，我们就要预测“那天能不能玩”。</li></ul><p id="78bd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">当天“是”的可能性=  P(前景=下雨|是)*P(湿度=高|是)*P(风=弱|是)* P(是)</p><p id="9785" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">当天“否”的可能性= </strong> P(前景=下雨|否)*P(湿度=高|否)*P(风=弱|否)* P(否)</p><ul class=""><li id="1139" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated">现在我们将这些值归一化，然后</li></ul><p id="0a88" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi"> P(是)= 0.0199/(0.0199+0.0166)= 0.55</strong></p><p id="a7e5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">P(No)= 0.0166/(0.0199+0.0166)= 0.45</strong></p><ul class=""><li id="b60f" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated">我们的模型预测明天有<strong class="in hi"> 55% </strong>的几率有比赛。</li></ul><h1 id="d99b" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">行业中的朴素贝叶斯</h1><p id="3b26" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">现在你已经知道了什么是朴素贝叶斯，它是如何工作的，让我们看看它在行业中的应用。</p><p id="dadf" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">新闻分类:</strong></p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/db8394c7da03f534264b42ab42102c77.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*Vk-nfEYRN-mibKPBiBnveQ.png"/></div></figure><p id="df56" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">从我们的第一个工业应用开始，它是<strong class="in hi">新闻分类，</strong>或者我们可以使用术语文本分类来拓宽这个算法的范围。web上的新闻正在快速增长，每个新闻网站都有自己不同的布局和新闻分类。公司使用网络爬虫从新闻文章内容的HTML页面中提取有用的文本来构建全文RSS。每条新闻内容都被<strong class="in hi">符号化</strong>(分类)。为了获得更好的分类结果，我们从文档中移除不太重要的单词，即stop单词。我们应用朴素贝叶斯分类器对基于新闻编码的新闻内容进行分类。</p><p id="d47c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">垃圾邮件过滤:</strong></p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es md"><img src="../Images/8260946741a9fd81cbcfb7ed60e53bb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*CH4v_-dWDfJ_PeIhnVdxcA.png"/></div></figure><p id="c3dd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">朴素贝叶斯分类器是一种流行的电子邮件过滤统计技术。他们通常使用单词包特征来识别垃圾邮件，这是文本分类中常用的方法。朴素贝叶斯分类器的工作原理是将标记(通常是单词，有时是其他东西)的使用与垃圾邮件和非垃圾邮件相关联，然后使用贝叶斯定理来计算电子邮件是或不是垃圾邮件的概率。</p><p id="6c44" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">特定的单词在垃圾邮件和合法邮件中出现的概率是特定的。例如，大多数电子邮件用户会经常在垃圾邮件中遇到单词“彩票”和“幸运抽奖”，但在其他电子邮件中很少看到。电子邮件中的每个单词都会影响电子邮件的垃圾邮件概率，或者只影响最有趣的单词。这个贡献被称为<strong class="in hi">后验概率</strong>，并使用<strong class="in hi">贝叶斯定理进行计算。</strong>然后，计算电子邮件中所有单词的垃圾邮件概率，如果总数超过某个阈值(比如95%)，过滤器会将该电子邮件标记为垃圾邮件。</p><p id="7d35" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">医疗诊断:</strong></p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es me"><img src="../Images/f7bbd525fc9cf8adb4cb2ae1acb8472d.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*mtQpkYmtA29AM6qqGbzXiA.jpeg"/></div></figure><p id="02c4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如今，现代医院配备了很好的监测和其他数据收集设备，导致通过健康检查和医疗连续收集大量数据。吸引医生的朴素贝叶斯方法的主要优点之一是<strong class="in hi">“所有可用的信息都用来解释决策”</strong>。这种解释对于医学诊断和预后来说似乎是“自然的”,即接近于医生诊断患者的方式。</p><p id="34e4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在处理医疗数据时，朴素贝叶斯分类器会考虑来自许多属性的证据，以做出最终预测，并为其决策提供透明的解释，因此被认为是支持医生决策的最有用的分类器之一。</p><p id="4a38" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">天气预报:</strong></p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/235741900a590029566470696ee594e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*RgWTGk0bo7ak0qRbXmteCw.png"/></div></figure><p id="a62c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">天气是我们日常生活中最具影响力的因素之一，在某种程度上，它可能会影响一个依赖农业等职业的国家的经济。多年来，天气预报一直是气象部门的一个挑战性问题。即使在技术和科学进步之后，天气预报的准确性仍然不够。</p><p id="3169" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">使用基于贝叶斯方法的天气预测模型，其中<strong class="in hi">后验概率</strong>用于计算输入数据实例的每个类别标签的<strong class="in hi">似然度</strong>，具有<strong class="in hi">最大似然度</strong>的那个被认为是结果输出。</p><h1 id="ebb1" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">朴素贝叶斯的逐步实现</h1><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/9f443779c15fcefb25a8fc537239f4bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*fN_EjBlo_qBz3hPX3-h_pg.png"/></div></figure><p id="f4ab" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这里我们有一个由768名21岁及以上女性的观察数据组成的数据集。该数据集描述了对患者的即时测量，如年龄、血检、怀孕次数。每个记录都有一个类别值，用于指示患者是否在5年内患过糖尿病。糖尿病患者的数值为<strong class="in hi"> 1 </strong>，非糖尿病患者的数值为<strong class="in hi"> 0 </strong>。</p><p id="c9c6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，让我们继续我们的朴素贝叶斯博客，并逐一了解所有步骤。我把整个过程分成了以下几个步骤:</p><ul class=""><li id="e26f" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">处理数据</strong></li><li id="b7ea" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">汇总数据</strong></li><li id="b3db" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">进行预测</strong></li><li id="1e56" class="jk jl hh in b io jt is ju iw jv ja jw je jx ji jp jq jr js bi translated"><strong class="in hi">评估准确度</strong></li></ul><h2 id="5e5e" class="lg jz hh bd ka lh li lj ke lk ll lm ki iw ln lo km ja lp lq kq je lr ls ku lt bi translated">步骤1:处理数据</h2><p id="05aa" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">我们需要做的第一件事是加载我们的数据文件。数据是CSV格式，没有标题行或任何引号。我们可以使用open函数打开文件，并使用CSV模块中的reader函数读取数据行。</p><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="a13e" class="lg jz hh mh b fi ml mm l mn mo"><strong class="mh hi">import</strong> <!-- -->csv<br/><strong class="mh hi">import</strong> <!-- -->math<br/><strong class="mh hi">import</strong> <!-- -->random</span><span id="c440" class="lg jz hh mh b fi mp mm l mn mo">def loadCsv(filename):<br/>lines = csv.reader(open(r'C:UsersKislayDesktoppima-indians-diabetes.data.csv'))<br/>dataset = list(lines)<br/><strong class="mh hi">for</strong> <!-- -->i in range(len(dataset)):<br/>dataset[i] = [<strong class="mh hi">float</strong>(x) <strong class="mh hi">for</strong> <!-- -->x in dataset[i]]<br/><strong class="mh hi">return</strong> <!-- -->dataset</span></pre><p id="2d8e" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在我们需要将数据分成训练和测试数据集。</p><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="c7af" class="lg jz hh mh b fi ml mm l mn mo">def splitDataset(dataset, splitRatio):<br/>trainSize = <strong class="mh hi">int</strong>(len(dataset) * splitRatio)<br/>trainSet = []<br/>copy = list(dataset)<br/><strong class="mh hi">while</strong> <!-- -->len(trainSet) &amp;lt; trainSize:<br/>index = random.randrange(len(copy))<br/>trainSet.append(copy.pop(index))<br/><strong class="mh hi">return</strong> <!-- -->[trainSet, copy]</span></pre><h2 id="9023" class="lg jz hh bd ka lh li lj ke lk ll lm ki iw ln lo km ja lp lq kq je lr ls ku lt bi translated">第二步:总结数据</h2><p id="4563" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">收集的训练数据摘要包括每个属性的平均值和标准偏差(按类值)。在进行预测以计算属于每个类值的特定属性值的概率时，这些是必需的。</p><p id="3a59" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们可以将汇总数据的准备工作分解为以下子任务:</p><ul class=""><li id="a903" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">按类别分离数据</strong></li></ul><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="0813" class="lg jz hh mh b fi ml mm l mn mo">def separateByClass(dataset): separated = {}<br/>for i in range(len(dataset)):<br/>vector = dataset[i]<br/>if (vector[-1] not in separated):<br/>separated[vector[-1]] = [] separated[vector[-1]].append(vector) return separated</span></pre><ul class=""><li id="19c1" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">计算的意思是</strong></li></ul><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="445e" class="lg jz hh mh b fi ml mm l mn mo">def mean(numbers):<br/><strong class="mh hi">return</strong> <!-- -->sum(numbers)/<strong class="mh hi">float</strong>(len(numbers))</span></pre><ul class=""><li id="ec3e" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">计算标准偏差</strong></li></ul><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="7be7" class="lg jz hh mh b fi ml mm l mn mo">def stdev(numbers):<br/>avg = mean(numbers)<br/>variance = sum([pow(x-avg,2) <strong class="mh hi">for</strong> <!-- -->x in numbers])/<strong class="mh hi">float</strong>(len(numbers)-1)<br/><strong class="mh hi">return</strong> <!-- -->math.sqrt(variance)</span></pre><ul class=""><li id="c37a" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">汇总数据集</strong></li></ul><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="0b1a" class="lg jz hh mh b fi ml mm l mn mo">def summarize(dataset):<br/>summaries = [(mean(attribute), stdev(attribute)) <strong class="mh hi">for</strong> <!-- -->attribute in zip(*dataset)]<br/>del summaries[-1]<br/><strong class="mh hi">return</strong> <!-- -->summaries</span></pre><ul class=""><li id="1146" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">按类别汇总属性</strong></li></ul><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="c58b" class="lg jz hh mh b fi ml mm l mn mo">def summarizeByClass(dataset):<br/>separated = separateByClass(dataset)<br/>summaries = {}<br/><strong class="mh hi">for</strong> <!-- -->classValue, instances in separated.items():<br/>summaries[classValue] = summarize(instances)<br/><strong class="mh hi">return</strong> <!-- -->summaries</span></pre><h2 id="532d" class="lg jz hh bd ka lh li lj ke lk ll lm ki iw ln lo km ja lp lq kq je lr ls ku lt bi translated">第三步:做预测</h2><p id="173c" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">我们现在准备使用从我们的训练数据准备的摘要来进行预测。进行预测包括计算给定数据实例属于每个类的概率，然后选择概率最大的类作为预测。我们需要执行以下任务</p><ul class=""><li id="ee55" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">计算高斯概率密度函数</strong></li></ul><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="d830" class="lg jz hh mh b fi ml mm l mn mo">def calculateProbability(x, mean, stdev):<br/>exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))<br/><strong class="mh hi">return</strong> <!-- -->(1/(math.sqrt(2*math.pi)*stdev))*exponent</span></pre><ul class=""><li id="0a7a" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">计算类别概率</strong></li></ul><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="f809" class="lg jz hh mh b fi ml mm l mn mo">def calculateClassProbabilities(summaries, inputVector):<br/>probabilities = {}<br/><strong class="mh hi">for</strong> <!-- -->classValue, classSummaries in summaries.items():<br/>probabilities[classValue] = 1<br/><strong class="mh hi">for</strong> <!-- -->i in range(len(classSummaries)):<br/>mean, stdev = classSummaries[i]<br/>x = inputVector[i]<br/>probabilities[classValue] *= calculateProbability(x, mean, stdev)<br/><strong class="mh hi">return</strong> <!-- -->probabilities</span></pre><ul class=""><li id="9146" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">做个预测</strong></li></ul><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="463a" class="lg jz hh mh b fi ml mm l mn mo">def predict(summaries, inputVector):<br/>probabilities = calculateClassProbabilities(summaries, inputVector)<br/>bestLabel, bestProb = None, -1<br/><strong class="mh hi">for</strong> <!-- -->classValue, probability in probabilities.items():<br/><strong class="mh hi">if</strong> <!-- -->bestLabel is None or probability &amp;gt; bestProb:<br/>bestProb = probability<br/>bestLabel = classValue<br/><strong class="mh hi">return</strong> <!-- -->bestLabel</span></pre><ul class=""><li id="e8d0" class="jk jl hh in b io ip is it iw jm ja jn je jo ji jp jq jr js bi translated"><strong class="in hi">获得精度</strong></li></ul><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="e986" class="lg jz hh mh b fi ml mm l mn mo">def getAccuracy(testSet, predictions):<br/>correct = 0<br/><strong class="mh hi">for</strong> <!-- -->x in range(len(testSet)):<br/><strong class="mh hi">if</strong> <!-- -->testSet[x][-1] == predictions[x]:<br/>correct += 1<br/><strong class="mh hi">return</strong> <!-- -->(correct/<strong class="mh hi">float</strong>(len(testSet)))*100.0</span></pre><p id="4dce" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">最后，我们定义我们的主函数，在这里我们调用我们定义的所有这些方法，一个接一个地得到我们创建的模型的精度。</p><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="b3ad" class="lg jz hh mh b fi ml mm l mn mo">def main():<br/>filename = 'pima-indians-diabetes.data.csv'<br/>splitRatio = 0.67<br/>dataset = loadCsv(filename)<br/>trainingSet, testSet = splitDataset(dataset, splitRatio)<br/>print('Split {0} rows into train = {1} and test = {2} rows'.format(len(dataset),len(trainingSet),len(testSet)))</span><span id="07e2" class="lg jz hh mh b fi mp mm l mn mo">#prepare model<br/>summaries = summarizeByClass(trainingSet)</span><span id="356a" class="lg jz hh mh b fi mp mm l mn mo">#test model<br/>predictions = getPredictions(summaries, testSet)<br/>accuracy = getAccuracy(testSet, predictions)<br/>print('Accuracy: {0}%'.format(accuracy))</span><span id="122e" class="lg jz hh mh b fi mp mm l mn mo">main()</span></pre><p id="35ca" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">输出:</strong></p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mq"><img src="../Images/c9a50c06c17f504a307fb7c0297271be.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/0*SPArgdRn9qNWJnQL.png"/></div></figure><p id="d688" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这里你可以看到我们模型的准确率是66 %。现在，该值因型号和分流比而异。</p><p id="f6ff" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这里你可以看到我们模型的准确率是66 %。现在，该值因型号和分流比而异。</p><p id="82b4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在我们已经看到了朴素贝叶斯分类器中涉及的步骤，Python附带了一个库<strong class="in hi"> SKLEARN </strong>，它使得上述所有步骤都易于实现和使用。让我们继续我们的朴素贝叶斯教程，看看这是如何实现的。</p><h1 id="7e08" class="jy jz hh bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">带SKLEARN的朴素贝叶斯</h1><h2 id="3c2c" class="lg jz hh bd ka lh li lj ke lk ll lm ki iw ln lo km ja lp lq kq je lr ls ku lt bi translated">导入库和加载数据集</h2><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="0875" class="lg jz hh mh b fi ml mm l mn mo">from sklearn <strong class="mh hi">import</strong> <!-- -->datasets<br/>from sklearn <strong class="mh hi">import</strong> <!-- -->metrics<br/>from sklearn.naive_bayes <strong class="mh hi">import</strong> <!-- -->GaussianNB</span><span id="1d0a" class="lg jz hh mh b fi mp mm l mn mo">&amp;amp;nbsp;<br/>dataset = datasets.load_iris()</span></pre><h2 id="2813" class="lg jz hh bd ka lh li lj ke lk ll lm ki iw ln lo km ja lp lq kq je lr ls ku lt bi translated">使用Sklearn创建我们的朴素贝叶斯模型</h2><p id="5d3c" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">这里我们有一个GaussianNB()方法，它执行与上面解释的代码完全相同的功能</p><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="9128" class="lg jz hh mh b fi ml mm l mn mo">model = GaussianNB() model.fit(dataset.data, dataset.target)<br/>expected = dataset.target predicted = model.predict(dataset.data)</span></pre><h2 id="3334" class="lg jz hh bd ka lh li lj ke lk ll lm ki iw ln lo km ja lp lq kq je lr ls ku lt bi translated">获得准确性和统计数据</h2><p id="c557" class="pw-post-body-paragraph il im hh in b io kw iq ir is kx iu iv iw ky iy iz ja kz jc jd je la jg jh ji ha bi translated">在这里，我们将创建一个分类报告，其中包含判断一个模型所需的各种统计数据。之后，我们将创建一个混淆矩阵，让我们清楚地了解模型的准确性和拟合度。</p><pre class="lc ld le lf fd mg mh mi mj aw mk bi"><span id="94e9" class="lg jz hh mh b fi ml mm l mn mo">print(metrics.classification_report(expected, predicted)) print(metrics.confusion_matrix(expected, predicted))</span></pre><p id="35a8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">分类报告:</strong></p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es mr"><img src="../Images/089896731edc008351983981f322b9dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*jluesB5tZH7YBlBNM6B8sA.png"/></div></figure><p id="03d9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">混淆矩阵:</strong></p><figure class="lc ld le lf fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/8521ff33812604453f85e5318b9e413d.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*BRiRtGkxCEWv1gpZqquyQg.png"/></div></figure><p id="2c45" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如你所见，有了这个强大的库，所有的数百行代码都可以总结成几行代码。</p><p id="88b7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如你所见，有了这个强大的库，所有的数百行代码都可以总结成几行代码。</p><p id="e507" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">就这样，我们结束了这篇朴素贝叶斯教程博客。我希望你喜欢这个博客。如果你正在读这篇文章，那么恭喜你！你不再是朴素贝叶斯的新手了。现在就在您的系统上尝试这个简单的例子。</p><p id="ab42" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果你想查看更多关于Python、DevOps、Ethical Hacking等市场最热门技术的文章，那么你可以参考<a class="ae mt" href="https://www.edureka.co/blog/?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=naive-bayes-tutorial" rel="noopener ugc nofollow" target="_blank"> Edureka的官方网站。</a></p><p id="8ede" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">请留意本系列中的其他文章，它们将解释数据科学的各个方面。</p><blockquote class="mu mv mw"><p id="e5f0" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 1。</em> <a class="ae mt" rel="noopener" href="/edureka/data-science-tutorial-484da1ff952b"> <em class="hh">数据科学教程</em> </a></p><p id="06bc" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 2。</em> <a class="ae mt" rel="noopener" href="/edureka/math-and-statistics-for-data-science-1152e30cee73"> <em class="hh">数据科学</em> </a>数学与统计</p><p id="4e46" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 3。</em><a class="ae mt" rel="noopener" href="/edureka/linear-regression-in-r-da3e42f16dd3"><em class="hh">R中的线性回归</em> </a></p><p id="83ac" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 4。</em> <a class="ae mt" rel="noopener" href="/edureka/machine-learning-algorithms-29eea8b69a54"> <em class="hh">机器学习算法</em> </a></p><p id="fc43" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 5。</em><a class="ae mt" rel="noopener" href="/edureka/logistic-regression-in-r-2d08ac51cd4f"><em class="hh">R中的逻辑回归</em> </a></p><p id="77e0" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 6。</em> <a class="ae mt" rel="noopener" href="/edureka/classification-algorithms-ba27044f28f1"> <em class="hh">分类算法</em> </a></p><p id="620d" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 7。</em> <a class="ae mt" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="1aea" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 8。</em> <a class="ae mt" rel="noopener" href="/edureka/a-complete-guide-on-decision-tree-algorithm-3245e269ece"> <em class="hh">决策树中的R </em> </a></p><p id="aec0" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated">9。 <a class="ae mt" rel="noopener" href="/edureka/introduction-to-machine-learning-97973c43e776"> <em class="hh">机器学习入门</em> </a></p><p id="662c" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 10。</em> <a class="ae mt" rel="noopener" href="/edureka/naive-bayes-in-r-37ca73f3e85c"> <em class="hh">朴素贝叶斯在R </em> </a></p><p id="0d5d" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 11。</em> <a class="ae mt" rel="noopener" href="/edureka/statistics-and-probability-cf736d703703"> <em class="hh">统计与概率</em> </a></p><p id="1501" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 12。</em> <a class="ae mt" rel="noopener" href="/edureka/decision-trees-b00348e0ac89"> <em class="hh">如何创建一个完美的决策树？</em>T47】</a></p><p id="1ab4" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 13。</em> <a class="ae mt" rel="noopener" href="/edureka/data-scientists-myths-14acade1f6f7"> <em class="hh">关于数据科学家角色的十大误区</em> </a></p><p id="4711" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 14。</em> <a class="ae mt" rel="noopener" href="/edureka/data-science-projects-b32f1328eed8"> <em class="hh">顶级数据科学项目</em> </a></p><p id="d9de" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 15。</em> <a class="ae mt" rel="noopener" href="/edureka/data-analyst-vs-data-engineer-vs-data-scientist-27aacdcaffa5"> <em class="hh">数据分析师vs数据工程师vs数据科学家</em> </a></p><p id="3693" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 16。</em> <a class="ae mt" rel="noopener" href="/edureka/types-of-artificial-intelligence-4c40a35f784"> <em class="hh">人工智能的种类</em> </a></p><p id="8605" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 17。</em><a class="ae mt" rel="noopener" href="/edureka/r-vs-python-48eb86b7b40f"><em class="hh">R vs Python</em></a></p><p id="6828" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 18。</em> <a class="ae mt" rel="noopener" href="/edureka/ai-vs-machine-learning-vs-deep-learning-1725e8b30b2e"> <em class="hh">人工智能vs机器学习vs深度学习</em> </a></p><p id="3ae5" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 19。</em> <a class="ae mt" rel="noopener" href="/edureka/machine-learning-projects-cb0130d0606f"> <em class="hh">机器学习项目</em> </a></p><p id="9895" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 20。</em> <a class="ae mt" rel="noopener" href="/edureka/data-analyst-interview-questions-867756f37e3d"> <em class="hh">数据分析师面试问答</em> </a></p><p id="edd8" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 21。</em> <a class="ae mt" rel="noopener" href="/edureka/data-science-and-machine-learning-for-non-programmers-c9366f4ac3fb"> <em class="hh">面向非程序员的数据科学和机器学习工具</em> </a></p><p id="a794" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 22。</em> <a class="ae mt" rel="noopener" href="/edureka/top-10-machine-learning-frameworks-72459e902ebb"> <em class="hh">十大机器学习框架</em> </a></p><p id="64fc" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated">23。 <a class="ae mt" rel="noopener" href="/edureka/statistics-for-machine-learning-c8bc158bb3c8"> <em class="hh">统计机器学习</em> </a></p><p id="3678" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 24。</em> <a class="ae mt" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="1786" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 25。</em> <a class="ae mt" rel="noopener" href="/edureka/breadth-first-search-algorithm-17d2c72f0eaa"> <em class="hh">广度优先搜索算法</em> </a></p><p id="4bd4" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated">26。<a class="ae mt" rel="noopener" href="/edureka/linear-discriminant-analysis-88fa8ad59d0f"><em class="hh">R中的线性判别分析</em> </a></p><p id="1735" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 27。</em> <a class="ae mt" rel="noopener" href="/edureka/prerequisites-for-machine-learning-68430f467427"> <em class="hh">机器学习的先决条件</em> </a></p><p id="24aa" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated">28。 <a class="ae mt" rel="noopener" href="/edureka/r-shiny-tutorial-47b050927bd2"> <em class="hh">互动WebApps使用R闪亮</em> </a></p><p id="b9d8" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 29。</em> <a class="ae mt" rel="noopener" href="/edureka/top-10-machine-learning-books-541f011d824e"> <em class="hh">十大机器学习书籍</em> </a></p><p id="ca1c" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated">30。 <a class="ae mt" rel="noopener" href="/edureka/supervised-learning-5a72987484d0"> <em class="hh">监督学习</em> </a></p><p id="2588" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 31。</em> <a class="ae mt" rel="noopener" href="/edureka/10-best-books-data-science-9161f8e82aca"> <em class="hh"> 10本最好的数据科学书籍</em> </a></p><p id="81a1" class="il im jj in b io ip iq ir is it iu iv mx ix iy iz my jb jc jd mz jf jg jh ji ha bi translated"><em class="hh"> 32。</em> <a class="ae mt" rel="noopener" href="/edureka/machine-learning-with-r-c7d3edf1f7b"> <em class="hh">机器学习使用R </em> </a></p></blockquote></div><div class="ab cl na nb go nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ha hb hc hd he"><p id="ad3c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="jj">原载于2018年8月7日</em><a class="ae mt" href="https://www.edureka.co/blog/naive-bayes-tutorial/" rel="noopener ugc nofollow" target="_blank"><em class="jj">https://www.edureka.co</em></a><em class="jj">。</em></p></div></div>    
</body>
</html>