# Pinterest 上的大规模 Hadoop 升级

> 原文：<https://medium.com/pinterest-engineering/large-scale-hadoop-upgrade-at-pinterest-a23a112deb73?source=collection_archive---------2----------------------->

![](img/e978e19c47b0f9e21cad640d4802a8fd.png)

张永军|软件工程师；威廉·汤姆|软件工程师；王绍文|软件工程师；Bhavin Pathak |软件工程师；批处理平台团队

Pinterest 的批处理平台 Monarch 由超过 30 个 [Hadoop YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) 集群组成，17k+节点完全建立在 [AWS EC2](https://aws.amazon.com/ec2/?ec2-whats-new.sort-by=item.additionalFields.postDateTime&ec2-whats-new.sort-order=desc) 之上。2021 年初，Monarch 还在 Hadoop 2.7.1 上，已经五岁了。由于反向移植上游变更(特性和错误修复)的复杂性日益增加，我们决定是时候投资进行版本升级了。我们选定了 Hadoop 2.10.0，这是当时 Hadoop 2 的最新版本。

本文分享了我们将 Monarch 升级到 Hadoop 2.10.0 的经验。为简单起见，我们用 Hadoop 2.10 指代 Hadoop 2.10.0，用 Hadoop 2.7 指代 Hadoop 2.7.1。

# 挑战

自从 Pinterest 的批处理平台开始(大约 2016 年)以来，我们一直在使用 Hadoop 2.7。随着时间的推移，我们的平台处理的工作负载不断增长和发展，作为回应，我们进行了数百次内部更改来满足这些需求。这些内部补丁大多数都是 Pinterest 特有的，需要大量的时间投入才能将其移植到 Hadoop 2.10。

大多数业务关键型批处理工作负载都在 Monarch 上运行，因此我们的最高优先级是以不会对这些工作负载造成集群停机或性能/SLA 影响的方式执行升级。

# 升级策略

因为许多用户定义的应用程序与 Hadoop 2.7 紧密耦合，所以我们决定将升级过程分为两个独立的阶段。第一阶段是将平台本身从 Hadoop 2.7 升级到 Hadoop 2.10，第二阶段是将用户定义的应用升级到使用 2.10。

在升级的第一阶段，我们将允许用户的工作继续使用 Hadoop 2.7 依赖项，同时我们专注于平台升级。这增加了额外的开销，因为我们需要使 Hadoop 2.7 作业与 Hadoop 2.10 平台兼容，但这将使我们有更多的时间从事第二阶段的工作。

由于平台和用户应用的规模，上述两个阶段都需要逐步完成:

*   我们需要逐个升级 Monarch 集群
*   我们需要将用户应用程序批量升级到 2.10 版，而不是 2.7 版

当时，我们没有一个灵活的构建管道来允许我们构建两个独立版本的作业工件，它们具有独立的 hadoop direct 和 transitive 依赖关系。同样，我们要求所有用户(公司的其他工程师)单独验证从 2.7 到 2.10 的 10，000 多项不同的工作迁移也是不合理的。为了支持上述增量升级，我们需要在迁移之前运行许多验证，以确保使用 Hadoop 2.7 构建的绝大多数应用程序将继续在 2.10 集群中工作。

我们为升级过程提出的高级步骤是:

1.  Hadoop 2.10 发布准备:将 Hadoop 2.7 内部分支上的所有补丁移植到普通的 Apache Hadoop 2.10
2.  将 Monarch 集群逐步升级到 Hadoop 2.10(一个接一个)
3.  逐步(分批)升级用户应用程序以使用 Hadoop 2.10

# Hadoop 2.10 版本准备

Pinterest 2.7 版本包括许多在开源 Hadoop 2.7 基础上进行的内部更改，这些更改需要移植到 Hadoop 2.10。然而，Hadoop 2.7 和 Hadoop 2.10 之间有显著的变化。正因为如此，将 Pinterest Hadoop 2.7 的变化应用到 vanilla Hadoop 2.10 上是一项重要的任务。

以下是我们在 Hadoop 2.7 上制作，然后移植到 Hadoop 2.10 的一些内部补丁示例:

*   Monarch 构建在 EC2 之上，使用 S3 作为持久存储。任务的输入和输出通常在 S3 上。添加了 DirectOutputFileCommitter，使任务能够将结果直接写入目标位置，以避免在 S3 复制结果文件的开销。
*   添加应用程序主服务器和历史服务器端点，以便为给定作业的所有任务获取特定计数器的值。
*   在提供容器日志时添加范围支持，这允许获取指定容器日志的一部分。
*   为日志聚合添加节点 Id 分区，以便集群中不同节点的日志可以写入不同的 S3 分区，这有助于避免达到 S3 访问速率限制。
*   新创建的 Namenodes 可能有不同的 IP 地址，这是一个在故障转移时解析 NN RPC 套接字地址的附加功能。
*   如果分配的映射器数量与总映射器数量之比超过配置的阈值，则禁用抢占减少器。
*   将磁盘使用监视器线程添加到 AM，这样，如果磁盘使用超过了配置的限制，应用程序将被终止。

# 将 Monarch 集群升级到 Hadoop 2.10

## 集群升级方法探索

我们评估了将 Monarch 集群升级到 Hadoop 2.10 的多种方法。每种方法都有自己的优点和缺点，我们将在下面进行概述。

**方法一:使用 CCR**

正如在[高效资源管理](https://blog.planview.com/how-to-manage-resource-effectively-and-efficiently/)文章中提到的，我们开发了跨集群路由(CCR)来平衡各个集群之间的工作负载。为了最大限度地减少对现有 2.7 集群的影响，一种选择是构建新的 Hadoop 2.10 集群，并将工作负载逐步迁移到新的集群。如果出现任何问题，我们可以将工作负载路由回其原始集群，修复问题，然后再次路由回 2.10 集群。

我们从这种方法开始，并在一些小型生产和开发集群上对其进行了评估。没有任何重大问题，但是我们发现了一些缺点:

*   我们必须为每个集群迁移构建一个新的并行集群。对于大的纱簇(高达几千个节点)，这变得昂贵
*   工作负载需要批量迁移，这非常耗时。因为 Monarch 是一个如此大的平台，这个升级过程可能需要很长时间才能完成。

**方式二:滚动升级**

理论上，我们可以尝试工作节点的滚动升级，但是滚动升级可能会影响集群上的所有工作负载。如果我们遇到任何问题，回滚将是昂贵的。

**方法三:就地升级**

利用与我们将集群从一种实例类型就地升级到另一种实例类型类似的方法，我们可以:

1.  将新实例类型的几个 canary 主机作为节点的新自动扩展组(canary ASG)插入到集群中
2.  评估相对于基本 ASG(现有实例类型)的金丝雀 ASG
3.  扩展金丝雀 ASG
4.  ASG 基地的规模

通常，这适用于没有服务级别变化的小型基础架构级别变化。作为一项探索，我们想看看我们能否在 Hadoop 2.10 升级中做到同样的事情。我们必须做出的一个重要假设是，Hadoop 2.7 和 2.10 组件之间的通信是兼容的。这种方法的步骤是:

1.  将 Hadoop 2.10 canary 工作节点(运行 HDFS 数据节点和 YARN 节点管理器)添加到 Hadoop 2.7 集群
2.  发现并解决出现的问题
3.  增加 Hadoop 2.10 工作节点的数量，减少 Hadoop 2.7 工作节点的数量，直到 2.7 节点完全被 2.10 节点取代
4.  升级所有管理器节点(名称节点、日志节点、资源管理器、历史服务器等)。这个过程的工作方式类似于用 Hadoop 2.10 节点替换工作节点。

在将这种有风险的方法应用到生产集群之前，我们对 dev Monarch 集群进行了广泛的评估。除了一些我们将在后面描述的小问题之外，这是一次无缝升级体验。

**最终确定升级方法**

如前所述，作业工件最初是用 Hadoop 2.7 依赖项构建的。这意味着他们可能将 Hadoop 2.7 jars 带入分布式缓存。然后在运行时，我们将用户类路径放在集群上的库路径之前。这可能会导致 Hadoop 2.10 canary 节点上的依赖问题，因为 Hadoop 2.7 和 2.10 可能依赖于不同版本的第三方 jar。

在使用方法 I 对一些小型集群进行升级后，我们发现这种方法需要太长时间来完成所有 Monarch 集群的升级。此外，考虑到我们最大的 monarch 集群的规模(多达 3k 个节点),我们无法在如此短的时间内获得足够的 EC2 实例来替换这些集群！).我们评估了利弊，决定采用方法三，因为我们可能会大大加快升级过程，并且大多数依赖问题可以很快得到解决。如果我们无法快速解决某些作业的问题，我们可以使用 CCR 将作业路由到另一个 Hadoop 2.7 集群，然后花时间解决问题。

## 问题和解决方案

在我们最终确定了方法三之后，我们的主要焦点就变成了识别任何问题并尽快解决它们。概括地说，我们遇到了三类问题:由于 Hadoop 2.7 和 Hadoop 2.10 之间的不兼容而导致的服务级别问题，用户定义的应用程序中的依赖性问题，以及其他各种问题。

**不兼容行为问题**

*   重启 Hadoop 2.10 NM 导致容器被杀死。我们发现 Hadoop 2.10 引入了一个默认为 FALSE 的新配置*yarn . node manager . recovery . supervised*。为了防止重启 NMs 时容器被杀死，我们需要设置 TRUE。当启用此配置时，正在运行的节点管理器在退出时不会尝试清理容器，因为假设它会立即重启并恢复容器。
*   在 2.10 金丝雀节点上安排 AM 时作业停滞:在 [MAPREDUCE-6515](https://issues.apache.org/jira/browse/MAPREDUCE-6515) 中添加了应用程序优先级，假定该字段始终在 PB 响应中设置。在版本分离的集群(2 . 7 . 1 resource manager+2.10 worker)中，情况并非如此，因为 RM 返回的 PB 响应将不包含 appPriority 字段。我们检查这个字段是否在 protobuf 中，如果不在，我们忽略更新 applicationPriority。
*   [HADOOP-13680](https://issues.apache.org/jira/browse/HADOOP-13680) 使得 *fs.s3a.readahead.range* 从 Hadoop 2.8 开始使用 getLongBytes，支持“32M”格式的值(内存后缀 K，M，G，T，P)。但是，Hadoop 2.7 代码无法处理这种格式。这会中断混合 Hadoop 版本集群中的作业。我们对 Hadoop 2.7 添加了一个修复，使其与 Hadopop 2.10 行为兼容。
*   Hadoop 2.10 意外地在 io.serialization config 的多个值之间引入了空格，这导致了 ClassNotFound 错误。我们进行了修复，删除了配置值中的空格。

**依赖问题**

当我们执行 Hadoop 2.7 到 2.10 的就地升级时，我们面临的大多数依赖关系问题都是由于 Hadoop 服务和用户应用程序之间共享的不同版本的依赖关系造成的。解决方案要么是修改用户的作业以与 Hadoop 平台依赖项兼容，要么是在我们的作业工件或 Hadoop 平台分发中屏蔽版本。以下是一些例子:

*   Hadoop 2.7 jars 被放入分布式缓存，并导致 Hadoop 2.10 canary 节点上的依赖性问题。我们在 Hadoop 2.7 版本中实现了一个解决方案，以防止这些 jar 被添加到分布式缓存中，以便所有主机都使用已经部署到主机的 Hadoop jars。
*   Woodstox-core 包。Hadoop-2.10.0 依赖于 woodstox-core-5.0.3.jar，而一些应用程序依赖于另一个依赖于 wstx-asl-3.2.7.jar 的模块。woodstox-core-5.0.3.jar 和 wstx-asl-3.2.7.jar 之间的不兼容导致了作业失败。我们的解决方案是在 Hadoop 2.10 中屏蔽 woodstox-core-5.0.3.jar。
*   我们有一些基于 Hadoop 2.7 实现的内部库或类。它们不能在 Hadoop 2.10 上运行。例如，我们有一个名为 s3DoubleWrite 的类，它同时将输出写入两个 S3 位置。开发它是为了帮助我们在 3 个存储桶之间迁移日志。由于我们不再需要那个类，我们弃用它来解决依赖性问题。
*   一些 Hadoop 2.7 库被打包到用户的 bazel jars 中，并在运行时导致一些依赖问题。我们采取的解决方案是将用户应用程序从 Hadoop jars 中分离出来。更多细节可以在后面的相关章节中找到。

**其他杂项问题**

*   我们在 dev 集群上执行的验证之一是确保我们可以在升级过程的中途回滚。当我们试图将 NameNode 回滚到 Hadoop 2.7 时，revert 出现了一个问题。我们发现 NameNode 没有收到来自升级后的 DataNodes 的块报告。我们确定的解决方法是手动触发阻止报告。我们后来发现了潜在的问题 HDFS-12749 (DN 可能不会在 NN 重启后向 NN 发送块报告)并将其反向传输。
*   当捆绑了 Hadoop 2.7 jars 的 Hadoop 流作业部署到 Hadoop 2.10 节点时，预期的 2.7 jar 不可用。这是因为我们使用集群提供的 jar 来满足大多数用户工件分发的依赖性，以减少工件的大小。然而，所有 Hadoop 依赖项都有编码在 jar 名称中的版本。解决方案是使 Hadoop 流作业捆绑包 Hadoop jars 不带版本字符串，以便提供的 Hadoop 依赖项在运行时始终位于类路径中，而不管它运行的节点是 Hadoop 2.7 还是 2.10。

# 将用户应用升级到 Hadoop 2.10

为了将用户应用升级到 Hadoop 2.10，我们需要确保 Hadoop 2.10 在编译时和运行时都能使用。第一步是确保 Hadoop 2.7 jars 没有与用户 jar 一起发布，以便在运行时使用部署到集群的 Hadoop jar(2.7 节点中使用 2.7 jar，2.10 节点中使用 2.10 jar)。然后我们改变了用户应用构建环境，使用 Hadoop 2.10 而不是 2.7。

## 将用户应用从 Hadoop jars 中分离出来

在 Pinterest，大多数数据管道都使用 Bazel 制造的胖罐子。这些 jar 包含所有的依赖项，包括升级前的 Hadoop 2.7 客户端库。我们总是支持来自那些 fat jar 的类，而不是来自本地环境的类，这意味着当在具有 Hadoop 2.10 的集群上运行那些 fat jar 时，我们仍将使用 Hadoop 2.7 类。

为了永久解决这个问题(在 2.10 集群中使用 2.7 jars)，我们决定将用户的 Bazel jars 与 Hadoop 库分离；也就是说，我们不再在胖用户 Bazel jar 中提供 Hadoop jar，已经部署到集群节点的 Hadoop jar 将在运行时使用。

Bazel java_binary rules 有一个名为 deploy_env 的参数，它的值是表示这个二进制文件的部署环境的其他 java_binary 目标的列表。我们设置这个属性来从用户 jar 中排除所有 Hadoop 依赖项及其子依赖项。这里的挑战是，许多用户应用程序都依赖于 Hadoop 所依赖的库。这些公共库很难识别，因为它们没有被明确指定，因为它们已经作为节点管理器部署的一部分在 Hadoop workers 上提供了。在测试期间，我们投入了大量的精力来识别这些类型的情况，并修改了用户的 bazel 规则，以显式地添加那些隐藏的依赖关系。

## 将 Hadoop bazel 目标从 2.7 升级到 2.10

在将用户应用与 Hadoop Jars 分离之后，我们需要将 Hadoop bazel 目标从 2.7 升级到 2.10，这样我们就可以确保构建和运行时环境中使用的 Hadoop 版本是一致的。Hadoop 2.7 和 Hadoop 2.10 在这个过程中又出现了一些依赖冲突。我们通过构建测试确定了这些依赖项，并相应地将它们升级到正确的版本。

# 摘要

将 17k 以上的节点从一个 Hadoop 版本升级到另一个版本，同时不对应用程序造成重大中断，这是一个挑战。我们设法做到了高质量、合理的速度和成本效益。我们希望以上分享的经验能对社区有益。

# 确认

感谢批处理平台团队的、郭衡哲、桑迪普·库马尔、波格丹一世·皮西卡、康奈尔·多纳吉，他们在整个升级过程中提供了很多帮助。感谢 Soam Acharya、Keith Regier 致力于解决 FGAC 集群中遇到的问题。感谢金俊成、李凡和王春燕一路走来的支持。感谢工作流团队、查询团队和我们平台用户团队的支持。

*要在 Pinterest 了解更多关于工程的知识，请查看我们的* [*工程博客*](https://medium.com/pinterest-engineering) *，并访问我们的*[*Pinterest Labs*](https://www.pinterestlabs.com/?utm_source=medium&utm_medium=blog-link&utm_campaign=zhang-et-al-march-30-2022)*网站。要查看和申请空缺职位，请访问我们的* [*招聘*](https://www.pinterestcareers.com/?utm_source=medium&utm_medium=blog-link&utm_campaign=zhang-et-al-march-30-2022) *页面*