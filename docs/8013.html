<html>
<head>
<title>Computational Complexity of Deep Learning: Solution Approaches</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的计算复杂性:解决方法</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/computational-complexity-of-deep-learning-a-birds-eye-view-2250b7c098a1?source=collection_archive---------0-----------------------#2021-05-27">https://medium.com/walmartglobaltech/computational-complexity-of-deep-learning-a-birds-eye-view-2250b7c098a1?source=collection_archive---------0-----------------------#2021-05-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5171" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jc">沃尔玛全球技术部数据科学总监Vijay Agneeswaran博士。沃尔玛全球技术部首席数据科学家Anirban Chatterjee和沃尔玛全球技术部员工数据科学家Kunal Banerjee。</em>T3】</strong></p><p id="02cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇博客的灵感来自于本·洛里卡在<a class="ae jd" href="https://thedataexchange.media/why-you-should-optimize-your-deep-learning-inference-platform/" rel="noopener ugc nofollow" target="_blank">数据交易所</a>上主持的一个关于“为什么你应该优化你的深度学习推理平台”的播客。</p><p id="949c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi je translated">eep learning在广泛的任务中取得了显著的成功，例如成为围棋冠军击败机器的一部分(<a class="ae jd" href="https://www.theatlantic.com/technology/archive/2016/03/the-invisible-opponent/475611/" rel="noopener ugc nofollow" target="_blank"> AlphaGo击败了世界围棋冠军Lee Sedol </a>)以及作为最近几个月计算机视觉、语音处理、语言翻译等领域多项进步的单一驱动者。</p><p id="b410" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么深度学习如此成功？深度学习能从大数据中学习的根本原因是什么？为什么传统的ML不能像深度学习那样高效地从现在可用于不同任务的大型数据集学习？这些问题可以通过理解深度学习的可学性来回答——也称为Vapnik-Chervonenkis维度，如图1 [1]中的曲线所示。该曲线捕捉了传统ML和DL的性能与用于训练模型的数据量的关系。可以观察到，当数据集较小时，传统的ML比DL具有更好的性能，但是随着数据集进入大数据区，DL的性能保持增长，几乎呈指数增长。这就是我们在特定任务上看到深度学习如此显著的性能提升的原因，其中有大量带标签的数据集可用于训练(图像分类是典型的例子)。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es jn"><img src="../Images/276be63c2d6cd89a3a20aa5fe552bd4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P9ABi4PpafOSlMZ7dPzmGw.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx">Figure 1: Source Credit [1].</figcaption></figure><p id="ca19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最近观察到的一个趋势是深度神经网络正在被过度参数化，这意味着参数的数量远大于用于训练的可用数据点的数量。已经证明，对于训练过度参数化的神经网络，某些特征允许通过局部试探法和二次激活有效地找到全局最优模型[2]。过度参数化网络的一个例子是最先进的图像识别系统nosys student[3]，它有大约4.8亿个参数，或者是微软的图灵NLG，它有170亿个参数。OpenAI的GPT3将这一点发挥到了极致，它有175B个参数。尽管像加里·马库斯这样的一些研究人员已经表明GPT3可以生成自然语言，但这是毫无意义的——这是一个单独的主题，需要进一步研究。</p><p id="56f0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">过度参数化网络的计算需求与数据点数量和参数数量的乘积成比例(并且假设在过度参数化设置中，所需的参数数量随着数据点数量的增加而增加)，这本质上意味着训练过度参数化深度学习的计算需求与数据点数量成二次关系。图2中的图表量化了常用深度学习模型的计算要求，由[4]提供。从图中可以看出，性能最好的预训练架构是那些计算复杂度非常高的架构(如NASNet-A-Large)，位于图中的最右侧。还应该记住，它们不是具有最大模型复杂性的模型(如气泡大小所示)。另一个重要的观察结果是，几乎所有的模型都能够在高端GPU上实现实时或超实时性能，这可能意味着拥有GPU可以解决计算问题。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es jn"><img src="../Images/84a504ef74d8f818392c5d403ab8bed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*twB72lpdBORRbzYnmJAMpA.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx">Figure 2: Source Credit [4]</figcaption></figure><p id="8443" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们试图说明的一点是，虽然GPU解决了一些计算复杂性，并有助于采用深度学习，但AlexNet或ResNext-101等先进系统中实际使用的计算能力增长得更快。</p><p id="3d43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从能源角度来看，深度学习的碳足迹也是巨大的。从图3[5]中的表格可以看出，训练BERT或Transformers等常见NLP模型的成本相当高，网络架构搜索(NAS)是一种调整模型超参数并获得最佳模型的方法，是环境的最高负担。将CO2e列中的成本与1984年1名乘客从纽约到旧金山的航空旅行成本进行比较。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es jn"><img src="../Images/db7418016eb442ef986f1e44e7df70d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tirQvUEuSskWIoYFNvr4Vg.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx">Figure 3: Source Credit [5]</figcaption></figure><h1 id="34be" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">解决方法</h1><p id="cd86" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">GPU一直处于深度学习革命的前沿，从Hinton的基础论文开始，当时他们在GPU上使用深度学习解决了ImageNet挑战[6]以及过去几年在计算机视觉、语言翻译和语音处理等各种挑战中的许多其他深度学习胜利。谷歌还推出了加速深度学习推理的专门硬件，称为张量处理单元(TPUs) [7]，现在已经可以在谷歌云中使用。TPU实现了深度学习的显著加速，并且消耗了更少的功率。Nvidia也推出了自己的加速深度学习的专业硬件，称为<a class="ae jd" href="https://blogs.nvidia.com/blog/2018/01/07/drive-xavier-processor/" rel="noopener ugc nofollow" target="_blank"> Xavier处理器</a>，每秒钟可以进行30万亿次运算，仅消耗30瓦的功率。由于这些专业化正面临回报递减，因此正在探索包括量子计算在内的其他硬件进步。一项有趣的工作是在[8]中，作者展示了如何在HPC(如ORNL的泰坦超级计算机)、神经形态计算(针对尖峰神经网络)和量子计算(波尔兹曼机器网络)上实现具有层内互连的复杂神经架构。</p><p id="2923" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第二个广泛的工作试图通过稀疏化经过训练的神经网络来降低深度学习的计算复杂性，以便它需要更少的推理计算。这方面的三种常用方法包括剪枝、低秩压缩和量化。例如,[9]中的工作使用低成本协作层来加速网络中的每个卷积层。一劳永逸网络[10]中使用的渐进收缩技术是一种广义的多维剪枝方法。在这个方向上另一个有趣的努力是BNNs ( <a class="ae jd" href="https://towardsdatascience.com/binary-neural-networks-future-of-low-cost-neural-networks-bcc926888f3f" rel="noopener" target="_blank">二进制神经网络</a>)，这是一种量化网络的方法。低秩近似(LRA)是一种用两个或更多更小的矩阵来代替大矩阵乘法以降低计算复杂度的技术。[11]提出了力正则化，以将深度学习滤波器协调到更相关的状态，从而实现更有效的LRA。</p><p id="0f0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">第三项工作是寻找小型深度学习架构，这在计算上更有效。例如，神经架构搜索(NAS)最初是使用强化学习[12]提出的，通过搜索超参数的可能空间来获得最佳可能的架构(接近人类制作的架构)。如此发现的体系结构可能有效地执行推理。然而，它不能保证是低能量模型，加上整个搜索过程本身计算量很大。ENAS被提出来缓解[13]中的一些问题——这里的关键改进是使子模型共享参数(权重),而不是从头开始训练每个子模型。寻找更小架构的另一个有趣方法是[14]中概述的彩票假设，它认为找到这样一个理想的性能架构就像赢得一张彩票(接近拥有理想的初始化参数)，并描述了一种实现相同目标的方法。另一个有助于减少深度网络模型规模的相关方法是“知识提炼”[15]。在这种方法中，我们首先训练一个称为“教师模型”的大模型。接下来，我们训练一个较小的模型，称为“学生模型”，它试图模仿教师模型的行为。学生模型通过尝试复制教师模型的特定层的输出以及最终损失来做到这一点。</p><p id="293d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">沿着类似的思路，有一个思维过程，即深度学习可能需要以不同的方式增强，以使其更好地用于现实世界的应用，特别是从类人学习的角度来看[16]。谷歌和T2最近都试图加入基本的直觉物理学(如重力、形状等)。)转化为深度学习模型。可微分神经计算机(DNC)是一种用随机存取读写存储器增强的神经网络，它保持端到端的可微分性，已被应用于解决方块拼图和寻找图中节点之间的路径。在围棋比赛中击败人类冠军的机器AlphaGo是一个深度学习系统，它增强了基于模型的树搜索[17]，这是一个经典的人工智能构建模块。这个领域最近的一家初创公司是<a class="ae jd" href="https://deci.ai/" rel="noopener ugc nofollow" target="_blank"> deci。AI </a>，它提出了一项名为自动神经架构构建(AutoNAC)的技术，以现有的神经网络为输入，将其转换为计算效率更高的网络。</p><p id="35fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一种规避深度学习计算限制的方法是转移到其他可能尚未发现的机器学习类型。例如，“专家”模型在计算上可能更高效，但是如果这些专家不能像灵活模型那样高效地探索和识别起作用的因素，那么它们的性能就会停滞不前(图4) [2]。这种技术已经优于深度学习模型的一个例子是那些可以更直接地应用工程和物理知识的技术:对已知物体(例如车辆)的识别[18] [19]。机器学习的符号方法的最新发展更进了一步，使用符号方法有效地学习和应用某种意义上的“专家知识”，例如[20]从数据中学习物理定律，或方法[21] [22]将神经符号推理应用于场景理解、强化学习和自然语言处理任务，建立系统的高级符号表示，以便能够用更少的数据更有效地理解和探索它。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lg"><img src="../Images/b80aa0704c5b4f810905a50cc31410ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*54ZoI62kkECuZneBXnA0Ig.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx">Figure 4: Source Credit [2]</figcaption></figure><h1 id="d2df" class="kd ke hh bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">参考</h1><p id="9175" class="pw-post-body-paragraph ie if hh ig b ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb ha bi translated">[1] M. Alom，T. Taha，C. Yakopcic，s .韦斯特博格，P. Sidike，M. Nasrin，M. Hasan，B. Van Essen，A. Awwal和v .阿莎丽，深度学习理论和架构的最新调查，第8卷(3)，电子，2019年，第292页。</p><p id="91af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2] N. C. Thompson，K. Greenewald，K. Lee和G. F. Manso，深度学习的计算极限，第4卷，麻省理工学院数字经济研究简报倡议，2020年。</p><p id="5ebd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]qi . Xie，M.-T. Luong，E. Hovy和Q. V. Le，“对有噪声的学生进行自我训练可提高ImageNet分类”，IEEE计算机视觉和模式识别会议，西雅图，2020年。</p><p id="3c9b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4] S. Bianco，R. Cadene，L. Celona和P. Napoletano，“代表性深度神经网络架构的基准分析”，<em class="jc"> IEEE Access，</em>第6卷，第64270–64277页，2018年。</p><p id="562e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[5] E. Strubell，A. Ganesh和A. McCallum，“NLP中深度学习的能源和政策考虑”，<em class="jc">计算语言学协会(ACL) </em>第57届年会，意大利佛罗伦萨，2019年。</p><p id="de26" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[6] A .克里热夫斯基、l .苏茨基弗和G. E .辛顿。，“深度卷积神经网络的ImageNet分类”，<em class="jc">《美国计算机学会通讯》，</em>第60卷第6期，第84–90页，2017。</p><p id="36bd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[7] N. P. Jouppi和e. al，“张量处理单元的数据中心内性能分析”，载于<em class="jc">第44届计算机架构年度国际研讨会会议录(ISCA '17) </em>，纽约州，2017年。</p><p id="e264" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[8] T. E. Potok，C. Schuman，S. Young，R. Patton，F. Spedalieri，J. Liu，K.-T. Yao，G. Rose和G. Chakma，“关于高性能、神经形态和量子计算机的复杂深度学习网络的研究”，<em class="jc"> ACM计算系统中的新兴技术杂志，</em>第14卷第2期，2018年7月。</p><p id="819a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[9] X. Dong，J. Huang，Y. Yang和S. Yan，“更多即更少:推理复杂度更低的更复杂网络”，载于<em class="jc"> IEEE计算机视觉与模式识别大会(CVPR) </em>，2017。</p><p id="2464" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[10]蔡海华，甘春华，王天庭，张志军，韩少南，“一劳永逸:培养一个网络，并使之专业化，以实现高效部署”，<em class="jc">(ICLR)</em>，2020年。</p><p id="6a0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[11]魏文等。al，“为更快的深度神经网络协调过滤器”，在<em class="jc"> IEEE计算机视觉国际会议(ICCV) </em>，2017年，意大利威尼斯。</p><p id="d44c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[12] B. Zoph和Q. V. Le，“具有强化学习的神经架构搜索”，载于2016年<em class="jc">国际学习表征会议</em>。</p><p id="8765" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[13] H. Pham，M. Y. Guan，B. Zoph，Q. V. Le和J. Dean，“通过参数共享实现高效的神经架构搜索”，载于<em class="jc">国际机器学习会议(ICML) </em>，2018年。</p><p id="4e19" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[14] J. Frankle和M. Carbin，“彩票假说:寻找稀疏的、可训练的神经网络”，载于2019年<em class="jc">国际学习表征会议(ICLR) </em>。</p><p id="f1a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[15] G. Hinton，O. Vinyals和J. Dean，“在神经网络中提取知识”，NIPS深度学习和表征学习研讨会<em class="jc">，2015年。</em></p><p id="a0ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[16] B. M. Lake，T. D. Ullman，J. B. T. Gershman和J. Samuel，“构建像人一样学习和思考的机器”，<em class="jc">行为脑科学，</em>第40卷，2017年11月。</p><p id="c0cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">17戴维·西尔弗等人。al，“用深度神经网络和树搜索掌握围棋博弈”，<em class="jc">《自然》，</em>第529卷，第484–503页，2016。</p><p id="f8cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[18] T. He和S. Soatto，“Mono3d+:具有双尺度3d假设和任务先验的单目3d车辆检测”，载于2019年<em class="jc">AAAI人工智能会议论文集</em>。</p><p id="be4f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[19] V. Tzoumas，P. Antonante和L. Carlone，“离群值-稳健的空间感知:硬度、通用算法和保证”，载于<em class="jc"> arXiv，stat。ML </em>，2019。</p><p id="82bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[20] S.-M. Udrescu和M. Tegmar，《Ai feynman:一种受物理学启发的符号回归方法》，<em class="jc">科学进展，</em>第6卷，第16期，2020年。</p><p id="dcbf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">21m . Asai和C. Muise。“通过立方体空间先验学习神经符号描述性规划模型:回家之旅(to STRIPS)”，载于<em class="jc">国际人工智能联合会议(IJCAI) </em>，2020年。</p><p id="3269" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[22] K. Yi，J. Wu，C. Gan，A. Torralba，P. Kohli和J. Tenenbaum，“神经-符号vqa:从视觉和语言理解中解开推理”，载于<em class="jc">神经信息处理系统进展(NIPS) </em>，2018年。</p><p id="b411" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">工具</p><p id="05ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1.<a class="ae jd" href="https://quic.github.io/aimet-pages/index.html" rel="noopener ugc nofollow" target="_blank">https://quic.github.io/aimet-pages/index.html</a>—看看这是否可以用来优化TensorFlow笔记本电脑以及PyTorch笔记本电脑的DeepSpeed。</p><p id="94ef" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.<a class="ae jd" href="https://github.com/IntelLabs/distiller" rel="noopener ugc nofollow" target="_blank">https://github.com/IntelLabs/distiller</a>—自动压缩框架——不确定是否可以对所有类型的模型进行探索。</p><p id="42e8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.https://github.com/forresti/SqueezeNet——较老的框架之一——必须看看这是否是当前的。</p></div></div>    
</body>
</html>