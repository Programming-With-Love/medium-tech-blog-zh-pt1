<html>
<head>
<title>Knowledge distillation of multilingual BERT for Walmart’s conversational AI assistant</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向沃尔玛对话式人工智能助理的多语言BERT知识提炼</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/knowledge-distillation-of-multilingual-bert-for-walmarts-conversational-ai-assistant-98ca7c2c3835?source=collection_archive---------1-----------------------#2021-05-25">https://medium.com/walmartglobaltech/knowledge-distillation-of-multilingual-bert-for-walmarts-conversational-ai-assistant-98ca7c2c3835?source=collection_archive---------1-----------------------#2021-05-25</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/61a89040084b688eb84236af9ac0fd7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oTN36mUvyTgKD8yd1jVzWQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Walmart’s askSam AI assistant</figcaption></figure><h2 id="f91f" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">介绍</h2><p id="bac8" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">沃尔玛的对话式人工智能助手已经通过谷歌助手和Siri平台实现语音购物两年多了。我们为我们的<a class="ae km" href="https://corporate.walmart.com/newsroom/2020/07/29/helping-associates-succeed-at-work-while-elevating-customer-service-safety" rel="noopener ugc nofollow" target="_blank">商店员工</a>扩展了人工智能助手，因为他们可以极大地受益于其自然语言理解(NLU)能力，并更好地为我们的客户服务。商店经理一直在使用该助手来获取产品在商店中的位置，检查不同商品的库存，查询各部门的最高销售额，甚至检查同事的日程安排。</p><p id="da29" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">我们的askSam AI助手的核心组件是NLU管道，这是一个强大的、最先进的机器学习模型的集合。意图分类和<a class="ae km" href="https://en.wikipedia.org/wiki/Named-entity_recognition" rel="noopener ugc nofollow" target="_blank">命名实体识别</a> (NER)是至关重要的NLU任务，能够理解我们同事的意图，并从他们的文本和语音查询中提取相关实体。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ks"><img src="../Images/62c44fd348a72c6c71e1e30b2841bdc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1zittNha_PvBIeRX-9JycQ.gif"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Product Entity tagging in the Walmart askSam app</figcaption></figure><p id="2a0c" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">语音助手最近也被我们的国际商场员工所采用。我们必须扩展助理的NLU能力，以便能够理解美国和墨西哥讲西班牙语的同事并与之交谈。这导致了多语言零售语音助手的开发，使我们能够在更多国家如加拿大和智利扩展业务。下表总结了我们的助理常见的一些西班牙语语音查询及其对应的NER标签。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kx"><img src="../Images/56883e79cbddae6e03cfba2d92ce8331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMNf7_LflIFT02PuH8XdiA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Walmart askSam voice queries and NER tags</figcaption></figure><p id="3aec" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">我们的对话助手是一个对延迟极其敏感的应用程序。对我们来说，研究针对延迟和内存占用进行优化的深度学习模型架构非常重要，这样我们就可以高效地扩展和服务全球超过一百万个多语言查询。在这篇博客中，我们讨论了我们如何针对西班牙语和英语查询启用快速NER，重点关注多语言<a class="ae km" href="https://blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank"> BERT </a> <a class="ae km" href="https://en.wikipedia.org/wiki/Knowledge_distillation" rel="noopener ugc nofollow" target="_blank">知识提炼</a>。</p><p id="2134" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">大多数开源<a class="ae km" href="https://arxiv.org/abs/1911.02116" rel="noopener ugc nofollow" target="_blank">多语言transformer </a>模型都是海量的，有超过1.1亿个参数，计算成本很高。缺少西班牙语标记的零售数据、沃尔玛墨西哥目录中大量未标记的数据、母语口音的多样性、部署基础设施的成本是我们在为多语言下游NER任务选择架构时考虑的几个因素。多语言BERT的知识提炼帮助我们提出了一个轻量级、超快速的生产友好版本，与大多数产品属性相关实体的教师模型相比，该版本保留了超过97%的F1分数，并显示了大约4倍的推理加速和2倍的压缩。</p><h2 id="3edc" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">微调多语言BERT</h2><p id="d061" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">我们最初根据我们的西班牙语和英语对话数据微调了Google research发布的预先训练好的<a class="ae km" href="https://github.com/google-research/bert/blob/master/multilingual.md" rel="noopener ugc nofollow" target="_blank">多语言BERT </a>。对于西班牙语数据，我们准备了一个数据集，其中包含手动筛选的语音查询模板，反映了我们的员工在商店中的使用模式。使用墨西哥沃尔玛目录提供的沃尔玛产品和品牌、释义技术和语言翻译API，我们能够生成大约一百万个英语和西班牙语标签样本。这被认为是我们多语言NER任务的基线实验。尽管这种模型架构对于实时日志中看不见的西班牙语查询表现得非常好，但我们看到，与为我们的英语NLU系统部署的生产提炼的BERT模型相比，模型延迟仍然非常高。</p><p id="b2cf" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">在研究了几种蒸馏技术和云产品后，我们发现由微软研究院<a class="ae km" href="https://arxiv.org/pdf/2004.05686.pdf" rel="noopener ugc nofollow" target="_blank">发表的研究报告xtremediate</a>在多语言NER任务的性能、模型压缩和加速方面显示了有希望的结果。通过与微软研究团队的密切合作，我们将多级蒸馏技术应用于大规模变压器模型，为我们的沃尔玛助理的西班牙NLU用例衍生出学生变压器模型。</p><h2 id="e2b3" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">变压器蒸馏架构</h2><p id="1b1e" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">本文详述的蒸馏过程提供了选择以下配置的灵活性:</p><ol class=""><li id="266b" class="ky kz hh jt b ju kn jy ko je la ji lb jm lc kl ld le lf lg bi translated">教师模型的架构(姆伯特，XLM-罗伯塔等。)</li><li id="ceb9" class="ky kz hh jt b ju lh jy li je lj ji lk jm ll kl ld le lf lg bi translated">学生模型的架构(TinyBERT、miniLM、BiLSTM等)。)</li><li id="d7c4" class="ky kz hh jt b ju lh jy li je lj ji lk jm ll kl ld le lf lg bi translated">分词器(BERT WordPiece tokenizer，SentencePiece tokenizer等。)</li></ol><p id="3efd" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">我们对教师模型的<a class="ae km" href="https://huggingface.co/bert-base-multilingual-cased" rel="noopener ugc nofollow" target="_blank"> mBERT </a>和<a class="ae km" href="https://huggingface.co/transformers/model_doc/xlmroberta.html" rel="noopener ugc nofollow" target="_blank">XLM-罗伯塔</a>以及学生模型的<a class="ae km" href="https://arxiv.org/abs/1909.10351" rel="noopener ugc nofollow" target="_blank"> TinyBERT </a>和<a class="ae km" href="https://arxiv.org/abs/2002.10957" rel="noopener ugc nofollow" target="_blank"> miniLM </a>进行了实验，以权衡蒸馏造成的性能损失和可允许的推理延迟增益。</p><p id="bd12" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">教师的训练目标、深度网络的层数、注意头的数量、教师模型预先训练的数据、教师模型可以保持的上下文量、嵌入大小和其他超参数极大地影响了学生模型的提取目标。</p><h2 id="95a3" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">标记化</h2><p id="5805" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">我们用mBERT作为教师模型，用<a class="ae km" href="https://www.tensorflow.org/tutorials/tensorflow_text/subwords_tokenizer#applying_wordpiece" rel="noopener ugc nofollow" target="_blank">单词块</a>分词器和S <a class="ae km" href="https://arxiv.org/pdf/1808.06226.pdf" rel="noopener ugc nofollow" target="_blank">单词块</a>分词器进行了实验。我们也用句子标记器评估了XLM-罗伯塔。内部标准化也在零售特定标点符号化器之前执行，忽略特定语言的重音和大小写。</p><h2 id="5bf8" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">资料组</h2><p id="6b52" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">我们在NER的三个不同的内部标记的沃尔玛对话数据集上执行了我们的蒸馏实验，即购物的搜索细化、文本购物的多产品实体识别和askSam产品搜索用例。由于注释器可用性、零售数据的语言翻译API的不准确性以及首次启动我们的西班牙语理解助手等限制，标记的英语数据的百分比高于标记的西班牙语数据。我们的实体被广泛地分为特定于产品属性的实体，如产品、品牌，以及通用实体，如邮政编码、时间参考。我们还利用了沃尔玛墨西哥目录中扩展产品组合和当天交付商品的大量未标记数据，用于特定实体的产品。</p><h2 id="fa92" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">蒸馏特征</h2><p id="e526" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">领域特定的标记数据帮助教师模型适应零售领域NER任务。学生模型也被训练为使用标记数据最小化c <a class="ae km" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">罗斯熵</a>损失。未标记的数据用于通过优化以下约束使学生能够向老师学习。</p><p id="6e43" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">1.最小化教师和学生的内部表征之间的<a class="ae km" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> KL </a>差异——未标记数据上的表征损失。</p><p id="2c2e" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">2.通过比较学生相对于教师的分类分数来最小化<a class="ae km" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> MSE </a>损失——未标记数据的MSE logit损失。</p><p id="92d0" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">3.利用教师的mBERT嵌入、非线性投影和嵌入上的<a class="ae km" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank"> SVD </a>来对齐教师和学生的输出空间。</p><p id="3947" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated">上一节中讨论的损失函数在蒸馏过程中以逐步解冻的逐级方式进行了优化，以便某一级学习以前一级学习的参数为条件的参数。这样，我们利用标记和未标记的数据来最好地学习学生模型参数。</p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lm"><img src="../Images/1da2afa97185cec5d43d923237c45fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmA_yIXRKtc49IhQFOQdfw.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Distillation architecture</figcaption></figure><h2 id="331d" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">蒸馏实验结果</h2><p id="064a" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">下表总结了对经过提炼的学生模型的性能和延迟测量。</p><p id="10f4" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated"><em class="ln">教师模型:Bert-base-multilingual-cased with word piece tokenizer</em></p><p id="31c1" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated"><em class="ln">数据集:沃尔玛内部对话式人工智能数据集</em></p><p id="c95b" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated"><em class="ln">学生型号:TinyBERT_L-4_H-312_v2和多语种-MiniLM-L12-H384。</em></p><figure class="kt ku kv kw fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lo"><img src="../Images/6e1c2adf1912d079ee4e553efd584ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iy6yGzW_fya5f3O8aBHqpQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Teacher, Student Model Metrics Comparison</figcaption></figure><h2 id="c26e" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">学生模型性能</h2><p id="ae69" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">教师和学生模型是根据我们的实时日志中的一组看不见的查询进行评估的。我们看到，对于我们的大多数产品和通用实体，TinyBERT学生的英语查询的性能下降平均不到4%。对于西班牙语查询，我们注意到TinyBERT学生模型上的产品实体的性能下降了大约9%。在MiniLM学生模型中，我们看到产品实体的F1分数仅下降了约2%,而我们仍处于允许的延迟裕量范围内。我们看到西班牙语的通用实体的性能下降更多，这可能是由于数据集和我们的实时生产日志中对这些实体的低表示和支持。</p><h2 id="54d6" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">学生模型延迟</h2><p id="fde7" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">TinyBERT模型的GPU (Tesla V100)推理延迟约为4毫秒，miniLM模型约为8毫秒。通过生成一个<a class="ae km" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>运行时，我们能够进一步将延迟减少到1毫秒以下，而与经过提炼的学生模型相比，性能没有任何下降。</p><h2 id="6f7e" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">学生模型足迹</h2><p id="03ed" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">在内存占用方面，与最初的12层mBERT模型相比，我们能够使用TinyBERT student将模型的大小减少4倍，使用miniLM student将模型的大小减少2倍。</p><h2 id="5873" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">摘要</h2><p id="2af1" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">借助多语言transformer蒸馏流程和miniLM学生模型，我们能够将多语言伯特NER模型架构的推理延迟降低到几毫秒以内，同时保留超过97%的F1分数。这也为我们的多语言下游NER任务的计算和部署基础设施带来了显著的成本优势，使我们能够扩展并服务于全球客户。</p><h2 id="20f2" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">未来的工作</h2><p id="e29b" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">随着我们在更多地区的扩张，我们希望在我们的助手上启用更多语言。从蒸馏的角度来看，我们希望通过试验以下技术来弥补学生模型的性能下降:</p><ol class=""><li id="2e61" class="ky kz hh jt b ju kn jy ko je la ji lb jm lc kl ld le lf lg bi translated">针对本地语言，试验任务特定的标记数据与未标记数据的比率。</li><li id="46ce" class="ky kz hh jt b ju lh jy li je lj ji lk jm ll kl ld le lf lg bi translated">从更大的多语言模型和表现更好的教师模型中提取精华。</li><li id="64bb" class="ky kz hh jt b ju lh jy li je lj ji lk jm ll kl ld le lf lg bi translated">改进了学生模型的初始化。</li><li id="6b4d" class="ky kz hh jt b ju lh jy li je lj ji lk jm ll kl ld le lf lg bi translated">调查特定语言的句子片断标记器。</li><li id="3acb" class="ky kz hh jt b ju lh jy li je lj ji lk jm ll kl ld le lf lg bi translated">提取注意力。</li><li id="05e5" class="ky kz hh jt b ju lh jy li je lj ji lk jm ll kl ld le lf lg bi translated">NER联合分类任务和其他深度学习模型架构的自定义提取。</li></ol><h2 id="2616" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">参考</h2><div class="lp lq ez fb lr ls"><a href="https://arxiv.org/abs/2004.05686" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab dw"><div class="lu ab lv cl cj lw"><h2 class="bd hi fi z dy lx ea eb ly ed ef hg bi translated">XtremeDistil:大规模多语言模型的多阶段提取</h2><div class="lz l"><h3 class="bd b fi z dy lx ea eb ly ed ef dx translated">深度和大型预训练语言模型是各种自然语言处理任务的最新技术…</h3></div><div class="ma l"><p class="bd b fp z dy lx ea eb ly ed ef dx translated">arxiv.org</p></div></div></div></a></div><div class="lp lq ez fb lr ls"><a href="https://arxiv.org/abs/1910.01108" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab dw"><div class="lu ab lv cl cj lw"><h2 class="bd hi fi z dy lx ea eb ly ed ef hg bi translated">蒸馏伯特，伯特的蒸馏版本:更小，更快，更便宜，更轻</h2><div class="lz l"><h3 class="bd b fi z dy lx ea eb ly ed ef dx translated">随着大规模预训练模型的迁移学习在自然语言处理(NLP)中变得越来越普遍…</h3></div><div class="ma l"><p class="bd b fp z dy lx ea eb ly ed ef dx translated">arxiv.org</p></div></div></div></a></div><div class="lp lq ez fb lr ls"><a href="https://arxiv.org/abs/1503.02531" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab dw"><div class="lu ab lv cl cj lw"><h2 class="bd hi fi z dy lx ea eb ly ed ef hg bi translated">提取神经网络中的知识</h2><div class="lz l"><h3 class="bd b fi z dy lx ea eb ly ed ef dx translated">提高几乎任何机器学习算法的性能的一个非常简单的方法是训练许多不同的模型…</h3></div><div class="ma l"><p class="bd b fp z dy lx ea eb ly ed ef dx translated">arxiv.org</p></div></div></div></a></div><h2 id="26d6" class="it iu hh bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">承认</h2><p id="bc07" class="pw-post-body-paragraph jr js hh jt b ju jv jw jx jy jz ka kb je kc kd ke ji kf kg kh jm ki kj kk kl ha bi translated">沃尔玛全球技术对话人工智能团队和<a class="mb mc ge" href="https://medium.com/u/ce33163d1ee?source=post_page-----98ca7c2c3835--------------------------------" rel="noopener" target="_blank">伊曼·米雷扎伊</a>、<a class="mb mc ge" href="https://medium.com/u/42dcc11aa043?source=post_page-----98ca7c2c3835--------------------------------" rel="noopener" target="_blank">阿德里安·桑切斯</a>和<a class="mb mc ge" href="https://medium.com/u/4f942b6a9cd4?source=post_page-----98ca7c2c3835--------------------------------" rel="noopener" target="_blank">西姆拉尔·乔德里</a>在这项工作中帮助进行了多次实验。</p><p id="0913" class="pw-post-body-paragraph jr js hh jt b ju kn jw jx jy ko ka kb je kp kd ke ji kq kg kh jm kr kj kk kl ha bi translated"><a class="ae km" href="https://www.microsoft.com/en-us/research/people/submukhe/" rel="noopener ugc nofollow" target="_blank">微软研究团队的Subho Mukherjee </a>和<a class="ae km" href="https://www.linkedin.com/in/yukaishi/" rel="noopener ugc nofollow" target="_blank"> Steven Shi </a>分享大规模多语言模型提炼的经验和最佳实践。</p></div></div>    
</body>
</html>