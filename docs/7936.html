<html>
<head>
<title>The Journey of Open AI GPT models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">开放人工智能GPT模型之旅</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2?source=collection_archive---------0-----------------------#2020-11-10">https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2?source=collection_archive---------0-----------------------#2020-11-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/dad2612970c330c787bc77a3dac42bdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQ8qPcJB7xdd8wEXEcD9Bg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo Credit : <a class="ae it" href="https://pixabay.com/photos/road-winding-street-bridge-1030789/" rel="noopener ugc nofollow" target="_blank">Image</a> by <a class="ae it" href="https://pixabay.com/photos/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1030789" rel="noopener ugc nofollow" target="_blank">Free-Photos</a> from <a class="ae it" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1030789" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="0aef" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">OpenAI的生成式预训练转换器(GPT)模型通过引入非常强大的语言模型，在自然语言处理(NLP)社区掀起了风暴。这些模型可以执行各种自然语言处理任务，如<a class="ae it" href="https://en.wikipedia.org/wiki/Question_answering" rel="noopener ugc nofollow" target="_blank">问题回答</a>、<a class="ae it" href="https://en.wikipedia.org/wiki/Textual_entailment#:~:text=Textual%20entailment%20(TE)%20in%20natural,hypothesis%20(h)%2C%20respectively." rel="noopener ugc nofollow" target="_blank">文本蕴涵</a>、文本摘要等。没有任何监督训练。这些语言模型只需要很少甚至不需要例子就能理解任务，并且执行起来相当于甚至优于以监督方式训练的最先进的模型。</p><p id="e225" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，我们将介绍这些模型的发展历程，并了解它们是如何在两年的时间内发展的。我们将在此讨论以下主题:</p><p id="696e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">1.GPT-1论文讨论(<a class="ae it" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">通过生成性预训练</a>提高语言理解)。</p><p id="e563" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.讨论GPT-2论文(<a class="ae it" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无监督的多任务学习者</a>)及其随后对GPT-1的改进。</p><p id="804f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">3.讨论GPT-3论文<a class="ae it" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank">(语言模型很少被学习</a>)以及使其成为迄今为止NLP见过的最强大的模型之一的改进。</p><p id="27aa" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本文假设读者熟悉NLP术语和<a class="ae it" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">转换器</a>架构的基础知识。</p><p id="c434" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们从逐一理解这些文件开始。为了使这个旅程更容易理解，我将每篇论文分成四个部分:论文中讨论的目标和概念、使用的数据集、模型架构和实现细节，以及它们的性能评估。</p><h1 id="13a8" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">通过生成性预训练提高语言理解</strong> (GPT-1):</h1><p id="f3d4" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">在这项工作之前，大多数最新的NLP模型都是专门针对特定任务进行训练的，如情感分类、文本蕴涵等。使用监督学习。然而，监督模型有两个主要限制:</p><p id="bc56" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">他们需要大量的带注释的数据来学习一个特定的任务，这通常是不容易得到的。</p><p id="0352" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">二。除了他们所接受的训练之外，他们不能概括任务。</p><p id="6c2c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本文提出使用未标记数据学习生成语言模型，然后通过提供特定下游任务(如分类、情感分析、文本蕴涵等)的示例来微调该模型。</p><blockquote class="kv kw kx"><p id="5202" class="iu iv ky iw b ix iy iz ja jb jc jd je kz jg jh ji la jk jl jm lb jo jp jq jr ha bi translated">无监督学习用作监督微调模型的预训练目标，因此得名生成性预训练。</p></blockquote><p id="47ab" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">让我们浏览一下本文中讨论的概念和方法。</p><p id="2aca" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">1.<strong class="iw hi">学习目标和概念</strong>:这种用于NLP任务的<em class="ky">半监督</em>学习(无监督预训练，然后监督微调)有以下三个组成部分:</p><p id="4ab5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">a.<strong class="iw hi">无监督语言建模</strong>(预训练):对于无监督学习，使用标准语言模型目标。</p><figure class="ld le lf lg fd ii er es paragraph-image"><div class="er es lc"><img src="../Images/0cfed1346451535bb49bcc10920d71ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*Zrg8WFl_Zc7FDtSVgLC9eg.png"/></div></figure><p id="ccdd" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">其中T是无监督数据{ t1，…，TN }中的特征集，k是上下文窗口的大小，θ是使用随机梯度下降训练的神经网络的参数。</p><p id="d365" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">b.<strong class="iw hi">监督微调</strong>:该部分旨在最大化观察标签y的可能性，给定特征或标记x_1，…，x_n。</p><figure class="ld le lf lg fd ii er es paragraph-image"><div class="er es lh"><img src="../Images/9234f22c1f2ced83ffe09b4c71cba1ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*5hDwpxGf2KGPlNOvmcBX6g.png"/></div></figure><p id="3245" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">其中C是由训练示例组成的带标签的数据集。</p><p id="1044" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">不是简单地最大化等式(ii)中提到的目标，作者添加了一个<a class="ae it" href="#e96b" rel="noopener ugc nofollow"> <strong class="iw hi">辅助学习目标</strong> </a>用于监督微调，以获得更好的概括和更快的收敛。修改后的培训目标表述为:</p><figure class="ld le lf lg fd ii er es paragraph-image"><div class="er es li"><img src="../Images/ea6230f915a859f4b0c77a23a5f33baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*pFWB54O7V8HtWu97H0wUIw.png"/></div></figure><p id="acf5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">其中，L₁(C是学习语言模型的辅助目标，λ是赋予该辅助学习目标的权重。λ被设置为0.5。</p><p id="8ef5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">通过向transformer模型添加一个linear和一个softmax层来获得下游任务的任务标签，从而实现了受监督的微调。</p><p id="c07a" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">c.<strong class="iw hi">特定于任务的输入转换</strong>:为了在微调过程中对模型的架构做出最小的改变，特定下游任务的输入被转换成有序序列。令牌按以下方式重新排列:</p><p id="10b6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">—输入序列中添加了开始和结束标记。</p><p id="9cc6" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">—在示例的不同部分之间添加了分隔符标记，以便输入可以按有序序列发送。对于像问题回答，多项选择问题等任务。每个示例都发送了多个序列。例如，训练示例包括用于问答任务的上下文、问题和答案的序列。</p><p id="e8c3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.<strong class="iw hi">数据集</strong> : GPT一号使用了<a class="ae it" href="https://yknzhu.wixsite.com/mbweb" rel="noopener ugc nofollow" target="_blank">图书语料库</a>数据集来训练语言模型。BooksCorpus有大约7000本未出版的书籍，这些书籍有助于在看不见的数据上训练语言模型。该数据不太可能在下游任务的测试集中找到。此外，这个语料库有大量连续的文本，这有助于模型学习大范围的依赖关系。</p><p id="b87b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">3.<strong class="iw hi">模型架构和实现细节</strong> : GPT一号使用了12层的只解码的变换器结构，带掩蔽自注意来训练语言模型。模型的架构在很大程度上保持不变，如关于变压器的<a class="ae it" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">原著</a>中所述。<a class="ae it" href="#6c42" rel="noopener ugc nofollow">屏蔽</a>有助于实现语言模型目标，其中语言模型无法访问当前单词右侧的后续单词。</p><p id="94c9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以下是实现细节:</p><p id="0178" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">a.<strong class="iw hi">用于无监督训练</strong>:</p><ul class=""><li id="fab3" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated"><a class="ae it" href="#acc9" rel="noopener ugc nofollow">字节对编码</a> (BPE)使用了40，000个合并的词汇。</li><li id="d0ef" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">模型使用768维状态将标记编码到单词嵌入中。位置嵌入也在培训中学习。</li><li id="2ad3" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">使用12层模型，每个自我注意层中有12个注意头。</li><li id="4cd0" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">对于位置式前馈层，使用3072维状态。</li><li id="ab39" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">使用Adam optimiser，学习率为2.5e-4。</li><li id="1c46" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">注意、剩余和嵌入辍学用于正规化，辍学率为0.1。L2正则化的修改版本也用于无偏倚的权重。</li><li id="e4c8" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">使用GELU作为激活函数。</li><li id="1e06" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">该模型在大小为64和序列长度为512的小批量上训练了100个时期。该模型共有117M个参数。</li></ul><p id="710f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">b.<strong class="iw hi">用于监督微调</strong>:</p><ul class=""><li id="4df0" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">对于大多数下游任务，有监督的微调只需要3个时期。这表明模型在预训练期间已经学习了很多关于语言的知识。因此，最小的微调就足够了。</li><li id="be70" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">来自无监督预训练的大多数超参数用于微调。</li></ul><p id="9467" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">4.<strong class="iw hi">绩效与总结</strong>:</p><blockquote class="kv kw kx"><p id="dc8b" class="iu iv ky iw b ix iy iz ja jb jc jd je kz jg jh ji la jk jl jm lb jo jp jq jr ha bi translated">在与模型进行比较的12项任务中，GPT-1在9项任务中的表现优于经过专门训练的监督型最新模型。</p></blockquote><p id="b3ef" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这款机型的另一个重大成就是它在各种任务上体面的<a class="ae it" href="#b2d9" rel="noopener ugc nofollow"><strong class="iw hi"/><strong class="iw hi">零射击性能</strong> </a>。该论文证明了该模型在不同的自然语言处理任务(如问答、模式解析、情感分析等)上的零命中率性能已经得到了发展。由于预先训练。</p><p id="f48b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">GPT-1证明语言模型是一个有效的预训练目标，可以帮助模型更好地概括。该架构有助于迁移学习，并且只需很少的微调就可以执行各种NLP任务。该模型显示了生成性预训练的力量，并为其他模型开辟了道路，这些模型可以通过更大的数据集和更多的参数更好地释放这种潜力。</p><h1 id="4253" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">语言模型是无监督的多任务学习器(</strong> GPT-2):</h1><p id="15ad" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">GPT-2模型的发展主要是在使用更大的数据集和向模型添加更多参数以学习更强的语言模型方面。让我们看看GPT新协议模型的重大发展和本文中讨论的概念:</p><ol class=""><li id="3837" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lx lp lq lr bi translated"><strong class="iw hi">学习目标和概念</strong>:以下是本文在NLP背景下讨论的两个重要概念。</li></ol><ul class=""><li id="4be1" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated"><strong class="iw hi">任务条件</strong>:我们已经看到语言模型的训练目标被公式化为P(输出|输入)。然而，GPT-2旨在使用相同的无监督模型学习多项任务。为此，学习目标应该修改为P(输出|输入，任务)。这种修改被称为任务条件化，在这种情况下，模型被期望为不同的任务产生相同输入的不同输出。一些模型在架构级别实现任务调节，其中模型被输入和任务两者。对于语言模型，输出、输入和任务都是自然语言的序列。因此，通过向模型提供示例或自然语言指令来执行任务，来执行语言模型的<em class="ky">任务条件。任务条件形成了零射击任务转移的基础，我们将在下面讨论。</em></li><li id="53cf" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated"><strong class="iw hi">零镜头学习和零短任务转移</strong>:GPT 2的一个有趣的能力是<a class="ae it" href="#21cf" rel="noopener ugc nofollow">零镜头任务转移</a>。<a class="ae it" href="#b2d9" rel="noopener ugc nofollow">零触发学习</a>是零触发任务转移的一种特殊情况，在这种情况下，没有提供任何示例，模型根据给定的指令理解任务。GPT 2号的输入不是像GPT 1号那样重新安排序列，而是以一种期望模型理解任务的性质并提供答案的格式给出的。这样做是为了模拟零射击任务转移行为。例如，对于英语到法语的翻译任务，给模型一个英语句子，后跟单词法语和提示(:)。该模型应该理解这是一个翻译任务，并给出英语句子的法语对应物。</li></ul><p id="5ae9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.<strong class="iw hi">数据集</strong>:为了创建一个广泛且高质量的数据集，作者们抓取了Reddit平台，并从高投票率文章的出站链接中提取数据。由此产生的名为WebText的数据集包含来自800多万个文档的40GB文本数据。该数据集用于训练GPT-2，并且与用于训练GPT-1模型的图书语料库数据集相比是巨大的。由于许多测试集包含维基百科文章，所有维基百科文章都从网络文本中删除。</p><p id="91cf" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">3.<strong class="iw hi">模型架构和实现细节</strong> : GPT二号有15亿个参数。这比GPT 1号(117米参数)多10倍。与GPT 1号的主要区别是:</p><ul class=""><li id="68b9" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">GPT-2有48层，使用1600维向量来嵌入单词。</li><li id="7597" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">使用了50，257个标记的更大词汇表。</li><li id="3df1" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">使用了512的较大批量和1024个令牌的较大上下文窗口。</li><li id="c818" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">层标准化被移动到每个子块的输入，并且在最终自关注块之后添加了附加层标准化。</li><li id="233c" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">初始化时，残差层的权重按1/√N进行缩放，其中N是残差层的数量。</li></ul><p id="57a9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">作者用117M(与GPT-1相同)、345M、762M和1.5B (GPT-2)的参数训练了四个语言模型。每一个后续的模型都比前一个模型更容易混淆。<em class="ky">这确立了同一数据集上语言模型的</em> <a class="ae it" href="#0d4b" rel="noopener ugc nofollow"> <em class="ky">困惑度</em> </a> <em class="ky">随着参数个数的增加而减小。</em>此外，参数数量最多的模型在每个下游任务中表现更好。</p><p id="6a95" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">4.<strong class="iw hi">表现和总结</strong> : GPT-2在几个下游任务数据集上进行了评估，如阅读理解、总结、翻译、问题回答等。让我们来看看其中的一些任务以及GPT 2号在这些任务上的表现:</p><ul class=""><li id="5907" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">GPT-2在零触发设置中为8个语言建模数据集中的7个改进了当时现有的最先进水平。</li><li id="3f5f" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated"><a class="ae it" href="https://arxiv.org/pdf/1511.02301.pdf" rel="noopener ugc nofollow" target="_blank">儿童图书数据集</a>评估语言模型在名词、介词、命名实体等词类别上的性能。GPT-2将常用名词和命名实体识别的最新准确率提高了约7%。</li><li id="135a" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated"><a class="ae it" href="https://arxiv.org/abs/1606.06031" rel="noopener ugc nofollow" target="_blank"> LAMBADA </a>数据集评估模型在识别长期依存关系和预测句子最后一个单词方面的性能。GPT-2将困惑度从99.8降低到8.6，并显著提高了准确性。</li><li id="aa45" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">GPT-2在阅读理解任务中的表现优于四个基线模型中的三个。</li><li id="ef3c" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">在法语到英语的翻译任务中，GPT-2在零镜头设置方面比大多数无监督模型表现得更好，但并不优于最先进的无监督模型。</li><li id="e882" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">GPT-2在文本摘要方面表现不佳，其性能类似于或低于为摘要而训练的经典模型。</li></ul><blockquote class="kv kw kx"><p id="6305" class="iu iv ky iw b ix iy iz ja jb jc jd je kz jg jh ji la jk jl jm lb jo jp jq jr ha bi translated">GPT-2能够在零触发的8个测试语言建模数据集的7个上实现最先进的结果。</p></blockquote><p id="18d4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">GPT-2表明，在更大的数据集上进行训练和拥有更多的参数提高了语言模型理解任务的能力，并超过了零镜头设置中许多任务的最先进水平。该论文指出，随着模型容量的增加，性能以对数线性方式增加。此外，语言模型的复杂度下降并没有表现出饱和，而是随着参数数量的增加而持续下降。事实上，GPT-2不适合网络文本数据集，更多时间的训练可以进一步减少困惑。这表明GPT-2的模型大小不是极限，建立更大的语言模型将减少困惑，并使语言模型在自然语言理解方面更好。</p><h1 id="5dee" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">语言模型很少镜头学习者</strong> (GPT-3):</h1><p id="491b" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">为了寻求建立非常强大的语言模型，这种模型不需要微调，只需要很少的演示就可以理解并执行任务，Open AI建立了具有1750亿个参数的GPT-3模型。这个模型的参数比微软强大的图灵NLG语言模型多10倍，比GPT-2多100倍。由于大量的参数和广泛的数据集GPT-3已被训练，它在零炮和少炮设置的下游NLP任务中表现良好。由于它的大容量，它具有写文章的能力，很难与人类写的文章区分开来。它还可以执行从未明确训练过的即时任务，如对数字求和、编写SQL查询和代码、解读句子中的单词、根据任务的自然语言描述编写React和JavaScript代码等。让我们了解一下新GPT协议中提到的概念和发展，以及该模型的一些更广泛的影响和局限性:</p><ol class=""><li id="ee8d" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lx lp lq lr bi translated"><strong class="iw hi">学习目标和概念</strong>:让我们讨论一下本文中讨论的两个概念。</li></ol><ul class=""><li id="8434" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated"><strong class="iw hi">情境学习</strong>:大型语言模型使用它们接受训练的文本数据开发模式识别和其他技能。在学习预测给定上下文单词的下一个单词的主要目标时，语言模型也开始识别数据中的模式，这有助于它们最小化语言建模任务的损失。后来，这种能力在零射击任务转移期间帮助模型。当只有很少的例子和/或它需要做什么的描述时，语言模型将这些例子的模式与它过去从类似数据中学到的知识相匹配，并使用这些知识来执行任务。这是大型语言模型的强大功能，随着模型参数数量的增加而增加。</li><li id="ab93" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated"><strong class="iw hi">少拍、单拍和零拍设置</strong>:如前所述，少拍、单拍和零拍设置是零拍任务转移的特例。在少镜头设置中，为模型提供任务描述和尽可能多的适合模型上下文窗口的例子。在一次设置中，模型仅提供一个示例，而在零次设置中，不提供任何示例。随着模型容量的增加，模型的少炮、一炮和零炮能力也随之提高。</li></ul><p id="7a24" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">2.<strong class="iw hi">数据集</strong> : GPT-3在五个不同的语料库上进行训练，每个语料库都有一定的权重。高质量的数据集被更频繁地采样，并且模型在其上被训练超过一个时期。使用的五个数据集是Common Crawl、WebText2、Books1、Books2和Wikipedia。</p><p id="50c9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">3.<strong class="iw hi">模型和实现细节</strong>:GPT三号的架构与GPT二号相同。与GPT-2的几个主要区别是:</p><ul class=""><li id="5701" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">GPT-3有96层，每层有96个注意头。</li><li id="4e6f" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">GPT-3的单词嵌入量从GPT-2的1600增加到12888。</li><li id="2a68" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">上下文窗口大小从GPT-2的1024个令牌增加到GPT-3的2048个令牌。</li><li id="18c2" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">使用亚当优化器，β_1=0.9，β_2=0.95，ε= 10^(-8).</li><li id="fc85" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">交替密集和局部带状稀疏注意模式被使用。</li></ul><p id="4168" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">4.<strong class="iw hi">性能和总结</strong> : GPT-3在一系列语言建模和自然语言处理数据集上进行了评估。对于语言建模数据集，如LAMBADA和Penn Tree Bank，GPT-3在少镜头或零镜头设置下的表现优于当前水平。对于其他数据集，它无法击败最先进的技术，但改善了零炮最先进的性能。GPT-3在自然语言处理任务中也表现得相当好，如闭卷问答、模式解析、翻译等。，经常击败最先进的或性能可媲美微调模型。对于大多数任务，与一次和零次拍摄相比，该模型在少量拍摄设置中表现更好。</p><p id="42fe" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">除了在传统的自然语言处理任务上对模型进行评估之外，还在算术加法、单词解读、新闻文章生成、学习和使用新单词等综合任务上对模型进行了评估。对于这些任务，性能也随着参数数量的增加而增加，并且该模型在少镜头设置中的性能优于一次和零镜头设置。</p><p id="a1e3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">5.<strong class="iw hi">局限性和更广泛的影响</strong>:本文讨论了GPT-3模型的几个弱点和有待改进的地方。让我们在这里总结一下。</p><ul class=""><li id="3f70" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lo lp lq lr bi translated">虽然GPT-3能够产生高质量的文本，但有时它在形成长句和一遍又一遍地重复文本序列时开始失去连贯性。此外，GPT-3在自然语言推理(确定一个句子是否暗示另一个句子)、填空、一些阅读理解任务等任务上表现不佳。该论文引用GPT模型的单向性作为这些局限性的可能原因，并建议在这种规模下训练双向模型来克服这些问题。</li><li id="5052" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">该论文指出的另一个限制是GPT-3的通用语言建模目标，该目标对每个标记进行同等加权，并且缺乏标记的面向任务或目标的预测的概念。针对这一点，本文提出了增加学习目标、使用强化学习来微调模型、增加其他模态等方法。</li><li id="5579" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">GPT-3的其他限制包括由于其沉重的架构而导致的从模型进行复杂和昂贵的推理，语言和模型生成的结果的可解释性较差，以及关于什么帮助模型实现其少量学习行为的不确定性。</li><li id="d471" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lo lp lq lr bi translated">除了这些限制之外，GPT-3还具有滥用其类人文本生成能力进行网络钓鱼、垃圾邮件、传播错误信息或进行其他欺诈活动的潜在风险。此外，由GPT-3生成的文本具有它所训练的语言的偏见。GPT-3生成的文章可能有性别、民族、种族或宗教偏见。因此，谨慎使用这些模型并在使用之前监控它们生成的文本变得极其重要。</li></ul><h2 id="44ca" class="ly jt hh bd ju lz ma mb jy mc md me kc jf mf mg kg jj mh mi kk jn mj mk ko ml bi translated"><strong class="ak">结局注</strong>:</h2><p id="f709" class="pw-post-body-paragraph iu iv hh iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr ha bi translated">本文通过三篇论文总结了OpenAI GPT模型的发展历程及其演变。这些模型无疑是非常强大的语言模型，通过仅使用指令和少量示例执行大量任务，彻底改变了自然语言处理领域。尽管这些模型在自然语言理解方面还不能与人类相提并论，但它们无疑为实现这一目标指明了前进的方向。</p><h2 id="d4d4" class="ly jt hh bd ju lz ma mb jy mc md me kc jf mf mg kg jj mh mi kk jn mj mk ko ml bi translated"><strong class="ak">词汇:</strong></h2><ol class=""><li id="e96b" class="lj lk hh iw b ix kq jb kr jf mm jj mn jn mo jr lx lp lq lr bi translated">辅助学习目标是与主要学习目标一起学习的附加训练目标或任务，通过使模型更通用来提高模型的性能。这篇<a class="ae it" href="https://arxiv.org/abs/1704.07156" rel="noopener ugc nofollow" target="_blank">论文</a>提供了这个概念的更多细节。</li><li id="6c42" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lx lp lq lr bi translated">屏蔽指的是用一些其他虚拟标记来移除或替换句子中的单词，使得模型在训练时无法访问这些单词。</li><li id="acc9" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lx lp lq lr bi translated">字节对编码是一种数据压缩技术，其中频繁出现的连续字节对被替换为数据中不存在的字节，以压缩数据。为了重建原始数据，使用包含替换字节映射的表。这篇<a class="ae it" href="https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10" rel="noopener" target="_blank">博客</a>详细解释了BPE。</li><li id="b2d9" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lx lp lq lr bi translated">零镜头学习或行为指的是一个模型在过去没有看到任何这种例子的情况下执行任务的能力。在零镜头学习期间没有梯度更新发生，并且模型应该理解任务而不看任何例子。</li><li id="21cf" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lx lp lq lr bi translated">零镜头任务迁移或元学习指的是在很少或没有例子的情况下呈现模型的设置，以使其理解任务。术语零炮来自于不执行梯度更新的事实。模型应该理解基于例子和指令的任务。</li><li id="0d4b" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lx lp lq lr bi translated">困惑是语言模型的标准评估标准。困惑是测试集的逆概率，它由测试集中的字数归一化。具有较低复杂度的语言模型被认为比具有较高复杂度的语言模型更好。阅读<a class="ae it" href="https://towardsdatascience.com/perplexity-in-language-models-87a196019a94" rel="noopener" target="_blank">这篇</a>博客，了解更多关于困惑的解释。</li></ol><p id="016d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">参考文献:</strong></p><ol class=""><li id="0e1a" class="lj lk hh iw b ix iy jb jc jf ll jj lm jn ln jr lx lp lq lr bi translated">拉德福德，a .，纳拉辛汉，k .，萨利曼斯，t .和苏茨基弗，I .，2018年。通过生成性预训练提高语言理解能力。</li><li id="7da6" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lx lp lq lr bi translated">a .、Wu、j .、Child、r .、Luan、d .、Amodei、d .和Sutskever，I .，2019年。语言模型是无人监督的多任务学习者。<em class="ky"> OpenAI博客</em>，<em class="ky"> 1 </em> (8)，第9页。</li><li id="49eb" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lx lp lq lr bi translated">布朗，汤姆·b，本杰明·曼，尼克·赖德，梅勒妮·苏比亚，贾里德·卡普兰，普拉富拉·达瑞瓦尔，阿尔温德·尼拉坎坦等人《语言模型是很少出手的学习者》<em class="ky"> arXiv预印本arXiv:2005.14165 </em> (2020)。</li><li id="1ba5" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lx lp lq lr bi translated">Rei，m，2017。用于序列标记的半监督多任务学习。<em class="ky"> arXiv预印本arXiv:1704.07156 </em>。</li><li id="3c6a" class="lj lk hh iw b ix ls jb lt jf lu jj lv jn lw jr lx lp lq lr bi translated">Waswani，a .，Shazeer，n .，Parmar，n .，Uszkoreit，j .，Jones，l .，Gomez，A.N .，Kaiser，l .和Polosukhin，I .，2017年。你需要的只是关注。在<em class="ky">辊隙</em>中。</li></ol><p id="8a8d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">注意:为了简洁起见，博客的链接在参考文献中没有重复。</p></div></div>    
</body>
</html>