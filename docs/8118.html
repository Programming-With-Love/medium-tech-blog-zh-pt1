<html>
<head>
<title>Saving and Retrieving ML Models Using PySpark in Cloud Platform</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">云平台中使用PySpark保存和检索ML模型</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/saving-and-retrieving-ml-models-using-pyspark-in-cloud-platform-d8b1db9e91b1?source=collection_archive---------2-----------------------#2022-05-10">https://medium.com/walmartglobaltech/saving-and-retrieving-ml-models-using-pyspark-in-cloud-platform-d8b1db9e91b1?source=collection_archive---------2-----------------------#2022-05-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/6b47cfc5cc9f3e0a098ea5efdc5582b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fSx16LfqCdAMgxDBGlEew.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Machine learning pipeline for cloud applications</figcaption></figure><p id="89e8" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">PySpark已经成为许多数据科学和机器学习(ML)爱好者扩展数据科学和ML模型的首选平台，因为它具有卓越且易于使用的并行计算能力，以及与外部Python库轻松集成的能力。通常，需要在训练后保存一个模型，以便以后被其他应用程序检索和使用。在本文中，我将解释使用PySpark保存和检索ML模型的两种方法。第一种方法使用Spark的原生<a class="ae jr" href="https://spark.apache.org/mllib/%22%20/t%20%22_blank" rel="noopener ugc nofollow" target="_blank"> MLlib </a>模块，而第二种方法基于一种定制方法，主要专注于使用最少的存储量存储大量的模型文件。这些方法在谷歌云平台的(GCP) <a class="ae jr" href="https://cloud.google.com/sdk/gcloud/reference/dataproc" rel="noopener ugc nofollow" target="_blank"> dataproc </a>集群和<a class="ae jr" href="https://cloud.google.com/storage" rel="noopener ugc nofollow" target="_blank">存储</a>中得到了应用和验证，所以下面分享的代码片段大多是针对GCP的。</p><p id="4189" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这篇文章假设你已经熟悉使用PySpark和熊猫UDF。如果没有，这里有一个快速简单的<a class="ae jr" rel="noopener" href="/walmartglobaltech/multi-time-series-forecasting-in-spark-cc42be812393">总结</a>。此外，本文跳过了一些琐碎的步骤，如数据清理，因为这里的主要焦点是保存和加载ML模型。</p><h1 id="a5fb" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">获取数据</h1><p id="a288" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">这里的数据集作为<code class="du kv kw kx ky b">parquet</code>文件存储在Google Storage (GS)中。以下代码片段将数据集加载到PySpark DataFrame中。</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="9332" class="lh jt hh ky b fi li lj l lk ll">sc = SparkContext()</span><span id="d61f" class="lh jt hh ky b fi lm lj l lk ll">#........Read data from parquet files........#<br/>dataDir = "gs://mlmodels/data/*"<br/>dataTable = spark.read.parquet(dataDir)<br/>dataTable.createOrReplaceTempView("inputTable")</span><span id="cce8" class="lh jt hh ky b fi lm lj l lk ll">inputDf = spark.sql('select * from inputTable ')</span></pre><h1 id="15f6" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">方法1 —使用PySpark ML </strong> lib模块</h1><p id="4015" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">MLlib是Spark的原生API，用于大规模构建ML模型。下面的代码片段首先将输入数据分成训练集和测试集，然后使用训练数据训练随机森林(RF)模型。(<em class="ln">注:参数值应根据输入数据选择。)</em></p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="0bb8" class="lh jt hh ky b fi li lj l lk ll">from pyspark.mllib.tree import RandomForest, RandomForestModel<br/><br/><em class="ln"># Split the data into training and test sets <br/></em>(trainingData, testData) = inputDf.randomSplit([0.8, 0.2])<br/><br/><em class="ln"><br/></em>model = RandomForest.trainRegressor(trainingData, numTrees = 10, categoricalFeaturesInfo={}, featureSubsetStrategy="auto",<br/>impurity='variance', maxDepth=5)</span></pre><p id="36aa" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">使用save方法保存训练好的模型非常简单:</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="550b" class="lh jt hh ky b fi li lj l lk ll">model.save(sc,"gs://mlmodels/models/myRF")</span></pre><p id="3733" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">生成的目录包含几个文件夹，其中有多个拼花文件。根据数据的大小和复杂程度，文件的数量可能会有所不同(在我的例子中，每个RF型号几乎有350个文件)。</p><p id="3076" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">为了检索先前保存的模型，我们需要使用load方法:</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="987e" class="lh jt hh ky b fi li lj l lk ll">savedModel = RandomForestModel.load(sc,"gs://mlmodels/models/myRF")</span></pre><h1 id="601c" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">方法2 —使用joblib和gsutil</h1><blockquote class="lo lp lq"><p id="9057" class="it iu ln iv b iw ix iy iz ja jb jc jd lr jf jg jh ls jj jk jl lt jn jo jp jq ha bi translated">为什么需要第二种方法？</p><p id="b0db" class="it iu ln iv b iw ix iy iz ja jb jc jd lr jf jg jh ls jj jk jl lt jn jo jp jq ha bi translated"><em class="hh">如您所见，使用MLlib简单明了。但是用这种方法保存和检索模型有点耗费时间和资源。对于大多数用例来说，这可能不是问题，但是如果您正在构建一个必须使用尽可能少的存储空间尽可能快地生成、保存和加载多个模型(在我的例子中，有100多个模型)的应用程序，那么使用MLlib可能不是最佳选择。这让我们想到了第二种方法。</em></p></blockquote><p id="daa4" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">对于这种方法，我们根据它们的ID对数据集进行分组，然后应用熊猫UDF为每个组创建一个RF模型。对于每个组，首先将其数据框架分为训练集和测试集(80:20的比例)。然后用训练数据集实例化和训练RF模型。</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="64a5" class="lh jt hh ky b fi li lj l lk ll">from sklearn.model_selection import train_test_split</span><span id="036c" class="lh jt hh ky b fi lm lj l lk ll">#........Define schema for output dataframe here........#<br/>outSchema = StructType([...])</span><span id="b45f" class="lh jt hh ky b fi lm lj l lk ll">#........Define pandas UDF........#<br/><a class="ae jr" href="http://twitter.com/pandas_udf" rel="noopener ugc nofollow" target="_blank">@pandas_udf</a>(outSchema, PandasUDFType.GROUPED_MAP)<br/>def generate model(df):<br/>   label = np.array(df['label'])<br/>   features = df.drop('label', axis = 1)</span><span id="0381" class="lh jt hh ky b fi lm lj l lk ll">   # splitting training and testing data<br/>   train_features, test_features, train_labels, test_labels = train_test_split(features, label, test_size = 0.2, random_state = 42)<br/>   <br/>   # Instantiate model with decision trees<br/>   model = RandomForestRegressor(bootstrap = True, max_depth = 5, max_features = 'auto', min_samples_leaf = 2, min_samples_split = 8, n_estimators = 10, oob_score = True, random_state = 42)<br/>    <br/>   # Train the model on training data<br/>   model.fit(train_features, train_labels)</span><span id="f3d9" class="lh jt hh ky b fi lm lj l lk ll">   ...</span><span id="b26e" class="lh jt hh ky b fi lm lj l lk ll">outputDf = inputDf.groupby('id').apply(generate_model)</span></pre><blockquote class="lo lp lq"><p id="7478" class="it iu ln iv b iw ix iy iz ja jb jc jd lr jf jg jh ls jj jk jl lt jn jo jp jq ha bi translated">保存模型</p></blockquote><p id="d1b3" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在我们有了自己的模型，是时候拯救它了。让我们将问题分成两部分:1)我们需要找到一种方法将模型对象保存到文件中，2)将相应的文件上传到GS bucket中。</p><p id="c059" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">对于第一部分，我们使用一个名为<em class="ln"> joblib的包。</em><code class="du kv kw kx ky b">joblib.dump()</code>方法允许我们将一个对象压缩并存储到一个文件中。它有三个参数——要转储的对象、应该转储的目录位置以及转储时使用的压缩算法。我根据<em class="ln"> joblib </em> <a class="ae jr" href="https://joblib.readthedocs.io/en/latest/generated/joblib.dump.html" rel="noopener ugc nofollow" target="_blank"> docs </a>使用<code class="du kv kw kx ky b">compress = 3</code>，这个算法是大小和速度之间的一个很好的折衷。现在，让我们将模型保存在一个本地目录中，并根据相应的组ID对其进行命名。</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="ed41" class="lh jt hh ky b fi li lj l lk ll">import joblib</span><span id="19bb" class="lh jt hh ky b fi lm lj l lk ll">fileName = str(id) + "_rf.joblib"<br/>joblib.dump(model, "./" + fileName, compress = 3)</span></pre><p id="1c78" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这将创建一个单独的<code class="du kv kw kx ky b">.joblib</code>文件，而不是方法1中的多个拼花文件。大小取决于定型数据的数量，但仍然比MLlib为同一模型生成的文件的总大小小得多。此外，这里的执行时间也快得多。</p><p id="2476" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">一旦我们有了模型文件，就该上传它了。令人惊讶的是，在我项目的大部分时间里，这是我一直坚持的部分！如果这是一个Python项目，而不是PySpark，解决方案将会很简单。我们将使用GCP的<em class="ln">存储库</em>并将模型文件从本地目录上传到GS bucket，如下所示:</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="7d76" class="lh jt hh ky b fi li lj l lk ll">from google.cloud import storage</span><span id="eb2b" class="lh jt hh ky b fi lm lj l lk ll">srcPath = "./" + fileName<br/>bucketName = "mlmodels/"</span><span id="423c" class="lh jt hh ky b fi lm lj l lk ll">storage_client = storage.Client()<br/>bucket = storage_client.bucket(bucketName)<br/>blob = bucket.blob('models')</span><span id="da09" class="lh jt hh ky b fi lm lj l lk ll">with open(srcPath, "w") as f:<br/>    blob.upload_from_file(f)</span></pre><p id="9420" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但对PySpark来说，这个过程相当复杂。第一个问题是PySpark对UDF熊猫的处理方式。为了实现可扩展性，Spark将数据集组分发到工作节点。因此，模型文件是在工作者的本地存储中创建的。但<em class="ln">存储</em>库只有在从主节点执行时才起作用(不确定为什么，但在我的情况下就是这样！).为了从工作者节点上传文件，我们需要一种不同的方法。当我通过终端登录到各个节点时，我发现<a class="ae jr" href="https://cloud.google.com/storage/docs/gsutil" rel="noopener ugc nofollow" target="_blank"><em class="ln">gsutil</em></a><em class="ln"/>命令(<code class="du kv kw kx ky b">cp</code>或<code class="du kv kw kx ky b">rsync</code>)都是功能性的。要在我们的应用程序中使用<em class="ln"> gsutil </em>命令，我们需要使用子流程包中的<code class="du kv kw kx ky b">Popen</code>方法。在PySpark中，应用程序文件的默认本地目录是<code class="du kv kw kx ky b">/hadoop/spark/tmp/</code> <em class="ln">。</em>我们的代码是这样的:</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="9c44" class="lh jt hh ky b fi li lj l lk ll">from subprocess import Popen, PIPE</span><span id="3756" class="lh jt hh ky b fi lm lj l lk ll">srcPath = "/hadoop/spark/tmp/" + fileName<br/>destPath = "gs://mlmodels/models/"</span><span id="e0f1" class="lh jt hh ky b fi lm lj l lk ll">p = Popen("gsutil -m rsync -r "+srcPath+" "+destPath, shell=True, stdout=PIPE, stderr=PIPE)</span></pre><p id="284d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">但使用这个之后，我发现只有保存在主节点中的模型被上传了！相同的命令在通过终端发送时有效(ssh-ing到工作节点)，但在应用程序中不起作用！我真的不知道为什么！经过多日对无数在线资源的搜索，我终于在这里找到了解决方案<a class="ae jr" href="https://stackoverflow.com/questions/45973386/dataproc-pyspark-workers-have-no-permission-to-use-gsutil/45991873#45991873" rel="noopener ugc nofollow" target="_blank">，感谢霍天华！以下是他的解释:</a></p><blockquote class="lo lp lq"><p id="a356" class="it iu ln iv b iw ix iy iz ja jb jc jd lr jf jg jh ls jj jk jl lt jn jo jp jq ha bi translated">对于在shade容器中运行的东西，尽管作为用户<code class="du kv kw kx ky b">yarn</code>运行，如果您只运行<code class="du kv kw kx ky b">sudo su yarn</code>，您将在Dataproc节点上看到<code class="du kv kw kx ky b">~</code>解析为<code class="du kv kw kx ky b">/var/lib/hadoop-yarn</code>，shade实际上将<code class="du kv kw kx ky b">yarn.nodemanager.user-home-dir</code>传播为容器的homedir，这默认为<code class="du kv kw kx ky b">/home/</code>。因此，即使可以<code class="du kv kw kx ky b">sudo -u yarn gsutil ...</code>，它的行为方式也不同于same容器中的gsutil，自然，只有<code class="du kv kw kx ky b">root</code>能够在基本<code class="du kv kw kx ky b">/home/</code>目录中创建目录。</p></blockquote><p id="c483" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">解决方案相当简单——我们需要在<em class="ln"> gsutil </em>命令之前添加HOME =/var/lib/Hadoop-three。<em class="ln">注:丹尼斯提出了其他解决方案，我没有尝试；更多细节请阅读他的答案。)</em></p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="b1c1" class="lh jt hh ky b fi li lj l lk ll">p = Popen("<!-- -->HOME=/var/lib/hadoop-yarn <!-- -->gsutil -m rsync -r "+srcPath+" "+destPath, shell=True, stdout=PIPE, stderr=PIPE)</span></pre><p id="f0d4" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated"><em class="ln">万岁！</em>魅力十足！所有工作节点的所有模型文件都已上载到指定的GS存储桶。</p><p id="f068" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">以下是我在解决这个问题时发现的其他一些技巧:</p><p id="53c5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">I)最初使用<code class="du kv kw kx ky b">/hadoop/spark/tmp/</code>作为保存模型文件的目录可能有风险，因为这是Spark的默认临时目录，其他一些进程可能会覆盖你的文件。相反，请使用其他目录。但是worker节点中的大多数目录都是写保护的，所以您可能需要花时间来找到合适的目录。在我的情况下，我发现<code class="du kv kw kx ky b">/usr/local/man/</code>是一个。</p><p id="343e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">ii)由于Spark的<em class="ln">【懒惰】</em>和并行计算，很难预测模型文件将在何时何地被创建。如果您在转储后立即上传模型文件，您可能会丢失一些文件。原因是在执行上传代码时，相应的文件尚未创建。因此，最好在一个UDF中生成并转储模型文件，然后在另一个中上传模型文件。这将确保所有文件在上传前完全转储。最后的代码片段是:</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="5789" class="lh jt hh ky b fi li lj l lk ll">#........Define schema for output dataframe here........#<br/>outSchema = StructType([...])<br/>uploadSchema = StructType([...])</span><span id="323d" class="lh jt hh ky b fi lm lj l lk ll">#........UDF for generating and dumping models........#<br/><a class="ae jr" href="http://twitter.com/pandas_udf" rel="noopener ugc nofollow" target="_blank">@pandas_udf</a>(outSchema, PandasUDFType.GROUPED_MAP)<br/>def generate model(df):<br/>    label = np.array(df['label'])<br/>    features = df.drop('label', axis = 1)</span><span id="d9a3" class="lh jt hh ky b fi lm lj l lk ll">    # splitting training and testing data<br/>    train_features, test_features, train_labels, test_labels = train_test_split(features, label, test_size = 0.2, random_state = 42)<br/>   <br/>    # Instantiate model with decision trees<br/>    model = RandomForestRegressor(bootstrap = True, max_depth = 5, max_features = 'auto', min_samples_leaf = 2, min_samples_split = 8, n_estimators = 10, oob_score = True, random_state = 42)<br/>    <br/>    # Train the model on training data<br/>    model.fit(train_features, train_labels)</span><span id="ca2e" class="lh jt hh ky b fi lm lj l lk ll">    fileName = str(id) + "_rf.joblib"<br/>    joblib.dump(model, "/usr/local/man/" + fileName, compress = 3)<br/>    ...</span><span id="529c" class="lh jt hh ky b fi lm lj l lk ll">#........UDF for uploading models to gcs bucket........#<br/><a class="ae jr" href="http://twitter.com/pandas_udf" rel="noopener ugc nofollow" target="_blank">@pandas_udf</a>(uploadSchema, PandasUDFType.GROUPED_MAP)<br/>def upload_model(df):<br/>    destPath = gs://mlmodels/models/<br/>    srcPath = "/usr/local/man/*"<br/>    p = Popen("HOME=/var/lib/hadoop-yarn gsutil -m rsync -r "+srcPath+" "+destPath, shell=True, stdout=PIPE, stderr=PIPE)<br/>    return pd.DataFrame({"comment":"Success"},index=[0])</span><span id="94e0" class="lh jt hh ky b fi lm lj l lk ll">outputDf = inputDf.groupby('id').apply(generate_model)<br/>uploadDf = outputDf.groupby('id').apply(upload_model)</span></pre><p id="2850" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">请注意，熊猫UDF总是需要返回一个数据帧。在第二个UDF中，尽管我们没有要返回的内容，但是我们返回了一个伪数据帧。此外，outputDf应该有足够多的组，以便在所有工作节点中执行第二个UDF。</p><p id="a5bc" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">iii)同样，由于Spark的“<em class="ln">lazy”</em>和并行计算，第二个UDF可能在所有文件完成上传之前返回(即，上传已启动但尚未完成)。所以事后最好加个延迟，以防万一。</p><blockquote class="lo lp lq"><p id="8c3b" class="it iu ln iv b iw ix iy iz ja jb jc jd lr jf jg jh ls jj jk jl lt jn jo jp jq ha bi translated">检索模型</p></blockquote><p id="076a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">为了使用上述方法检索先前保存到GS的模型文件，我们需要将文件从GS下载到集群节点(假设我们将使用所有的模型文件)。第一步是获取GS bucket中的文件列表，然后我们可以将它们逐个添加到Spark上下文中。这会将所有文件下载到主节点。代码如下所示:</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="f9e5" class="lh jt hh ky b fi li lj l lk ll">from subprocess import Popen, PIPE</span><span id="2f21" class="lh jt hh ky b fi lm lj l lk ll">srcPath = "gs://mlmodels/models/"<br/>p = Popen("gsutil ls " + srcPath, shell=True, stdout=PIPE, stderr=PIPE)<br/>saved_files = str(p.stdout.read())[2:-1].split("\\n")[1:-1]</span><span id="66b6" class="lh jt hh ky b fi lm lj l lk ll">for filePath in saved_files:<br/>   sc.addFile(filePath)</span></pre><p id="5868" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">一旦所有文件下载完毕，我们可以使用<code class="du kv kw kx ky b">joblib.load()</code>来加载模型。我们需要提供相应模型文件的本地路径，可以通过<code class="du kv kw kx ky b">SparkFiles.get()</code>方法检索。由于所有文件都存储在主节点中，这种方法也可以用于熊猫UDF，因为所有工作节点都可以访问主节点的文件系统。</p><pre class="kz la lb lc fd ld ky le lf aw lg bi"><span id="447f" class="lh jt hh ky b fi li lj l lk ll">from pyspark import SparkFiles </span><span id="9b2e" class="lh jt hh ky b fi lm lj l lk ll">model = joblib.load(SparkFiles.get(fileName))</span></pre><h1 id="46a4" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">结论</h1><p id="32e1" class="pw-post-body-paragraph it iu hh iv b iw kq iy iz ja kr jc jd je ks jg jh ji kt jk jl jm ku jo jp jq ha bi translated">第二种方法的代码可能比使用MLlib更复杂，但在生成大量模型时，它在执行时间和所需存储方面更有效。不过需要注意的是，在我的例子中，每个组的数据集都不是很大，所以第二种方法可能会导致非常大的数据集出现内存不足的异常。</p><p id="198a" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">我花了很长时间才完成，因为有很多障碍要克服，但我喜欢这段旅程。我真诚地希望这篇文章能帮助人们更快地找到类似问题的解决方案。如果您有任何建议或问题，请发表评论。</p><h1 id="b752" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">资源</h1><ol class=""><li id="04fd" class="lu lv hh iv b iw kq ja kr je lw ji lx jm ly jq lz ma mb mc bi translated">https://spark.apache.org/mllib/</li><li id="bc4d" class="lu lv hh iv b iw md ja me je mf ji mg jm mh jq lz ma mb mc bi translated">谷歌云data proc—【https://cloud.google.com/sdk/gcloud/reference/dataproc T2】</li><li id="bd70" class="lu lv hh iv b iw md ja me je mf ji mg jm mh jq lz ma mb mc bi translated">谷歌云存储—【https://cloud.google.com/storage T4】</li><li id="941a" class="lu lv hh iv b iw md ja me je mf ji mg jm mh jq lz ma mb mc bi translated">Spark中的多时间序列预测—<a class="ae jr" rel="noopener" href="/walmartglobaltech/multi-time-series-forecasting-in-spark-cc42be812393">https://medium . com/walmartglobaltech/multi-Time-Series-Forecasting-in-Spark-cc 42 be 812393</a></li><li id="0a1e" class="lu lv hh iv b iw md ja me je mf ji mg jm mh jq lz ma mb mc bi translated"><em class="ln"> gsutil </em>工具—<a class="ae jr" href="https://cloud.google.com/storage/docs/gsutil" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/storage/docs/gsutil</a></li></ol></div></div>    
</body>
</html>