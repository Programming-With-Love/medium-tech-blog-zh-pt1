<html>
<head>
<title>Neural Networks Part 1: Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络第1部分:逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/neural-networks-part-1-logistic-regression-least-square-error-f78c79159cb7?source=collection_archive---------2-----------------------#2019-09-30">https://medium.com/walmartglobaltech/neural-networks-part-1-logistic-regression-least-square-error-f78c79159cb7?source=collection_archive---------2-----------------------#2019-09-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><h2 id="aaea" class="hf hg hh bd b fp hi hj hk hl hm hn dx ho translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="022c" class="pw-subtitle-paragraph in hq hh bd b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je dx translated">神经网络的单个神经元</h2></div><p id="e964" class="pw-post-body-paragraph jf jg hh jh b ji jj ir jk jl jm iu jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">必修学习:线性回归基础知识<a class="ae kb" href="http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="4279" class="pw-post-body-paragraph jf jg hh jh b ji jj ir jk jl jm iu jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">我们从神经网络的基本单元——单激活神经元开始。具有单个神经元的神经网络与逻辑回归相同。因此，神经网络可以被认为是逻辑回归单元的网络集合。</p><p id="a0fc" class="pw-post-body-paragraph jf jg hh jh b ji jj ir jk jl jm iu jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated"><strong class="jh hr">注意:上述对于仅具有Sigmoid激活函数的神经网络是正确的，因为逻辑回归使用Sigmoid函数。不要担心，这将在随后的博客中得到澄清</strong></p><h1 id="83b0" class="kc kd hh bd ke kf kg kh ki kj kk kl km iw kn ix ko iz kp ja kq jc kr jd ks kt bi translated">建立符号以备将来使用</h1><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/0cb6829535e91851d3cf2601f73adcef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rt8CzpgVkZbvA1VhUpJIQw.png"/></div></div></figure><p id="3b86" class="pw-post-body-paragraph jf jg hh jh b ji jj ir jk jl jm iu jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated"><strong class="jh hr">注意，符号中的上标“(I)”只是训练集的一个索引，与取幂无关。</strong></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lg"><img src="../Images/687b7b534eb02760c63ac5ba31e8e2bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEF9Kk60w3r5sEi_L3HZaQ.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx">Fig: Single Neuron (Created using inkscape)</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ll"><img src="../Images/57c814aa25c981b475cc65d9b9073a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sD8c6bvCx36h_dJai-JOLw.png"/></div></div></figure><p id="bfeb" class="pw-post-body-paragraph jf jg hh jh b ji jj ir jk jl jm iu jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated"><strong class="jh hr">注意:我们可以为逻辑回归使用更好的损失函数，但是为了简单起见，我们使用最小二乘误差</strong></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lm"><img src="../Images/2e22b286d667570494d7491a486952ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAxugo4HsMy7P0s2_5i0mg.png"/></div></div></figure><h1 id="1ef9" class="kc kd hh bd ke kf kg kh ki kj kk kl km iw kn ix ko iz kp ja kq jc kr jd ks kt bi translated">派生物</h1><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ln"><img src="../Images/94b666ac6f2d8fe4c1359c636a3a1f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mu2lbguVUv0NkVjrYdkkKg.png"/></div></div></figure><h1 id="69bc" class="kc kd hh bd ke kf kg kh ki kj kk kl km iw kn ix ko iz kp ja kq jc kr jd ks kt bi translated">梯度下降</h1><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lo"><img src="../Images/1b6fd9da1c0952925478ab4a59102b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qzIt7JAeX-9NbOLQJZC4JQ.png"/></div></div></figure><p id="92e0" class="pw-post-body-paragraph jf jg hh jh b ji jj ir jk jl jm iu jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated"><strong class="jh hr">注:<em class="lp"> J </em>是我们的损失函数，<em class="lp"> j </em>用于索引</strong></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lq"><img src="../Images/71e8d8f5d5c6eeb0c7547b27bac3ea61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Crlg5HyjYWBg1-Xo9C9nw.png"/></div></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lr"><img src="../Images/bfbf43605a0183fd029e0eb8b587f806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PBJZKLqOaIPCocaXRALExQ.png"/></div></div></figure><ol class=""><li id="6f79" class="ls lt hh jh b ji jj jl jm jo lu js lv jw lw ka lx ly lz ma bi translated">对训练样本的单个梯度求和使得梯度更新更加平滑</li><li id="b854" class="ls lt hh jh b ji mb jl mc jo md js me jw mf ka lx ly lz ma bi translated">不求平均的学习率取决于训练数据的大小<strong class="jh hr"> <em class="lp"> m </em> </strong>或批量大小</li><li id="50b4" class="ls lt hh jh b ji mb jl mc jo md js me jw mf ka lx ly lz ma bi translated">通过平均，梯度大小与批次大小无关。这允许在使用不同批次大小或训练数据大小<strong class="jh hr"> <em class="lp"> m </em> </strong>时进行比较。</li></ol><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mg"><img src="../Images/fc5cee04ad43bc0f581f632fd1911595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3oN3BH8JjBXkQm6tDyTmcw.png"/></div></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mh"><img src="../Images/c35aaaef1e482342d79d8af635bef9ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQC4Ud6gPxXY_FCNxqTUAA.png"/></div></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mi"><img src="../Images/b4c9f83a406a80889f1d8fe96077474b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17K80hipVdGs4J1RzsaSOw.png"/></div></div></figure><h2 id="8931" class="mj kd hh bd ke mk ml mm ki mn mo mp km jo mq mr ko js ms mt kq jw mu mv ks hn bi translated">培训步骤:</h2><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mw"><img src="../Images/1668fccf907500a38e4596583962a05d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xk2moBBOw77Zg_0mr0d3GA.png"/></div></div></figure><p id="e327" class="pw-post-body-paragraph jf jg hh jh b ji jj ir jk jl jm iu jn jo jp jq jr js jt ju jv jw jx jy jz ka ha bi translated">上述步骤的代码片段:</p><pre class="kv kw kx ky fd mx my mz na aw nb bi"><span id="7698" class="mj kd hh my b fi nc nd l ne nf">#Accumulate gradient with respect to bias and weights<br/>    grad_bias = 0<br/>    grad_w = np.zeros(len(W))<br/>    for i in range(X_train.shape[0]):        <br/>        grad_bias += (YP[i] - y_train[i])*(YP[i])*(1-YP[i]) #dJ/db<br/>        for j in range(len(W)):<br/>            #dJ/dW_j<br/>            grad_w[j] += (YP[i] - y_train[i])*(YP[i])*(1-YP[i])*(X_train[i][j])<br/>        <br/>    #Update bias<br/>    bias = bias - grad_bias*lr/X_train.shape[0]</span></pre><h1 id="1975" class="kc kd hh bd ke kf kg kh ki kj kk kl km iw kn ix ko iz kp ja kq jc kr jd ks kt bi translated">随机梯度下降</h1><p id="928c" class="pw-post-body-paragraph jf jg hh jh b ji ng ir jk jl nh iu jn jo ni jq jr js nj ju jv jw nk jy jz ka ha bi translated">当训练数据量mm较大时，我们选择<strong class="jh hr"><em class="lp"/></strong>&lt;<strong class="jh hr"><em class="lp">m</em></strong>的批量大小。我们将训练数据分成大小为<strong class="jh hr"><em class="lp">【m’</em></strong>的批次。我们按如下方式更新每批的重量和偏差:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es nl"><img src="../Images/8d263dc9707c8156a8cdbb196ec04608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_Tgk30lmJwGyl3HCoYyvQ.png"/></div></div></figure><h2 id="f71c" class="mj kd hh bd ke mk ml mm ki mn mo mp km jo mq mr ko js ms mt kq jw mu mv ks hn bi translated">SGD的优势</h2><ol class=""><li id="2d5b" class="ls lt hh jh b ji ng jl nh jo nm js nn jw no ka lx ly lz ma bi translated">比正常的梯度下降快得多</li><li id="6ce4" class="ls lt hh jh b ji mb jl mc jo md js me jw mf ka lx ly lz ma bi translated">当整个训练数据无法放入系统的RAM(可用内存)时，这是更好的选择</li></ol><h1 id="28fb" class="kc kd hh bd ke kf kg kh ki kj kk kl km iw kn ix ko iz kp ja kq jc kr jd ks kt bi translated">密码</h1><p id="cb87" class="pw-post-body-paragraph jf jg hh jh b ji ng ir jk jl nh iu jn jo ni jq jr js nj ju jv jw nk jy jz ka ha bi translated"><a class="ae kb" href="https://github.com/rakesh-malviya/MLCodeGems/blob/master/notebooks/Neural_networks/2-neural-networks-part-1-logistic-regression-least-square-error.ipynb" rel="noopener ugc nofollow" target="_blank">这里的</a>是逻辑回归的python实现。</p><h1 id="690e" class="kc kd hh bd ke kf kg kh ki kj kk kl km iw kn ix ko iz kp ja kq jc kr jd ks kt bi translated">参考资料:</h1><ol class=""><li id="bf0e" class="ls lt hh jh b ji ng jl nh jo nm js nn jw no ka lx ly lz ma bi translated"><a class="ae kb" href="http://cs229.stanford.edu/notes" rel="noopener ugc nofollow" target="_blank">http://cs229.stanford.edu/notes</a></li></ol></div></div>    
</body>
</html>