<html>
<head>
<title>Load-testing TensorFlow Serving and FastAPI on GKE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在GKE上对TensorFlow服务和FastAPI进行负载测试</h1>
<blockquote>原文：<a href="https://medium.com/google-developer-experts/load-testing-tensorflow-serving-and-fastapi-on-gke-411bc14d96b2?source=collection_archive---------2-----------------------#2022-07-17">https://medium.com/google-developer-experts/load-testing-tensorflow-serving-and-fastapi-on-gke-411bc14d96b2?source=collection_archive---------2-----------------------#2022-07-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e345" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">由</em> <a class="ae jd" href="https://github.com/deep-diver" rel="noopener ugc nofollow" target="_blank"> <em class="jc">陈成</em> </a> <em class="jc">和</em> <a class="ae jd" href="https://github.com/sayakpaul" rel="noopener ugc nofollow" target="_blank"> <em class="jc">萨亚克</em> </a> <em class="jc"> (ML-GDEs) </em></p><p id="7f3c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇文章中，我们将分享在众多部署配置中对深度学习模型进行负载测试的经验和发现。这些配置涉及基于REST的部署，使用<a class="ae jd" href="https://fastapi.tiangolo.com/" rel="noopener ugc nofollow" target="_blank"> FastAPI </a>和<a class="ae jd" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"> TensorFlow服务</a>。通过这种方式，我们旨在让读者对两者之间的差异有一个全面的了解。</p><p id="8e5f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章不是关于代码，而是关于我们在执行部署时必须做出的架构决策。我们将首先概述我们的设置，包括技术规格。我们还将分享我们对一些关键设计选择及其影响的评论。</p><h1 id="05a5" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">技术设置</h1><p id="0505" class="pw-post-body-paragraph ie if hh ig b ih kc ij ik il kd in io ip ke ir is it kf iv iw ix kg iz ja jb ha bi translated">如上所述，我们雇佣了两个有希望的候选人进行部署— <a class="ae jd" href="https://fastapi.tiangolo.com/" rel="noopener ugc nofollow" target="_blank"> FastAPI </a>和<a class="ae jd" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"> TensorFlow Serving </a>。两者都具有丰富的功能，并在设计中嵌入了目标规格(稍后将详细介绍)。</p><p id="c724" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了执行我们的测试，我们使用了一个<a class="ae jd" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">预训练的ResNet50模型</a>，它可以将各种图像分为不同的类别。然后，我们在两种不同的设置中提供该模型——FastAPI和TensorFlow服务，它们在设置中有一些共同点:</p><ul class=""><li id="f3f9" class="kh ki hh ig b ih ii il im ip kj it kk ix kl jb km kn ko kp bi translated">使环境集装箱化。</li><li id="1375" class="kh ki hh ig b ih kq il kr ip ks it kt ix ku jb km kn ko kp bi translated">Kubernetes为可伸缩性编排一个容器节点集群。我们使用<a class="ae jd" href="https://cloud.google.com/kubernetes-engine" rel="noopener ugc nofollow" target="_blank"> Kubernetes引擎</a> (GKE)来管理这个。</li><li id="0aab" class="kh ki hh ig b ih kq il kr ip ks it kt ix ku jb km kn ko kp bi translated"><a class="ae jd" href="https://github.com/features/actions" rel="noopener ugc nofollow" target="_blank"> GitHub动作</a>在GKE上自动展开部署。</li></ul><p id="c4d5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的部署平台(Kubernetes集群上的节点)是基于CPU的。我们在流程的任何阶段都不使用GPU。为了进一步优化FastAPI提供的模型的运行时，我们使用了<a class="ae jd" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>。<a class="ae jd" href="https://github.com/sayakpaul/ml-deployment-k8s-fastapi/blob/feat-locust/notebooks/TF_to_ONNX.ipynb" rel="noopener ugc nofollow" target="_blank">这本笔记本</a>提供了ONNX模型和原始TensorFlow模型之间的延迟比较。TensorFlow服务尚不允许我们为ONNX模型提供服务，但我们仍然可以构建一个CPU优化的TensorFlow服务映像，并利用其他一些选项来减少延迟并提高系统的整体吞吐量。我们将在后面的文章中讨论这些。</p><p id="b9be" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以在以下存储库中找到所有代码并了解部署是如何执行的:</p><div class="kv kw ez fb kx ky"><a href="https://github.com/sayakpaul/ml-deployment-k8s-fastapi" rel="noopener  ugc nofollow" target="_blank"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hi fi z dy ld ea eb le ed ef hg bi translated">GitHub-sayak Paul/ml-deployment-k8s-fastapi:这个项目展示了如何服务于ONNX优化的…</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">这个项目展示了如何将ONNX优化的图像分类模型作为一个web服务提供给FastAPI、Docker和…</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">github.com</p></div></div><div class="lh l"><div class="li l lj lk ll lh lm ln ky"/></div></div></a></div><div class="kv kw ez fb kx ky"><a href="https://github.com/deep-diver/ml-deployment-k8s-tfserving" rel="noopener  ugc nofollow" target="_blank"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hi fi z dy ld ea eb le ed ef hg bi translated">GitHub-deep-diver/ml-deployment-k8s-TF serving:这个项目展示了如何服务一个基于TF的映像…</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">这个项目展示了如何将一个基于TF的图像分类模型作为一个web服务使用TFServing、Docker和…</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">github.com</p></div></div><div class="lh l"><div class="lo l lj lk ll lh lm ln ky"/></div></div></a></div><p id="88bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在资源库中，您可以找到示例笔记本和详细的代码设置说明。因此，我们不会逐行讨论代码，而是在必要时揭示最重要的部分。</p><p id="b0c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文的其余部分，我们将讨论FastAPI和TensorFlow服务部署实验的关键考虑因素，包括它们的动机、限制和我们的实验结果。</p><p id="c592" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">随着</em><a class="ae jd" href="https://cloud.google.com/vertex-ai" rel="noopener ugc nofollow" target="_blank"><em class="jc">Vertex AI</em></a><em class="jc">等无服务器产品的出现，部署机器学习(ML)模型并安全可靠地扩展它们变得前所未有的简单。这些服务有助于极大地缩短上市时间，并提高开发人员的整体工作效率。也就是说，在某些情况下，您可能希望对事情进行更精细的控制。这是我们最初想做这些实验的原因之一。</em></p><h1 id="8819" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">考虑</h1><p id="3c57" class="pw-post-body-paragraph ie if hh ig b ih kc ij ik il kd in io ip ke ir is it kf iv iw ix kg iz ja jb ha bi translated">FastAPI和TensorFlow服务有自己的约束和设计选择，它们会影响部署。在本节中，我们将简要概述这些考虑事项。</p><p id="d843" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">框架选择:</strong> FastAPI已经迅速成为部署REST APIs的好工具。此外，它利用了一个基于C++的事件系统，该系统已在Node.js中得到验证。另一方面，TensorFlow Serving是一个生产就绪框架，可部署标准TensorFlow模型，同时支持基于REST和gRPC的接口。由于FastAPI不是一个经过实战检验的ML部署框架，但许多组织将它用于此目的，我们希望比较它和TensorFlow服务之间的性能。</p><p id="35cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">部署基础设施:</strong>我们选择GKE是因为Kubernetes是现代IT行业事实上的部署平台(当使用GCP时)，而GKE让我们专注于ML部分而不用担心基础设施，因为它是一个完全托管的谷歌云平台(GCP)服务。我们的主要兴趣是如何为基于CPU的环境部署模型，因此我们为FastAPI服务器准备了一个CPU优化的ONNX模型和一个CPU优化的TensorFlow服务映像。</p><p id="2b6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">更多或更少服务器之间的权衡:</strong>我们开始对FastAPI和TensorFlow服务设置进行实验，使用尽可能简单的配备2vCPU和4GB RAM的虚拟机，然后我们逐渐将规格升级到8vCPU和64GB RAM。另一方面，我们将Kubernetes集群中的节点数量从8个减少到2个，因为这是部署更便宜的服务器与部署更便宜的服务器之间的权衡。</p><p id="a4e0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">有利于多核环境的选项:</strong>我们希望了解高端虚拟机是否能够超越简单虚拟机，即使节点较少也能利用多核环境。为此，我们对使用<code class="du lp lq lr ls b">uvicorn</code>和<code class="du lp lq lr ls b">gunicorn</code>进行FastAPI部署的不同工作人员进行了试验，根据CPU核心的数量设置TensorFlow服务部署的<code class="du lp lq lr ls b"><a class="ae jd" href="https://www.tensorflow.org/api_docs/python/tf/config/threading/set_inter_op_parallelism_threads" rel="noopener ugc nofollow" target="_blank">inter_op_parallelism</a></code>和<code class="du lp lq lr ls b"><a class="ae jd" href="https://www.tensorflow.org/api_docs/python/tf/config/threading/set_intra_op_parallelism_threads" rel="noopener ugc nofollow" target="_blank">intra_op_parallelism</a></code>线程的数量。</p><p id="f7e5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">动态批处理和其他考虑:</strong>tensor flow Serving等现代ML框架通常支持动态批处理、初始模型预热、不同模型的多个版本的多个部署等开箱即用。出于在线预测的目的，我们没有仔细测试这些功能。但是，根据官方文件，动态批处理能力也值得探索，以增强性能。我们已经看到，默认的批处理配置可以减少一点延迟，尽管结果没有包括在这篇博文中。</p><p id="15ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="jc">自己在FastAPI服务器中实现这些特性并不简单，因此使用TensorFlow服务而不是FastAPI之类的纯REST API服务器框架是一个优势。</em></p><h1 id="c636" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">实验</h1><p id="9e4a" class="pw-post-body-paragraph ie if hh ig b ih kc ij ik il kd in io ip ke ir is it kf iv iw ix kg iz ja jb ha bi translated">我们准备了以下环境。我们在最小的2vCPU和4GB RAM的机器上使用了一个<code class="du lp lq lr ls b">uvicorn</code> worker，但是在更大的机器上我们选择了<code class="du lp lq lr ls b">gunicorn</code>而不是<code class="du lp lq lr ls b">uvicorn</code>。建议在Kubernetes环境中使用<code class="du lp lq lr ls b">uvicorn</code>，但是根据<a class="ae jd" href="https://fastapi.tiangolo.com/deployment/server-workers/?h=gunicorn#uvicorn-with-workers" rel="noopener ugc nofollow" target="_blank">官方FastAPI的文档</a>，在某些情况下更倾向于使用<code class="du lp lq lr ls b">gunicorn</code>，所以我们想要探索这两者。</p><p id="1398" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据FastAPI的官方指导原则，使用公式<a class="ae jd" href="https://docs.gunicorn.org/en/stable/design.html#how-many-workers" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">(2 x $ num _ cores)+1</strong></a>设置<code class="du lp lq lr ls b">gunicorn</code>工人的数量。在TensorFlow服务中，<code class="du lp lq lr ls b">intra_op_parallelism_threads</code>的数量被设置为等于CPU内核的数量，而<code class="du lp lq lr ls b">inter_op_parallelism_threads</code>的数量被设置为2到8，用于实验目的，因为它控制线程的数量，以并行执行独立的操作。下面我们详细介绍了我们对每个Kubernetes集群的vCPUs数量、RAM大小和节点数量进行的调整。请注意，vCPUs的数量和RAM大小分别适用于集群节点。</p><p id="bd48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用<a class="ae jd" href="https://locust.io/" rel="noopener ugc nofollow" target="_blank"> Locust </a>进行负载测试。我们已经运行每个负载测试5分钟了。我们每秒钟都会产生请求，以清楚地了解TensorFlow服务如何随着客户端数量的增加而变化。因此，您可以假设每秒请求数并没有反映真实世界中客户端试图随时发送请求的情况。</p><p id="eab1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">2颗CPU，4GB内存，8个节点:</strong></p><p id="a72b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用1、2和4个<code class="du lp lq lr ls b">uvicorn</code>工作人员对FastAPI部署进行了负载测试，对于TensorFlow服务负载测试，<code class="du lp lq lr ls b">inter_op_parallelism_threads</code>的数量被设置为2、4和8。</p><p id="6da6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">4个虚拟CPU，8GB内存，4个节点:</strong></p><p id="ca76" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据公式，对于FastAPI部署，<code class="du lp lq lr ls b">gunicorn</code>工作人员的数量被设置为7，对于TensorFlow服务负载测试，<code class="du lp lq lr ls b">inter_op_parallelism_threads</code>的数量被设置为2、4和8。</p><p id="3a63" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">8个虚拟CPU，16GB内存，2个节点:</strong></p><p id="f5b8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据公式，对于FastAPI部署，<code class="du lp lq lr ls b">gunicorn</code>工作人员的数量被设置为17，对于TensorFlow服务负载测试，<code class="du lp lq lr ls b">inter_op_parallelism_threads</code>的数量被设置为2、4和8。</p><p id="ef79" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">8个虚拟CPU，64GB内存，2个节点:</strong></p><p id="649b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据公式，对于FastAPI部署，<code class="du lp lq lr ls b">gunicorn</code>工作线程的数量被设置为7，对于TensorFlow服务负载测试，<code class="du lp lq lr ls b">inter_op_parallelism_threads</code>的数量被设置为2、4和8。</p><p id="5ace" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以在上述存储库中找到试验这些不同配置的代码。每个实验的部署通过<a class="ae jd" href="https://kustomize.io/" rel="noopener ugc nofollow" target="_blank"> Kustomize </a>提供以覆盖基本配置，基于文件的配置通过<a class="ae jd" href="https://kubernetes.io/docs/concepts/configuration/configmap/" rel="noopener ugc nofollow" target="_blank"> ConfigMap </a>注入。</p><h1 id="abc3" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">结果</h1><p id="a4bd" class="pw-post-body-paragraph ie if hh ig b ih kc ij ik il kd in io ip ke ir is it kf iv iw ix kg iz ja jb ha bi translated">各种配置上的所有负载测试的结果可以在上面提到的存储库中找到。本节仅包括FastAPI和TensorFlow服务的最佳结果。图1显示了这样的结果。以下是我们发现的分别适用于FastAPI和TensorFlow服务的最佳配置:</p><ul class=""><li id="b98f" class="kh ki hh ig b ih ii il im ip kj it kk ix kl jb km kn ko kp bi translated"><strong class="ig hi"> FastAPI </strong> : 8个节点，2个<code class="du lp lq lr ls b">uvicorn</code>工人，2个CPU，4GB内存</li><li id="e99a" class="kh ki hh ig b ih kq il kr ip ks it kt ix ku jb km kn ko kp bi translated"><strong class="ig hi"> TensorFlow </strong>服务:2个节点，8个<code class="du lp lq lr ls b">intra_op_parallelism_threads</code>，8个<code class="du lp lq lr ls b">inter_op_parallelism_threads</code>，8个8vCPUs，16GB RAM</li></ul><p id="a2a5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">硬件基于GCP提供的E2系列虚拟机。</p><p id="3fd6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面我们展示了这两种配置的各种结果。我们还提供了额外的结果，以显示TensorFlow服务在配备了更高RAM容量的虚拟机上在延迟和吞吐量方面的表现。</p><figure class="lu lv lw lx fd ly er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es lt"><img src="../Images/568a4636afc6a0f68105ecd8bbd4c039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vvjotsteFVQiLrjg"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx">F<strong class="bd jg">igure 1:</strong> Comparison between FastAPI and TensorFlow Serving (<a class="ae jd" href="https://i.ibb.co/6JVRqyS/download-3.png" rel="noopener ugc nofollow" target="_blank">original</a>).</figcaption></figure><p id="29db" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过选择最佳选项，我们分别观察了两种部署的以下方面。</p><ul class=""><li id="3eb7" class="kh ki hh ig b ih ii il im ip kj it kk ix kl jb km kn ko kp bi translated">FastAPI在部署在更多、更小(CPU和RAM更少)的机器上时往往表现更好，而且在Kubernetes环境中，<code class="du lp lq lr ls b">uvicorn</code> workers似乎比<code class="du lp lq lr ls b">gunicorn</code> workers工作得更好。</li><li id="a00f" class="kh ki hh ig b ih kq il kr ip ks it kt ix ku jb km kn ko kp bi translated">TensorFlow服务在更少、更大(更多CPU和RAM)的机器上部署时效率更高。通过实验找到合适的<code class="du lp lq lr ls b">inter_op_parallelism_threads</code>数量很重要。数字越大，即使节点配备了高容量硬件，也不一定能保证性能越好。</li></ul><p id="a7b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于两种部署的比较，TensorFlow服务更注重可靠性，而不是吞吐量性能。如图1所示，TensorFlow服务的<strong class="ig hi">请求/秒</strong>略低于ONNX优化的FastAPI部署。我们认为它牺牲了一些吞吐量性能来实现可靠性。这是TensorFlow发球的预期行为，如<a class="ae jd" href="https://www.tensorflow.org/tfx/serving/performance#objectives" rel="noopener ugc nofollow" target="_blank">官方文件</a>所述。另一方面，与TensorFlow服务相比，FastAPI似乎更有能力处理更多的请求，但延迟并不稳定，因为一些请求的响应时间不规则地更长，随着请求数量的增加，甚至会出现一些故障。</p><p id="a6ed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">尽管处理尽可能多的请求很重要，但在处理生产系统时，保持服务器尽可能可靠也非常重要。在性能和可靠性之间有一个权衡，所以你必须小心选择正确的。然而，TensorFlow服务的吞吐量性能似乎足够接近FastAPI。此外，由于我们已经在基于CPU的部署场景中测试了一个简单的图像分类模型，带有优化ONNX模型的FastAPI可能是一个不错的选择。然而，如果您想要考虑更丰富的功能，如动态批处理和在模型之间高效共享GPU资源，我们相信TensorFlow服务是正确的选择。</p><h1 id="ea18" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">关于gRPC和张量流服务的注记</h1><p id="22e3" class="pw-post-body-paragraph ie if hh ig b ih kc ij ik il kd in io ip ke ir is it kf iv iw ix kg iz ja jb ha bi translated">我们正在处理用于部署的图像分类模型，该模型的输入将包括图像。因此，根据图像分辨率和保真度，请求有效负载的大小可能会螺旋上升。因此，确保消息传输尽可能轻量级尤为重要。一般来说，gRPC中的消息传输比REST快得多。<a class="ae jd" href="https://blog.dreamfactory.com/grpc-vs-rest-how-does-grpc-compare-with-traditional-rest-apis" rel="noopener ugc nofollow" target="_blank">这篇文章</a>很好地讨论了REST和gRPC APIs之间的主要区别。</p><p id="47cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">TensorFlow服务可以<a class="ae jd" href="https://www.tensorflow.org/tfx/serving/docker" rel="noopener ugc nofollow" target="_blank">无缝地为gRPC </a>的模型提供服务，但是比较gRPC API和REST API的性能并不容易。这就是为什么我们没有在这篇文章中提到它。感兴趣的读者可以看看<a class="ae jd" href="https://github.com/deep-diver/ml-deployment-k8s-tfserving" rel="noopener ugc nofollow" target="_blank">这个库</a>，它遵循了类似的设置，但是使用了gRPC服务器。</p><h1 id="9b0b" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">费用</h1><p id="45da" class="pw-post-body-paragraph ie if hh ig b ih kc ij ik il kd in io ip ke ir is it kf iv iw ix kg iz ja jb ha bi translated">为此，我们使用了<a class="ae jd" href="https://cloud.google.com/products/calculator" rel="noopener ugc nofollow" target="_blank"> GCP成本估算器</a>。每个实验配置的定价假定每月24小时有效(这对我们的实验来说足够了)。</p><figure class="lu lv lw lx fd ly er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es mi"><img src="../Images/462e9de07d67e934892fd37e44f15ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-PRCQI_xJHSgjVH_enfDQ.png"/></div></div></figure><h1 id="06ac" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">结论</h1><p id="0a45" class="pw-post-body-paragraph ie if hh ig b ih kc ij ik il kd in io ip ke ir is it kf iv iw ix kg iz ja jb ha bi translated">在这篇文章中，我们讨论了从我们对标准图像分类模型进行负载测试的经验中获得的一些重要教训。我们考虑了两个向最终用户公开模型的行业级框架——FastAPI和TensorFlow服务。虽然我们执行负载测试的设置可能不完全类似于在野外发生的情况，但我们希望我们的发现至少可以作为社区的一个良好起点。尽管这篇文章用一个图像分类模型展示了我们的方法，但是这些方法应该是与任务无关的。</p><p id="5aed" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了简洁起见，我们没有在两个API中做太多工作来进一步提高模型的效率。借助现代CPU、软件堆栈和操作系统级优化，可以改善模型的延迟和吞吐量。我们将感兴趣的读者重定向到以下可能相关的资源:</p><ul class=""><li id="fff7" class="kh ki hh ig b ih ii il im ip kj it kk ix kl jb km kn ko kp bi translated"><a class="ae jd" href="https://huggingface.co/blog/bert-cpu-scaling-part-1" rel="noopener ugc nofollow" target="_blank">在现代CPU上扩展类伯特模型推理—第一部分</a></li><li id="4d40" class="kh ki hh ig b ih kq il kr ip ks it kt ix ku jb km kn ko kp bi translated"><a class="ae jd" href="https://huggingface.co/blog/bert-cpu-scaling-part-2" rel="noopener ugc nofollow" target="_blank">在现代CPU上扩展类伯特模型推理—第二部分</a></li><li id="25aa" class="kh ki hh ig b ih kq il kr ip ks it kt ix ku jb km kn ko kp bi translated"><a class="ae jd" href="https://cloud.google.com/architecture/load-testing-and-monitoring-aiplatform-models" rel="noopener ugc nofollow" target="_blank">负载测试和监控AI平台模型</a></li><li id="b274" class="kh ki hh ig b ih kq il kr ip ks it kt ix ku jb km kn ko kp bi translated"><a class="ae jd" href="https://cloud.google.com/architecture/best-practices-for-ml-performance-cost" rel="noopener ugc nofollow" target="_blank">机器学习的性能和成本优化最佳实践</a></li></ul><h1 id="8a00" class="je jf hh bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">承认</h1><p id="bb54" class="pw-post-body-paragraph ie if hh ig b ih kc ij ik il kd in io ip ke ir is it kf iv iw ix kg iz ja jb ha bi translated">我们感谢谷歌的ML开发者项目团队为我们的实验提供了GCP积分。我们也感谢<a class="ae jd" href="https://www.linkedin.com/in/hanneshapke" rel="noopener ugc nofollow" target="_blank">汉尼斯·哈普克</a>和<a class="ae jd" href="https://www.linkedin.com/in/robert-crowe" rel="noopener ugc nofollow" target="_blank">罗伯特·克罗</a>为我们提供了有益的反馈和指导。</p></div></div>    
</body>
</html>