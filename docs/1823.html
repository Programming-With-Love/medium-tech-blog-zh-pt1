<html>
<head>
<title>Understanding TF-IDF for Machine Learning | Capital One</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解用于机器学习的TF-IDF | Capital One</h1>
<blockquote>原文：<a href="https://medium.com/capital-one-tech/understanding-tf-idf-for-machine-learning-capital-one-dea9ab4a586d?source=collection_archive---------6-----------------------#2021-11-08">https://medium.com/capital-one-tech/understanding-tf-idf-for-machine-learning-capital-one-dea9ab4a586d?source=collection_archive---------6-----------------------#2021-11-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="fc88" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">词频简介——逆文档频率</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/01af489b1d8bd512d03041e61ec06516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*240aNO7G2ic6lK3z.jpg"/></div></div></figure><p id="5d2a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">TF-IDF代表<em class="ke">术语频率-逆文档频率</em>，它是一种在<a class="ae kf" href="https://en.wikipedia.org/wiki/Information_retrieval" rel="noopener ugc nofollow" target="_blank">信息检索(IR) </a>和机器学习领域中使用的度量，可以量化文档集合(也称为语料库)中的文档中的字符串表示(单词、短语、词条等)的重要性或相关性。</p><h1 id="9e71" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">TF-IDF概述</h1><p id="89c9" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">TF-IDF可以分解为两部分<em class="ke"> TF(词频)</em>和<em class="ke"> IDF(逆文档频)。</em></p><h2 id="2f97" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">什么是TF(词频)？</h2><p id="344f" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">术语频率通过查看您所关注的特定术语<em class="ke">相对于文档的频率来工作。定义频率有多种方法:</em></p><ul class=""><li id="cc54" class="lr ls hh jk b jl jm jo jp jr lt jv lu jz lv kd lw lx ly lz bi translated">单词在文档中出现的次数(原始计数)。</li><li id="3928" class="lr ls hh jk b jl ma jo mb jr mc jv md jz me kd lw lx ly lz bi translated">根据文档长度调整的术语频率(原始出现次数除以文档中的字数)。</li><li id="a309" class="lr ls hh jk b jl ma jo mb jr mc jv md jz me kd lw lx ly lz bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Logarithmic_scale" rel="noopener ugc nofollow" target="_blank">对数标度</a>频率(如log(1 +原始计数))。</li><li id="30d9" class="lr ls hh jk b jl ma jo mb jr mc jv md jz me kd lw lx ly lz bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Boolean" rel="noopener ugc nofollow" target="_blank">布尔频率</a>(例如，在文档中，如果该术语出现，则为1；如果该术语没有出现，则为0)。</li></ul><h2 id="b068" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">什么是IDF(逆文档频率)？</h2><p id="591b" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">逆文档频率查看一个单词在语料库中的常见(或不常见)程度。IDF的计算如下，其中<strong class="jk hi"> <em class="ke"> t </em> </strong>是我们要衡量其共性的术语(单词)，而<strong class="jk hi"> <em class="ke"> N </em> </strong>是语料库(D)中文档的数量(D)。分母只是术语<strong class="jk hi"> <em class="ke"> t </em> </strong>出现的文档数。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/5f264a154a19c03ce6bd3e3e487aa00f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVGoyvkcBePbzXrhGJiDAw.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx">Image Source: <a class="ae kf" href="https://monkeylearn.com/blog/what-is-tf-idf/" rel="noopener ugc nofollow" target="_blank">https://monkeylearn.com/blog/what-is-tf-idf/</a></figcaption></figure><p id="cf73" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">注意:一个术语可能根本没有出现在语料库中，这可能导致被零除的错误。处理这种情况的一种方法是将现有计数加1。从而使分母(1 +计数)。下面是一个流行库</em><a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html" rel="noopener ugc nofollow" target="_blank"><em class="ke">scikit-learn</em></a><em class="ke">如何处理这个问题的例子。</em></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mj"><img src="../Images/83f806c6166f8b8c072ae8d24627575d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WkAzU-as6UfYNPHDUM8wsw.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx"><em class="mk">Image Source: </em><a class="ae kf" href="https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d" rel="noopener" target="_blank"><em class="mk">https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d</em></a></figcaption></figure><p id="3fbd" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们需要IDF的原因是帮助纠正诸如“of”、“as”、“The”等词语。因为它们经常出现在英语语料库中。因此，通过取逆文档频率，我们可以最小化频繁项的权重，同时使不频繁项具有更高的影响。</p><p id="1f09" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">最后，IDF也可以从背景语料库(用于校正采样偏差)或手头实验中使用的数据集提取。</p><h2 id="0d92" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">综合起来:TF-IDF</h2><p id="b812" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">总而言之，推动TF-IDF的关键直觉是，术语的重要性与其在文档中的出现频率成反比。TF为我们提供了一个术语在文档中出现的频率信息，而IDF则为我们提供了一个术语在文档集合中相对罕见的信息。通过将这些值相乘，我们可以得到最终的TF-IDF值。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/d98187e2fb51dd6ae69f4067130e746d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWIlf6HkOqXTZsq7-xogDw.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx"><em class="mk">Image Source: </em><a class="ae kf" href="https://monkeylearn.com/blog/what-is-tf-idf/" rel="noopener ugc nofollow" target="_blank"><em class="mk">https://monkeylearn.com/blog/what-is-tf-idf/</em></a></figcaption></figure><p id="7a50" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">TF-IDF得分越高，术语就越重要或相关；随着一个术语变得越来越不相关，它的TF-IDF分数将接近0。</p><h1 id="37ad" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">在哪里使用TF-IDF</h1><p id="74fc" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">正如我们所看到的，TF-IDF是确定一个术语在文档中的重要性的一个非常方便的指标。但是TF-IDF是怎么用的呢？TF-IDF主要有三种应用。这些是在<em class="ke">机器学习，信息检索，</em>和<em class="ke">文本摘要/关键词提取。</em></p><h2 id="236c" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">在机器学习和自然语言处理中使用TF-IDF</h2><p id="76fa" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">机器学习算法经常使用数字数据，因此当处理文本数据或任何<a class="ae kf" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>任务(ML/AI处理文本的子领域)时，首先需要通过称为<a class="ae kf" href="https://towardsdatascience.com/understanding-nlp-word-embeddings-text-vectorization-1a23744f7223" rel="noopener" target="_blank">矢量化</a>的过程将数据转换为数字数据的矢量。TF-IDF矢量化包括计算语料库中与该文档相关的每个单词的TF-IDF得分，然后将该信息放入一个向量中(参见下图中的示例文档“A”和“B”)。因此，您的语料库中的每个文档都有自己的向量，并且该向量对整个文档集合中的每个单词都有一个TF-IDF得分。一旦有了这些向量，您就可以将它们应用于各种用例，例如通过使用<a class="ae kf" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>比较两个文档的TF-IDF向量来查看它们是否相似。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/27b33086ff10d4f9b47d4da804d17d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABN7eyelof5lWf2eXX2CqQ.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx"><em class="mk">A = “The car is driven on the road”; B = “The truck is driven on the highway” Image from freeCodeCamp — How to process textual data using TF-IDF in Python (https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/)</em></figcaption></figure><h2 id="b3e1" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">TF-IDF在信息检索中的应用</h2><p id="e2f5" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">TF-IDF在信息检索领域也有用例，一个常见的例子是搜索引擎。由于TF-IDF可以告诉您基于文档的术语的相关重要性，搜索引擎可以使用TF-IDF来帮助基于相关性对搜索结果进行排序，与用户更相关的结果具有更高的TF-IDF分数。</p><h2 id="300b" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">TF-IDF在文本摘要和关键词提取中的应用</h2><p id="4b4d" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">因为TF-IDF基于相关性对单词进行加权，所以可以使用这种技术来确定具有最高相关性的单词是最重要的。这可以用来帮助更有效地总结文章，或者简单地确定文档的关键字(甚至标签)。</p><h1 id="153c" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">向量和单词嵌入:TF-IDF vs Word2Vec vs单词包vs BERT</h1><p id="042c" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">如上所述，TF-IDF可以用来将文本矢量化成更适合ML &amp; NLP技术的格式。然而，虽然它是一种流行的NLP算法，但它不是唯一的一种。</p><h2 id="c186" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">一袋单词</h2><p id="4eb1" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> Bag of Words (BoW) </a>简单统计文档中的词频。因此，文档的向量具有该文档的语料库中每个单词的频率。单词袋和TF-IDF之间的关键区别在于前者不包含任何类型的逆文档频率(IDF ),而只是一个频率计数(TF)。</p><h2 id="868d" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">Word2Vec</h2><p id="bdb3" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>是一种算法，它使用浅层2层而非深层神经网络来摄取语料库并产生向量集。TF-IDF和word2vec之间的一些关键区别在于，TF-IDF是一种统计度量，我们可以将其应用于文档中的术语，然后使用它来形成向量，而word2vec将为术语生成向量，然后可能需要做更多的工作来将向量集转换为单一向量或其他格式。此外，TF-IDF不考虑语料库中单词的上下文，而word2vec则考虑。</p><h2 id="63bc" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">BERT —变压器的双向编码器表示</h2><p id="21f5" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> BERT </a>是谷歌开发的一种ML/NLP技术，它使用基于ML模型的<a class="ae kf" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener ugc nofollow" target="_blank">转换器将短语、单词等转换成向量。TF-IDF和BERT之间的主要区别如下:TF-IDF不考虑单词的语义或上下文，而BERT考虑。此外，BERT使用深度神经网络作为其架构的一部分，这意味着它的计算成本可能比没有此类要求的TF-IDF高得多。</a></p><h1 id="e0b8" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">使用TF-IDF的利弊</h1><h2 id="6b49" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">使用TF-IDF的优点</h2><p id="8521" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">TF-IDF最大的优势来自于它有多简单好用。它计算简单，计算成本低，并且是相似性计算的简单起点(通过TF-IDF矢量化+余弦相似性)。</p><h2 id="74e1" class="ld kh hh bd ki le lf lg km lh li lj kq jr lk ll ks jv lm ln ku jz lo lp kw lq bi translated">使用TF-IDF的缺点</h2><p id="1c6c" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">需要注意的是，TF-IDF不能帮助传递语义。它根据单词的权重来考虑单词的重要性，但它不一定能推导出单词的上下文并以这种方式理解重要性。</p><p id="cb21" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">同样如上所述，像BoW一样，TF-IDF忽略词序，因此像“英国女王”这样的复合名词不会被视为“单一单位”。这也延伸到否定的情况，比如“不付账”和“付账”，顺序有很大的不同。在使用NER工具和下划线的两种情况下，“queen_of_england”或“not_pay”都是将短语作为一个单位处理的方法。</p><p id="3a64" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">另一个缺点是，由于TF-IDF会遭受维数灾难，所以它会遭受内存低效。回想一下，TF-IDF向量的长度等于词汇表的大小。在某些分类环境中，这可能不是问题，但在其他环境中，如聚类，随着文档数量的增加，这可能会变得难以处理。因此，研究上面提到的一些替代方法(BERT，Word2Vec)可能是必要的。</p><h1 id="f3a9" class="kg kh hh bd ki kj kk kl km kn ko kp kq in kr io ks iq kt ir ku it kv iu kw kx bi translated">结论</h1><p id="f13e" class="pw-post-body-paragraph ji jj hh jk b jl ky ii jn jo kz il jq jr la jt ju jv lb jx jy jz lc kb kc kd ha bi translated">TF-IDF(Term Frequency-Inverse Document Frequency)是一种简便的算法，它使用单词的频率来确定这些单词与给定文档的相关程度。这是一种相对简单但直观的单词加权方法，允许它作为各种任务的一个很好的出发点。这包括构建搜索引擎、总结文档或信息检索和机器学习领域的其他任务。</p></div><div class="ab cl ml mm go mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ha hb hc hd he"><p id="dbc9" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">披露声明:2021资本一。观点是作者个人的观点。除非本帖中另有说明，否则Capital One不隶属于所提及的任何公司，也不被这些公司认可。使用或展示的所有商标和其他知识产权是其各自所有者的财产。</em></p><p id="0629" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">最初发表于</em><a class="ae kf" href="https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/" rel="noopener ugc nofollow" target="_blank">T5【https://www.capitalone.com】</a><em class="ke">。</em></p></div></div>    
</body>
</html>