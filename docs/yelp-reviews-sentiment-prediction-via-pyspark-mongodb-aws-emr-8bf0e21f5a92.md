# Yelp é€šè¿‡ PySparkã€MongoDBã€AWS EMR è¯„è®ºæƒ…ç»ªé¢„æµ‹

> åŸæ–‡ï¼š<https://medium.com/quick-code/yelp-reviews-sentiment-prediction-via-pyspark-mongodb-aws-emr-8bf0e21f5a92?source=collection_archive---------3----------------------->

ä½œè€…:[å°¼æŸ¥Â·é²å¥‡æ‹‰ç“¦ç‰¹](https://medium.com/u/f8ecd6cdacf9?source=post_page-----8bf0e21f5a92--------------------------------)ï¼Œ[è’‚å¨œÂ·å½­](https://medium.com/u/f9b481f9624?source=post_page-----8bf0e21f5a92--------------------------------)ï¼Œ[éº¦è‰²Â·æ](https://medium.com/u/13c0d3a0ad3?source=post_page-----8bf0e21f5a92--------------------------------)

æƒ…æ„Ÿåˆ†ææˆ–è§‚ç‚¹æŒ–æ˜æ˜¯ä¸€ç§å¸¸è§çš„è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ï¼Œç”¨äºç¡®å®šæ–‡æœ¬æ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢çš„ã€‚åœ¨ä¸€ä¸ªé¡¾å®¢è¶Šæ¥è¶Šå¤šåœ°åœ¨ç½‘ä¸Šå‘è¡¨æ„è§çš„ä¸–ç•Œé‡Œï¼Œä¼ä¸šäº†è§£è‡ªå·±çš„ç½‘ä¸Šå£°èª‰è‡³å…³é‡è¦ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ Apache Spark çš„æœºå™¨å­¦ä¹ åº“åœ¨ Yelp æ•°æ®é›†ä¸Šæ¢ç´¢ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œä»¥é¢„æµ‹ç»™å®šè¯„è®ºæ–‡æœ¬çš„æƒ…ç»ªã€‚æˆ‘ä»¬è¿˜å°†åˆ†æå“ªäº›æœ¯è¯­å¯¹ç§¯ææˆ–æ¶ˆæçš„é¤é¦†è¯„è®ºè´¡çŒ®æœ€å¤§ã€‚

ç”±äºç‰¹å¾ç³»æ•°çš„å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**çº¿æ€§æ”¯æŒå‘é‡æœº**å’Œ**é€»è¾‘å›å½’**æ¥é¢„æµ‹è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢ã€‚å®ƒä»¬éƒ½æ˜¯ NLP åº”ç”¨çš„æœ‰æ•ˆåˆ†ç±»ç®—æ³•ã€‚

1.  **åœ¨ AWS EMR å®ä¾‹ä¸Šè®¾ç½® MongoDB å’Œ Zeppelin Notebook**

æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ª AWS EMR å®ä¾‹ï¼Œæœ‰ 1 ä¸ªä¸»å®ä¾‹å’Œ 3 ä¸ªå·¥ä½œå®ä¾‹(æ¯ä¸ªå®ä¾‹éƒ½æœ‰ä¸€ä¸ª m3.xlargeï¼Œæœ‰ 15 GB å†…å­˜å’Œ 8 ä¸ªå†…æ ¸)ã€‚è¿™å¯¹äº 1.5 M çš„è¯„è®ºå­é›†å¾ˆæœ‰æ•ˆï¼Œä½†æ˜¯æ›´å¤šçš„è¯„è®ºå¯ä»¥é€šè¿‡å…·æœ‰æ›´å¤§å†…å­˜å¤§å°çš„èŠ‚ç‚¹æˆ–è€…å…·æœ‰æ›´å¤šèŠ‚ç‚¹çš„é›†ç¾¤æ¥å¤„ç†ã€‚åŸå§‹æ•°æ®é›†å­˜å‚¨åœ¨äºšé©¬é€Š S3 æ¡¶ä¸­ã€‚MongoDB è®¾ç½®åœ¨ä¸»èŠ‚ç‚¹ä¸Šï¼ŒYelp çš„è¡¨ä» S3 å¯¼å…¥ï¼Œæˆ‘ä»¬ä»é‚£é‡ŒåŠ è½½åˆ° Zeppelin Notebookã€‚

![](img/edb5be498f5ca9779e665c04dff0482a.png)

è¦å¯åŠ¨ PySpark å’Œ MongoDB ä¹‹é—´çš„è¿æ¥:

```
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("yelp).getOrCreate()#connect to 'database' called yelp, 'collection' called review
review = spark.read.format("com.mongodb.spark.sql.DefaultSource")\
                .option("uri", "mongodb://127.0.0.1:27017/yelp.review").load()
```

**2ã€‚é¢„å¤„ç†è¯„å®¡æ•°æ®**

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°æ¸…ç†è¯„è®ºæ–‡æœ¬ï¼Œåˆ é™¤ä»»ä½•æ ‡ç‚¹ç¬¦å·æˆ–æ•°å­—ã€‚

```
import string
import redef remove_punct(text):
    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\r\\t\\n]')
    nopunct = regex.sub(" ", text)  
    return nopunct
```

æˆ‘ä»¬è¿˜éœ€è¦å°†æ˜Ÿçº§è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç­‰çº§ï¼Œ1 è¡¨ç¤ºæ­£é¢è¯„ä»·ï¼Œ0 è¡¨ç¤ºè´Ÿé¢è¯„ä»·ã€‚å¦‚æœè¯„ä»·è¶…è¿‡ 4 æ˜Ÿï¼Œåˆ™ä¸ºæ­£é¢è¯„ä»·ï¼›å¦‚æœä½äº 4 æ˜Ÿï¼Œåˆ™ä¸ºè´Ÿé¢è¯„ä»·ã€‚4 æ˜Ÿä»¥ä¸Šçš„é—¨æ§›æ˜¯å› ä¸ºäººä»¬å€¾å‘äºé«˜ä¼°é¤å…(ä¾‹å¦‚ï¼Œäººä»¬é€šå¸¸ä¸è®¤ä¸º 3 æ˜Ÿé¤å…æ˜¯ä¸€å®¶å¥½é¤å…)ã€‚

```
def convert_rating(rating):
    if rating >=4:
        return 1
    else:
        return 0
```

æˆ‘ä»¬å°†æŠŠè¿™äº›å‡½æ•°è½¬æ¢æˆ PySpark çš„ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼Œå¹¶åœ¨æŸ¥è¯¢æ•´ä¸ªâ€œreviewâ€é›†åˆæ—¶å°†å®ƒä»¬åº”ç”¨äº text å’Œ rating åˆ—ã€‚

```
from pyspark.sql.functions import udfpunct_remover = udf(lambda x: remove_punct(x))
rating_convert = udf(lambda x: convert_rating(x))#select 1.5 mn rows of reviews text and corresponding star rating with punc removed and ratings converted
resultDF = df.select('review_id', punct_remover('text'), rating_convert('stars')).limit(1500000)#user defined functions change column names so we rename the columns back to its original names
resultDF = resultDF.withColumnRenamed('<lambda>(text)', 'text')
resultDF = resultDF.withColumnRenamed('<lambda>(stars)', 'stars')
```

æˆ‘ä»¬è¿˜éœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°(åˆ†è§£æˆå•è¯)å¹¶åˆ é™¤åœç”¨è¯(å¸¸è§çš„è‹±è¯­æœ¯è¯­ï¼Œå¦‚â€œandâ€ã€â€œthenâ€ã€â€œå› æ­¤â€)ã€‚è¿™äº›éƒ½å°†ä½¿ç”¨ç°æœ‰çš„ SparkML å’Œç®¡é“åº“æ¥å®Œæˆã€‚

```
from pyspark.ml.feature import *#tokenizer and stop word remover
tok = Tokenizer(inputCol="text", outputCol="words")
#stop word remover
stopwordrm = StopWordsRemover(inputCol='words', outputCol='words_nsw')# Build the pipeline 
pipeline = Pipeline(stages=[tok, stopwordrm])
# Fit the pipeline 
review_tokenized = pipeline.fit(resultDF).transform(resultDF).cache()
```

æˆ‘ä»¬ç¼“å­˜æ¸…ç†åçš„æ•°æ®ï¼Œè¿™æ ·æ¯æ¬¡è°ƒç”¨æ•°æ®å¸§æ—¶éƒ½ä¸éœ€è¦é‡æ–°è®¡ç®—ã€‚

**3)ç‰¹å¾é€‰æ‹©:é€‰æ‹©å¹¶æ•´åˆæœ€é‡è¦çš„ n å…ƒæ–‡æ³•(å¯é€‰)**

ä¸ºäº†æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬å°è¯•æ•´åˆäºŒå…ƒæ¨¡å‹å’Œä¸‰å…ƒæ¨¡å‹ã€‚è¿™æ˜¯åœ¨åœç”¨è¯è¢«ç§»é™¤ä¹‹å‰é€šè¿‡æ ‡è®°åŒ–çš„æ–‡æœ¬åˆ—æ¥å®Œæˆçš„ã€‚

ä¸ºäº†é€‰æ‹©æœ€é‡è¦çš„ n gramï¼Œæˆ‘ä»¬å°†äºŒå…ƒæ¨¡å‹æˆ–ä¸‰å…ƒæ¨¡å‹çš„ [TF-IDF çŸ©é˜µ](http://datameetsmedia.com/bag-of-words-tf-idf-explained/)åˆ†åˆ«è¾“å…¥åˆ°æ”¯æŒå‘é‡æœºæ¨¡å‹ä¸­ï¼Œå¹¶æŒ‘é€‰å‡ºå…·æœ‰æœ€å¤§ç³»æ•°æƒé‡(æ­£è´Ÿ)çš„ n gramã€‚è¿™å¾ˆæœ‰æ•ˆï¼Œå› ä¸º [SVM å·²ç»è¢«è¯æ˜æ˜¯ç‰¹å¾é€‰æ‹©çš„å¥½æ–¹æ³•](http://proceedings.mlr.press/v3/chang08a/chang08a.pdf)ã€‚

æˆ‘ä»¬å°†è¿™äº› ngrams è¿æ¥å›è¯„è®ºä¸­ï¼Œä»¥ä¾¿å®ƒä»¬ä½œä¸ºä¸€ä¸ªå•è¯è€Œä¸æ˜¯ä¸¤ä¸‰ä¸ªå•ç‹¬çš„å•è¯è¢«çº³å…¥æ¨¡å‹ä¸­(ä¾‹å¦‚ï¼Œâ€œä¼Ÿå¤§çš„å®¢æˆ·æœåŠ¡â€å˜æˆäº†â€œä¼Ÿå¤§çš„å®¢æˆ·æœåŠ¡â€)ã€‚

æœ€å¥½çš„æ¨¡å‹ç”±ä¸€å…ƒæ¨¡å‹å’Œä¸‰å…ƒæ¨¡å‹ç»„æˆã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•é€‰æ‹©ä¸‰å…ƒæ¨¡å‹çš„ä¸€ä¸ªä¾‹å­:

```
# add ngram column
n = 3
ngram = NGram(inputCol = 'words', outputCol = 'ngram', n = n)
add_ngram = ngram.transform(review_tokenized)# count vectorizer and tfidf
cv_ngram = CountVectorizer(inputCol='ngram', outputCol='tf_ngram')
cvModel_ngram = cv_ngram.fit(add_ngram)
cv_df_ngram = cvModel_ngram.transform(add_ngram)# create TF-IDF matrix
idf_ngram = IDF().setInputCol('tf_ngram').setOutputCol('tfidf_ngram')
tfidfModel_ngram = idf_ngram.fit(cv_df_ngram)
tfidf_df_ngram = tfidfModel_ngram.transform(cv_df_ngram)# split into training & testing set
splits_ngram = tfidf_df_ngram.select(['tfidf_ngram', 'label']).randomSplit([0.8,0.2],seed=100)
train_ngram = splits_ngram[0].cache()
test_ngram = splits_ngram[1].cache()# Convert feature matrix to LabeledPoint vectors
train_lb_ngram = train_ngram.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))
test_lb_ngram = train_ngram.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))# fit SVM model of only trigrams
numIterations = 50
regParam = 0.3
svm = SVMWithSGD.train(train_lb_ngram, numIterations, regParam=regParam)#extract top 20 trigrams based on weights
top_ngram = svm_coeffs_df_ngram.sort_values('weight')['ngram'].values[:20]
bottom_ngram = svm_coeffs_df_ngram.sort_values('weight', ascending=False)['ngram'].values[:20]
ngram_list = list(top_ngram) + list(bottom_ngram)
```

è¦åœ¨åŸæ–‡ä¸­è¿æ¥è¿™äº›é¡¶çº§ä¸‰å…ƒæ¨¡å‹:

```
# replace the word with selected ngram
def ngram_concat(text):
    text1 = text.lower()
    for ngram in ngram_list:
        if ngram in text1:
            new_ngram = ngram.replace(' ', '_')
            text1 = text1.replace(ngram, new_ngram)
    return text1ngram_df = udf(lambda x: ngram_concat(x))
ngram_df = review_tokenized.select(ngram_df('text'), 'label')\
                          .withColumnRenamed('<lambda>(text)', 'text')
```

ä¸€æ—¦å®ç°äº†è¿™ä¸€ç‚¹ï¼Œå°±éœ€è¦é‡å¤æ­¥éª¤ 2)ä¸­çš„ç®¡é“ï¼Œä»¥ä½¿ç”¨ä¸²è”çš„ ngram æ›´æ–°æ ‡è®°åŒ–çš„åˆ—(è€Œä¸æ˜¯å°†ç®¡é“åº”ç”¨äº resultDFï¼Œè€Œæ˜¯å°†å…¶åº”ç”¨äº ngram_df)ã€‚

**4)æ„å»ºæœ€ç»ˆçš„â€œå•è¯åŒ…â€TF-IDF ç‰¹å¾çŸ©é˜µ**

è¦æ„å»ºæœ€ç»ˆçš„ TF-IDF ç‰¹å¾çŸ©é˜µ:

```
# count vectorizer and tfidf
cv = CountVectorizer(inputCol='words_nsw', outputCol='tf')
cvModel = cv.fit(review_tokenized)
count_vectorized = cvModel.transform(review_tokenized)tfidfModel = idf.fit(count_vectorized)
tfidf_df = tfidfModel.transform(count_vectorized)
```

**5)æ”¯æŒå‘é‡æœº**

ä¸ºäº†æµ‹è¯•æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†:

```
# split into training and testing set
splits = tfidf_df.select(['tfidf', 'label']).randomSplit([0.8,0.2],seed=100)
train = splits[0].cache()
test = splits[1].cache()
```

æˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸Šå°† TF-IDF ç‰¹å¾çŸ©é˜µæ‹Ÿåˆåˆ° SVM æ¨¡å‹ã€‚ä¸€ä¸ªè¶…å‚æ•° regParam ç”¨äºæ§åˆ¶è¿‡åº¦æ‹Ÿåˆã€‚å€¼ 0.3 åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šéƒ½äº§ç”Ÿäº†æœ€å¥½çš„ f1 åˆ†æ•°ã€‚

```
numIterations = 50
regParam = 0.3
svm = SVMWithSGD.train(train_lb, numIterations, regParam=regParam)test_lb = test.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))
scoreAndLabels_test = test_lb.map(lambda x: (float(svm.predict(x.features)), x.label))
score_label_test = spark.createDataFrame(scoreAndLabels_test, ["prediction", "label"])
```

**6)æ­£è§„åŒ–(å¼¹æ€§ç½‘)ç‰©æµå›å½’**

ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬è¿˜åº”ç”¨äº†æ­£åˆ™åŒ–é€»è¾‘å›å½’ã€‚ [SVM ä¸“æ³¨äºå¯»æ‰¾æœ€å¤§åŒ–æœ€è¿‘ç‚¹åˆ°è¾¹ç¼˜çš„è·ç¦»çš„åˆ†ç¦»å¹³é¢ï¼Œè€Œé€»è¾‘å›å½’æœ€å¤§åŒ–æ•°æ®çš„æ¦‚ç‡](http://www.cs.toronto.edu/~kswersky/wp-content/uploads/svm_vs_lr.pdf)(å³ç¦»è¶…å¹³é¢è¶Šè¿œè¶Šå¥½)ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿˜è°ƒæ•´äº†æœ€ä½³è¶…å‚æ•° RegParam å’Œ ElasticNetParam ä»¥è·å¾—æœ€ä½³ç»“æœã€‚æˆ‘ä»¬å°†æ¨¡å‹æ‹Ÿåˆå¦‚ä¸‹:

```
# Elastic Net Logit
lambda_par = 0.02
alpha_par = 0.3
lr = LogisticRegression().\
        setLabelCol('label').\
        setFeaturesCol('tfidf').\
        setRegParam(lambda_par).\
        setMaxIter(100).\
        setElasticNetParam(alpha_par)
lrModel = lr.fit(train)
lr_pred = lrModel.transform(test)
```

**7)è¯„ä¼°**

è¯·æ³¨æ„ï¼Œç”±äºæ•°æ®é›†ä¸å¹³è¡¡(æ­£é¢è¯„ä»·å¤šäºè´Ÿé¢è¯„ä»·)ï¼Œf1 åˆ†æ•°è¢«é€‰ä¸ºè¯„ä¼°åŸºç¡€ã€‚å‡†ç¡®åº¦ä¸èƒ½è¯´æ˜å…¨éƒ¨æƒ…å†µï¼Œè€Œ f1-ä½œä¸ºå‡†ç¡®åº¦å’Œå¬å›ç‡çš„åŠ æƒå¹³å‡å€¼-å¯ä»¥æ­ç¤ºæ¨¡å‹åœ¨è¯†åˆ«é¢„æµ‹ç›¸å…³æ€§å’Œæ­£ç¡®é¢„æµ‹çœŸæ­£ç›¸å…³ç»“æœçš„ç™¾åˆ†æ¯”æ–¹é¢çš„è¡¨ç°ã€‚

è¯„ä¼° SVM æ¨¡å‹:

```
f1_eval = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
svm_f1 = f1_eval.evaluate(score_label_test)
print("F1 score: %.4f" % svm_f1)
```

è¯„ä¼°ç‰©æµå›å½’æ¨¡å‹:

```
f1_eval = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
lr_f1 = f1_eval.evaluate(lr_pred)
print("F1 score: %.4f" % lr_f1)
```

**åœ¨æµ‹è¯•é›†ä¸Šï¼ŒSVM æ¨¡å‹çš„ f1 å€¼æœ€é«˜ï¼Œä¸º 87.32%ã€‚**é€»è¾‘å›å½’æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„ f1 å€¼ä¸º 84.21%ã€‚

**8)ç‰¹å¾åˆ†æ**

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¥è‡ªæœ€ç»ˆ SVM æ¨¡å‹çš„ç³»æ•°æƒé‡æ¥æŸ¥çœ‹å“ªäº›æœ¯è¯­å¯¹æ­£é¢è¯„è®ºæˆ–è´Ÿé¢è¯„è®ºçš„è´¡çŒ®æœ€å¤§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸æœ€ç§¯æçš„ç³»æ•°ç›¸å…³è”çš„æœ¯è¯­ä»£è¡¨é‚£äº›å¯¹ç§¯æè¯„ä»·è´¡çŒ®æœ€å¤§çš„æœ¯è¯­ã€‚ä¸æœ€è´Ÿç³»æ•°ç›¸å…³è”çš„æœ¯è¯­è¡¨ç¤ºå¯¹è´Ÿé¢è¯„ä»·è´¡çŒ®æœ€å¤§çš„é‚£äº›æœ¯è¯­ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå•è¯äº‘ï¼Œç”¨è“è‰²è¡¨ç¤ºæœ€â€œç§¯æâ€çš„è¯è¯­ï¼Œç”¨çº¢è‰²è¡¨ç¤ºæœ€â€œæ¶ˆæâ€çš„è¯è¯­ã€‚

![](img/a052e489fe05a16038c9393dfc1f0e0c.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæœ€ç§¯æçš„è¯è¯­åŒ…æ‹¬â€œå‹å¥½çš„å‘˜å·¥â€ã€â€œç¾å‘³â€ã€â€œå‡ºè‰²çš„å®¢æˆ·æœåŠ¡â€ã€â€œç¾å‘³çš„é£Ÿç‰©â€ã€‚è¿™è¡¨æ˜é¡¾å®¢æ»¡æ„çš„é‡è¦å› ç´ æ˜¯å‘˜å·¥ã€é£Ÿç‰©å£å‘³å’ŒæœåŠ¡ã€‚å¯ä»¥é€šè¿‡åˆ é™¤è¯¸å¦‚â€œæ£’æäº†â€ã€â€œæ¨èâ€ç­‰çŸ­è¯­æ¥è¿›ä¸€æ­¥æ”¹è¿›ã€‚ä»¥ä¾¿æ›´å¤šæœ‰æ„ä¹‰çš„æ ‡è®°å¯ä»¥å‡ºç°ã€‚

æœ€è´Ÿé¢çš„è¯è¯­åŒ…æ‹¬â€œåˆ†é’Ÿâ€ã€â€œä»·æ ¼è¿‡é«˜â€ã€â€œæ·¡è€Œæ— å‘³â€ã€â€œé£Ÿç‰©è¿˜å¯ä»¥â€ã€â€œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„â€ã€‚è¿™è¡¨æ˜ï¼Œè´Ÿé¢è¯„ä»·æ˜¯ç”±æ¼«é•¿çš„ç­‰å¾…æ—¶é—´ã€ä»·æ ¼è¿‡é«˜çš„é£Ÿç‰©ã€ç³Ÿç³•çš„é£Ÿç‰©å‘³é“é€ æˆçš„ï¼Œè¿™ç§ç»å†å¹¶ä¸è¢«è§†ä¸ºä»€ä¹ˆç‰¹åˆ«çš„äº‹æƒ…ã€‚

è¿™ç¯‡æ–‡ç« æ€»ç»“äº†äº§ç”Ÿæœ€ä½³ç»“æœçš„æ­¥éª¤ã€‚è¦æŸ¥çœ‹æˆ‘ä»¬å°è¯•çš„æ‰€æœ‰æ­¥éª¤çš„æ‰€æœ‰ä»£ç ï¼Œè¯·æŸ¥çœ‹è¿™ä¸ª [GitHub](https://github.com/nicharuc/yelp-sentiment-prediction/blob/master/yelp_nlp_svm.md) ã€‚

**è¯·ç‚¹å‡»ğŸ‘æŒ‰é’®ä¸‹é¢å‡ ä¸‹ï¼Œä»¥ç¤ºæ”¯æŒï¼â¬‡â¬‡è°¢è°¢ï¼ä¸è¦å¿˜è®°éµå¾ªä¸‹é¢çš„å¿«é€Ÿä»£ç ã€‚**

> æ‰¾åˆ°å…³äºå„ç§ç¼–ç¨‹è¯­è¨€çš„[å¿«é€Ÿä»£ç ](http://www.quickcode.co/)çš„å…è´¹è¯¾ç¨‹ã€‚è·å– [Messenger](https://www.messenger.com/t/1493528657352302) çš„æ–°æ›´æ–°ã€‚