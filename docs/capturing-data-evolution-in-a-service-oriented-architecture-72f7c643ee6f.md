# 在面向服务的体系结构中捕获数据演变

> 原文：<https://medium.com/airbnb-engineering/capturing-data-evolution-in-a-service-oriented-architecture-72f7c643ee6f?source=collection_archive---------1----------------------->

## 建立 Airbnb 的变更数据捕获系统(SpinalTap ),以便能够实时传播和应对数据突变。

![](img/a27a65f768253d8b80b970f09fa7459c.png)

The dining hall in the San Francisco Office is always gleaming with natural sunlight!

> 劳动节周末就要到了！卡里姆渴望一个受之无愧的假期。他登录 Airbnb，开始计划去旧金山的旅行，偶然发现了 Dany 提供的一个很棒的列表。他预订了。
> 
> 过了一会儿，丹妮收到通知，说他的房子已经被预订了。他查看了他的列表日历，果然，那些日期都被预订了。他还注意到建议的每日价格在该时间段有所增加。"嗯，那段时间一定有很多人想参观这个城市"他喃喃自语。丹妮将他的列表标记为该周剩余时间可用...
> 
> 在东海岸的一路上，Sara 正在她纽约切尔西的舒适公寓里品茶，准备去旧金山公司总部出差。她已经倒霉了一段时间，正准备休息一下，这时丹妮的清单出现在她的搜索地图上。她检查了细节，看起来棒极了！她开始写信息:“亲爱的丹妮，我要去旧金山旅行，你的地方看起来很适合我呆在这里……”

在过去几年里，适应数据演变已经成为 Airbnb 许多新兴应用程序的经常性需求。上面的场景描述了这样的例子，其中动态定价、可用性和预订工作流需要近乎实时地对系统中不同组件的变化做出反应。从基础设施的角度来看，随着我们在数据和服务数量方面的持续增长，设计可扩展的架构是必不可少的。然而，作为努力实现面向服务的架构的一部分，在微服务之间传播有意义的数据模型变化的有效方式，同时维护保留数据所有权边界的解耦架构也同样重要。

作为回应，我们创造了 SpinalTap 一种可扩展、高性能、可靠、无损的**变更数据捕获**服务，能够跨不同数据源类型以低延迟检测数据突变，并将它们作为标准化事件传播给下游消费者。SpinalTap 已经成为 Airbnb 基础设施和衍生数据处理平台不可或缺的组成部分，几个关键管道都依赖于该平台。在这篇博客中，我们将概述系统架构、用例、保证以及它是如何扩展的。

# 背景

变更数据捕获(CDC)是一种设计模式，能够捕获数据的变更并通知参与者，以便他们能够做出相应的反应。这遵循发布-订阅模型，其中对数据集的更改是感兴趣的主题。

## 要求

为了适应我们的用例，系统的某些高级需求是可取的:

*   **无损**:对数据丢失零容忍，这是对基于流的会计审计管道等关键应用程序的要求
*   **可扩展:**水平可随着负载和数据集群规模的增加而扩展，以避免系统随着增量增长而重复设计
*   **Performant:** 更改以接近实时(亚秒)的速度传播给订阅的用户
*   **一致:**实施排序和时间线一致性，以保留特定数据记录的变更顺序
*   **容错**:高度可用，具有可配置的冗余度，可抵御故障
*   **可扩展的**:一个通用的框架，可以适应不同的数据源和接收器类型

## 考虑的解决方案

文献中提出了几种用于构建 CDC 系统的解决方案，其中引用最多的是:

*   **轮询:**通过跟踪状态属性(如**最后更新的**或**版本**)，时间驱动策略可用于定期检查数据存储的记录是否已提交任何更改
*   **触发器:**对于支持数据库触发器(例如:MySQL)的存储引擎，基于行的操作触发的存储过程，可以用来以无缝的方式将更改传播到其他数据表
*   **双重写入:**在请求期间，可以将数据更改传递给应用层中的订阅用户，比如在写提交之后通过发出事件或调度 RPC
*   **审计跟踪:**大多数数据存储解决方案维护一个事务日志(或变更日志)来记录和跟踪提交给数据库的变更。这通常用于集群节点之间的复制和恢复操作(例如意外的服务器关闭或故障转移)。

使用数据库 changelog 来检测更改有几个可取的特性:与触发器和轮询策略相比，从日志中读取允许一种异步的非侵入性方法来捕获更改。它还支持提交时间的强一致性和排序保证，并保留事务边界信息，这两项都是双重写入无法实现的。这允许重放某个时间点的事件。考虑到这一点，SpinalTap 就是基于这种方法设计的。

# 体系结构

![](img/c7d458e65067785b6ceee601f3eff3a3.png)

High level workflow overview

在高层次上，SpinalTap 被设计为一个通用的解决方案，它抽象了变更捕获工作流，足以轻松适应不同的基础设施依赖(数据存储、事件总线、消费者服务)。该架构由 3 个主要组件组成，有助于提供足够的抽象来实现这些品质:

## 来源

![](img/6014249cded805b1b2e4994484735aa9.png)

Left: Source component diagram; Right: Source event workflow

**源**表示来自特定数据存储的变更事件流的来源。只要有一个可访问的 changelog 来传输事件，就可以很容易地用不同的数据源类型扩展源抽象。从 changelog 解析的事件被过滤、处理并转换成相应的突变。一个**突变**是一个应用层构造，它代表了对一个数据实体的单个更改(插入、更新或删除)。它包括变更后的&之前的实体值、全局唯一标识符、交易信息和源自原始源事件的元数据。源还负责检测**数据** **模式演化**，并相应地传播具有相应突变的模式信息。这对于在客户端反序列化实体值或重放早期状态的事件时确保一致性非常重要。

## **目的地**

![](img/8eb0a990d5e140a132c78b63516f09e4.png)

Left: Destination component diagram; Right: Destination event workflow

**目的地**表示经过处理并转换为标准化事件后的突变接收器。目的地还跟踪最后一次成功发布的变异，这被用来导出要检查的源状态位置。该组件抽象出所使用的传输介质和格式。在 Airbnb，我们使用 [Apache Kafka](http://kafka.apache.org/) 作为事件总线，因为它在我们的基础设施中被广泛使用。 [Apache Thrift](https://thrift.apache.org/) 被用作数据格式，以提供标准化的变异模式定义和跨语言支持(Ruby & Java)。

通过对系统进行基准测试，发现一个主要的性能瓶颈是突变发布。考虑到我们的系统设置被选择为支持强一致性而不是延迟，这种情况变得更加严重。为了缓解这种情况，我们进行了一些优化:

![](img/e1e948b6483f6724c46bff56e9f6c395.png)

**缓冲的目的地:**为了避免源在等待发布突变时被阻塞，我们使用了内存中的有界队列来缓冲从源发出的事件(消费者-生产者模式)。当目的地发布突变时，源将事件添加到缓冲区。一旦可用，目的地将清空缓冲区并处理下一批突变。

![](img/facf1e3d377a76f2338ff092eca4a05c.png)

**目标池:**对于在传入事件率中显示不稳定尖峰行为的源，内存中的缓冲区偶尔会饱和，从而导致性能间歇性下降。为了将系统从不规则的负载模式中解放出来，我们采用了应用程序级的划分，将源事件划分到由线程池管理的一组可配置的缓冲目的地。事件被多路复用到线程目的地，同时保留排序模式。这使我们能够实现高吞吐量，同时不影响延迟或一致性。

## **管道**

![](img/a521e47c10bcee806822cbdd89c3750c.png)

Left: Pipe component diagram; Right: Pipe manager coordinating the pipe lifecycles

**管道**协调给定源和目的地之间的工作流。它代表并行的基本单位。它还负责定期检查源状态，并管理事件流的生命周期。在错误行为的情况下，管道执行正常关闭并启动故障恢复过程。根据最后一个状态检查点，采用保活机制来确保在出现故障的情况下重新启动源流。这允许从间歇性故障中自动修复，同时保持数据完整性。**管道管理器**负责在给定的集群节点上创建、更新和删除管道，以及管道生命周期(开始/停止)。它还确保对管道配置的任何更改都相应地在运行时传播。

# 集群管理

![](img/e04672daed86a8a84cfc98d17bd6d09e.png)

Left: Cluster resource management; Middle: Node failure recovery; Right: Isolation with instance tagging

为了实现某些理想的架构方面——如可伸缩性、容错和隔离——我们采用了一个集群管理框架( [Apache Helix](https://helix.apache.org/) )来协调跨计算资源的流处理分布。这帮助我们实现了**确定性负载平衡**和**水平扩展**，并在集群中自动重新分配源处理器。

为了提高**高可用性**和可配置的**容错能力，**每个源都被指定为集群节点的某个子集来处理事件流。我们使用一个 Leader-Standby 状态模型，在该模型中，在任何给定的时间点，只有一个节点从一个源传输事件，而子集群中的其余节点都处于备用状态。如果主节点出现故障，那么备用节点之一将承担主节点的责任。

为了支持源类型处理之间的**隔离**，集群中的每个节点都标记有可以委托给它的源类型。流处理分布在集群节点上，同时保持这种隔离标准。

为了解决来自**网络分区**的不一致性，特别是在一个以上的节点承担来自特定源(分裂大脑)的流的领导权的情况下，我们维护每个源的全局领导者时期，其在领导者转换时自动递增。前导时期随着每个突变而传播，并且通过忽略具有比最近观察到的时期更小的时期的事件，客户端过滤因此减轻了不一致性。

# 担保

某些保证对于系统的维护是必不可少的，以适应所有的下游用例。

**数据完整性**:系统维护一个*至少一次交付*的保证，其中底层数据存储的任何改变*最终*传播到客户端。这意味着 changelog 中的任何事件都不会永久丢失，并且会在我们的 SLA 指定的时间窗口内交付。我们还确保不会发生数据损坏，并且变异内容保持与源事件的奇偶校验。

**事件排序:**根据定义的分区方案执行排序。我们保持每个数据记录(行)的排序，即给定数据库表中特定行的所有更改都将以*提交顺序* **接收。**

**时间线一致性:**时间线上的一致性要求在给定的时间范围内按时间顺序接收变化，即给定突变集的两个序列不交错发送。裂脑场景可能会损害这种保证，但如前所述，可以通过纪元隔离来缓解。

# **验证**

![](img/30877718d745e289de6f1f0da9fa4025.png)

The SpinalTap validation framework

通过设计来证明 SpinalTap 的保证没有违规是不够的，我们需要一种更务实的数据驱动方法来验证我们的假设。为了解决这一问题，我们开发了**一个连续的在线端到端验证管道**，负责验证消费者端收到的变异是否属实，并断言在预生产和生产环境中都没有检测到错误行为。

为了实现可靠的验证工作流，使用的突变被分区并存储在本地磁盘上，对源事件应用相同的分区方案。一旦接收到对应于一个分区的事件的所有突变，就通过一系列测试，用原始的源分区来验证该分区文件，这些测试断言了前面描述的保证。特别是对于 MySQL，binlog 文件被认为是一个干净的分区边界。

我们在沙盒环境中设置了离线集成测试，以**防止**任何回归被部署到生产中。通过消费每个源流的实时事件，验证器也可以在生产中在线使用。这有助于**检测**我们的测试管道中没有捕捉到的任何违规，并通过将源状态回滚到前一个检查点来自动**修复**。这就强制要求在任何问题得到解决之前不进行流式传输，并且*最终*保证一致性和数据完整性。

# 模型突变

![](img/1b8e30f51c481f9874349f9f8deafa41.png)

Left: Sync vs Async app workflow; Right: Propagating model mutations to downstream consumers

消费者服务直接利用给定服务数据库的 SpinalTap 事件的一个缺点是，数据模式被泄露，产生了不必要的耦合。此外，处理封装在自有服务中的数据突变的域逻辑也需要复制到消费者服务中。

为了缓解这种情况，我们在 SpinalTap 之上构建了一个模型流库，它允许服务侦听来自服务数据存储的事件，将它们转换为域模型突变，并在消息总线中重新注入它们。这有效地允许**数据模型突变**成为服务接口的一部分，并将请求/响应周期与异步数据接收和事件传播分离开来。它还有助于解耦域依赖性，促进事件驱动的通信，并通过隔离同步&异步应用程序工作流为服务提供性能&容错改进。

# 用例

SpinalTap 在我们的基础架构中有许多使用案例，其中最突出的是:

**高速缓存无效:**CDC 系统的一个常见应用是高速缓存无效，其中对后备数据存储的改变被高速缓存无效器服务或进程检测到，从而驱逐(或更新)相应的高速缓存条目。选择异步方法允许我们将缓存机制与请求路径和服务于生产流量的应用程序代码分离。这种模式在服务中广泛使用，以保持真实数据存储源和我们的分布式缓存集群(例如 Memcached、Redis)之间的一致性。

**搜索索引:**Airbnb 有多个搜索产品使用实时索引(例如评论搜索、收件箱搜索、支持机票搜索)。SpinalTap 被证明非常适合构建从数据存储到搜索后端的索引管道(例如 ElasticSearch)，特别是由于它的有序和至少一次交付语义。服务可以轻松地消费相应主题的事件，并转换突变以更新索引，这有助于确保搜索的新鲜度和低延迟。

**离线处理:** SpinalTap 还用于将在线数据存储以流的方式导出到我们的离线大数据处理系统(如 Hive、Airstream)，这需要高吞吐量、低延迟和适当的可扩展性。该系统在历史上还用于我们的数据库快照管道，以持续构建我们的在线数据库的备份，并将它们存储在 HBase 中。这极大地减少了放置每日备份的时间，并允许以更精细的时间粒度(例如:每小时)拍摄快照。

**信令:**在分布式架构中传播数据变更的另一个常见用例是作为一种信令机制，其中依赖的服务可以近乎实时地订阅和响应来自另一个服务的数据变更。例如，可用性服务将通过订阅预订服务的更改来阻止列表的日期，以便在预订时得到通知。风险、安全、支付、搜索和定价工作流是这种模式在我们的生态系统中应用的几个例子。

# 结论

在过去的几年里，SpinalTap 已经成为我们基础设施不可或缺的一部分，是一个为我们的许多核心工作流程提供动力的系统。这对于那些寻找一个可靠的通用框架的平台尤其有用，该框架可以很容易地与您的基础设施集成。在 Airbnb，SpinalTap 用于传播来自 MySQL、DynamoDB 和我们内部存储解决方案的数据突变。Kafka 目前是首选的事件总线，但是系统的可扩展性也允许我们考虑其他媒介(例如:Kinesis)。

最后，我们已经[开源了](https://github.com/airbnb/SpinalTap)几个我们的库组件，并且还在审核剩余的模块以进行正式发布。非常欢迎投稿！

*你真诚的，*

*spinal tap 战队(* [*贾德阿比-萨姆拉*](https://www.linkedin.com/in/jad-abi-samra-b9456a88/) *，* [*李涛邓*](https://www.linkedin.com/in/litaodeng/) *，* [*【王*](https://www.linkedin.com/in/zuofeiw/) *)*