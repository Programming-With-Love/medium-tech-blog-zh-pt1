<html>
<head>
<title>PySpark — How Local File Reads &amp; Writes Can Help Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark —本地文件读写如何提升性能</h1>
<blockquote>原文：<a href="https://medium.com/capital-one-tech/pyspark-how-local-file-reads-writes-can-help-performance-capital-one-a2add4b3037c?source=collection_archive---------3-----------------------#2021-07-12">https://medium.com/capital-one-tech/pyspark-how-local-file-reads-writes-can-help-performance-capital-one-a2add4b3037c?source=collection_archive---------3-----------------------#2021-07-12</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="26ff" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">解决数据帧中内存堆和垃圾收集问题的快速指南</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/2ad605593c56b98f93384f6f64fbe198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VUowYoqDLqjJp05p.jpg"/></div></div></figure><p id="0c65" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">想象一下…</p><p id="935d" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">你正在一个数据集上写一长串曲折的火花变换。一切都很顺利，但是在大约十次连接和数百行代码之后，您会注意到数据集从需要几分钟变成了几个小时，最终导致垃圾收集、堆空间或其他内存问题。您开始读取并尝试不同的Spark调用，调整数据的混洗方式，重新写入，重新分配本地内存，所有这些都无济于事。</em></p><p id="0797" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">如果你在<a class="ae kf" href="https://spark.apache.org/docs/latest/api/python/index.html" rel="noopener ugc nofollow" target="_blank"> PySpark </a>(或者一般的<a class="ae kf" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Spark </a>)中工作，Spark <em class="ke">应该</em>在幕后做很多优化。然而，如果您在不同的数据集上有许多连接或其他昂贵的计算，Spark可能会感到困惑。</p><p id="3dde" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">如果Spark无法优化您的工作，您可能会遇到垃圾收集或堆空间问题。如果您已经尝试调用了<code class="du kg kh ki kj b">repartition</code>、<code class="du kg kh ki kj b">coalesce</code>、<code class="du kg kh ki kj b">persist</code>和<code class="du kg kh ki kj b">cache</code>，但都没有成功，那么可能是时候考虑让Spark将数据帧写入本地文件并读回它了。将数据帧写入文件可以帮助Spark清除由于Spark延迟评估而导致的内存消耗。</p><p id="ca69" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">然而，作为一个警告，如果你将一个中间数据帧写到一个文件中，你不能一直重复使用相同的路径。</em>由于数据无法流入您正在尝试覆盖的同一目录，因此尝试对您正在覆盖的同一路径进行读写会产生问题。这是因为Spark只将一小部分数据读入内存，而将大部分数据留在磁盘上，直到需要的时候。如果您读取一个数据集，然后试图将它写出到相同的路径，就会在Spark通常的操作顺序中产生冲突。</p><h1 id="7ad3" class="kk kl hh bd km kn ko kp kq kr ks kt ku in kv io kw iq kx ir ky it kz iu la lb bi translated">我是如何第一次遇到这个问题的</h1><p id="484f" class="pw-post-body-paragraph ji jj hh jk b jl lc ii jn jo ld il jq jr le jt ju jv lf jx jy jz lg kb kc kd ha bi translated">我是Capital One的软件工程师，偶尔会使用Python和Spark。我最近遇到了一个Spark转换的问题，它看起来在逻辑上是正确的，但却导致了内存和性能问题。有很多转变，但都和我想象的一样简单。如果不是堆空间问题，那就是垃圾收集；无论是在转换还是测试中。在Jupyter笔记本上一个接一个地运行命令，仍然很难找到性能受到影响的地方。在运行测试时，扩展我的笔记本使用的内存，或者Pycharm使用的内存，并没有产生任何影响。</p><p id="eb5e" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">下面的功能很小，只有几行代码，但是可以解决很多困惑。我结合了StackOverflow的建议，以及Capital One其他更有经验的开发人员的帮助，设法解决了这个问题。和往常一样，这个函数在某些地方不起作用，但是我们会在看完代码后详细讨论。</p><h1 id="98fc" class="kk kl hh bd km kn ko kp kq kr ks kt ku in kv io kw iq kx ir ky it kz iu la lb bi translated">把密码给我！</h1><p id="3b34" class="pw-post-body-paragraph ji jj hh jk b jl lc ii jn jo ld il jq jr le jt ju jv lf jx jy jz lg kb kc kd ha bi translated">考虑下面的Python代码。为了方便起见，我尝试将PEP8格式化，所以如果你是这门语言的新手，你可以向你的团队保证你知道并遵循PEP8标准(并且肯定不是来自不遵循这些标准的Java团队):</p><pre class="ix iy iz ja fd lh kj li lj aw lk bi"><span id="8e22" class="ll kl hh kj b fi lm ln l lo lp">def clear_computation_graph(data_frame, spark_session): <br/>    """Returns a 'cleared' dataframe after saving it for PySpark to work from </span><span id="f93a" class="ll kl hh kj b fi lq ln l lo lp">    This will 'clear' the computation graph for you since<br/>    occasionally spark will poorly optimize commands. <br/>    This is useful to avoid having too many nested operations<br/>    especially more complex ones that tank performance. <br/>    Keyword arguments: <br/>    data_frame -- the dataframe you want to clear the graph for<br/>    spark_session -- your current spark session <br/>    """ <br/>    with tempfile.TemporaryDirectory() as path:  <br/>        data_frame.write.parquet(path, mode="overwrite") <br/>        data_frame = spark_session.read.parquet(path) <br/>        data_frame.cache() <br/>        data_frame.count() <br/>        return data_frame</span></pre><h1 id="1c81" class="kk kl hh bd km kn ko kp kq kr ks kt ku in kv io kw iq kx ir ky it kz iu la lb bi translated">说明</h1><p id="6075" class="pw-post-body-paragraph ji jj hh jk b jl lc ii jn jo ld il jq jr le jt ju jv lf jx jy jz lg kb kc kd ha bi translated"><em class="ke">我们为什么要做这些事情？</em></p><p id="57e8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这样做的目的是创建一个<a class="ae kf" href="https://docs.python.org/3/library/tempfile.html#tempfile.TemporaryDirectory" rel="noopener ugc nofollow" target="_blank">临时目录</a>，它只为这个函数而存在。它将在返回后删除自身及其内容。然后，它会将您的数据帧写入一个parquet文件，并立即读取出来。然后，它将数据帧缓存到本地存储器，执行<a class="ae kf" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" rel="noopener ugc nofollow" target="_blank">动作</a>，并返回数据帧。</p><p id="7650" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">写入删除自身的<a class="ae kf" href="https://docs.python.org/3/library/tempfile.html#tempfile.TemporaryDirectory" rel="noopener ugc nofollow" target="_blank">临时目录</a>可以避免产生内存泄漏。此外，我们并不真的关心我们写到什么目录，所以我们得到了一个额外的好处，不必了解我们当前的目录结构，或者传递一个位置来保存东西的额外工作。</p><p id="b874" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">写入一个拼花文件并立即读回会“清除”计算图，以帮助Spark从新开始到该点。</p><p id="9141" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">缓存是一个延迟求值的操作，这意味着Spark不会运行那个命令，直到调用了一个“<a class="ae kf" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" rel="noopener ugc nofollow" target="_blank">动作</a>”。动作会导致火花图计算到该点。Count是一个操作，用于确保Spark实际运行至此为止的所有命令，并将数据帧缓存在内存中。我们不关心这里有什么动作。缓存和计数使数据帧在内存中保持可用，因为磁盘上的目录会自行清理，否则您会丢失内容。</p><h1 id="8a9f" class="kk kl hh bd km kn ko kp kq kr ks kt ku in kv io kw iq kx ir ky it kz iu la lb bi translated">这种解决方案在哪些地方不起作用？</h1><p id="efce" class="pw-post-body-paragraph ji jj hh jk b jl lc ii jn jo ld il jq jr le jt ju jv lf jx jy jz lg kb kc kd ha bi translated">在某些情况下，此解决方案不起作用:</p><ul class=""><li id="9aaf" class="lr ls hh jk b jl jm jo jp jr lt jv lu jz lv kd lw lx ly lz bi translated">如果你已经有了Spark优化工具(Spark不能像这样优化用户定义的函数)。</li><li id="9da3" class="lr ls hh jk b jl ma jo mb jr mc jv md jz me kd lw lx ly lz bi translated">如果你没有足够的内存来写一个本地文件。</li><li id="b7b0" class="lr ls hh jk b jl ma jo mb jr mc jv md jz me kd lw lx ly lz bi translated">如果你的数据帧太大，以至于你不能让Spark执行一个<a class="ae kf" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" rel="noopener ugc nofollow" target="_blank">动作</a>，或者在内存中缓存你的数据帧。</li><li id="0755" class="lr ls hh jk b jl ma jo mb jr mc jv md jz me kd lw lx ly lz bi translated">如果您的Spark转换很简单，并且通过调整可以更好地解决性能问题，比如移除<a class="ae kf" rel="noopener" href="/@david.mudrauskas/looping-over-spark-an-antipattern-e10ac54824a0">循环</a>。</li></ul><p id="e073" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">缓存应该只保留给非常小的数据集，这些数据集可能会像小型CSV表一样被反复读取。如果您正在处理一个大型的Parquet数据集，这将干扰Spark的本机优化，并使您的查询效率更低，并可能导致内存耗尽问题。</p><p id="572e" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">另一种方法是只写入随机或临时目录，忽略缓存/计数。在某些情况下，与内存相比，扩展磁盘空间非常便宜。所以看起来像这样</p><pre class="ix iy iz ja fd lh kj li lj aw lk bi"><span id="12ac" class="ll kl hh kj b fi lm ln l lo lp">def saveandload(df, path): <br/>    """ Save a Spark dataframe to disk and immediately read back.    <br/>    This can be used as part of a checkpointing scheme as well as  <br/>    breaking Spark's computation graph. <br/>    """ <br/>    df.write.parquet(path, mode="overwrite") <br/>    return spark.read.parquet(path) </span><span id="d6cf" class="ll kl hh kj b fi lq ln l lo lp"><br/>my_df = saveandload(my_df, "/tmp/abcdef")</span></pre><p id="aa36" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">但是，等等，这到底是为什么？这些手术相当昂贵。</em></p><p id="6185" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">从理论上讲，与仅仅缓存相比，这个函数的效率会很低，Spark的工作方式让你不需要这么做。用户定义的函数对Spark来说是困难的，因为它不能优化它们内部发生的事情。保存到内存和调用一个动作本身已经是很昂贵的操作了。但是，在某些情况下，尝试缓存和持久存储数据帧实际上并不会中断所有的计算。如果有很多复杂的连接或其他逻辑，Spark可能需要硬中断，从它认为的新数据帧开始。</p><h1 id="a495" class="kk kl hh bd km kn ko kp kq kr ks kt ku in kv io kw iq kx ir ky it kz iu la lb bi translated">让这个解决方案成为你自己的</h1><p id="944d" class="pw-post-body-paragraph ji jj hh jk b jl lc ii jn jo ld il jq jr le jt ju jv lf jx jy jz lg kb kc kd ha bi translated">这其中很多都是可以互换的——如果不能将数据帧写入本地，可以写入S3存储桶。您不必将数据帧保存为拼花文件，甚至不必使用<a class="ae kf" href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes" rel="noopener ugc nofollow" target="_blank">覆盖</a>。你应该可以使用任何火花动作来代替计数。<code class="du kg kh ki kj b"><a class="ae kf" href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence" rel="noopener ugc nofollow" target="_blank">Cache</a></code> <a class="ae kf" href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence" rel="noopener ugc nofollow" target="_blank">可以切换为任意存储级别</a>。</p><p id="116e" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">如果我调用这个方法会是什么样子呢？</p><p id="9ad1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">大概是这样:</em></p><p id="b5bf" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><a class="ae kf" href="https://spark.apache.org/docs/latest/sql-getting-started.html" rel="noopener ugc nofollow" target="_blank">你有一个spark会话</a>:</p><pre class="ix iy iz ja fd lh kj li lj aw lk bi"><span id="c89a" class="ll kl hh kj b fi lm ln l lo lp">from pyspark.sql import SparkSession </span><span id="cee7" class="ll kl hh kj b fi lq ln l lo lp">val spark_session = SparkSession <br/>    .builder() <br/>    .appName("Spark SQL basic example") <br/>    .config("spark.some.config.option", "some-value") <br/>    .getOrCreate()</span><span id="86aa" class="ll kl hh kj b fi lq ln l lo lp">val complex_dataframe = spark.read.csv("/src/resources/file.csv") ... <br/>#Some transformations on complex_dataframe <br/>...</span></pre><p id="8cfe" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">一旦您在运行时发现内存问题，您只需调用来清除数据帧:</p><pre class="ix iy iz ja fd lh kj li lj aw lk bi"><span id="f185" class="ll kl hh kj b fi lm ln l lo lp">complex_dataframe = clear_computation_graph(complex_dataframe, spark_session)</span></pre><p id="1a53" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这就是你所需要的！然后，您可以继续在数据帧上运行转换。</p><h1 id="3662" class="kk kl hh bd km kn ko kp kq kr ks kt ku in kv io kw iq kx ir ky it kz iu la lb bi translated">非常感谢我在第一资本的同事</h1><p id="66d3" class="pw-post-body-paragraph ji jj hh jk b jl lc ii jn jo ld il jq jr le jt ju jv lf jx jy jz lg kb kc kd ha bi translated">第一资本拥有不可思议的开发人员和工程师。我有幸与他们中的一些人一起工作，并在他们的关注、经验和指导下找到了这个解决方案。我特别要感谢Robin Neufeld和Cleland Loszewski在这方面的帮助。谢谢大家！</p></div><div class="ab cl mf mg go mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ha hb hc hd he"><p id="2d76" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">披露声明:2021资本一。观点是作者个人的观点。除非本帖中另有说明，否则Capital One不隶属于所提及的任何公司，也不被这些公司认可。使用或展示的所有商标和其他知识产权是其各自所有者的财产。</em></p><p id="f802" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">最初发表于</em><a class="ae kf" href="https://www.capitalone.com/tech/software-engineering/pyspark-performance-optimization-local-file/" rel="noopener ugc nofollow" target="_blank">T5【https://www.capitalone.com】</a><em class="ke">。</em></p></div></div>    
</body>
</html>