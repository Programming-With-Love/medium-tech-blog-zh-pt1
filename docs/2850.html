<html>
<head>
<title>Recurrent Neural Networks (RNN) Tutorial — Analyzing Sequential Data Using TensorFlow In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络(RNN)教程-使用Python中的张量流分析序列数据</h1>
<blockquote>原文：<a href="https://medium.com/edureka/recurrent-neural-networks-df945afd7441?source=collection_archive---------1-----------------------#2018-11-28">https://medium.com/edureka/recurrent-neural-networks-df945afd7441?source=collection_archive---------1-----------------------#2018-11-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/ee1333ea56b08fd34c999cd187aca5a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*n8oJhwXBRC40wvzsqsYfcA.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">Recurrent Neural Networks — Edureka</figcaption></figure><p id="dcdd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在本文中，让我们讨论递归神经网络工作背后的概念。递归神经网络在图像和视频识别、音乐创作和机器翻译方面有着广泛的应用<strong class="ir hi"/>。</p><p id="c8bf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将检验以下概念:</p><ul class=""><li id="ad68" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">为什么不是前馈网络？</li><li id="6787" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">什么是递归神经网络？</li><li id="fe56" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">如何训练递归神经网络？</li><li id="cf21" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">消失和爆炸渐变</li><li id="1c9d" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">长短期记忆(LSTM)网络</li><li id="c031" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">LSTM用例</li></ul><h1 id="4eff" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">为什么不是前馈网络？</h1><p id="b197" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">考虑一个<strong class="ir hi">图像分类</strong>用例，其中<strong class="ir hi">训练</strong>神经网络<strong class="ir hi">对各种<strong class="ir hi">动物的图像</strong>进行分类。</strong></p><p id="529c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，假设你<strong class="ir hi">在一张<strong class="ir hi">猫</strong>或<strong class="ir hi">狗的<strong class="ir hi">图像</strong>中给</strong>喂食，</strong>网络实际上提供了一张<strong class="ir hi">输出</strong>，上面分别有一张<strong class="ir hi">对应标签</strong>给猫或狗<strong class="ir hi">的图像。</strong></p><p id="eb71" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请考虑下图:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es le"><img src="../Images/1d75d378a546f3ac1312a4a1e6ee98df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*kPW3NIPp4sIqOBFmPxLpOQ.png"/></div></figure><p id="01ed" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，作为<strong class="ir hi">象</strong>的<strong class="ir hi">第一输出</strong>将不会影响作为<strong class="ir hi">狗的<strong class="ir hi">先前输出</strong>的<strong class="ir hi">。</strong>这意味着<strong class="ir hi">‘t’</strong>时刻的输出与<strong class="ir hi">‘t-1’</strong>时刻的输出<strong class="ir hi">无关。</strong></strong></p><p id="81e2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑这个<strong class="ir hi">场景</strong>，其中您将要求<strong class="ir hi">使用<strong class="ir hi">先前获得的输出</strong>中的</strong></p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/989e18ec36e9eb5af9e9253607dbf47f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ex5wdP7f896HuF_mZRXoVA.png"/></div></div></figure><p id="0432" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这个概念类似于<strong class="ir hi">读</strong>一本<strong class="ir hi">书。</strong>随着<strong class="ir hi">进入每一页</strong>，你需要<strong class="ir hi">理解<strong class="ir hi">前几页</strong>的</strong>才能让<strong class="ir hi">完全理解<strong class="ir hi">信息</strong>在大多数<strong class="ir hi">情况下。</strong></strong></p><p id="8fce" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">利用<strong class="ir hi">前馈网络</strong>，在时间<strong class="ir hi">‘t+1’</strong>的<strong class="ir hi">新</strong>输出与时间t、t-1或t-2的输出<strong class="ir hi">没有关系。</strong></p><p id="cd9a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，当<strong class="ir hi">预测</strong>一个句子中的<strong class="ir hi">单词</strong>时，前馈网络<strong class="ir hi">不能使用</strong>，因为它与<strong class="ir hi">之前的</strong>组<strong class="ir hi">单词没有<strong class="ir hi">绝对关系。</strong></strong></p><p id="0789" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是，有了<strong class="ir hi">递归神经网络，</strong>这个挑战可以被<strong class="ir hi">克服。</strong></p><p id="bd09" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请考虑下图:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/c37bb9a020bb563eb496885266fd3ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Hq7bCQwITA_-mD9ki3uQPA.png"/></div></figure><p id="9fad" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，我们在<strong class="ir hi">‘t-1’</strong>处有<strong class="ir hi">某些输入</strong>，这些输入<strong class="ir hi">被</strong>馈入网络。这些<strong class="ir hi">输入</strong>也将在时间‘t-1’导致<strong class="ir hi">相应的输出</strong>。</p><p id="f4c8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在<strong class="ir hi">下一个时间戳，来自<strong class="ir hi">前一个</strong>输入‘t-1’的</strong>信息沿着与在‘t’的<strong class="ir hi">输入</strong>一起被提供<strong class="ir hi">，以最终<strong class="ir hi">也在‘t’提供</strong>输出<strong class="ir hi"/>。</strong></p><p id="56e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">此过程<strong class="ir hi">重复，</strong>确保<strong class="ir hi">最新输入</strong>被<strong class="ir hi">知晓</strong>并且可以使用从先前<strong class="ir hi">时间戳</strong>获得的<strong class="ir hi">信息</strong>。</p><p id="0c5f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，在这篇<strong class="ir hi">递归神经网络</strong>文章中，我们需要看看<strong class="ir hi">什么是</strong>递归神经网络(RNNs) <strong class="ir hi">。</strong></p><h1 id="d042" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">什么是递归神经网络？</h1><p id="61ca" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">递归网络是一种<strong class="ir hi">类型</strong>的<strong class="ir hi">人工神经网络</strong>，设计用于<strong class="ir hi">识别数据序列中的模式</strong>，如<strong class="ir hi">文本、</strong>基因组、笔迹、口语词、数字时间<strong class="ir hi">系列</strong>传感器发出的数据、<strong class="ir hi">股票市场</strong>和<strong class="ir hi">政府机构。</strong></p><p id="07b4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了更清楚，考虑下面的<strong class="ir hi">类比:</strong></p><p id="ab42" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你定期去<strong class="ir hi">健身房</strong><strong class="ir hi"/>并且<strong class="ir hi">教练</strong>已经给了你以下<strong class="ir hi">时间表</strong>供你锻炼:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/31a1d68b6715c1645bbbeae28ad8be0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*dzBFGdFLj2sHr1Z3blAaNQ.png"/></div></figure><p id="6f03" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">注意，所有这些<strong class="ir hi">练习</strong>都是每周按顺序<strong class="ir hi">重复<strong class="ir hi"/>。</strong>首先，让我们使用一个<strong class="ir hi">前馈网络</strong>来尝试和<strong class="ir hi">预测</strong>锻炼的类型。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lq"><img src="../Images/ae46f077014c0849b65dd9358182d017.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*I7KwcO-tTc0To2ch8xRBVQ.png"/></div></figure><p id="c027" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输入为<strong class="ir hi">日、月、</strong>和<strong class="ir hi">健康状态。</strong>必须使用这些输入来<strong class="ir hi">训练</strong>神经网络，以便为我们提供</p><p id="9988" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与<strong class="ir hi">演习中的<strong class="ir hi">预测</strong>。</strong></p><p id="3b7f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然而，考虑到输入，这将<strong class="ir hi">不是非常精确</strong>。为了<strong class="ir hi">解决</strong> <strong class="ir hi">这个问题，</strong>我们可以利用<strong class="ir hi">递归神经网络</strong>的概念，如下所示:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lr"><img src="../Images/b7e7ff7ef52982d00b97259d08fb4867.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*du0pKFlYM4neKOVM2HHoyg.png"/></div></figure><p id="decb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这种情况下，将<strong class="ir hi">输入</strong>视为前一天在<strong class="ir hi">完成的<strong class="ir hi">锻炼</strong>。</strong></p><p id="b0ae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，如果你昨天做了肩部训练，你今天可以做二头肌训练，这也是第22周休息的T21。</p><p id="226e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是，如果您碰巧在健身房错过了<strong class="ir hi"/><strong class="ir hi"/>，那么来自<strong class="ir hi">之前参加的时间戳</strong>的数据可以被<strong class="ir hi">视为</strong>，如下所示:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/63627ca6091ccd44e78252fbd152418f.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*EEwFucZmT6Z6_wUUtArkfA.png"/></div></figure><p id="711e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果一个<strong class="ir hi">模型</strong>基于数据被训练，它可以<strong class="ir hi">从<strong class="ir hi">先前的练习中获得</strong>，</strong>模型的输出将<strong class="ir hi">非常精确。</strong></p><p id="d953" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">总而言之，让我们用<strong class="ir hi">将<strong class="ir hi">数据</strong>转换成<strong class="ir hi">向量。</strong>嗯，<strong class="ir hi">什么是矢量？</strong></strong></p><p id="c0a2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">向量</strong>是输入到<strong class="ir hi">模型的<strong class="ir hi">数字</strong>如果<strong class="ir hi">做了</strong>练习</strong>或<strong class="ir hi">没有做，则</strong>到<strong class="ir hi">表示</strong>。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lt"><img src="../Images/4fd8faebf315a175c431149cf3c6a039.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*8dqWY4fMcw5e4lVLEZNyXg.png"/></div></figure><p id="9a29" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，如果您有一个<strong class="ir hi">肩部练习，</strong>对应的<strong class="ir hi">节点</strong>将是<strong class="ir hi">‘1’</strong>，<strong class="ir hi">练习</strong>的<strong class="ir hi">休息</strong>节点将被<strong class="ir hi">映射到<strong class="ir hi">‘0’。</strong></strong></p><p id="02dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们来看看<strong class="ir hi">神经网络的<strong class="ir hi">工作</strong>背后的<strong class="ir hi">数学</strong>。</strong>考虑以下图表:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/15676193b6531434f1df84e76427853a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MplrorEyhCPnfHB5qG5w1w.png"/></div></div></figure><p id="1434" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将<strong class="ir hi">‘w’</strong>视为<strong class="ir hi">权重矩阵</strong>，将<strong class="ir hi">‘b’</strong>视为<strong class="ir hi">偏差:</strong></p><p id="8ec3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在时间<strong class="ir hi"> t=0时，</strong>输入为<strong class="ir hi">【x0】</strong>，任务是计算出什么是<strong class="ir hi">【h0】。</strong>将<strong class="ir hi"> t=0 </strong>代入<strong class="ir hi">方程</strong>，得到函数<strong class="ir hi"> h(t)值。</strong>接下来，当应用于<strong class="ir hi">新公式时，使用<strong class="ir hi">先前计算的值</strong>找出<strong class="ir hi">‘y0’</strong>的值。</strong></p><p id="bbf5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这个过程是<strong class="ir hi">重复</strong>到<strong class="ir hi">模型中所有<strong class="ir hi">时间戳</strong>的</strong>到<strong class="ir hi">列车</strong>的一个<strong class="ir hi">模型。</strong></p><p id="867c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">那么，<strong class="ir hi"/>递归神经网络<strong class="ir hi">是如何训练的呢？</strong></p><h1 id="e6e8" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">训练递归神经网络</h1><p id="57d2" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">递归神经网络使用反向传播算法进行训练，<strong class="ir hi">但对于每个<strong class="ir hi">时间戳，<strong class="ir hi">应用</strong>。</strong>俗称<strong class="ir hi">穿越时间的反向传播(BTT)。</strong></strong></p><p id="a5e6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">反向传播有一些问题，如:</p><ul class=""><li id="1c51" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi">消失渐变</strong></li><li id="0570" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">爆炸渐变</strong></li></ul><p id="1745" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们考虑一下每一个因素，以了解正在发生的事情</p><h2 id="a11a" class="lu kc hh bd kd lv lw lx kh ly lz ma kl ja mb mc kp je md me kt ji mf mg kx mh bi translated">消失梯度</h2><p id="6832" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">当使用<strong class="ir hi">反向传播</strong>时，<strong class="ir hi">的目标</strong>是<strong class="ir hi">计算</strong>误差<strong class="ir hi">实际上是通过<strong class="ir hi">找出</strong>实际输出</strong>和<strong class="ir hi">模型输出</strong>之间的<strong class="ir hi">差值</strong>并将其提高到2的幂。</p><p id="9ca1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请考虑下图:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/20adb1372cdf414bc5ce33cda3b85050.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*sI3oWfA3_qRAq3nJbSO0Dw.png"/></div></figure><p id="1b57" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计算出<strong class="ir hi">误差后，相对于<strong class="ir hi">重量</strong>中的<strong class="ir hi">变化</strong>计算出</strong>中的<strong class="ir hi">变化</strong>。但是随着每个<strong class="ir hi">学习</strong>速率的增加，这个速率必须乘以<strong class="ir hi"/>。</p><p id="b9a7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，<strong class="ir hi">学习率</strong>与变化<strong class="ir hi">的<strong class="ir hi">乘积</strong>将</strong>引导至<strong class="ir hi">重量的<strong class="ir hi">实际变化</strong>值。</strong></p><p id="e026" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如图<strong class="ir hi">所示，对于每个训练迭代，权重<strong class="ir hi">的这种变化</strong>被添加到旧的<strong class="ir hi">组的</strong> <strong class="ir hi">权重</strong>中。</strong>这里的问题是当 <strong class="ir hi">中的<strong class="ir hi">变化</strong> <strong class="ir hi">乘以</strong>时，<strong class="ir hi">值</strong>比<strong class="ir hi">小很多。</strong></strong></p><p id="5c80" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设你正在<strong class="ir hi">预测</strong>一个<strong class="ir hi">句子</strong>说，<strong class="ir hi">“我要去</strong> <strong class="ir hi">法国”</strong>而你想预测<strong class="ir hi">“我要去法国，那里说的语言是_ _ _ _ _”</strong></p><p id="03dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">大量的迭代</strong>将导致新的权重<strong class="ir hi">可以忽略不计</strong>，这导致<strong class="ir hi">权重没有</strong>被更新<strong class="ir hi">。</strong></p><h2 id="b92e" class="lu kc hh bd kd lv lw lx kh ly lz ma kl ja mb mc kp je md me kt ji mf mg kx mh bi translated">爆炸梯度</h2><p id="7d81" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">爆炸梯度的工作与类似<strong class="ir hi">但是<strong class="ir hi">的重量在这里</strong>剧烈变化<strong class="ir hi"/>而不是<strong class="ir hi">可以忽略的变化。</strong>注意下图中<strong class="ir hi">的微小变化</strong>:</strong></p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/7d903b5dff39c3977051e551fc62e5ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*KtG0EFWX8Fi-WH_vLvbUfA.png"/></div></figure><p id="af6c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们需要<strong class="ir hi">克服这两个</strong>，首先是<strong class="ir hi">挑战</strong>的<strong class="ir hi">位</strong>。考虑下面的图表:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/9c93447dcb8bcb0d93c74a04424beb21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPEtqI7tEocLPShWHsZaTw.png"/></div></div></figure><p id="9c9b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">继续这篇关于递归神经网络的博客，我们将<strong class="ir hi">进一步讨论LSTM网络。</strong></p><h1 id="9a10" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">长短期记忆网络</h1><p id="c730" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">长短期记忆网络通常被称为“LSTMs”。</p><p id="457e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它们是一种<strong class="ir hi">特殊类型的</strong>递归神经网络，能够<strong class="ir hi">或</strong>学习长期依赖关系。</p><p id="4616" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">什么是长期依赖？</strong></p><p id="3f3a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">很多时候，模型中只需要<strong class="ir hi">最近的数据</strong>来<strong class="ir hi">执行操作。</strong>但是从<strong class="ir hi">数据</strong>中可能会有一个<strong class="ir hi">要求</strong>，该数据是在过去<strong class="ir hi">中<strong class="ir hi">获得的</strong>。</strong></p><p id="ba49" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们看看下面的例子:</p><p id="f9b0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑一个<strong class="ir hi">语言模型</strong>试图根据前面的单词预测<strong class="ir hi">下一个单词</strong>。如果我们试图<strong class="ir hi">预测</strong>句子中的<strong class="ir hi">最后一个词</strong>说<strong class="ir hi">“云在天上”</strong>。</p><p id="7880" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里的上下文<strong class="ir hi">非常简单</strong>，最后一个单词始终以<strong class="ir hi">天空</strong>结束。在这种情况下，<strong class="ir hi">过去信息</strong>和<strong class="ir hi">当前需求</strong>之间的差距可以通过使用<strong class="ir hi">递归神经网络<strong class="ir hi">非常容易地<strong class="ir hi">桥接</strong>。</strong></strong></p><p id="2bf2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，像<strong class="ir hi">消失</strong>和<strong class="ir hi">爆炸梯度</strong>不存在<strong class="ir hi">的问题</strong>，这使得LSTM网络可以轻松处理<strong class="ir hi">长期依赖</strong>。</p><p id="9819" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">LSTM有一个<strong class="ir hi">链状的</strong>神经网络层。在标准的递归神经网络中，重复模块由一个<strong class="ir hi">单一函数</strong>组成，如下图所示:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mi"><img src="../Images/d211820e5d8333cd30ac32f03c297717.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*9y_qpq-ZOhpnoZpKzRTmaA.png"/></div></figure><p id="9331" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如上图所示，该层中有一个<strong class="ir hi">双曲正切函数</strong>。该功能是一个<strong class="ir hi">挤压功能。</strong>那么，<strong class="ir hi">什么是挤压功能呢？</strong></p><p id="6451" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">它是一个<strong class="ir hi">功能</strong>，基本上用于-1到+1 的<strong class="ir hi">范围内，并根据<strong class="ir hi">输入来<strong class="ir hi">操纵</strong>的<strong class="ir hi">值</strong>。</strong></strong></p><p id="2894" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们考虑LSTM网络的<strong class="ir hi">结构</strong>:</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/7acb0e2c7598d86df53d2f1538ca3e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*lxCd3xU-gLdAb_IVf52m4A.png"/></div></figure><p id="2b35" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如图所示，对于LSTM网络，各层中的每个功能都有自己的结构。单元状态是图中的水平线，它的作用类似于传送带，在数据通道上线性传送某些数据。</p><p id="34e1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们考虑一种循序渐进的方法来更好地理解LSTM网络。</p><h2 id="b53a" class="lu kc hh bd kd lv lw lx kh ly lz ma kl ja mb mc kp je md me kt ji mf mg kx mh bi translated"><strong class="ak">第一步:</strong></h2><p id="3c7b" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated"><strong class="ir hi"> LSTM </strong>的第一步是<strong class="ir hi">识别</strong>那些<strong class="ir hi">不需要</strong>并将<strong class="ir hi">从<strong class="ir hi">单元状态中丢弃</strong>的信息。</strong>这个决定是由一个<strong class="ir hi">s形层</strong>称为<strong class="ir hi">忘浇口</strong>层<strong class="ir hi">做出的。</strong></p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/76c0620244f1a623052850c9531044c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*TmepLUuEwKth2ZYmUGK6zg.png"/></div></figure><p id="d8ae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上面的<strong class="ir hi">高亮层</strong>就是前面提到的<strong class="ir hi">的<strong class="ir hi">乙状结肠层</strong>。</strong></p><p id="0359" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过考虑<strong class="ir hi">新输入</strong>和<strong class="ir hi">先前时间戳</strong>，最终<strong class="ir hi">将</strong>引向<strong class="ir hi">输出</strong>一个在0和1 之间的数字<strong class="ir hi">用于该<strong class="ir hi">单元状态中的每个</strong>数字<strong class="ir hi">，从而<strong class="ir hi">计算</strong>完成<strong class="ir hi">。</strong></strong></strong></p><p id="0be7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">作为典型的二进制，<strong class="ir hi"> 1 </strong>表示向<strong class="ir hi"> kee </strong> p信元状态，而<strong class="ir hi"> 0 </strong>表示向<strong class="ir hi"> trash </strong> it信元状态。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mk"><img src="../Images/b47abac08509bb364c03eadad84b410d.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*yfIBK5Xnpx308uo3Msu5GQ.png"/></div></figure><p id="0e72" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑<strong class="ir hi">性别分类，</strong>在使用<strong class="ir hi">网络时，考虑<strong class="ir hi">最新</strong>和<strong class="ir hi">正确的性别</strong>真的很重要。</strong></p><h2 id="6bc2" class="lu kc hh bd kd lv lw lx kh ly lz ma kl ja mb mc kp je md me kt ji mf mg kx mh bi translated"><strong class="ak">第二步:</strong></h2><p id="db9a" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">下一步是<strong class="ir hi">决定，</strong>什么<strong class="ir hi">新信息</strong>我们要将<strong class="ir hi">储存</strong>在单元格状态。整个过程包括以下步骤:</p><ul class=""><li id="0974" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">一个称为“输入门层”的<strong class="ir hi"> sigmoid层</strong>决定<strong class="ir hi">哪些值</strong>将被<strong class="ir hi">更新。</strong></li><li id="600a" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">双曲正切层</strong>创建一个<strong class="ir hi">新候选值</strong>的<strong class="ir hi">向量</strong>，它可以被<strong class="ir hi">添加到状态</strong>。</li></ul><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es ml"><img src="../Images/8d50b9004ce1e22d2517d6a6608e5e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*boIcqQefGpXi7TQT2GFHiw.png"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/0c25f7a013489fe1d82f4d50c6c4eddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*jp8kPBR2Gcoa3B06tAzFHA.png"/></div></figure><p id="43e9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">来自<strong class="ir hi">先前时间戳</strong>的输入和新输入通过<strong class="ir hi"> sigmoid函数</strong>被<strong class="ir hi">传递</strong>，该函数给出值<strong class="ir hi"> i(t)。</strong>该值然后被<strong class="ir hi">乘以c(t) </strong>，然后被添加到<strong class="ir hi">单元状态。</strong></p><p id="af54" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在下一步中，这些<strong class="ir hi">两个</strong>被<strong class="ir hi">组合</strong>到<strong class="ir hi">更新</strong>到<strong class="ir hi">状态。</strong></p><h2 id="2646" class="lu kc hh bd kd lv lw lx kh ly lz ma kl ja mb mc kp je md me kt ji mf mg kx mh bi translated"><strong class="ak">第三步:</strong></h2><p id="b97a" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">现在，我们将<strong class="ir hi">更新</strong>旧的<strong class="ir hi">单元状态ct1，</strong>进入<strong class="ir hi">新的</strong>单元状态<strong class="ir hi"> Ct。</strong></p><p id="5046" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，我们<strong class="ir hi">用f(t)乘</strong>旧的状态<strong class="ir hi">(ct1)</strong>，<strong class="ir hi">忘记</strong>我们<strong class="ir hi">决定</strong>早于<strong class="ir hi">离开</strong>的事情。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mn"><img src="../Images/fc5d4d6efce98aa2153c0fb48bc9f970.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*ocx62MLYPieZ18R0HG7yiw.png"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mm"><img src="../Images/0bba340cf98109656c093ecbcb7aa481.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*JiZfq61LoXNi1PQyb24MHw.png"/></div></figure><p id="b460" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后，我们<strong class="ir hi">将I _ t * c \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\</strong></p><p id="0d8c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第二步，我们决定使用<strong class="ir hi">数据</strong>中的<strong class="ir hi">数据</strong>，该数据仅在<strong class="ir hi">阶段需要。</strong></p><p id="216b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第三步，我们实际上<strong class="ir hi">实现</strong>它。</p><p id="b1fd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在之前讨论的<strong class="ir hi">的语言案例示例中，</strong>处的<strong class="ir hi">旧性别</strong>将被<strong class="ir hi">丢弃</strong>，而<strong class="ir hi">新性别</strong>将被<strong class="ir hi">考虑。</strong></p><h2 id="2b2a" class="lu kc hh bd kd lv lw lx kh ly lz ma kl ja mb mc kp je md me kt ji mf mg kx mh bi translated"><strong class="ak">第四步:</strong></h2><p id="ee3e" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们将运行一个<strong class="ir hi"> sigmoid层</strong>，它决定了<strong class="ir hi">单元状态</strong>的哪些<strong class="ir hi">部分</strong>将被<strong class="ir hi">输出。</strong></p><p id="34f0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后，我们通过<strong class="ir hi"> tanh </strong>设置<strong class="ir hi">单元状态</strong>(将值推到1和1之间)</p><p id="b544" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">后来，我们<strong class="ir hi">将</strong>乘以<strong class="ir hi">s形门</strong>的<strong class="ir hi">输出</strong>，这样我们只输出我们决定的<strong class="ir hi">部分</strong>。</p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es mo"><img src="../Images/47a28f286e9afae56744857deb364687.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*-TV7vgUVcMlWwiFCux3hCA.png"/></div></figure><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/63f56ba3410feb62facec4ef3eb948b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*dusm2RXHPqSTxZJb18lWWg.png"/></div></figure><p id="c6dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这一步中的<strong class="ir hi">计算</strong>非常简单<strong class="ir hi">最终<strong class="ir hi">导致<strong class="ir hi">输出。</strong></strong></strong></p><p id="2143" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然而，<strong class="ir hi">输出</strong>仅由<strong class="ir hi">输出</strong>组成，在<strong class="ir hi">之前的</strong>步骤中决定将<strong class="ir hi">结转</strong>，而不是所有的<strong class="ir hi">输出</strong>。</p><p id="de25" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">总结所有4个步骤:</p><p id="c5a2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在<strong class="ir hi">第一个</strong>步骤中，<strong class="ir hi">我们找出了需要丢弃的东西。</strong></p><p id="deb4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第二个</strong>步骤包括向网络添加新的输入。</p><p id="3099" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第三个</strong>步骤是<strong class="ir hi">合并先前获得的输入，以生成新的单元状态。</strong></p><p id="bb38" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，我们按照要求达到了<strong class="ir hi">的输出。</strong></p><p id="b86a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，让我们考虑一个<strong class="ir hi">有趣的用例。</strong></p><h1 id="f050" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">用例:长短期记忆网络</h1><p id="931c" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们将<strong class="ir hi">考虑到</strong>的用例是<strong class="ir hi">在一个示例短篇故事中预测</strong>的<strong class="ir hi">下一个单词</strong>。</p><p id="4510" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们可以从<strong class="ir hi">向</strong>输入一个<strong class="ir hi"> LSTM </strong>网络开始，该网络具有来自作为<strong class="ir hi">输入</strong>和1个标记符号的3个<strong class="ir hi">符号</strong>的文本的<strong class="ir hi">正确序列</strong>。</p><p id="9015" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最终，神经网络将<strong class="ir hi">学习</strong>以<strong class="ir hi">正确预测</strong>下一个符号<strong class="ir hi">！</strong></p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/bef1037a67ae0263f06d217c4b67ce4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*09R6HkY-080l6C-BiBsTSg.png"/></div></figure><h2 id="ced1" class="lu kc hh bd kd lv lw lx kh ly lz ma kl ja mb mc kp je md me kt ji mf mg kx mh bi translated"><strong class="ak">数据集:</strong></h2><p id="e4d3" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">使用由<strong class="ir hi"> 112个独特符号组成的<strong class="ir hi">样本短篇故事</strong>来训练LSTM。逗号</strong>和<strong class="ir hi">句号</strong>也被<strong class="ir hi">认为</strong>是<strong class="ir hi">的唯一符号</strong>。</p><p id="c7f7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">很久以前，老鼠们开了一个全体会议，考虑采取什么措施来智取它们共同的敌人——猫。有些人这样说，有些人那样说，但最后一只年轻的老鼠站起来说，他有一个建议，他认为会满足的情况。你们都会同意，他说，我们的主要危险在于敌人向我们逼近的狡猾和奸诈的方式。现在，如果我们能收到一些她接近的信号，我们就能轻易地逃离她。因此，我斗胆提议买一个小铃铛，用丝带系在猫的脖子上。通过这种方法，我们应该总是知道她什么时候在附近，当她在附近的时候，我们可以很容易地退休。这个提议得到了普遍的掌声，直到一只老老鼠站起来说，这一切都很好，但是谁来给猫系上铃铛呢？老鼠们面面相觑，谁也不说话。然后老老鼠说提出不可能的补救办法很容易。”</p><h2 id="e156" class="lu kc hh bd kd lv lw lx kh ly lz ma kl ja mb mc kp je md me kt ji mf mg kx mh bi translated"><strong class="ak">培训:</strong></h2><p id="47b2" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我们已经知道<strong class="ir hi">lstm</strong>只能<strong class="ir hi">理解实数。</strong>所以第一个<strong class="ir hi">要求</strong>是<strong class="ir hi">根据<strong class="ir hi">出现的<strong class="ir hi">频率</strong>将</strong>唯一符号转换为<strong class="ir hi">唯一整数</strong>值。</strong></p><p id="d25c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这样做将创建一个<strong class="ir hi">定制的字典</strong>，我们可以<strong class="ir hi">稍后使用它来<strong class="ir hi">映射</strong>这些值。</strong></p><figure class="lf lg lh li fd ii er es paragraph-image"><div class="er es lo"><img src="../Images/c6a9bad6febacb5b7fcceeee3a42d704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*39zXPqRF0nGv_xQihhPoAA.png"/></div></figure><p id="192c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，<strong class="ir hi">某些符号</strong>被映射为<strong class="ir hi">整数</strong>，如图所示。</p><p id="1c12" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">网络将创建一个<strong class="ir hi"> 112元素向量</strong>，该向量由这些唯一整数值中的每一个的<strong class="ir hi">出现</strong>的<strong class="ir hi">概率</strong>组成。</p><p id="c296" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">实现:</strong></p><p id="abdc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">代码使用Tensorflow实现，如下所示:</p><pre class="lf lg lh li fd mq mr ms mt aw mu bi"><span id="e17b" class="lu kc hh mr b fi mv mw l mx my">import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.contrib import rnn<br/>import random<br/>import collections<br/>import time<br/> <br/>start_time = time.time()<br/> <br/>def elapsed(sec):<br/>    if sec&lt;60:<br/>        return str(sec) + " sec"<br/>    elif sec&lt;(60*60): return str(sec/60) + " min" else: return str(sec/(60*60)) + " hr" # Target log path logs_path = '/tmp/tensorflow/rnn_words' writer = tf.summary.FileWriter(logs_path) <br/># Text file containing words for training training_file = 'Story.txt' def read_data(fname): with open(fname) as f: content = f.readlines() content = [x.strip() for x in content] content = [content[i].split() for i in range(len(content))] content = np.array(content) content = np.reshape(content, [-1, ]) return content training_data = read_data(training_file) print("Loaded training data...") def build_dataset(words): count = collections.Counter(words).most_common() dictionary = dict() for word, _ in count: dictionary[word] = len(dictionary) reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) return dictionary, reverse_dictionary dictionary, reverse_dictionary = build_dataset(training_data) vocab_size = len(dictionary) <br/># Parameters learning_rate = 0.001 training_iters = 50000 display_step = 1000 n_input = 3 <br/># number of units in RNN cell n_hidden = 512 <br/># tf Graph input x = tf.placeholder("float", [None, n_input, 1]) y = tf.placeholder("float", [None, vocab_size]) <br/># RNN output node weights and biases weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocab_size])) } biases = { 'out': tf.Variable(tf.random_normal([vocab_size])) } def RNN(x, weights, biases): <br/># reshape to [1, n_input] x = tf.reshape(x, [-1, n_input])<br/># Generate a n_input-element sequence of inputs <br/># (eg. [had] [a] [general] -&gt; [20] [6] [33])<br/>    x = tf.split(x,n_input,1)<br/> <br/>    # 2-layer LSTM, each layer has n_hidden units.<br/>    # Average Accuracy= 95.20% at 50k iter<br/>    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])<br/> <br/>    # 1-layer LSTM with n_hidden units but with lower accuracy.<br/>    # Average Accuracy= 90.60% 50k iter<br/>    # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above<br/>    # rnn_cell = rnn.BasicLSTMCell(n_hidden)<br/> <br/>    # generate prediction<br/>    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)<br/> <br/>    # there are n_input outputs but<br/>    # we only want the last output<br/>    return tf.matmul(outputs[-1], weights['out']) + biases['out']<br/> <br/>pred = RNN(x, weights, biases)<br/> <br/># Loss and optimizer<br/>cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))<br/>optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)<br/> <br/># Model evaluation<br/>correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))<br/> <br/># Initializing the variables<br/>init = tf.global_variables_initializer()<br/> <br/># Launch the graph<br/>with tf.Session() as session:<br/>    session.run(init)<br/>    step = 0<br/>    offset = random.randint(0,n_input+1)<br/>    end_offset = n_input + 1<br/>    acc_total = 0<br/>    loss_total = 0<br/> <br/>    writer.add_graph(session.graph)<br/> <br/>    while step &lt; training_iters: # Generate a minibatch. Add some randomness on selection process. if offset &gt; (len(training_data)-end_offset):<br/>            offset = random.randint(0, n_input+1)<br/> <br/>        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]<br/>        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])<br/> <br/>        symbols_out_onehot = np.zeros([vocab_size], dtype=float)<br/>        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0<br/>        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])<br/> <br/>        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \<br/>                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})<br/>        loss_total += loss<br/>        acc_total += acc<br/>        if (step+1) % display_step == 0:<br/>            print("Iter= " + str(step+1) + ", Average Loss= " + \<br/>                  "{:.6f}".format(loss_total/display_step) + ", Average Accuracy= " + \<br/>                  "{:.2f}%".format(100*acc_total/display_step))<br/>            acc_total = 0<br/>            loss_total = 0<br/>            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]<br/>            symbols_out = training_data[offset + n_input]<br/>            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]<br/>            print("%s - [%s] vs [%s]" % (symbols_in,symbols_out,symbols_out_pred))<br/>        step += 1<br/>        offset += (n_input+1)<br/>    print("Optimization Finished!")<br/>    print("Elapsed time: ", elapsed(time.time() - start_time))<br/>    print("Run on command line.")<br/>    print("\ttensorboard --logdir=%s" % (logs_path))<br/>    print("Point your web browser to: <a class="ae mz" href="http://localhost:6006/" rel="noopener ugc nofollow" target="_blank">http://localhost:6006/</a>")<br/>    while True:<br/>        prompt = "%s words: " % n_input<br/>        sentence = input(prompt)<br/>        sentence = sentence.strip()<br/>        words = sentence.split(' ')<br/>        if len(words) != n_input:<br/>            continue<br/>        try:<br/>            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]<br/>            for i in range(32):<br/>                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])<br/>                onehot_pred = session.run(pred, feed_dict={x: keys})<br/>                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())<br/>                sentence = "%s %s" % (sentence,reverse_dictionary[onehot_pred_index])<br/>                symbols_in_keys = symbols_in_keys[1:]<br/>                symbols_in_keys.append(onehot_pred_index)<br/>            print(sentence)<br/>        except:<br/>            print("Word not in dictionary")</span></pre><p id="6ff0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="mp">这就把我们带到了“递归神经网络”这篇文章的结尾。我希望这篇文章对你有所帮助，并增加了你的知识价值。</em></p><p id="6864" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你希望查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，那么你可以参考<a class="ae mz" href="https://www.edureka.co/blog/?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank"> Edureka的官方网站。</a></p><p id="27e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释深度学习的各个其他方面。</p><blockquote class="na nb nc"><p id="1554" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">1.<a class="ae mz" rel="noopener" href="/edureka/tensorflow-tutorial-ba142ae96bca"> TensorFlow教程</a></p><p id="ff73" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">2.<a class="ae mz" rel="noopener" href="/edureka/pytorch-tutorial-9971d66f6893"> PyTorch教程</a></p><p id="309d" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">3.<a class="ae mz" rel="noopener" href="/edureka/perceptron-learning-algorithm-d30e8b99b156">感知器学习算法</a></p><p id="1fd3" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">4.<a class="ae mz" rel="noopener" href="/edureka/neural-network-tutorial-2a46b22394c9">神经网络教程</a></p><p id="a136" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">5.<a class="ae mz" rel="noopener" href="/edureka/backpropagation-bd2cf8fdde81">什么是反向传播？</a></p><p id="7374" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">6.<a class="ae mz" rel="noopener" href="/edureka/convolutional-neural-network-3f2c5b9c4778">卷积神经网络</a></p><p id="fcc8" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">7.<a class="ae mz" rel="noopener" href="/edureka/capsule-networks-d7acd437c9e">胶囊神经网络</a></p><p id="7120" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">8.<a class="ae mz" rel="noopener" href="/edureka/recurrent-neural-networks-df945afd7441"/><a class="ae mz" rel="noopener" href="/edureka/tensorflow-object-detection-tutorial-8d6942e73adc">tensor flow中的物体检测</a></p><p id="6efc" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">9.<a class="ae mz" rel="noopener" href="/edureka/autoencoders-tutorial-cfdcebdefe37">自动编码器教程</a></p><p id="756a" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">10.<a class="ae mz" rel="noopener" href="/edureka/restricted-boltzmann-machine-tutorial-991ae688c154">受限玻尔兹曼机教程</a></p><p id="5035" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">11.<a class="ae mz" rel="noopener" href="/edureka/pytorch-vs-tensorflow-252fc6675dd7"> PyTorch vs TensorFlow </a></p><p id="0375" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">12.<a class="ae mz" rel="noopener" href="/edureka/deep-learning-with-python-2adbf6e9437d">用Python进行深度学习</a></p><p id="2fb0" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">13.<a class="ae mz" rel="noopener" href="/edureka/artificial-intelligence-tutorial-4257c66f5bb1">人工智能教程</a></p><p id="e24f" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">14.<a class="ae mz" rel="noopener" href="/edureka/tensorflow-image-classification-19b63b7bfd95">张量流图像分类</a></p><p id="2a2c" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">15.<a class="ae mz" rel="noopener" href="/edureka/artificial-intelligence-applications-7b93b91150e3">人工智能应用</a></p><p id="6e6f" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">16.<a class="ae mz" rel="noopener" href="/edureka/become-artificial-intelligence-engineer-5ac2ede99907">如何成为一名人工智能工程师？</a></p><p id="05c4" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">17.<a class="ae mz" rel="noopener" href="/edureka/q-learning-592524c3ecfc">问学习</a></p><p id="e21a" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">18.<a class="ae mz" rel="noopener" href="/edureka/apriori-algorithm-d7cc648d4f1e"> Apriori算法</a></p><p id="5969" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">19.<a class="ae mz" rel="noopener" href="/edureka/introduction-to-markov-chains-c6cb4bcd5723">马尔可夫链与Python </a></p><p id="bd13" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">20.<a class="ae mz" rel="noopener" href="/edureka/artificial-intelligence-algorithms-fad283a0d8e2">人工智能算法</a></p><p id="a686" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">21.<a class="ae mz" rel="noopener" href="/edureka/best-laptop-for-machine-learning-a4a5f8ba5b">机器学习的最佳笔记本电脑</a></p><p id="a504" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">22.<a class="ae mz" rel="noopener" href="/edureka/top-artificial-intelligence-tools-36418e47bf2a">12大人工智能工具</a></p><p id="f5ca" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">23.<a class="ae mz" rel="noopener" href="/edureka/artificial-intelligence-interview-questions-872d85387b19">人工智能(AI)面试问题</a></p><p id="2e6e" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">24.<a class="ae mz" rel="noopener" href="/edureka/theano-vs-tensorflow-15f30216b3bc"> Theano vs TensorFlow </a></p><p id="3153" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">25.<a class="ae mz" rel="noopener" href="/edureka/what-is-a-neural-network-56ae7338b92d">什么是神经网络？</a></p><p id="a537" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">26.<a class="ae mz" rel="noopener" href="/edureka/pattern-recognition-5e2d30ab68b9">模式识别</a></p><p id="d34d" class="ip iq mp ir b is it iu iv iw ix iy iz nd jb jc jd ne jf jg jh nf jj jk jl jm ha bi translated">27.<a class="ae mz" rel="noopener" href="/edureka/alpha-beta-pruning-in-ai-b47ee5500f9a">人工智能中的阿尔法贝塔剪枝</a></p></blockquote></div><div class="ab cl ng nh go ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ha hb hc hd he"><p id="f766" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="mp">原载于2018年11月28日</em><a class="ae mz" href="https://www.edureka.co/blog/recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank"><em class="mp">www.edureka.co</em></a><em class="mp">。</em></p></div></div>    
</body>
</html>