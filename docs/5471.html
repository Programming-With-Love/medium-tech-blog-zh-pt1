<html>
<head>
<title>Lessons From AlphaZero (part 4): Improving the Training Target</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AlphaZero的教训(第四部分):提高培训目标</h1>
<blockquote>原文：<a href="https://medium.com/oracledevs/lessons-from-alphazero-part-4-improving-the-training-target-6efba2e71628?source=collection_archive---------0-----------------------#2018-06-27">https://medium.com/oracledevs/lessons-from-alphazero-part-4-improving-the-training-target-6efba2e71628?source=collection_archive---------0-----------------------#2018-06-27</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/48cbfb3ff35f2ad0d6756c20d5cc565e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDuPnEf2GbYQbWIuvvlxIw.jpeg"/></div></div></figure><p id="c804" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是我们关于实施AlphaZero的经验教训系列的第四部分。查看 <a class="ae jo" rel="noopener" href="/oracledevs/lessons-from-implementing-alphazero-7e36e9054191"> <em class="jn">第一部分</em></a><em class="jn"/><a class="ae jo" rel="noopener" href="/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68"><em class="jn">第二部分</em> </a> <em class="jn">，</em> <a class="ae jo" rel="noopener" href="/oracledevs/lessons-from-alphazero-part-3-parameter-tweaking-4dceb78ed1e5"> <em class="jn">第三部分</em> </a> <em class="jn">。</em></p><p id="264f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们AlphaZero方法中的一项创新涉及网络价值输出的目标。我们已经为网络的价值输出找到了一个替代的训练目标，它优于AlphaZero方法。</p><h2 id="8984" class="jp jq hh bd jr js jt ju jv jw jx jy jz ja ka kb kc je kd ke kf ji kg kh ki kj bi translated">培训目标审查</h2><p id="900a" class="pw-post-body-paragraph ip iq hh ir b is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji ko jk jl jm ha bi translated">原始论文为神经网络指定了两个输出:策略输出和值输出。预测当前游戏位置中的最佳移动的策略输出对照由<strong class="ir hi"> π、</strong>表示的值进行训练，该值是基于自玩期间由MCTS搜索累积的访问计数的概率分布。预测游戏结果的值输出对照值<strong class="ir hi"> z </strong>进行训练，该值是从当前玩家的角度看自玩游戏的结果。换句话说，如果当前玩家最终赢得了对位置进行采样的游戏，则用于训练的<strong class="ir hi"> z </strong>值将为+1，而如果当前玩家输掉了游戏，则<strong class="ir hi"> z </strong>值将为-1。平局的比赛得分为0。如果此描述令人困惑，下图可能会有所帮助:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kp"><img src="../Images/df161af4927f011e340f046b9bb45ee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oTtENBrl4x7EZlLYQo0GQA.jpeg"/></div></div></figure><p id="7615" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们解开在AlphaZero自玩过程中使用<strong class="ir hi"> z </strong>生成训练目标的过程。从一个空棋盘开始(玩家1移动)，游戏进行如下:</p><ol class=""><li id="f14f" class="ku kv hh ir b is it iw ix ja kw je kx ji ky jm kz la lb lc bi translated">从当前状态运行800次模拟。</li><li id="9780" class="ku kv hh ir b is ld iw le ja lf je lg ji lh jm kz la lb lc bi translated">根据当前状态的子节点的访问计数生成策略。</li><li id="b153" class="ku kv hh ir b is ld iw le ja lf je lg ji lh jm kz la lb lc bi translated">概率上根据策略下一步棋，产生一个新的状态。</li><li id="970d" class="ku kv hh ir b is ld iw le ja lf je lg ji lh jm kz la lb lc bi translated">为对手执行以上两步。</li><li id="4db4" class="ku kv hh ir b is ld iw le ja lf je lg ji lh jm kz la lb lc bi translated">重复，直到游戏结束。游戏结果在{-1，0，1}。</li></ol><p id="484b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在游戏结束时，每个州都被贴上了政策和游戏结果的标签(注意要否定参与人2要移动的位置的值)。神经网络的两个输出对照该策略(<strong class="ir hi"> π </strong>)和博弈结果(<strong class="ir hi"> z </strong>)进行训练。如果MCTS模拟部分仍然模糊，这里有一个非常详细的解释<a class="ae jo" href="http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="b96c" class="jp jq hh bd jr js jt ju jv jw jx jy jz ja ka kb kc je kd ke kf ji kg kh ki kj bi translated">另一个培训目标</h2><p id="1725" class="pw-post-body-paragraph ip iq hh ir b is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji ko jk jl jm ha bi translated">训练价值输出还有另一个潜在的目标。在MCTS搜索过程中，每个节点通过备份步骤累积游戏的预期结果。这个值被称为节点的<strong class="ir hi"> Q </strong>值，简单地说就是<strong class="ir hi"> W </strong> / <strong class="ir hi"> N </strong>，其中<strong class="ir hi"> W </strong>是在模拟过程中沿着树向上传播的总分数，而<strong class="ir hi"> N </strong>是对节点的访问次数。搜索树的根节点代表游戏中的当前位置，因此它的<strong class="ir hi"> Q </strong>代表游戏从该位置的预期结果。当我们保存基于访问计数的<strong class="ir hi"> π </strong>值时，我们也可以将这个根节点的<strong class="ir hi"> Q </strong>值保存为<strong class="ir hi"> q </strong>，并根据<strong class="ir hi"> q </strong>而不是<strong class="ir hi"> z </strong>来训练网络。使用<strong class="ir hi"> q </strong>生成训练目标的过程如下所示:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kp"><img src="../Images/ec84d7c31d3b49408551f7e479b9a3d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rku_FgdvNhQ_Oiej68Ovaw.jpeg"/></div></div></figure><p id="45d5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">生成这个训练目标的自演步骤与上面列出的五个步骤完全相同。唯一的区别是游戏结束时的标签过程。不是使用游戏结果，而是用策略和根节点的Q值来标记每个状态。针对该策略(<strong class="ir hi"> π </strong>)和Q值(<strong class="ir hi"> q </strong>)训练神经网络的两个输出。</p><p id="d329" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">类似的目标被用于<a class="ae jo" href="https://arxiv.org/abs/1705.08439" rel="noopener ugc nofollow" target="_blank">深度学习和树搜索的快速和慢速思考</a>，有一个关键的区别。他们的MCTS使用完整播放，而AlphaZero使用截断策略，直接备份来自神经网络预测的值，而不是为每次模拟播放游戏。作者建议这个训练目标是一个很好的代表<strong class="ir hi"> z </strong>，但是需要一个更小的数据集才是有意义的。</p><p id="72c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">z和q的区别</strong></p><p id="de88" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当使用<strong class="ir hi"> z </strong>或<strong class="ir hi"> q </strong>进行训练时，我们试图将从MCTS学到的信息编码到网络中，但这两种方法之间存在有意义的差异。针对z的训练试图通过将决赛中的所有模拟浓缩为一个单独的离散值来编码游戏的预期结果:赢、输或平。相比之下，针对<strong class="ir hi"> q </strong>的训练试图仅使用当前位置的800次模拟将游戏的预期结果编码为连续值。</p><p id="5f75" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">基于这些描述，<strong class="ir hi"> z </strong>似乎更胜一筹，但是使用<strong class="ir hi"> z </strong>有一个很大的缺点:每场比赛只有一个结果，并且这个结果会受到随机性的严重影响。例如:想象一下，在游戏的早期，网络采取了正确的行动，但后来却选择了次优的行动，输掉了游戏。在这种情况下，<strong class="ir hi"> z </strong>将为-1，并且训练将错误地将低分与该位置相关联。</p><p id="2c21" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有了足够的训练数据，人们会希望这些错误被正确的比赛所掩盖。不幸的是，完全根除错误是不可能的，因为网络由于其概率策略而在自玩期间进行探索。我们推测这是在30次移动后降低温度对AlphaZero如此重要的原因之一。否则，游戏快结束时移动选择的随机性会损害<strong class="ir hi"> z </strong>的准确性。</p><p id="5a07" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">训练对抗<strong class="ir hi"> q </strong>不会遭遇随机性问题。网络最终输了比赛也没关系。如果模拟给出一个好的移动结果，网络将针对一个正值进行训练。从某种意义上来说，你可以把<strong class="ir hi"> q </strong>看作是800个自我游戏的平均值而不是一个。这些“游戏”是网络的猜测，因此它们不像800个真实的<strong class="ir hi"> z </strong>值那样准确，但它们可以通过一组小得多的自玩游戏实现比<strong class="ir hi"> z </strong>值更高的一致性。</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es li"><img src="../Images/9838c36fd300cc12fd9ad2f5180a66cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Snoau2IHc2pC4M4SG3NgDQ.jpeg"/></div></div></figure><p id="85d1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">不幸的是，<strong class="ir hi"> q </strong>并不是一个完美的解决方案。它可能会遭受一种叫做“T20地平线效应”的东西。当模拟返回肯定结果，但是存在刚好超出搜索范围的致命响应，即在800次模拟期间没有被访问时，这可能发生。另外<strong class="ir hi"> q </strong>对于最初几代训练中的早期移动有些无意义，因为网络不知道如何评估位置。在这种情况下，<strong class="ir hi"> q </strong>保持接近0，可能需要许多代的时间，输出值才会变得有意义。</p><p id="5e3b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">测试新目标</strong></p><p id="c08e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们从上面的分析中得出的结论是,<strong class="ir hi"> z </strong>和<strong class="ir hi"> q </strong>都有工作的潜力，但是每个都有独特的缺点。根据经验，我们可以使用<strong class="ir hi"> q </strong>而不是<strong class="ir hi"> z </strong>来成功训练一个网络。其实对于Connect Four这样的短游戏，对<strong class="ir hi"> q </strong>训练比对<strong class="ir hi"> z </strong>训练效果略好。但是，我们能利用我们的理解来显著改进AlphaZero的方法吗？</p><p id="f77e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的实验表明，我们可以通过一起使用<strong class="ir hi"> q </strong>和<strong class="ir hi"> z </strong>来获得更好的结果。组合<strong class="ir hi"> q </strong>和<strong class="ir hi"> z </strong>的一种方法是对每个示例位置进行平均，并使用该平均值来训练网络。这似乎带来了两者的好处:<strong class="ir hi"> z </strong>有助于抵消<strong class="ir hi"> q </strong>的视界效应，而<strong class="ir hi"> q </strong>有助于抵消<strong class="ir hi"> z </strong>的随机性。另一个有希望的方法是从针对<strong class="ir hi"> z </strong>的训练开始，但是经过几代之后线性下降到<strong class="ir hi"> q </strong>。</p><p id="5c57" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在训练Connect Four的早期周期中，平均和衰减的表现一样好。最终，衰减法能够实现比平均法略低的误差百分比。与单独使用<strong class="ir hi"> z </strong>或<strong class="ir hi"> q </strong>相比，这两种方法的训练速度都有显著提高。下面您可以看到我们的Connect Four模型的前20代使用四个目标中的每一个所实现的错误百分比的图表。在线性衰减的情况下，我们开始对第一代使用100% <strong class="ir hi"> z </strong>，到第20代过渡到100% <strong class="ir hi"> q </strong>。</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lj"><img src="../Images/79d9e3234a4f0f645a218cc578fc0687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oaJL2jbtwOcZrb7mB6hgOA.png"/></div></div></figure><p id="ca1f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们已经开始尝试使用<strong class="ir hi"> q </strong>和<strong class="ir hi"> z </strong>来训练更长时间的游戏，到目前为止，结果看起来很有希望。看到这种方法被应用到更大的AlphaZero项目中会很有趣，比如Leela Chess Zero。</p><p id="4765" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">第5部分现已推出。</p></div></div>    
</body>
</html>