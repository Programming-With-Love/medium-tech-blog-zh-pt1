<html>
<head>
<title>Top 5 Classification Algorithms You’ll Actually Use In Life</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你在生活中实际使用的五大分类算法</h1>
<blockquote>原文：<a href="https://medium.com/edureka/classification-algorithms-ba27044f28f1?source=collection_archive---------0-----------------------#2019-01-17">https://medium.com/edureka/classification-algorithms-ba27044f28f1?source=collection_archive---------0-----------------------#2019-01-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/7a96df3c5c90edc1edd3bc7d11d54f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*i-dEiJ4Nb8uMFhIVlsFjKA.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx">Classification Algorithms — Edureka</figcaption></figure><p id="2369" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi"> <em class="jn">分类算法</em> </strong>的思路相当简单<em class="jn">。</em>你通过分析训练数据集来预测目标类。</p><p id="fa16" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">本博客讨论以下概念:</p><ul class=""><li id="bbd0" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">什么是分类？</li><li id="4308" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">分类与聚类算法</li><li id="fd9f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">分类算法中的基本术语</li><li id="a062" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">分类算法的应用</li><li id="95a8" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">分类算法的类型</li><li id="2528" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">逻辑回归</li><li id="793c" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">决策图表</li><li id="5970" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">朴素贝叶斯分类器</li><li id="48d8" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">k最近邻</li><li id="06cd" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">SVM</li></ul><h1 id="3ecd" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">什么是分类？</h1><p id="da4f" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">我们使用训练数据集来获得更好的边界条件，这些边界条件可以用于确定每个目标类。一旦确定了边界条件，下一个任务就是预测目标类。整个过程被称为分类。</p><h2 id="f6bf" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated"><strong class="ak">目标类示例:</strong></h2><ul class=""><li id="4362" class="jo jp hh ir b is la iw lb ja lt je lu ji lv jm jt ju jv jw bi translated">分析客户数据，预测他是否会购买电脑配件<strong class="ir hi">(目标类别:是或否)</strong></li><li id="52ba" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">根据颜色、味道、大小、重量等特征对水果进行分类<strong class="ir hi">(目标类别:苹果、橙子、樱桃、香蕉)</strong></li><li id="1727" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">根据头发长度进行性别分类<strong class="ir hi">(目标类别:男性或女性)</strong></li></ul><p id="8eba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们用使用头发长度的性别分类来理解分类算法的概念(决不是我试图用性别来刻板化，这只是一个例子)。为了使用毛发长度作为特征参数来分类性别<strong class="ir hi">(目标类别)</strong>，我们可以使用任何分类算法来训练模型，以提出一些边界条件集合，这些边界条件可以用于使用毛发长度作为训练特征来区分男性和女性。在性别分类的情况下，边界条件可以是适当的头发长度值。假设<strong class="ir hi">区分边界</strong>毛发长度值为15.0 cm，那么我们可以说，如果毛发长度<strong class="ir hi">小于15.0 cm </strong>，那么性别可以是男性或者女性。</p><h1 id="0c78" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">分类算法与聚类算法</h1><p id="e483" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在聚类中，其思想不是像在分类中那样预测目标类，而是更多地试图通过考虑最满意的条件来对同类事物进行分组，<strong class="ir hi">同一组中的所有项目都应该是相似的，没有两个不同组的项目应该是不相似的。</strong></p><h2 id="05e0" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated"><strong class="ak">分组项目示例:</strong></h2><ul class=""><li id="88a1" class="jo jp hh ir b is la iw lb ja lt je lu ji lv jm jt ju jv jw bi translated">同时对相似语言类型的文档进行分组<strong class="ir hi">(相同语言的文档为一组。)</strong></li><li id="20f6" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">分类新闻文章时<strong class="ir hi">(同一新闻类别(体育)文章为一组)</strong></li></ul><p id="7400" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们以头发长度为例来理解性别聚类的概念。为了确定性别，可以使用不同相似性度量来对男性和女性进行分类。这可以通过找到两个头发长度之间的相似性来完成，并且如果相似性较小<strong class="ir hi">(头发长度的差异较小)</strong>，则将它们保持在同一组中。同样的过程可以继续下去，直到所有的头发长度适当地分为两类。</p><h1 id="f0ee" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">分类算法中的基本术语</h1><ul class=""><li id="e971" class="jo jp hh ir b is la iw lb ja lt je lu ji lv jm jt ju jv jw bi translated"><strong class="ir hi">分类器:</strong>将输入数据映射到特定类别的算法。</li><li id="2ad8" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">分类模型:</strong>分类模型试图从给定的用于训练的输入值中得出一些结论。它将预测新数据的分类标签/类别。</li><li id="1e51" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">特征:</strong>特征是被观察现象的个体可测量属性。</li><li id="f159" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">二元分类:</strong>分类有两种可能结果的任务。<strong class="ir hi">例如:性别分类(男/女)</strong></li><li id="4ea6" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">多类分类:</strong>两类以上的分类。在多类分类中，每个样本被分配给一个且仅一个目标标签。动物可以是猫或狗，但不能同时是猫和狗。</li><li id="d6d1" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi">多标签分类:</strong>将每个样本映射到一组目标标签(不止一个类别)的分类任务。一篇新闻可以同时涉及运动、人物和地点。</li></ul><h1 id="1136" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">分类算法的应用</h1><ul class=""><li id="ebfc" class="jo jp hh ir b is la iw lb ja lt je lu ji lv jm jt ju jv jw bi translated">垃圾邮件分类</li><li id="dfea" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">银行客户贷款支付意愿预测。</li><li id="d432" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">癌症肿瘤细胞鉴定。</li><li id="687b" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">情感分析</li><li id="c484" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">药品分类</li><li id="5b86" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">人脸关键点检测</li><li id="d143" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">汽车行驶中的行人检测。</li></ul><h1 id="cd6f" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">分类算法的类型</h1><p id="0777" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">分类算法可以大致分为以下几类:</p><ul class=""><li id="c5eb" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn">线性分类器</em> </strong></li></ul><ol class=""><li id="f8e6" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lw ju jv jw bi translated">逻辑回归</li><li id="2944" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">朴素贝叶斯分类器</li><li id="5c38" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">费希尔线性判别式</li></ol><ul class=""><li id="09af" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn">支持向量机</em> </strong></li></ul><ol class=""><li id="4451" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lw ju jv jw bi translated">最小二乘支持向量机</li></ol><ul class=""><li id="a725" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn">二次量词</em> </strong></li><li id="a4a0" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn">内核估计</em> </strong></li></ul><ol class=""><li id="ac48" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lw ju jv jw bi translated">k近邻</li></ol><ul class=""><li id="cb3a" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn">决策树</em> </strong></li></ul><ol class=""><li id="be5f" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lw ju jv jw bi translated">随机森林</li></ol><ul class=""><li id="1b34" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn">神经网络</em> </strong></li><li id="48ed" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn">学习矢量量化</em> </strong></li></ul><p id="b9d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面给出了一些流行的分类算法的例子。</p><h1 id="00e1" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">逻辑回归</h1><p id="4207" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">尽管这个名字可能令人困惑，但你可以放心。逻辑回归是一种分类，而不是回归算法。它根据给定的一组独立变量估计离散值<strong class="ir hi">(二进制值，如0/1、是/否、真/假)</strong>。简单来说，它基本上是通过将数据拟合到一个<strong class="ir hi"> <em class="jn"> logit函数</em>来预测事件发生的概率。</strong>因此，又称<strong class="ir hi"> <em class="jn"> logit回归</em> </strong>。获得的值将总是位于0和1之间，因为它预测了概率。</p><p id="1f41" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们试着通过另一个例子来理解这一点。</p><p id="95fa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设你的数学考试有一道算术题。它只能有两种结果，对吗？要么你解决它，要么你不解决它(这里我们不假设方法的要点)。现在想象一下，给你一个大范围的总数，试图理解你已经很好地理解了哪些章节。这项研究的结果大概是这样的——如果给你一个基于三角学的问题，你有70%的可能解决它。另一方面，如果是一道算术题，你得到答案的概率只有30%。这就是逻辑回归给你提供的。</p><p id="ec2b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我必须做数学计算，我会将结果的对数概率建模为预测变量的线性组合。</p><pre class="lx ly lz ma fd mb mc md me aw mf bi"><span id="c800" class="lf kd hh mc b fi mg mh l mi mj">odds= p/ (1-p) = probability of event occurrence / probability of event occurrence ln(odds) = ln(p/(1-p)) logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk)</span></pre><p id="7dcf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面给出的等式中，<em class="jn"> p </em>是感兴趣特征出现的概率。它选择最大化观察样本值的可能性的参数，而不是最小化误差平方和的参数(就像普通回归一样)。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mk"><img src="../Images/4a3ee1ee8e47b0220126420584c3aab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jr5p0gOq9NiTyJ2sHY2kJw.png"/></div></div></figure><p id="9ed7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，你们很多人可能会想，为什么要拿一根木头？为了简单起见，我们姑且说这是复制阶跃函数的最好的数学方法之一。我可以对此进行更深入的探讨，但这将违背我写这篇博客的目的。</p><h2 id="dcbf" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">r代码:</h2><pre class="lx ly lz ma fd mb mc md me aw mf bi"><span id="5ca1" class="lf kd hh mc b fi mg mh l mi mj">x &lt;- cbind(x_train,y_train)<br/># Train the model using the training sets and check score<br/>logistic &lt;- glm(y_train ~ ., data = x,family='binomial')<br/>summary(logistic)<br/>#Predict Output<br/>predicted= predict(logistic,x_test)</span></pre><p id="a375" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了改进模型，可以尝试许多不同的步骤:</p><ul class=""><li id="cb74" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">包括交互术语</li><li id="f184" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">移除功能</li><li id="52e5" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">规范技术</li><li id="5baa" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">使用非线性模型</li></ul><h1 id="4acd" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">决策树</h1><p id="2c00" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">现在，<strong class="ir hi"> <em class="jn">决策树</em> </strong>是目前为止我最喜欢的算法之一。它具有多种多样的特征，有助于实现分类和连续因变量，是一种监督学习算法，主要用于分类问题。该算法所做的是，根据最重要的属性将种群分成两个或更多同类集合，使各组尽可能不同。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mp"><img src="../Images/da00ac35e55aa9652a25f6f9d23f1da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_1aCdXADQ9evTk_nHCas6g.png"/></div></div></figure><p id="10e4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，你可以看到人口根据多种属性被分为四个不同的组，以识别“他们是否会玩”。</p><h2 id="6adf" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">r代码:</h2><pre class="lx ly lz ma fd mb mc md me aw mf bi"><span id="cc34" class="lf kd hh mc b fi mg mh l mi mj">library(rpart)<br/>x &lt;- cbind(x_train,y_train)<br/># grow tree <br/>fit &lt;- rpart(y_train ~ ., data = x,method="class")<br/>summary(fit)<br/>#Predict Output <br/>predicted= predict(fit,x_test)</span></pre><h1 id="ca0f" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">朴素贝叶斯分类器</h1><p id="adbc" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">这是一种基于预测器之间独立性假设或所谓的<em class="jn">贝叶斯定理</em>的分类技术。简单来说，<strong class="ir hi"> <em class="jn">朴素贝叶斯分类器</em> </strong>假设一个类中特定特征的存在与任何其他特征的存在无关。</p><p id="eaa8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">例如，如果一个水果是红色的，圆形的，直径约为3英寸，它就可以被认为是苹果。即使这些特征相互依赖或依赖于其他特征的存在，朴素贝叶斯分类器也会考虑所有这些属性，以独立地影响该水果是苹果的概率。</p><p id="b5d3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">建立贝叶斯模型很简单，尤其是在巨大数据集的情况下。除了简单之外，朴素贝叶斯也被认为优于复杂的分类方法。</p><p id="c255" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">贝叶斯定理提供了从<strong class="ir hi"> P(c) </strong>、<strong class="ir hi"> P(x) </strong>和<strong class="ir hi"> P(x|c) </strong>计算后验概率<strong class="ir hi"> P(c|x) </strong>的方法。后验概率的表达式如下。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es mq"><img src="../Images/150e9f0c1e5787a40f83524b95820315.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*aQWyW6yavYPuh8AUE2XR1A.png"/></div></figure><p id="20df" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里，</p><ul class=""><li id="5007" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn"> P </em> ( <em class="jn"> c|x </em>)是<em class="jn">类</em> ( <em class="jn">目标</em>)给定<em class="jn">预测器</em> ( <em class="jn">属性</em>)的后验概率。</strong></li><li id="2a79" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn"> P </em> ( <em class="jn"> c </em>)是<em class="jn">类</em>的先验概率。</strong></li><li id="28be" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn"> P </em> ( <em class="jn"> x|c </em>)为似然，即<em class="jn">预测器</em>给定<em class="jn">类</em>的概率。</strong></li><li id="23cc" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated"><strong class="ir hi"> <em class="jn"> P </em> ( <em class="jn"> x </em>)是<em class="jn">预测器</em>的先验概率。</strong></li></ul><p id="afc7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们通过一个例子来更好地理解这一点。因此，这里我有一个天气的训练数据集，即晴天、阴天和雨天，以及相应的二元变量“Play”。现在，我们需要根据天气情况对球员是否上场进行分类。让我们按照以下步骤来执行它。</p><p id="ab9f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">步骤1: </strong>将数据集转换成频率表</p><p id="8aa5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第二步:</strong>通过查找<strong class="ir hi">阴概率= 0.29 </strong>和<strong class="ir hi">出牌概率为0.64 </strong>这样的概率，创建一个似然表。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mr"><img src="../Images/f56e8c0ce482c6075adff928bc185011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_iCRS3JzFGpC3NsmCcIzw.png"/></div></div></figure><p id="d54d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第三步:</strong>现在，使用朴素贝叶斯方程计算每一类的后验概率。具有最高后验概率的类是预测的结果。</p><p id="74da" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">问题:</strong>天气晴朗玩家就会玩，这种说法正确吗？</p><p id="6fc3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们可以使用上面讨论的方法来解决它，所以<strong class="ir hi">P(Yes | Sunny)= P(Sunny | Yes)* P(Yes)/P(Sunny)</strong></p><p id="821a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里我们有<strong class="ir hi"> P (Sunny |Yes) = 3/9 = 0.33 </strong>，<strong class="ir hi"> P(Sunny) = 5/14 = 0.36 </strong>，<strong class="ir hi"> P( Yes)= 9/14 = 0.64 </strong></p><p id="f51e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，<strong class="ir hi"> P(是|晴)= 0.33 * 0.64 / 0.36 = 0.60 </strong>，概率较高。</p><p id="f2f7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">朴素贝叶斯使用类似的方法来预测基于各种属性的不同类别的概率。该算法主要用于文本分类和多类问题。</p><h2 id="fe93" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">r代码:</h2><pre class="lx ly lz ma fd mb mc md me aw mf bi"><span id="5d88" class="lf kd hh mc b fi mg mh l mi mj">library(e1071)<br/>x &lt;- cbind(x_train,y_train)<br/># Fitting model<br/>fit &lt;-naiveBayes(y_train ~ ., data = x)<br/>summary(fit)<br/>#Predict Output <br/>predicted= predict(fit,x_test)</span></pre><h1 id="cbc7" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">KNN (k-最近邻)</h1><p id="1d18" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated"><strong class="ir hi"> <em class="jn"> K最近邻</em> </strong>是一个简单的算法，用于分类和回归问题。它基本上存储所有可用的案例，以通过其k个邻居的多数投票对新案例进行分类。分配给该类的事例在通过距离函数(欧几里德、曼哈顿、闵可夫斯基和汉明)测量的K个最近邻中最为常见。</p><p id="d5a3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">前三种距离函数用于连续变量，而汉明距离函数用于分类变量。如果<strong class="ir hi"> K = 1 </strong>，则该案例被简单地分配给其最近邻的类别。有时，在执行kNN建模时，选择K是一个挑战。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es ms"><img src="../Images/b894325b498ebac8e1e35349325641d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4FeRsslkfggjsYkdaGWZQw.png"/></div></div></figure><p id="ba86" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">举一个我们现实生活中的例子，你就会很容易理解KNN。如果你暗恋班上的一个女孩/男孩，而你对他/她一无所知，你可能想和他们的朋友和社交圈谈谈，以获得他们的信息！</p><h2 id="0530" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">r代码:</h2><pre class="lx ly lz ma fd mb mc md me aw mf bi"><span id="6bcb" class="lf kd hh mc b fi mg mh l mi mj">library(knn)<br/>x &lt;- cbind(x_train,y_train)<br/># Fitting model<br/>fit &lt;-knn(y_train ~ ., data = x,k=5)<br/>summary(fit)<br/>#Predict Output <br/>predicted= predict(fit,x_test)</span></pre><h2 id="bebe" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">选择KNN之前需要考虑的事项:</h2><ul class=""><li id="da21" class="jo jp hh ir b is la iw lb ja lt je lu ji lv jm jt ju jv jw bi translated">KNN计算量很大</li><li id="05e3" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">变量应该被规范化，否则更高范围的变量可能会使它产生偏差</li><li id="3105" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">在使用kNN之前，更多地在预处理阶段工作，比如离群点、噪声去除</li></ul><h1 id="e2cc" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">SVM(支持向量机)</h1><p id="cbce" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在该算法中，我们将每个数据项绘制为n维空间中的一个点(其中n是您拥有的特征的数量)，每个特征的值是特定坐标的值。</p><p id="1530" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">例如，如果我们只有个人的两个特征，如身高和头发长度，我们将首先在二维空间中绘制这两个变量，其中每个点有两个坐标(这些坐标称为<strong class="ir hi">支持向量</strong></p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mt"><img src="../Images/149574d07244fba2f3c1b52ea7f5171e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uEZKUTiBXbvx-YZezrR5tA.png"/></div></div></figure><p id="839b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们将找到一些<em class="jn">线</em>来分割两个不同分类的数据组之间的数据。这将是一条线，使得两组中的每一组离最近点的距离最远。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mt"><img src="../Images/f269c5a06fea45369bcdfcb98af6d3ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XlTtohrt8AidIafQX9uFag.png"/></div></div></figure><p id="bd22" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的例子中，将数据分成两个不同分类组的线是蓝色的<em class="jn">线，因为两个最近的点离这条线最远。这一行是我们的<em class="jn">量词</em>。然后，根据测试数据在线两边的位置，这就是我们可以将新数据分类的类别。</em></p><h2 id="94e1" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">r代码:</h2><pre class="lx ly lz ma fd mb mc md me aw mf bi"><span id="f420" class="lf kd hh mc b fi mg mh l mi mj">library(e1071)<br/>x &lt;- cbind(x_train,y_train)<br/># Fitting model<br/>fit &lt;-svm(y_train ~ ., data = x)<br/>summary(fit)<br/>#Predict Output <br/>predicted= predict(fit,x_test)</span></pre><p id="19cf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">就这样，我们到了这篇文章的结尾。如果你想查看更多关于Python、DevOps、Ethical Hacking等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="27e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释数据科学的各个方面。</p><blockquote class="mv mw mx"><p id="0461" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 1。</em> <a class="ae mu" rel="noopener" href="/edureka/data-science-tutorial-484da1ff952b"> <em class="hh">数据科学教程</em> </a></p><p id="59a2" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 2。</em> <a class="ae mu" rel="noopener" href="/edureka/math-and-statistics-for-data-science-1152e30cee73"> <em class="hh">数据科学的数学与统计</em> </a></p><p id="f479" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 3。</em><a class="ae mu" rel="noopener" href="/edureka/machine-learning-with-r-c7d3edf1f7b"><em class="hh">R中的机器学习</em> </a></p><p id="08e8" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 4。</em> <a class="ae mu" rel="noopener" href="/edureka/machine-learning-algorithms-29eea8b69a54"> <em class="hh">机器学习算法</em> </a></p><p id="5e1a" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 5。</em><a class="ae mu" rel="noopener" href="/edureka/linear-regression-in-r-da3e42f16dd3"><em class="hh">R中的线性回归</em> </a></p><p id="2632" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 6。</em><a class="ae mu" rel="noopener" href="/edureka/logistic-regression-in-r-2d08ac51cd4f"><em class="hh">R中的逻辑回归</em> </a></p><p id="3def" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 7。</em> <a class="ae mu" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="5e3d" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 8。</em> <a class="ae mu" rel="noopener" href="/edureka/a-complete-guide-on-decision-tree-algorithm-3245e269ece"> <em class="hh">决策树中的R </em> </a></p><p id="b2b0" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 9。</em> <a class="ae mu" rel="noopener" href="/edureka/introduction-to-machine-learning-97973c43e776"> <em class="hh">机器学习入门</em> </a></p><p id="fee6" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 10。</em> <a class="ae mu" rel="noopener" href="/edureka/naive-bayes-in-r-37ca73f3e85c"> <em class="hh">朴素贝叶斯在R </em> </a></p><p id="e34b" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 11。</em> <a class="ae mu" rel="noopener" href="/edureka/statistics-and-probability-cf736d703703"> <em class="hh">统计与概率</em> </a></p><p id="a01f" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 12。</em> <a class="ae mu" rel="noopener" href="/edureka/decision-trees-b00348e0ac89"> <em class="hh">如何创建一个完美的决策树？</em>T73】</a></p><p id="b85c" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 13。</em> <a class="ae mu" rel="noopener" href="/edureka/data-scientists-myths-14acade1f6f7"> <em class="hh">关于数据科学家角色的10大误区</em> </a></p><p id="7d58" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 14。</em> <a class="ae mu" rel="noopener" href="/edureka/data-science-projects-b32f1328eed8"> <em class="hh">顶级数据科学项目</em> </a></p><p id="0bed" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 15。</em> <a class="ae mu" rel="noopener" href="/edureka/data-analyst-vs-data-engineer-vs-data-scientist-27aacdcaffa5"> <em class="hh">数据分析师vs数据工程师vs数据科学家</em> </a></p><p id="2b59" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated">16。 <a class="ae mu" rel="noopener" href="/edureka/types-of-artificial-intelligence-4c40a35f784"> <em class="hh">人工智能类型</em> </a></p><p id="2d04" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated">17。 <a class="ae mu" rel="noopener" href="/edureka/r-vs-python-48eb86b7b40f"> <em class="hh"> R vs Python </em> </a></p><p id="aefc" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated">18。 <a class="ae mu" rel="noopener" href="/edureka/ai-vs-machine-learning-vs-deep-learning-1725e8b30b2e"> <em class="hh">人工智能vs机器学习vs深度学习</em> </a></p><p id="5811" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 19。</em> <a class="ae mu" rel="noopener" href="/edureka/machine-learning-projects-cb0130d0606f"> <em class="hh">机器学习项目</em> </a></p><p id="6368" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated">20。 <a class="ae mu" rel="noopener" href="/edureka/data-analyst-interview-questions-867756f37e3d"> <em class="hh">数据分析师面试问答</em> </a></p><p id="c4c9" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 21。</em> <a class="ae mu" rel="noopener" href="/edureka/data-science-and-machine-learning-for-non-programmers-c9366f4ac3fb"> <em class="hh">面向非程序员的数据科学和机器学习工具</em> </a></p><p id="67c2" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh">二十二。</em> <a class="ae mu" rel="noopener" href="/edureka/top-10-machine-learning-frameworks-72459e902ebb"> <em class="hh">十大机器学习框架</em> </a></p><p id="fa5f" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 23。</em> <a class="ae mu" rel="noopener" href="/edureka/statistics-for-machine-learning-c8bc158bb3c8"> <em class="hh">用于机器学习的统计</em> </a></p><p id="14ec" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 24。</em> <a class="ae mu" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="4bcd" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 25。</em> <a class="ae mu" rel="noopener" href="/edureka/breadth-first-search-algorithm-17d2c72f0eaa"> <em class="hh">广度优先搜索算法</em> </a></p><p id="bd19" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 26。</em><a class="ae mu" rel="noopener" href="/edureka/linear-discriminant-analysis-88fa8ad59d0f"><em class="hh">R中的线性判别分析</em> </a></p><p id="330b" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 27。</em> <a class="ae mu" rel="noopener" href="/edureka/prerequisites-for-machine-learning-68430f467427"> <em class="hh">机器学习的先决条件</em> </a></p><p id="b985" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 28。</em> <a class="ae mu" rel="noopener" href="/edureka/r-shiny-tutorial-47b050927bd2"> <em class="hh">互动WebApps使用R闪亮</em> </a></p><p id="33e6" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 29。</em> <a class="ae mu" rel="noopener" href="/edureka/top-10-machine-learning-books-541f011d824e"> <em class="hh">机器学习十大书籍</em> </a></p><p id="ca89" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated">三十。 <a class="ae mu" rel="noopener" href="/edureka/unsupervised-learning-40a82b0bac64"> <em class="hh">无监督学习</em> </a></p><p id="8c60" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 31。</em> <a class="ae mu" rel="noopener" href="/edureka/10-best-books-data-science-9161f8e82aca"> <em class="hh"> 10本最好的数据科学书籍</em> </a></p><p id="4b70" class="ip iq jn ir b is it iu iv iw ix iy iz my jb jc jd mz jf jg jh na jj jk jl jm ha bi translated"><em class="hh"> 32。</em> <a class="ae mu" rel="noopener" href="/edureka/supervised-learning-5a72987484d0"> <em class="hh">监督学习</em> </a></p></blockquote></div><div class="ab cl nb nc go nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ha hb hc hd he"><p id="0605" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">原载于2019年1月17日</em><a class="ae mu" href="https://www.edureka.co/blog/classification-algorithms" rel="noopener ugc nofollow" target="_blank"><em class="jn">【www.edureka.co】</em></a><em class="jn">。</em></p></div></div>    
</body>
</html>