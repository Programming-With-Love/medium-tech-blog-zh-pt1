<html>
<head>
<title>Five steps to optimize AI deployments</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化人工智能部署的五个步骤</h1>
<blockquote>原文：<a href="https://medium.com/globant/five-steps-to-optimize-ai-deployments-b3f5e9d15dd7?source=collection_archive---------0-----------------------#2022-03-10">https://medium.com/globant/five-steps-to-optimize-ai-deployments-b3f5e9d15dd7?source=collection_archive---------0-----------------------#2022-03-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="e8fc" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">我如何将Docker图像从9.7 GB减少到3.7 GB</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/bf1a14a5e1213f10c7a1be9c45de12b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xxb7aj_-GrkKV9Vr"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source : Docker.com</figcaption></figure><p id="ee21" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi ki translated">随着许多组织专注于增强人工智能能力的研究，我们在<a class="ae kr" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习领域看到了许多行动和突破</a>。<a class="ae kr" rel="noopener" href="/@sanket.deshmukh/optimizing-container-sizes-for-ai-deployments-b3f5e9d15dd7"> </a>大多数最先进的模型都在<a class="ae kr" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> TFHub </a>，<a class="ae kr" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">hugging face</a>&amp;<a class="ae kr" href="https://pytorch.org/vision/stable/models.html" rel="noopener ugc nofollow" target="_blank">py torch</a>上开源，供大家实验和欣赏。然而，这些模型中的大多数都很笨重，对于基于CPU的推理和部署来说用处不大。</p><blockquote class="ks kt ku"><p id="c923" class="jm jn kv jo b jp jq ii jr js jt il ju kw jw jx jy kx ka kb kc ky ke kf kg kh ha bi translated"><strong class="jo hi">我们提高了推理速度，并将docker图像的大小从9.7 Gb减少到3.7 Gb(减少了3倍)，这也是本文的动机所在。</strong></p></blockquote><p id="f1cd" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">在这篇文章中，我们想介绍一些我们使用<a class="ae kr" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker </a>在云上部署深度学习模型时遵循的最佳实践和技巧。我们将讨论减小模型大小、提高推理速度、选择正确的模型、压缩docker图像的技巧以及代码重构的快速技巧。如果你是深度学习部署的新手，我在进一步阅读部分添加了优秀博客的参考。</p><h1 id="8332" class="kz la hh bd lb lc ld le lf lg lh li lj in lk io ll iq lm ir ln it lo iu lp lq bi translated">提示和最佳实践</h1><p id="c11c" class="pw-post-body-paragraph jm jn hh jo b jp lr ii jr js ls il ju jv lt jx jy jz lu kb kc kd lv kf kg kh ha bi translated">这篇文章的大部分内容都在谈论减少深度学习模型的大小，在最后，我们谈论一些压缩<a class="ae kr" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker </a>图像的通用技巧。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lw"><img src="../Images/a94e89fcb142884d39dbac69175e5dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AXH9Y3nxB7tyY1B3"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source : <a class="ae kr" href="https://www.pexels.com/photo/woman-reading-book-256455/" rel="noopener ugc nofollow" target="_blank">Pexels.com</a></figcaption></figure><h2 id="fea4" class="lx la hh bd lb ly lz ma lf mb mc md lj jv me mf ll jz mg mh ln kd mi mj lp mk bi translated">步骤1:使用量化来减少边</h2><p id="3f25" class="pw-post-body-paragraph jm jn hh jo b jp lr ii jr js ls il ju jv lt jx jy jz lu kb kc kd lv kf kg kh ha bi translated">让我们面对现实吧，当开发人员因为我们的模型太大而开始遇到API框架的内存问题时，这是一个额外的开销。虽然有许多方法来处理内存问题，但其中一个主要的方法是称为<a class="ae kr" href="https://in.mathworks.com/discovery/quantization.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jo hi">量化</strong> </a>的概念。</p><blockquote class="ks kt ku"><p id="86dc" class="jm jn kv jo b jp jq ii jr js jt il ju kw jw jx jy kx ka kb kc ky ke kf kg kh ha bi translated">摘自<a class="ae kr" href="https://pytorch.org/docs/stable/quantization.html" rel="noopener ugc nofollow" target="_blank"> PyTorch官方文档</a> : <br/>“量化是指以比浮点精度更低的位宽执行计算和存储张量的技术。量化模型对具有整数而非浮点值的张量执行一些或全部操作。这允许在许多硬件平台上使用更紧凑的模型表示和高性能的矢量化运算。”</p></blockquote><p id="d066" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">因此，量化一个模型会导致精确度的少量降低，但是在模型尺寸方面却有神奇的效果。量化通常可以在训练后或训练期间进行，所有领先的深度学习框架都提供了现成的量化选项。要了解更多关于量化的知识，请参考这个<a class="ae kr" href="http://How to accelerate and compress neural networks with quantization | by Tivadar Danka | Towards Data Science" rel="noopener ugc nofollow" target="_blank">博客</a>进行深入探讨。</p><h2 id="f816" class="lx la hh bd lb ly lz ma lf mb mc md lj jv me mf ll jz mg mh ln kd mi mj lp mk bi translated">步骤2:使用ONNX运行时提高推理速度</h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ml"><img src="../Images/a24c31497cf8656ec1e6a6a544e84f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xgSe3_IZYGJpAmV3"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source : <a class="ae kr" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank">ONNX Website</a></figcaption></figure><p id="501d" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated"><a class="ae kr" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>是开放神经网络交换的缩写，以其硬件优化和运行时而闻名。使用ONNX运行时进行推理可以加快推理速度。基于您正在使用的模型，可以直接选择ONNX来量化模型，然后使用ONNX会话来进行推断。</p><p id="6387" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">在我们的例子中，由于我们已经部署了<a class="ae kr" href="https://huggingface.co/docs/transformers/model_doc/t5" rel="noopener ugc nofollow" target="_blank"> T5模型</a> , <a class="ae kr" href="https://github.com/Ki6an/fastT5" rel="noopener ugc nofollow" target="_blank"> FastT5 </a>包为我们提供了量化和模型尺寸缩减，同时精度损失最小。下面的代码片段显示了量化T5模型并将其导出为ONNX格式以便更快推断的步骤。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lw"><img src="../Images/0b41630fc7dba60721ecbceb1eea11eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DbRihKiGweVBYEB1"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Fig. Showing Steps to Quantize the model using FastT5 Model</figcaption></figure><p id="e23c" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">在上面的代码中，我们执行函数<code class="du mm mn mo mp b">generate_onnx_representation()</code>，它负责下载和转换ONNX表示的模型。步骤2负责量化，在接下来的步骤中，我们导出模型。以下是上述步骤的结果:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mq"><img src="../Images/0d82f10ffc7da93f53b61764fed3af23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DfjDJN2TaN50ZhpLQMF_tA.png"/></div></div></figure><p id="c898" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们可以观察到，随着上述每个步骤的执行，模型大小是如何缩小的。对于ONNX的实际演练，请参考这个<a class="ae kr" href="https://towardsdatascience.com/onnx-easily-exchange-deep-learning-models-f3c42100fd77" rel="noopener" target="_blank">博客</a>。</p><h2 id="7e9f" class="lx la hh bd lb ly lz ma lf mb mc md lj jv me mf ll jz mg mh ln kd mi mj lp mk bi translated">第三步:选择合适的型号</h2><p id="fb3d" class="pw-post-body-paragraph jm jn hh jo b jp lr ii jr js ls il ju jv lt jx jy jz lu kb kc kd lv kf kg kh ha bi translated">大多数深度学习模型被部署为应用程序中的依赖项，或者作为API服务暴露给不同的应用程序。无论哪种方式，通常都有一个轻量级的模型，只需很少的修改就可以重用。我们需要做一个实地调查，看看是否有一个边缘设备兼容的模型可用于CPU绑定的推理。</p><p id="5ea7" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">例如，下图显示了最广泛使用的人脸检测模型之一——Facenet及其lite变体在大小和推理速度上的差异。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mr"><img src="../Images/fd738cb44790a7f1036cb5dabe1d7bf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*OCH-TnH9FfXnKPwC"/></div></figure><p id="3bbe" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">我们看到，当比较规模和推理速度时，<a class="ae kr" href="https://arxiv.org/pdf/1804.07573" rel="noopener ugc nofollow" target="_blank"> Mobile-Facenet </a>明显是<a class="ae kr" href="https://arxiv.org/abs/1503.03832" rel="noopener ugc nofollow" target="_blank"> Facenet </a>的赢家。这是以模型精度的最小损失为代价的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="er es mr"><img src="../Images/862807dacba920e02163097b3650434b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*OJh4rcxq8iFYDgYO"/></div></figure><p id="bc83" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">同样，预训练的<a class="ae kr" href="https://huggingface.co/docs/transformers/model_doc/t5" rel="noopener ugc nofollow" target="_blank"> T5 </a>和T5量化模型(<strong class="jo hi">大约)之间的模型大小也有显著差异。尺寸缩小3倍</strong>。因此，<strong class="jo hi">评估各种备选方案并选择最佳型号</strong>会显著影响整体规模</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ms"><img src="../Images/cbd39de1e31355d08fe68bd07e4751c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WsNomkFVaexJ6-m6"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Source : <a class="ae kr" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank">Docker.com</a></figcaption></figure><h2 id="827d" class="lx la hh bd lb ly lz ma lf mb mc md lj jv me mf ll jz mg mh ln kd mi mj lp mk bi translated">步骤4:压缩docker图像的技巧</h2><p id="aa75" class="pw-post-body-paragraph jm jn hh jo b jp lr ii jr js ls il ju jv lt jx jy jz lu kb kc kd lv kf kg kh ha bi translated">Docker是快速安全部署的行业选择。然而，当我们构建图像时，我们可以很容易地观察到图像大小是多么的臃肿。大多数时候，规模是如此之大，以至于构建没有完成就崩溃了(错误2)。这主要是由于两个主要原因:</p><ul class=""><li id="0c3e" class="mt mu hh jo b jp jq js jt jv mv jz mw kd mx kh my mz na nb bi translated">添加过多的运行命令<br/> <em class="kv"> </em>层是由Docker-file中的运行命令引入的。每个Run命令都会添加一个构建层，然后导致内存开销。</li><li id="527e" class="mt mu hh jo b jp nc js nd jv ne jz nf kd ng kh my mz na nb bi translated">使用<a class="ae kr" href="https://pypi.org/project/pip/" rel="noopener ugc nofollow" target="_blank"> PIP </a>命令安装Python框架。<br/> PIP在下载、提取、构建(DEB)工作流中执行。因此，每个包安装请求都必须首先下载所有的依赖项，构建二进制文件，然后安装它们。像PyTorch这样的框架并不是轻量级的，这个过程会消耗大量内存。</li></ul><p id="8ff7" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">以下是压缩Docker图像的提示:</p><ul class=""><li id="b7d0" class="mt mu hh jo b jp jq js jt jv mv jz mw kd mx kh my mz na nb bi translated"><strong class="jo hi">减少层次，即运行命令<br/> </strong>在一个单一的语句中组合层次将减少许多不必要的层次，并使用一个单一的命令执行大多数东西。例如，下面是我们最初的Docker文件:</li></ul><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="ab fe cl nh"><img src="../Images/fbb7a995bca070070854c48b0e977d9b.png" data-original-src="https://miro.medium.com/v2/0*Xdg83fFQxjTn5n_e"/></div></figure><p id="2461" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">可以重写为:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es ni"><img src="../Images/4a956123fc2f107aa48a846fd83cbd54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LqItaIPThX_ZyO-i"/></div></div></figure><p id="ebd0" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">这减少了所需的层数，从而加快了构建速度并减少了内存消耗。</p><ul class=""><li id="7946" class="mt mu hh jo b jp jq js jt jv mv jz mw kd mx kh my mz na nb bi translated"><strong class="jo hi">尽可能使用Python轮子<br/> </strong> <a class="ae kr" href="https://realpython.com/python-wheels/" rel="noopener ugc nofollow" target="_blank">轮子文件</a>对Python的安装过程是一个福音。它们预先构建了所需的所有依赖项和索引，并且非常轻便，可以通过互联网传输，并且需要的空间非常少。一个简单的<code class="du mm mn mo mp b">pip install torch == x.x</code>可以轻易消耗大约900 MB到1.3 GB的磁盘空间。因此，用兼容的linux PyTorch wheel文件(<a class="ae kr" href="https://download.pytorch.org/whl/torch_stable.html" rel="noopener ugc nofollow" target="_blank"> PyTorch稳定版本</a>)替换<code class="du mm mn mo mp b">torch==1.9.0</code>行会显著减少内存。</li></ul><h2 id="7ad1" class="lx la hh bd lb ly lz ma lf mb mc md lj jv me mf ll jz mg mh ln kd mi mj lp mk bi translated">步骤5:使用内存分析器并重构代码</h2><p id="5850" class="pw-post-body-paragraph jm jn hh jo b jp lr ii jr js ls il ju jv lt jx jy jz lu kb kc kd lv kf kg kh ha bi translated">最后，每当使用像<a class="ae kr" href="https://flask.palletsprojects.com/en/2.0.x/" rel="noopener ugc nofollow" target="_blank"> Flask </a>或<a class="ae kr" href="https://fastapi.tiangolo.com/" rel="noopener ugc nofollow" target="_blank"> FastAPI </a>这样的API框架部署深度学习模型时，确保我们针对内存泄漏分析我们的代码，并插入它们以避免由于<strong class="jo hi">内存溢出</strong>而导致的失败。我们特别面对Flask的这个问题，并使用<a class="ae kr" href="https://pythonspeed.com/articles/memory-profiler-data-scientists/" rel="noopener ugc nofollow" target="_blank"> FIL Profile </a>包来跟踪内存消耗并重新调整我们的代码。此外，这个<a class="ae kr" href="https://stackoverflow.com/questions/49991234/flask-app-memory-leak-caused-by-each-api-call" rel="noopener ugc nofollow" target="_blank">栈溢出帖子</a>对我们堵住漏洞帮助很大。</p><h1 id="6433" class="kz la hh bd lb lc ld le lf lg lh li lj in lk io ll iq lm ir ln it lo iu lp lq bi translated">结论</h1><p id="eb29" class="pw-post-body-paragraph jm jn hh jo b jp lr ii jr js ls il ju jv lt jx jy jz lu kb kc kd lv kf kg kh ha bi translated">以下快照捕获了Docker映像的初始大小、详细的逐步缩减和最终状态。</p><p id="2af1" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">下面显示了我的docker图像的初始大小。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lw"><img src="../Images/035c696643e0f69aaa5c5f4f9cc4b0fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XqaxqTtUuqRcbvMx"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Snapshot showing size of initial Docker image</figcaption></figure><p id="828c" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">以下是我通过应用上述步骤能够节省的金额</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nj"><img src="../Images/33d7fee12c4f1a54e2c9d806ad9c2be1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zWWK9zkonVFcU0LMNnxmGg.png"/></div></div></figure><p id="58c4" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">这是我最后得到的最终图像尺寸:</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es nk"><img src="../Images/417132be1958bca47a8b6771a2dfe61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zbVPT6G1CkL_a18q"/></div></div><figcaption class="ji jj et er es jk jl bd b be z dx">Snapshot showing final Docker image post optimization</figcaption></figure><p id="c868" class="pw-post-body-paragraph jm jn hh jo b jp jq ii jr js jt il ju jv jw jx jy jz ka kb kc kd ke kf kg kh ha bi translated">因此，使用上述最佳实践，我们能够将Docker映像大小从9.7 GB减少到3.7 GB，这对于部署在Azure应用服务或Azure功能上来说是非常健壮的。代码剖析和重构的使用帮助我们能够使执行时间和延迟与业务KPI相匹配。我希望这篇博客能帮助您减少部署时间。</p><h1 id="6ce5" class="kz la hh bd lb lc ld le lf lg lh li lj in lk io ll iq lm ir ln it lo iu lp lq bi translated">进一步阅读</h1><ul class=""><li id="8466" class="mt mu hh jo b jp lr js ls jv nl jz nm kd nn kh my mz na nb bi translated">DL部署初学者指南<br/> <a class="ae kr" rel="noopener" href="/swlh/end-to-end-machine-learning-from-data-collection-to-deployment-ce74f51ca203">端到端机器学习:从数据收集到部署🚀艾哈迈德·贝斯贝斯</a></li><li id="4d22" class="mt mu hh jo b jp nc js nd jv ne jz nf kd ng kh my mz na nb bi translated">Docker镜像优化策略:<br/><a class="ae kr" rel="noopener" href="/sciforce/strategies-of-docker-images-optimization-2ca9cc5719b6">https://medium . com/sci force/strategies-of-Docker-images-optimization-2c a9 cc 5719 b 6</a></li><li id="6c49" class="mt mu hh jo b jp nc js nd jv ne jz nf kd ng kh my mz na nb bi translated">模型量化讲解:<br/> <a class="ae kr" href="https://towardsdatascience.com/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7" rel="noopener" target="_blank">如何用量化加速压缩神经网络|作者蒂瓦达·卡丹|走向数据科学</a></li><li id="b834" class="mt mu hh jo b jp nc js nd jv ne jz nf kd ng kh my mz na nb bi translated">ONNX官网:<br/>【https://onnx.ai/ T4】</li><li id="45fc" class="mt mu hh jo b jp nc js nd jv ne jz nf kd ng kh my mz na nb bi translated">使用dockers <br/> <a class="ae kr" href="https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33" rel="noopener" target="_blank">部署DL模型的快速介绍https://towards data science . com/how-to-deploy-large-size-deep-learning-models-into-production-66b 851d 17 f 33</a></li></ul></div></div>    
</body>
</html>