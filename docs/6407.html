<html>
<head>
<title>Yelp Reviews Sentiment Prediction via PySpark, MongoDB, AWS EMR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Yelp通过PySpark、MongoDB、AWS EMR评论情绪预测</h1>
<blockquote>原文：<a href="https://medium.com/quick-code/yelp-reviews-sentiment-prediction-via-pyspark-mongodb-aws-emr-8bf0e21f5a92?source=collection_archive---------3-----------------------#2018-02-01">https://medium.com/quick-code/yelp-reviews-sentiment-prediction-via-pyspark-mongodb-aws-emr-8bf0e21f5a92?source=collection_archive---------3-----------------------#2018-02-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="d21f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者:<a class="jc jd ge" href="https://medium.com/u/f8ecd6cdacf9?source=post_page-----8bf0e21f5a92--------------------------------" rel="noopener" target="_blank">尼查·鲁奇拉瓦特</a>，<a class="jc jd ge" href="https://medium.com/u/f9b481f9624?source=post_page-----8bf0e21f5a92--------------------------------" rel="noopener" target="_blank">蒂娜·彭</a>，<a class="jc jd ge" href="https://medium.com/u/13c0d3a0ad3?source=post_page-----8bf0e21f5a92--------------------------------" rel="noopener" target="_blank">麦色·李</a></p><p id="15fa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">情感分析或观点挖掘是一种常见的自然语言处理问题，用于确定文本是正面还是负面的。在一个顾客越来越多地在网上发表意见的世界里，企业了解自己的网上声誉至关重要。</p><p id="6541" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将使用Apache Spark的机器学习库在Yelp数据集上探索一种简单的方法，以预测给定评论文本的情绪。我们还将分析哪些术语对积极或消极的餐馆评论贡献最大。</p><p id="2701" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于特征系数的可解释性，我们将使用<strong class="ig hi">线性支持向量机</strong>和<strong class="ig hi">逻辑回归</strong>来预测评论是正面还是负面。它们都是NLP应用的有效分类算法。</p><ol class=""><li id="d719" class="jf jg hh ig b ih ii il im ip jh it ji ix jj jb jk jl jm jn bi translated"><strong class="ig hi">在AWS EMR实例上设置MongoDB和Zeppelin Notebook</strong></li></ol><p id="808f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们设置了一个AWS EMR实例，有1个主实例和3个工作实例(每个实例都有一个m3.xlarge，有15 GB内存和8个内核)。这对于1.5 M的评论子集很有效，但是更多的评论可以通过具有更大内存大小的节点或者具有更多节点的集群来处理。原始数据集存储在亚马逊S3桶中。MongoDB设置在主节点上，Yelp的表从S3导入，我们从那里加载到Zeppelin Notebook。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jo"><img src="../Images/edb5be498f5ca9779e665c04dff0482a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwVFYxsKVhmEVnjEEtvHdA.png"/></div></div></figure><p id="02fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要启动PySpark和MongoDB之间的连接:</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="9d19" class="kf kg hh kb b fi kh ki l kj kk">from pyspark.sql import SparkSession<br/>spark = SparkSession.builder.appName("yelp).getOrCreate()</span><span id="9631" class="kf kg hh kb b fi kl ki l kj kk">#connect to 'database' called yelp, 'collection' called review<br/>review = spark.read.format("com.mongodb.spark.sql.DefaultSource")\<br/>                .option("uri", "mongodb://127.0.0.1:27017/yelp.review").load()</span></pre><p id="ce43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 2。预处理评审数据</strong></p><p id="3764" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们需要使用下面的函数清理评论文本，删除任何标点符号或数字。</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="fcc0" class="kf kg hh kb b fi kh ki l kj kk">import string<br/>import re</span><span id="f9b2" class="kf kg hh kb b fi kl ki l kj kk">def remove_punct(text):<br/>    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\r\\t\\n]')<br/>    nopunct = regex.sub(" ", text)  <br/>    return nopunct</span></pre><p id="95ce" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还需要将星级转换为二进制等级，1表示正面评价，0表示负面评价。如果评价超过4星，则为正面评价；如果低于4星，则为负面评价。4星以上的门槛是因为人们倾向于高估餐厅(例如，人们通常不认为3星餐厅是一家好餐厅)。</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="6254" class="kf kg hh kb b fi kh ki l kj kk">def convert_rating(rating):<br/>    if rating &gt;=4:<br/>        return 1<br/>    else:<br/>        return 0</span></pre><p id="5728" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将把这些函数转换成PySpark的用户自定义函数，并在查询整个“review”集合时将它们应用于text和rating列。</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="749b" class="kf kg hh kb b fi kh ki l kj kk">from pyspark.sql.functions import udf</span><span id="ac63" class="kf kg hh kb b fi kl ki l kj kk">punct_remover = udf(lambda x: remove_punct(x))<br/>rating_convert = udf(lambda x: convert_rating(x))</span><span id="dddf" class="kf kg hh kb b fi kl ki l kj kk">#select 1.5 mn rows of reviews text and corresponding star rating with punc removed and ratings converted<br/>resultDF = df.select('review_id', punct_remover('text'), rating_convert('stars')).limit(1500000)</span><span id="b691" class="kf kg hh kb b fi kl ki l kj kk">#user defined functions change column names so we rename the columns back to its original names<br/>resultDF = resultDF.withColumnRenamed('&lt;lambda&gt;(text)', 'text')<br/>resultDF = resultDF.withColumnRenamed('&lt;lambda&gt;(stars)', 'stars')</span></pre><p id="a777" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们还需要对文本进行标记(分解成单词)并删除停用词(常见的英语术语，如“and”、“then”、“因此”)。这些都将使用现有的SparkML和管道库来完成。</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="8d89" class="kf kg hh kb b fi kh ki l kj kk">from pyspark.ml.feature import *</span><span id="91f6" class="kf kg hh kb b fi kl ki l kj kk">#tokenizer and stop word remover<br/>tok = Tokenizer(inputCol="text", outputCol="words")<br/>#stop word remover<br/>stopwordrm = StopWordsRemover(inputCol='words', outputCol='words_nsw')</span><span id="cfae" class="kf kg hh kb b fi kl ki l kj kk"># Build the pipeline <br/>pipeline = Pipeline(stages=[tok, stopwordrm])<br/># Fit the pipeline <br/>review_tokenized = pipeline.fit(resultDF).transform(resultDF).cache()</span></pre><p id="41b6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们缓存清理后的数据，这样每次调用数据帧时都不需要重新计算。</p><p id="0933" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 3)特征选择:选择并整合最重要的n元文法(可选)</strong></p><p id="e736" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了提高机器学习模型的准确性，我们尝试整合二元模型和三元模型。这是在停用词被移除之前通过标记化的文本列来完成的。</p><p id="7820" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了选择最重要的n gram，我们将二元模型或三元模型的<a class="ae je" href="http://datameetsmedia.com/bag-of-words-tf-idf-explained/" rel="noopener ugc nofollow" target="_blank"> TF-IDF矩阵</a>分别输入到支持向量机模型中，并挑选出具有最大系数权重(正负)的n gram。这很有效，因为<a class="ae je" href="http://proceedings.mlr.press/v3/chang08a/chang08a.pdf" rel="noopener ugc nofollow" target="_blank"> SVM已经被证明是特征选择的好方法</a>。</p><p id="f00c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们将这些ngrams连接回评论中，以便它们作为一个单词而不是两三个单独的单词被纳入模型中(例如，“伟大的客户服务”变成了“伟大的客户服务”)。</p><p id="2cd5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最好的模型由一元模型和三元模型组成。下面是我们如何选择三元模型的一个例子:</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="84f6" class="kf kg hh kb b fi kh ki l kj kk"># add ngram column<br/>n = 3<br/>ngram = NGram(inputCol = 'words', outputCol = 'ngram', n = n)<br/>add_ngram = ngram.transform(review_tokenized)</span><span id="c7d4" class="kf kg hh kb b fi kl ki l kj kk"># count vectorizer and tfidf<br/>cv_ngram = CountVectorizer(inputCol='ngram', outputCol='tf_ngram')<br/>cvModel_ngram = cv_ngram.fit(add_ngram)<br/>cv_df_ngram = cvModel_ngram.transform(add_ngram)</span><span id="e593" class="kf kg hh kb b fi kl ki l kj kk"># create TF-IDF matrix<br/>idf_ngram = IDF().setInputCol('tf_ngram').setOutputCol('tfidf_ngram')<br/>tfidfModel_ngram = idf_ngram.fit(cv_df_ngram)<br/>tfidf_df_ngram = tfidfModel_ngram.transform(cv_df_ngram)</span><span id="2fd0" class="kf kg hh kb b fi kl ki l kj kk"># split into training &amp; testing set<br/>splits_ngram = tfidf_df_ngram.select(['tfidf_ngram', 'label']).randomSplit([0.8,0.2],seed=100)<br/>train_ngram = splits_ngram[0].cache()<br/>test_ngram = splits_ngram[1].cache()</span><span id="5ae8" class="kf kg hh kb b fi kl ki l kj kk"># Convert feature matrix to LabeledPoint vectors<br/>train_lb_ngram = train_ngram.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))<br/>test_lb_ngram = train_ngram.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))</span><span id="7e48" class="kf kg hh kb b fi kl ki l kj kk"># fit SVM model of only trigrams<br/>numIterations = 50<br/>regParam = 0.3<br/>svm = SVMWithSGD.train(train_lb_ngram, numIterations, regParam=regParam)</span><span id="c83d" class="kf kg hh kb b fi kl ki l kj kk">#extract top 20 trigrams based on weights<br/>top_ngram = svm_coeffs_df_ngram.sort_values('weight')['ngram'].values[:20]<br/>bottom_ngram = svm_coeffs_df_ngram.sort_values('weight', ascending=False)['ngram'].values[:20]<br/>ngram_list = list(top_ngram) + list(bottom_ngram)</span></pre><p id="9240" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要在原文中连接这些顶级三元模型:</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="7cba" class="kf kg hh kb b fi kh ki l kj kk"># replace the word with selected ngram<br/>def ngram_concat(text):<br/>    text1 = text.lower()<br/>    for ngram in ngram_list:<br/>        if ngram in text1:<br/>            new_ngram = ngram.replace(' ', '_')<br/>            text1 = text1.replace(ngram, new_ngram)<br/>    return text1</span><span id="5d10" class="kf kg hh kb b fi kl ki l kj kk">ngram_df = udf(lambda x: ngram_concat(x))<br/>ngram_df = review_tokenized.select(ngram_df('text'), 'label')\<br/>                          .withColumnRenamed('&lt;lambda&gt;(text)', 'text')</span></pre><p id="b189" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦实现了这一点，就需要重复步骤2)中的管道，以使用串联的ngram更新标记化的列(而不是将管道应用于resultDF，而是将其应用于ngram_df)。</p><p id="0fc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 4)构建最终的“单词包”TF-IDF特征矩阵</strong></p><p id="7526" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要构建最终的TF-IDF特征矩阵:</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="229c" class="kf kg hh kb b fi kh ki l kj kk"># count vectorizer and tfidf<br/>cv = CountVectorizer(inputCol='words_nsw', outputCol='tf')<br/>cvModel = cv.fit(review_tokenized)<br/>count_vectorized = cvModel.transform(review_tokenized)</span><span id="1b1b" class="kf kg hh kb b fi kl ki l kj kk">tfidfModel = idf.fit(count_vectorized)<br/>tfidf_df = tfidfModel.transform(count_vectorized)</span></pre><p id="6adf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 5)支持向量机</strong></p><p id="1f55" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了测试模型的性能，我们将数据分为训练集和测试集:</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="a2ba" class="kf kg hh kb b fi kh ki l kj kk"># split into training and testing set<br/>splits = tfidf_df.select(['tfidf', 'label']).randomSplit([0.8,0.2],seed=100)<br/>train = splits[0].cache()<br/>test = splits[1].cache()</span></pre><p id="af60" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在训练集上将TF-IDF特征矩阵拟合到SVM模型。一个超参数regParam用于控制过度拟合。值0.3在训练集和测试集上都产生了最好的f1分数。</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="3995" class="kf kg hh kb b fi kh ki l kj kk">numIterations = 50<br/>regParam = 0.3<br/>svm = SVMWithSGD.train(train_lb, numIterations, regParam=regParam)</span><span id="ad98" class="kf kg hh kb b fi kl ki l kj kk">test_lb = test.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))<br/>scoreAndLabels_test = test_lb.map(lambda x: (float(svm.predict(x.features)), x.label))<br/>score_label_test = spark.createDataFrame(scoreAndLabels_test, ["prediction", "label"])</span></pre><p id="dc95" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 6)正规化(弹性网)物流回归</strong></p><p id="c239" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用相同的训练数据，我们还应用了正则化逻辑回归。<a class="ae je" href="http://www.cs.toronto.edu/~kswersky/wp-content/uploads/svm_vs_lr.pdf" rel="noopener ugc nofollow" target="_blank"> SVM专注于寻找最大化最近点到边缘的距离的分离平面，而逻辑回归最大化数据的概率</a>(即离超平面越远越好)。在这种情况下，我们还调整了最佳超参数RegParam和ElasticNetParam以获得最佳结果。我们将模型拟合如下:</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="2ab1" class="kf kg hh kb b fi kh ki l kj kk"># Elastic Net Logit<br/>lambda_par = 0.02<br/>alpha_par = 0.3<br/>lr = LogisticRegression().\<br/>        setLabelCol('label').\<br/>        setFeaturesCol('tfidf').\<br/>        setRegParam(lambda_par).\<br/>        setMaxIter(100).\<br/>        setElasticNetParam(alpha_par)<br/>lrModel = lr.fit(train)<br/>lr_pred = lrModel.transform(test)</span></pre><p id="ea66" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 7)评估</strong></p><p id="5e4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请注意，由于数据集不平衡(正面评价多于负面评价)，f1分数被选为评估基础。准确度不能说明全部情况，而f1-作为准确度和召回率的加权平均值-可以揭示模型在识别预测相关性和正确预测真正相关结果的百分比方面的表现。</p><p id="6459" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">评估SVM模型:</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="f1ea" class="kf kg hh kb b fi kh ki l kj kk">f1_eval = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")<br/>svm_f1 = f1_eval.evaluate(score_label_test)<br/>print("F1 score: %.4f" % svm_f1)</span></pre><p id="03af" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">评估物流回归模型:</p><pre class="jp jq jr js fd ka kb kc kd aw ke bi"><span id="0cbb" class="kf kg hh kb b fi kh ki l kj kk">f1_eval = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")<br/>lr_f1 = f1_eval.evaluate(lr_pred)<br/>print("F1 score: %.4f" % lr_f1)</span></pre><p id="ec22" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">在测试集上，SVM模型的f1值最高，为87.32%。</strong>逻辑回归模型在测试集上的f1值为84.21%。</p><p id="58ca" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> 8)特征分析</strong></p><p id="ab5b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以使用来自最终SVM模型的系数权重来查看哪些术语对正面评论或负面评论的贡献最大。在这种情况下，与最积极的系数相关联的术语代表那些对积极评价贡献最大的术语。与最负系数相关联的术语表示对负面评价贡献最大的那些术语。</p><p id="8f4c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是一个单词云，用蓝色表示最“积极”的词语，用红色表示最“消极”的词语。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es km"><img src="../Images/a052e489fe05a16038c9393dfc1f0e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5E2UA4W_03UOMr1jK4DY6Q.png"/></div></div></figure><p id="ee7e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以看到，最积极的词语包括“友好的员工”、“美味”、“出色的客户服务”、“美味的食物”。这表明顾客满意的重要因素是员工、食物口味和服务。可以通过删除诸如“棒极了”、“推荐”等短语来进一步改进。以便更多有意义的标记可以出现。</p><p id="9b5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最负面的词语包括“分钟”、“价格过高”、“淡而无味”、“食物还可以”、“没什么特别的”。这表明，负面评价是由漫长的等待时间、价格过高的食物、糟糕的食物味道造成的，这种经历并不被视为什么特别的事情。</p><p id="316d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇文章总结了产生最佳结果的步骤。要查看我们尝试的所有步骤的所有代码，请查看这个<a class="ae je" href="https://github.com/nicharuc/yelp-sentiment-prediction/blob/master/yelp_nlp_svm.md" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p></div><div class="ab cl kn ko go kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ha hb hc hd he"><figure class="jp jq jr js fd jt"><div class="bz dy l di"><div class="ku kv l"/></div></figure><p id="6ecd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">请点击👏按钮下面几下，以示支持！⬇⬇谢谢！不要忘记遵循下面的快速代码。</strong></p><blockquote class="kw"><p id="d6c0" class="kx ky hh bd kz la lb lc ld le lf jb dx translated">找到关于各种编程语言的<a class="ae je" href="http://www.quickcode.co/" rel="noopener ugc nofollow" target="_blank">快速代码</a>的免费课程。获取<a class="ae je" href="https://www.messenger.com/t/1493528657352302" rel="noopener ugc nofollow" target="_blank"> Messenger </a>的新更新。</p></blockquote></div></div>    
</body>
</html>