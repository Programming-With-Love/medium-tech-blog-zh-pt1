<html>
<head>
<title>Introducing automatic object detection to visual search</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将自动对象检测引入视觉搜索</h1>
<blockquote>原文：<a href="https://medium.com/pinterest-engineering/introducing-automatic-object-detection-to-visual-search-e57c29191c30?source=collection_archive---------0-----------------------#2016-06-28">https://medium.com/pinterest-engineering/introducing-automatic-object-detection-to-visual-search-e57c29191c30?source=collection_archive---------0-----------------------#2016-06-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e95f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">视觉搜索工程师Dmitry Kislyuk</p><p id="edf8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当我们去年推出<a class="ae jc" href="https://engineering.pinterest.com/blog/introducing-new-way-visually-search-pinterest" rel="noopener ugc nofollow" target="_blank">视觉搜索</a>时，我们第一次看到了当你使用图像作为搜索查询时的可能性。现在，每个月都有超过1.3亿次的视觉搜索，因为人们会搜索他们在大头针上看到的物品、风格和颜色，并获得相关的推荐。这是一种全新的搜索方式，也是一项技术挑战。</p><p id="9d48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">今天，我们为Pinterest上最受欢迎的类别引入了自动对象检测，因此人们可以在大头针图像内直观地搜索产品。当我们展望视觉搜索的未来时，我们也开始预览新的相机搜索技术，这将为Pinners推荐他们在现实世界中找到的产品。Pinners将很快能够拍摄单个物体的照片，如运动鞋，并在Pinterest上获得推荐，甚至可以拍摄整个房间的照片，并获得多个项目的结果。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/fa15705be98d9b0a64f4f587ae165839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PCBauOs5tH63Ewef.jpg"/></div></div></figure><h2 id="5bf6" class="jp jq hh bd jr js jt ju jv jw jx jy jz ip ka kb kc it kd ke kf ix kg kh ki kj bi translated">Pinterest的深度学习</h2><p id="a640" class="pw-post-body-paragraph ie if hh ig b ih kk ij ik il kl in io ip km ir is it kn iv iw ix ko iz ja jb ha bi translated">视觉搜索是近年来深度学习的进步转变的许多领域之一。卷积神经网络将图像和视频表示为保留语义概念和视觉信息的特征向量，并在使用优化的最近邻技术时允许快速检索。去年11月，当我们发布了一个<a class="ae jc" href="https://engineering.pinterest.com/blog/introducing-new-way-visually-search-pinterest" rel="noopener ugc nofollow" target="_blank">视觉搜索产品</a>时，我们利用了这个想法，以及我们丰富注释的图像数据集，使在大头针图像中搜索变得像拖动庄稼一样简单。在我们的首次发布中，我们从10亿张Pinterest图片中提取了微调后的VGG模型的全连接-6层，并将其索引到分布式服务中，正如我们在<a class="ae jc" href="http://arxiv.org/abs/1505.07647" rel="noopener ugc nofollow" target="_blank"> KDD的论文</a>中所述。</p><p id="08cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您的浏览器不支持视频标签。</p><h2 id="bd65" class="jp jq hh bd jr js jt ju jv jw jx jy jz ip ka kb kc it kd ke kf ix kg kh ki kj bi translated">动机</h2><p id="01d1" class="pw-post-body-paragraph ie if hh ig b ih kk ij ik il kl in io ip km ir is it kn iv iw ix ko iz ja jb ha bi translated">由于一个图像可以包含几十个对象，我们希望尽可能简单地从它们中的任何一个开始发现体验。与自动完成改善文本搜索体验的方式相同，自动对象检测使视觉搜索体验更加无缝。视觉搜索中的对象检测也启用了新功能，如对象对对象匹配。例如，假设你在Pinterest或朋友家里发现了一张你喜欢的咖啡桌，很快你就能看到它在许多不同的家居环境中会是什么样子。</p><h2 id="21e9" class="jp jq hh bd jr js jt ju jv jw jx jy jz ip ka kb kc it kd ke kf ix kg kh ki kj bi translated">建筑物自动目标检测</h2><p id="489e" class="pw-post-body-paragraph ie if hh ig b ih kk ij ik il kl in io ip km ir is it kn iv iw ix ko iz ja jb ha bi translated">我们在构建自动对象检测中的第一个挑战是收集图像中感兴趣区域的标记包围盒作为我们的训练数据。自发布以来，我们已经处理了近10亿幅图像(视觉搜索)。通过在数百万张参与度最高的图片中汇总这种活动，我们可以了解Pinners对哪些对象感兴趣。我们将视觉上相似的结果的注释聚合到每个裁剪中，并在数百个对象类别中分配一个弱标签。下面的热图显示了这种情况的一个示例，其中形成了两个用户裁剪聚类，一个围绕“围巾”标注，另一个围绕“包”标注。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kp"><img src="../Images/3c397edbceefdd5c2fbc2be53e88dc30.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/0*LLK4GjZUXT-kB58-.png"/></div></figure><p id="0a0b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们的视觉搜索引擎可以使用任何图像作为查询——包括网络上看不见的内容，甚至是你的摄像头——检测必须实时进行，在几分之一秒内完成。我们广泛试验的最广泛使用的检测模型之一是更快的R-CNN，它使用深度网络在两个主要步骤中检测图像中的对象。首先，它通过在输入图像上运行完全卷积网络来产生特征图，从而识别图像中可能包含感兴趣对象的区域。对于特征图上的每个位置，网络考虑一组固定的区域，这些区域的大小和长宽比不同，并使用二进制softmax分类器来确定每个区域包含感兴趣对象的可能性。如果找到一个有希望的区域，网络还输出对该区域的调整，以便它更好地框住对象。</p><p id="2b72" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">一旦网络发现了感兴趣的区域，它就检查最有希望的区域，并试图将每个区域识别为特定类别的对象，或者如果没有发现对象，就丢弃它。对于每个候选区域，网络在卷积特征图的相应部分上执行空间汇集，从而产生具有固定大小的特征向量，而与区域的大小无关。该汇集的特征然后被用作检测网络的输入，该检测网络使用softmax分类器来将每个区域识别为背景或我们的对象类别之一。如果检测到对象，网络再次输出对区域边界的调整，以进一步改进检测质量。最后，对检测执行一轮非最大值抑制(NMS ),以过滤掉任何重复的检测，并将结果呈现给用户。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kq"><img src="../Images/46bd5fd6afc7de00d06d8399e44eefc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*otStGwgdlsRodJWx.png"/></div></div></figure><p id="d605" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用更快的R-CNN实现高速检测的关键技巧之一是在区域提议器和检测网络中使用的卷积特征是同一个。网络延迟的很大一部分花费在产生这个中间卷积特征图上，并且通过在两个网络组件之间共享它，我们减少了冗余计算的量。这使我们能够在几分之一秒内识别物体。</p><p id="900d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您的浏览器不支持视频标签。</p><p id="ae77" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">去年，我们部署了我们自己的这一模型的实现，以计算相关pin中的目标视觉相似性特征，这是我们的推荐产品之一，导致参与度增加了4%，如我们的<a class="ae jc" href="http://arxiv.org/abs/1511.04003" rel="noopener ugc nofollow" target="_blank">技术报告</a>中所详述的。</p><p id="a8c2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从那以后，我们一直致力于通过应用最近发表的深度残差网络(ResNets)的进展来提高该模型的准确性和效率。尽管最终的网络由100多个卷积层组成，但我们一直专注于减少该模型的GPU内存占用，以适合在AWS上部署，同时将延迟保持在300毫秒以下。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kr"><img src="../Images/c11924303de62832ab1876d03a1966cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/0*jipsfUFf8XQiaNYC.gif"/></div></figure><p id="a0cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">通过对Pinterest、网络或现实世界中任何地方的任何图像进行实时对象检测，Pinterest上的视觉搜索变得更加出色。物体检测将在未来几周在所有Pinners和平台上推出。</p><h2 id="e13e" class="jp jq hh bd jr js jt ju jv jw jx jy jz ip ka kb kc it kd ke kf ix kg kh ki kj bi translated">视觉搜索的未来</h2><p id="2f4e" class="pw-post-body-paragraph ie if hh ig b ih kk ij ik il kl in io ip km ir is it kn iv iw ix ko iz ja jb ha bi translated">我们还在开发一种技术，帮助人们在Pinterest上获得他们在现实世界中发现的产品的推荐，只需拍张照片。这将实现一种新的视觉搜索体验，结合图像检索，对象检测和我们的兴趣图的力量。请继续关注相机搜索技术的更多信息。</p><p id="e57b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="ks">鸣谢</em> </strong> <br/> <em class="ks">视觉搜索是Pinterest的一项合作成果，我们要感谢来自伯克利视觉与学习中心(BVLC)的徐克雷、Vishwa Patel、Andrew Zhai、Shirley Du、、Michelle Vu、Michael Feng和Kevin Jing，以及Eric Tzeng、Jeff Donahue和Trevor Darrell。此外，我们要感谢Mike Repass、Naveen Gavini和Albert Pereta，他们使产品发布成为可能。</em></p></div></div>    
</body>
</html>