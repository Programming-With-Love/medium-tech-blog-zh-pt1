<html>
<head>
<title>Auto-Correction and Suggestions Using LSTM based Char2Vec Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于LSTM的Char2Vec模型的自动校正及建议</h1>
<blockquote>原文：<a href="https://medium.com/version-1/auto-correction-and-suggestion-using-lstm-based-char2vec-model-e276d24471ea?source=collection_archive---------1-----------------------#2021-04-28">https://medium.com/version-1/auto-correction-and-suggestion-using-lstm-based-char2vec-model-e276d24471ea?source=collection_archive---------1-----------------------#2021-04-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/9f5d5272711dba3b0b6355963e81bbd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6fQIyY8bxB8OfsgXrZ6bQ.jpeg"/></div></div></figure><h1 id="3122" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">介绍</h1><p id="016d" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">你有没有好奇过你的手机是如何自动纠正你的短信的？或者谷歌如何在你拼错单词之前就知道你拼错了？</p><p id="32a2" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">让我告诉你，这是基于广为人知的自动纠错这个话题。在这篇文章中，我将带您浏览一个类似的基于字符的语言模型，这个模型是我为建议和自动更正单词而构建的。</p><p id="69ee" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">问题陈述:</strong></p><p id="9b5b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我和我在Version1创新实验室的团队一起参与了一个理解各种手写表单的项目。即使对于市场上最先进的玩家来说，理解手写文本也是一项艰巨的任务。原因有很多，包括没有标准的书写方式，字符之间的噪声和失真，扫描不良的表格图像或书写方向因人而异。由于这些挑战，我们在表单中得到了许多不正确识别的字段值。</p><p id="2f1b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">为了解决这个问题，我们在现有流程的基础上增加了一个额外的层，称为后处理阶段，它可以纠正这些表单字段。我已经使用了基于<strong class="jp hi"> Bi-LSTM </strong>的<strong class="jp hi"> Char2Vec </strong>模型来纠正这些单词(在本例中是爱尔兰姓氏)，并且对于两个字母的编辑单词(有两个字母拼错的单词)实现了94.36%的准确率<strong class="jp hi">。</strong></p><p id="95d9" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在我们深入研究技术细节和我如何构建这个模型之前，让我们先在下一节中简要了解一些基本原理。</p><h1 id="c061" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">基本原则</h1><ol class=""><li id="8eff" class="kq kr hh jp b jq jr ju jv jy ks kc kt kg ku kk kv kw kx ky bi translated"><strong class="jp hi">什么是单词嵌入？</strong></li></ol><p id="2c8b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">它只是单词的向量表示。具体来说，向量空间模型(VSM)是一种空间，其中文本被表示为向量，使得具有相似上下文和语义的单词彼此靠近。</p><figure class="la lb lc ld fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kz"><img src="../Images/70acedde766255842350113467c63b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sXNXYfAqfLUeiDXPCo130w.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx">Fig 1: Word Embedding with similar context have close proximity</figcaption></figure><p id="3703" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">相关词占用的空间更近，如上图所示。为了找到单词在空间中的位置，计算它们各自向量之间的距离。</p><p id="41c6" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> 2。如何计算单词嵌入？</strong></p><p id="e3ba" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">可以使用各种方法来计算，这些方法可以大致分为两类:</p><ol class=""><li id="1f09" class="kq kr hh jp b jq kl ju km jy li kc lj kg lk kk kv kw kx ky bi translated"><strong class="jp hi">基于频率的嵌入:</strong>基于计数的方法计算某个单词与其相邻单词共现的频率统计，然后将这些计数统计映射到每个单词的一个小而密集的向量。这些方法是<strong class="jp hi">计数向量</strong>、<strong class="jp hi"> TF-IDF向量</strong>和<strong class="jp hi">同现向量</strong></li><li id="98bf" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk kv kw kx ky bi translated"><strong class="jp hi">基于预测的嵌入:</strong>预测模型根据学习到的小而密集的嵌入向量，直接尝试预测来自其邻居的单词。方法有<strong class="jp hi">连续包字(CBOW) </strong>和<strong class="jp hi">跳克。</strong></li></ol><p id="c829" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">讨论所有的方法超出了这篇文章的范围。要了解更多，请点击<a class="ae lq" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">链接</em> </a>。现在，我们将专注于基于神经网络的方法，通常称为<strong class="jp hi"> Word2Vec </strong>，它是<strong class="jp hi"> CBOW </strong>和<strong class="jp hi"> Skip-Gram </strong>的混合。为了从这些算法中获得每个单词的嵌入，他们使用一个简单的三层神经网络:输入层、隐藏层和输出层。<strong class="jp hi">隐层和输出层之间的权重作为单词的单词向量表示。</strong></p><p id="3f2d" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">让我们简要了解两种基于预测的方法:</p><p id="9f6a" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> i. </strong> <strong class="jp hi"> CBOW </strong>:这个模型学习使用其邻域中的所有术语来预测目标单词。使用上下文向量的数量来预测目标单词。所考虑的相邻项由围绕目标单词的预定义窗口大小来确定。</p><p id="861c" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">二。</strong> <strong class="jp hi"> Skip-Gram </strong>:另一方面，这个模型学习根据一个相邻的单词来预测一个单词。简单来说，给定一个单词，它就学会在上下文中预测另一个单词。</p><figure class="la lb lc ld fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ls"><img src="../Images/cf6fc3f64d537fb9a090d183c9c3a87d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLbxTeFlrJNLSU17CzGICw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx">Fig 2: CBOW and Skip-Gram by Kavita Ganesan</figcaption></figure><p id="5975" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> 3。Word2Vec模型的挑战？</strong></p><p id="400d" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">如前所述，Word2Vec使用神经网络模型从大量文本中学习单词关联。该模型在对大型文本语料库进行训练时提取了一些关于词义的语义信息，但它们是在一个<strong class="jp hi">固定词汇</strong>中操作的(通常缺少一些很少使用的词)。</p><p id="3746" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在解决某些NLP问题时，这是一个巨大的缺点。如果文本中的几个重要术语不属于语义语言模型的词汇表，那么语义语言模型在解决某些类型的NLP任务时是低效的——该模型将无法解释这些单词。如果我们试图处理人类写的文本，我们可能会陷入这种境地(这些可能是回复、评论、文档或网上的帖子)。这些文本可能包括错别字、俚语、来自独特位置的生僻字，或者不包括在语言词典中的名字，因此也不包括在语义模型中。</p><p id="c4aa" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> 4。Char2Vec模型介绍？</strong></p><p id="d613" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">为了解决上面提到的问题，<a class="ae lq" href="https://hackernoon.com/chars2vec-character-based-language-model-for-handling-real-world-texts-with-spelling-errors-and-a3e4053a147d" rel="noopener ugc nofollow" target="_blank">直觉工程</a>创建了一个语言模型，该模型仅基于单词的拼写来创建单词嵌入，并收集相似矢量来相似地拼写单词。</p><p id="3df4" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">该模型用一个固定长度的向量来表示符号(单词)序列，单词拼写的相似性用向量之间的距离度量来表示。由于该模型不基于字典(不存储固定的具有对应向量表示的术语字典)，其初始化和使用不需要大量的计算能力。</p><p id="8b26" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在下图中，绿色的单词是正确的，黑色的是拼写相似但不在字典中的错别字。该模型能够根据向量距离度量对它们进行分组。</p><figure class="la lb lc ld fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lt"><img src="../Images/e8f0df41bedd6e972eba7efdc21d2615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RuWy9a1JCpkgTesT4nUlKQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx">Fig 3: Similarly spelt words are grouped</figcaption></figure><p id="ff4e" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我开发了一个类似的基于语言的字符模型，有一些改进，如使用<strong class="jp hi">双LSTM </strong>来获得嵌入和组合<strong class="jp hi"> KNN </strong>和<strong class="jp hi"> Levenshtein </strong>距离来找到最近的邻居，而不是基于余弦的相似性，这导致了准确性的显著提高。</p><p id="0fe4" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> 5。莱文斯坦距离</strong></p><p id="a86e" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> Levenshtein距离</strong>是一个<a class="ae lq" href="https://en.wikipedia.org/wiki/String_metric" rel="noopener ugc nofollow" target="_blank">字符串度量</a>，用于测量两个序列之间的差异。非正式地，两个单词之间的Levenshtein距离是将一个单词变成另一个单词所需的单个字符编辑(插入、删除或替换)的最小数量。例如,“小猫”和“坐着”之间的Levenshtein距离是3，因为至少需要3次编辑才能将一个变成另一个。</p><ol class=""><li id="7ae3" class="kq kr hh jp b jq kl ju km jy li kc lj kg lk kk kv kw kx ky bi translated"><strong class="jp hi">k</strong>itten→<strong class="jp hi">s</strong>itten(用“s”代替“k”)</li><li id="160d" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk kv kw kx ky bi translated">sitt<strong class="jp hi">e</strong>n→sitt<strong class="jp hi">I</strong>n(用“I”代替“e”)</li><li id="1fc6" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk kv kw kx ky bi translated">sittin → sittin <strong class="jp hi"> g </strong>(在末尾插入“g”)。</li></ol><p id="e9ce" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我已经使用Levenshtein距离在所有最近的邻居中找到与当前单词向量最接近的匹配。为了计算最近邻，我使用了基于人工神经网络搜索的k-最近邻(<a class="ae lq" href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods" rel="noopener ugc nofollow" target="_blank">近似最近邻</a>)使用<a class="ae lq" href="https://github.com/nmslib/nmslib" rel="noopener ugc nofollow" target="_blank"> nmslib </a>。</p><p id="30d9" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">6。LSTM和比-LSTM </p><p id="b070" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">长短期记忆</strong> ( <strong class="jp hi"> LSTM </strong>)是一种用于深度学习领域的人工递归神经网络(RNN)架构。与标准的前馈神经网络不同，LSTM有反馈连接。它不仅可以处理单个数据点，还可以处理整个数据序列。一个普通的LSTM单元由一个<strong class="jp hi">单元</strong>、一个<strong class="jp hi">输入门</strong>、一个<strong class="jp hi">输出门</strong>和一个<strong class="jp hi">遗忘门</strong>组成。该单元记忆任意时间间隔内的值，三个<em class="lr">门</em>调节进出该单元的信息流。我强烈推荐这个<a class="ae lq" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">博客</a>来更详细地了解LSTM。</p><figure class="la lb lc ld fd ii er es paragraph-image"><div class="er es lu"><img src="../Images/5bd186ec53ca5e9a984cfaa7319f3297.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*wEfQMA-XGZkp7aNozmwIqg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx">Fig 4: LSTM Unit</figcaption></figure><p id="c52a" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">双向LSTMs(双LSTM) </strong>，也使用相同的LSTM单位，但数据向两个方向流动，这意味着，我们从开始到结束和从结束到开始分别向学习算法提供一次原始数据。我使用这种双LSTM架构作为生成嵌入的基础模型，结果非常好。现在让我们在下一节看看我是如何实现这个模型的。</p><h1 id="fe8e" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">履行</h1><p id="d851" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">我所使用的方法是受<a class="ae lq" href="https://lettria.com/fr" rel="noopener ugc nofollow" target="_blank"><em class="lr">Lettria</em></a><em class="lr"/>在这篇<a class="ae lq" href="https://towardsdatascience.com/embedding-for-spelling-correction-92c93f835d79" rel="noopener" target="_blank"><em class="lr"/></a>文章中所做工作的启发。方法如下:</p><ul class=""><li id="a2fb" class="kq kr hh jp b jq kl ju km jy li kc lj kg lk kk lv kw kx ky bi translated">创建一个训练数据集。</li><li id="a120" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated">训练一个给出字符级单词嵌入的模型。</li><li id="f39b" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated">对整个词典进行矢量化处理，并为高效搜索建立索引。</li><li id="2f33" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated">向量化拼写错误的单词，并寻找它们最近的邻居。</li></ul><ol class=""><li id="5d2d" class="kq kr hh jp b jq kl ju km jy li kc lj kg lk kk kv kw kx ky bi translated"><strong class="jp hi">训练数据集创建</strong></li></ol><p id="6f95" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">训练数据已经使用<strong class="jp hi"> 6K+单词</strong>(爱尔兰姓氏)的字典创建。我使用插入、删除、置换和替换原始单词中的字符生成了几个拼写错误的单词。这导致为每个正确的单词创建四个相似的单词，从而产生<strong class="jp hi"> 24K+ </strong>相似的元组。例如:(Ankit，Ankt)，(Ankit，Anikt)，(Ankit，Ankeit)，(Ankit，Abkit)。</p><p id="0f53" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">类似地，我为每个正确的单词创建了四个不相似的单词，这导致了<strong class="jp hi"> 24k+ </strong>不相似的元组。例如:(安基特，苏米特)，(安基特，约翰)，(安基特，罗伯特)，(安基特，亚历山大)。所以整个训练数据集变成围绕<strong class="jp hi"> 48K+ </strong>元组。<strong class="jp hi">最后，</strong> <strong class="jp hi">我把相似对的目标赋值为0，不相似对的目标赋值为1。</strong></p><p id="943c" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> 2。模型训练和架构</strong></p><p id="da4e" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">该模型使用两层双LSTM来构建选定大小的嵌入。更高的维数以更长的计算时间为代价提供了更精确的结果。对于这个模型，我们已经满足于300维。</p><p id="3dd5" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">该模型通过传递一组单词来训练，这些单词或者是<strong class="jp hi"> <em class="lr">两个完全不同的单词，或者是一个单词和它的一个拼写错误，如上面的</em> </strong> <em class="lr">部分所述。</em>训练目标是<strong class="jp hi"> <em class="lr">对于相似元组最小化两个嵌入之间的差异，对于不同单词</em> </strong> <em class="lr">最大化两个嵌入之间的差异。</em></p><figure class="la lb lc ld fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/dcf9dc691769b6d54905a7d3e15bf01e.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*nUzkLKpLlWEkZsljTmqDdQ.jpeg"/></div><figcaption class="le lf et er es lg lh bd b be z dx">Fig 5: Training Model Architecture</figcaption></figure><p id="acd8" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">我已经在<a class="ae lq" href="https://azure.microsoft.com/en-us/services/databricks/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">数据块</em> </a> <em class="lr"> </em>上训练了这个模型50个时期。为了训练整个基于双LSTM的Char2Vec模型，使用GPU花费了大约3个小时。在典型的CPU机器上，这可能需要长达8个小时。</p><p id="9acb" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> 3。可视化</strong></p><p id="590e" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">一旦我们训练了我们的模型，下一步就是检查它是否能聚类拼写相似的单词。首先，我定义了一些正确的和错误的(拼写错误的)单词，然后通过训练好的模型对这些单词进行矢量化。通过用PCA将它们投影到2D平面上，我们得到了下面的图:</p><figure class="la lb lc ld fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lx"><img src="../Images/9211e9ff951adc373114d63d81fa3f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bAt4sunnL86yXrOPacSqvw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx">Fig 6: Similar Irish last names are grouped together</figcaption></figure><p id="1d2b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">大获成功！黑色的名字是正确的，红色的是拼写错误的。可以看出，拼写相似的单词彼此更接近。</p><p id="a595" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><em class="lr">注:“麦克马洪”这个词看起来与实际的姓氏相去甚远，但事实并非如此。如果我们从右边看，我们会发现它更接近实际的姓氏。</em></p><p id="d860" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> 4。用法修正</strong></p><p id="c8e7" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">下一步是检查我们是否能纠正这些拼写错误的单词。为此，我们将对6K+单词的整个字典进行矢量化。根据单词的大小，这个过程可能需要一些时间，但对于我的字典来说，在我的CPU上只需要1分钟。</p><p id="b848" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">最后一步是构建一个索引，使我们能够有效地搜索最近的向量。为此，我们使用了一个专门用于近似最近邻(ANN)搜索的库。</p><p id="8456" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">一旦我们对字典进行了矢量化并创建了索引，下一步就是使用这些索引找到当前拼写错误的单词的k个最近邻。在我的例子中，我保持k=80，这是默认值，它工作得非常好。一旦我们得到k个(这里是80个)邻居，我们就检查拼写错误的单词与其邻居的Levenshtein <strong class="jp hi"> </strong>距离，并过滤距离最小的单词。</p><figure class="la lb lc ld fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/7b6dcb77bc24c44cd89f5117c4f896c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*onUjcKJHtTY2UVjhsm9Ipg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx">Fig 7: Auto-correct suggestion output</figcaption></figure><p id="856c" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">在上面的输出中，我们可以看到该模型能够在所有情况下给出正确的姓氏“bolger”。编辑距离1表示输入中有一个错误或缺失的字母，编辑距离2表示输入中有两个错误或缺失的字母(拼写错误的单词)。</p><p id="f232" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi"> 5。评估</strong></p><p id="724b" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">上面的结果只是对一个单词(姓氏)进行了两次不同的编辑，但是我想对所有拼写错误的单词进行评估。</p><p id="b696" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">为了做到这一点，我使用了一个定制的指标来检查实际的单词是否出现在模型的自动更正建议中，如图7所示。如果实际的单词出现在建议中，我认为这个预测是正确的，否则就是不正确的。</p><figure class="la lb lc ld fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lz"><img src="../Images/989a78f5df652b9c0749ae74308803fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HzDJJmRTZV8QGZUAU7kNsg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx">Fig 8: Accuracy with 2 letter edit</figcaption></figure><p id="8e0d" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">最后，我用正确预测的总数除以预测的总数来计算准确度。如图8所示，我达到的总体精度是<strong class="jp hi"> 94.36% </strong>。</p><h1 id="742e" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">限制</h1><ul class=""><li id="d436" class="kq kr hh jp b jq jr ju jv jy ks kc kt kg ku kk lv kw kx ky bi translated">这个模型对于长度小于3的单词不太适用。</li><li id="a2e9" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated">如果嵌入和训练集大小增加，则训练时间显著增加。</li><li id="396f" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated">如果单词量很大，字典的矢量化需要时间。</li></ul><h1 id="a19c" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">其他一些应用</h1><ul class=""><li id="c470" class="kq kr hh jp b jq jr ju jv jy ks kc kt kg ku kk lv kw kx ky bi translated">改进了数据输入的表单验证，即该模型可用于纠正用户输入数据时的错误。</li><li id="7b01" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated">数据清理:修复数据库表中的坏数据</li><li id="a26f" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated">引人入胜的见解:许多问答平台，如Reddit、Quora和Stack Overflow，都有基于用户的输入。很多时候，用户在写作时会犯错别字或使用渣滓。机器很难理解这些词，这使得它很难进行分析。使用我们当前的模型，通过找到与这些错别字最接近的正确单词，可以解决这个问题。</li></ul><h1 id="8b31" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">结论</h1><p id="513e" class="pw-post-body-paragraph jn jo hh jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ha bi translated">字符级嵌入提供了出色的整体效率，尤其是对于较长的单词。双LSTM甚至更好地理解了序列和嵌入。未来的工作包括在各种其他场景中测试该模型并测量其准确性。</p><p id="8674" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated">希望这篇文章对你有帮助。如果你喜欢它，请在medium上关注我，为这个帖子鼓掌，并关注我未来的帖子。也可以在<a class="ae lq" href="https://www.linkedin.com/in/ankitk2109/" rel="noopener ugc nofollow" target="_blank"><strong class="jp hi"><em class="lr">LinkedIn</em></strong></a><strong class="jp hi"><em class="lr">或</em></strong><a class="ae lq" href="https://github.com/ankitk2109" rel="noopener ugc nofollow" target="_blank"><strong class="jp hi"><em class="lr">Github</em></strong></a><strong class="jp hi"><em class="lr">上与我联系。</em> </strong></p><h1 id="c530" class="ip iq hh bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">参考</h1><ul class=""><li id="a5d8" class="kq kr hh jp b jq jr ju jv jy ks kc kt kg ku kk lv kw kx ky bi translated"><a class="ae lq" href="https://hackernoon.com/chars2vec-character-based-language-model-for-handling-real-world-texts-with-spelling-errors-and-a3e4053a147d" rel="noopener ugc nofollow" target="_blank">https://hacker noon . com/chars 2 vec-character-based-language-model-for-handling-real-world-texts-with-spelling-errors-and-a3e 4053 a 147d</a></li><li id="da8e" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated"><a class="ae lq" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2017/06/word-embeddings-count-word 2 veec</a></li><li id="30fe" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated"><a class="ae lq" href="https://towardsdatascience.com/embedding-for-spelling-correction-92c93f835d79" rel="noopener" target="_blank">https://towards data science . com/embedding-for-spelling-correction-92c 93 f 835d 79</a></li><li id="327b" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated"><a class="ae lq" href="https://israelg99.github.io/2017-03-22-Vector-Representations-of-Words/" rel="noopener ugc nofollow" target="_blank">https://israelg 99 . github . io/2017-03-22-Vector-Representations-of-Words/</a></li><li id="bdb1" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated"><a class="ae lq" href="https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/" rel="noopener ugc nofollow" target="_blank">https://kavita-ganesan . com/comparison-between-cbow-skip gram-subword/</a></li><li id="1219" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated"><a class="ae lq" href="https://github.com/Lettria/Char2Vec" rel="noopener ugc nofollow" target="_blank">https://github.com/Lettria/Char2Vec</a></li><li id="4feb" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Levenshtein_distance</a></li><li id="9864" class="kq kr hh jp b jq ll ju lm jy ln kc lo kg lp kk lv kw kx ky bi translated"><a class="ae lq" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li></ul></div><div class="ab cl ma mb go mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ha hb hc hd he"><p id="7eea" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><strong class="jp hi">关于作者</strong></p><p id="6dca" class="pw-post-body-paragraph jn jo hh jp b jq kl js jt ju km jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk ha bi translated"><em class="lr"> Ankit Kumar目前是爱尔兰CeADAR的一名数据科学家，在</em> <a class="ae lq" href="https://www.version1.com/innovation/" rel="noopener ugc nofollow" target="_blank"> <em class="lr"> Version 1的创新实验室</em> </a> <em class="lr">工作，作为一名创新数据科学家，他为企业提供创造性的解决方案和概念证明，以确保Version 1处于突破性创新的前沿。</em></p></div></div>    
</body>
</html>