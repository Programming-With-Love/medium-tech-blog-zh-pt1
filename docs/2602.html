<html>
<head>
<title>Memahami Numerical Feature Scaling Untuk Meningkatkan Performace Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Memahami Numerical Feature Scaling Untuk Meningkatkan Performace Machine Learning</h1>
<blockquote>原文：<a href="https://medium.easyread.co/memahami-semua-tentang-numerical-feature-scaling-dalam-8-menit-d79c65e0ccb6?source=collection_archive---------0-----------------------#2021-11-06">https://medium.easyread.co/memahami-semua-tentang-numerical-feature-scaling-dalam-8-menit-d79c65e0ccb6?source=collection_archive---------0-----------------------#2021-11-06</a></blockquote><div><div class="fc ib ic id ie if"/><div class="ig ih ii ij ik"><div class=""/><p id="a359" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><em class="ki"> Artikel ini menjelaskan tentang feature scaling untuk meningkatkan performa model. </em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/1ea85a2eaf69ee350a473eb7e395c3e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Bq69p3OIRFuC4GmC"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Photo by <a class="ae kz" href="https://unsplash.com/@siora18?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Siora Photography</a> on <a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="059f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Salah satu tahapan penting dalam pipeline machine learning adalah feature scaling. Tahapan ini termasuk dalam tahap pipeline data preparation atau jika di dapur termasuk dalam proses persiapan bahan memasak. Untuk menghasilkan masakan yang enak, maka siapkan bumbu dan bahan masakan sesuai proposinya. Yap, sama dengan machine learning, agar model yang dihasilkan bagus, maka fitur yang dimasukkan juga harus sesuai dengan proporsinya. Artikel ini akan mengcover beberapa pertanyaan terkait feature scaling agar dapat meningkatkan kinerja model yaitu :</p><ol class=""><li id="235a" class="la lb in jm b jn jo jr js jv lc jz ld kd le kh lf lg lh li bi translated">Apa itu feature scaling?</li><li id="c39d" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">Mengapa sangat penting?</li><li id="2cf0" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">Kapan menggunakan feature scaling?</li><li id="f0c9" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh lf lg lh li bi translated">Apa saja tekniknya? dan bagaimana implementasinya?</li></ol></div><div class="ab cl lo lp hr lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ig ih ii ij ik"><h1 id="ca65" class="lv lw in bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">Apa itu feature scaling?</h1><p id="10da" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">Feature scaling merupakan teknik statistik yang berfungsi untuk menormalisasikan range pada fitur — fitur data sehingga seluruh fitur berada pada range yang sama. Bayangkan sebuah perusahaan memiliki data karyawan dengan rincian fitur umur (range : 20–55 tahun) dan gaji (range : 1.000.000–100.000.000). Kedua fitur karyawan tersebut berada pada range yang berbeda yaitu puluhan dan jutaan. Kenapa? Yap, karena satuan ukurnya berbeda, umur dengan satuan tahun, sementara gaji dengan satuan rupiah. Tapi, mesin hanya mengerti angka sehingga gaji akan selalu lebih besar daripada umur tanpa memperhatikan skala pengukurannya dan ini akan mempengaruhi hasil prediksi. Oleh karena itu, dengan feature scaling tiap fitur akan diubah dalam range yang sama seperti ke dalam range [-1, 1], [0,1] atau unitness . Tapi, kenapa fitur data karyawan tersebut harus di ubah kedalam range yang sama? Apa yang menjadi masalah?</p><h1 id="7537" class="lv lw in bd lx ly my ma mb mc mz me mf mg na mi mj mk nb mm mn mo nc mq mr ms bi translated">Kenapa penting?</h1><p id="6e1d" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">Jika model machine learning tidak melakukan proses feature scaling, maka hasil prediksi akan lebih condong atau lebih didominasi oleh nilai fitur terbesar, sementara fitur dengan nilai kecil akan sangat kecil mempengaruhi hasil prediksi. Sama seperti memasak, jika proposi garam lebih banyak maka masakan akan keasinan atau jika didominasi oleh gula maka akan kemanisan. Bumbu yang dimasukkan terbanyak akan mendominasi rasa masakan, oleh karena itu gunakan satuan sendok teh untuk mengukur gula dan garam. Berikut contoh visualisasi segmentasi karyawan berdasarkan weight dan height menggunakan KNN clustering.</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nd"><img src="../Images/09e960fb72b0c94279a08314a181eea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vjpPvXOlywYz86eYmE6EUQ.jpeg"/></div></div></figure><p id="8e8e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Berdasarkan <a class="ae kz" href="https://howtolearnmachinelearning.com/articles/feature_scaling_machine_learning/" rel="noopener ugc nofollow" target="_blank"> gambar tersebut </a> dapat disimpulkan bahwa :</p><ul class=""><li id="743b" class="la lb in jm b jn jo jr js jv lc jz ld kd le kh ne lg lh li bi translated">Gambar atas yaitu model tanpa fitur scaling, menghasilkan bahwa fitur height tidak mempengaruhi hasil segmentation karena hasil didominasi oleh weight .</li><li id="f34d" class="la lb in jm b jn lj jr lk jv ll jz lm kd ln kh ne lg lh li bi translated">Gambar bawah yaitu model dengan fitur scaling, menghasilkan bahwa semua fitur sama sama berpengaruh terhadap hasil prediksi yaitu segmentasi dilakukan berdasarkan fitur weight dan height.</li></ul><p id="6f59" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Oleh karena itu, menyamakan range fitur sangat penting agar hasil prediksi lebih akurat sehingga seluruh fitur dapat berkontribusi mempengaruhi hasil prediksi. Namun, kepentingan feature scaling dapat diartikan berbeda — beda tergantung algortimanya. Kasus diatas merupakan salah satu contoh algoritma berdasarkan distance. Lalu, bagaimana dengan algoritma lainnya? Apakah semua algoritma harus melalui proses feature scaling? Jawabannya tidak. Kenapa? Tergantung model algoritma dan tujuannya.</p><h1 id="89a2" class="lv lw in bd lx ly my ma mb mc mz me mf mg na mi mj mk nb mm mn mo nc mq mr ms bi translated">Kapan menggunakan feature scaling?</h1><p id="7d87" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">Feature scaling digunakan untuk model machine learning dengan kriteria berikut :</p><h2 id="3e21" class="nf lw in bd lx ng nh dn mb ni nj dp mf jv nk nl mj jz nm nn mn kd no np mr nq bi translated">Distance based algorithm</h2><p id="9f6f" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">Algoritma KNN, K-Means dan SVM merupakan contoh distance based algoritma. Seperti contoh kasus segmentasi karyawan diatas, dengan feature scaling maka seluruh fitur dapat berkontribusi mempengaruhi hasil prediksi model, namun apabila tidak, maka akan didominasi oleh fitur dengan nilai terbesar. Hal ini dikarenakan, cara kerja algoritmanya menggunakan metriks distance salah satunya seperti euclidean distance. Matriks ini mengukur jarak antar 2 point yang dapat dilihat pada rumus sebagai berikut.</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0cac74413cc5488145eeed12908f4fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*QjBTcGucUvfCe72JttO56w.png"/></div></figure><p id="994f" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Jika kita membuat salah satu <a class="ae kz" href="https://howtolearnmachinelearning.com/articles/feature_scaling_machine_learning/" rel="noopener ugc nofollow" target="_blank"> contoh implementasi </a> euclidean distance menggunakan kasus weight [45, 53] and height [1.5, 1.7]. Apabila dimasukin kedalam rumus, maka weight (x) menghasilkan 121, sementara height (y) menghasilkan 0.04, maka disimpulkan bahwa weight berkontribusi lebih besar dibandingkan height (121 &gt; 0.04) dan ini bukan karena fitur weight lebih penting, tetapi karena adanya perbedaan range value yang sangat besar.</p><h2 id="afe0" class="nf lw in bd lx ng nh dn mb ni nj dp mf jv nk nl mj jz nm nn mn kd no np mr nq bi translated">Gradient descent algoritma</h2><p id="57f5" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">Implementasi feature scaling dapat membantu menemukan titik cost minimal dengan cepat. Hal ini dikarenakan cara kerja gradien descent adalah dengan terus mencoba nilai koefisien yang berbeda, mengevaluasi cost dan memilih koefisien baru dengan nilai cost paling minimal. Apabila range data sangat besar atau tidak normal, maka sulit menemukan cost minimal dan akan terus beriterasi sehingga memakan waktu lebih lama, namun apabila data di rescale atau melakukan feature scaling kita dapat menghindari proses iterasi berlebihan karena titik minimal dapat lebih cepat ditemukan.</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/aa645310153b1db669978e27d9ab42cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*vayZwV0LsydyfOBbESPZiA.png"/></div></figure><p id="c7e2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Dari gambar diatas, dapat dilihat bahwa gradient descent tanpa feature scaling memiliki jalur yang lebih panjang ditandai dengan jalur zigzag dan bentuk gradient yang lebih besar untuk menemukan titik minimum, sementara gradient descent dengan feature scaling, jalurnya lebih pendek sehingga lebih cepat menemukan titik minimum costnya.</p><h2 id="8c29" class="nf lw in bd lx ng nh dn mb ni nj dp mf jv nk nl mj jz nm nn mn kd no np mr nq bi translated">Principles Component Analysis (PCA)</h2><p id="bc2a" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">Teknik PCA berguna untuk menangani curse dimensionality dengan mereduksi dimensi sehingga membentuk dimensi baru yang bervariance maksimum. Apabila komponen pertama (ex : tinggi badan) bervariansi lebih rendah daripada komponen kedua (ex : berat badan) dikarenakan perbedaan skala pengukuran yaitu meter dan kilogram, maka PCA akan menentukan bahwa arah variansi maksimum akan mengikuti fitur weight, jika data tidak di rescale. Hal ini dikarenakan range data mempengaruhi variansi, semakin besar rangenya maka semakin tinggi variansi nya, oleh karena itu perlu melakukan feature scaling sebelum melakukan PCA agar fitur fitur memiliki skala dan variansi yang sama sehingga arah variansi maksimumnya mengikuti semua fitur. Selanjutnya, kita akan melihat contoh pemodelan naive bayes terhadap data wine yang direduksi dimensinya menggunakan PCA dengan proses feature scaling dan tanpa feature scaling.</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nt"><img src="../Images/2de26a8eeb9d0045cb2d933b1554d39d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VSJEERHb0DWnqTCnCuFlrg.png"/></div></div></figure><p id="78e4" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated"><a class="ae kz" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html" rel="noopener ugc nofollow" target="_blank"> Gambar diatas </a> dapat disimpulkan bahwa pada gambar pertama yaitu model PCA + naive bayes tanpa melakukan feature scaling terlebih dahulu menghasilkan prediksi acak yang tidak dapat secara akurat mengkasifikasi objek, sementara gambar kedua yang mengimplementasikan feature scaling + PCA +Naive bayes dapat melakukan klasifikasi dengan sangat baik. Oleh karena itu, melakukan feature scaling sebelum PCA sangat penting karena berdampak pada hasil prediksi.</p><p id="914b" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Bagaimana algoritma lainnya? Yups. Penggunaan feature scaling harus disesuaikan dengan tujuan dan model algoritmanya.</p><blockquote class="nu"><p id="99d0" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">“Models that are smooth functions of the input, such as linear regression, logistic regression, or anything that involves a matrix, are affected by the scale of the input. If your model is sensitive to the scale of input features, feature scaling could help. As the name suggests, feature scaling changes the scale of the feature.”</p><p id="b8a2" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">By : Alice Zheng dan Amanda Casari (Feature Engineering on Machine Learning book)</p></blockquote><p id="184b" class="pw-post-body-paragraph jk jl in jm b jn oe jp jq jr of jt ju jv og jx jy jz oh kb kc kd oi kf kg kh ig bi translated">Namun, ada juga model yang tidak berefek apabila menggunakan feature scaling. Apa?</p><blockquote class="nu"><p id="50e2" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">“Naive Bayes, Linear Discriminant Analysis, and Tree-Based models are not affected by feature scaling. In Short, any Algorithm which is Not Distance-based is Not affected by Feature Scaling.”</p><p id="b44b" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">By : Alice Zheng dan Amanda Casari (Feature Engineering on Machine Learning book)</p></blockquote><p id="554a" class="pw-post-body-paragraph jk jl in jm b jn oe jp jq jr of jt ju jv og jx jy jz oh kb kc kd oi kf kg kh ig bi translated">Lau, bagaimana melakukan feature sceling pada data dengan benar? Apa saja tekniknya?</p><h1 id="4957" class="lv lw in bd lx ly my ma mb mc mz me mf mg na mi mj mk nb mm mn mo nc mq mr ms bi translated">Teknik dan implementasi feature scaling</h1><p id="a309" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">Terdapat 2 teknik feature scaling yang paling sering digunakan yaitu Min Max Scaling dan Standardization. Tapi apakah kamu tau disaat apa menggunakan teknik tersebut?</p><h2 id="53b8" class="nf lw in bd lx ng nh dn mb ni nj dp mf jv nk nl mj jz nm nn mn kd no np mr nq bi translated">Min Max Scaling</h2><p id="e907" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">Min Max Scaling merupakan teknik untuk mengecilkan range fitur menjadi range [0, 1] atau [-1, 1] apabila terdapat negative value.</p><blockquote class="nu"><p id="657b" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">“The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.”</p><p id="4255" class="nv nw in bd nx ny nz oa ob oc od kh dk translated"><a class="ae kz" href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler" rel="noopener ugc nofollow" target="_blank">作者:scikit learn </a></p></blockquote><p id="cf97" class="pw-post-body-paragraph jk jl in jm b jn oe jp jq jr of jt ju jv og jx jy jz oh kb kc kd oi kf kg kh ig bi translated">如果数据分布不正常或存在标准偏差，则最小最大规模可能会导致零条目或数据稀疏。</p><blockquote class="nu"><p id="cb93" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">“在我们不关心沿方差轴的标准化的情况下，这是完全可以接受的，例如，图像处理或神经网络期望值在0和1之间。”</p><p id="1f05" class="nv nw in bd nx ny nz oa ob oc od kh dk translated"><a class="ae kz" href="https://www.kaggle.com/getting-started/217186" rel="noopener ugc nofollow" target="_blank">作者:卡格尔</a></p></blockquote><p id="d6de" class="pw-post-body-paragraph jk jl in jm b jn oe jp jq jr of jt ju jv og jx jy jz oh kb kc kd oi kf kg kh ig bi translated">稀疏数据可以是一个数据类别的热编码，也可以是另一个数据类别的热编码。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oj"><img src="../Images/e4dc01d38d1dc7f780e75a80693209b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MKxP1QZah8Q7odu0PPqqw.jpeg"/></div></div></figure><p id="dcd5" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">数据分类编码的稀疏数据是一个很好的例子。最大规模最小化的准则是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ae0f942321b1d248dfbaaa323f7be20c.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*iWkbAKexmNnxRrqElzqtCw.png"/></div></figure><p id="9519" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Python通过库scikit learn实现了这些功能。例如，实现最小最大缩放器可以让python适应多库门塔什<a class="ae kz" href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler" rel="noopener ugc nofollow" target="_blank">科学学习</a>。</p><h2 id="7842" class="nf lw in bd lx ng nh dn mb ni nj dp mf jv nk nl mj jz nm nn mn kd no np mr nq bi translated">标准化</h2><p id="68bc" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">标准化技术将重标度分布数据定义为正态分布/高斯分布(零均值和单位标准差)。Kenapa perlu dilakukan？这一模型可能会导致短期或长期的正态分布。一个模型算法，它可以计算出所有的不确定性，并且可以计算出所有的变量。如果你能从其他地方获得更多的变量，那么这种方法将会成为一种主流模型，从而使评估者无法从其他地方获得更多的变量。</p><blockquote class="nu"><p id="9fb4" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">例如，学习算法的目标函数中使用的许多元素(如支持向量机的RBF核或线性模型的l1和l2正则化子)假设所有特征都以零为中心，并且具有相同顺序的方差。</p><p id="ca52" class="nv nw in bd nx ny nz oa ob oc od kh dk translated"><a class="ae kz" href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler" rel="noopener ugc nofollow" target="_blank">作者:scikit learn </a></p></blockquote><p id="e51b" class="pw-post-body-paragraph jk jl in jm b jn oe jp jq jr of jt ju jv og jx jy jz oh kb kc kd oi kf kg kh ig bi translated">因此，标准化可以更好地实现多样化的模型。</p><blockquote class="nu"><p id="0a83" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">“在处理方差(PCA、聚类、逻辑回归、支持向量机、感知器、神经网络)时，事实上标准定标器非常重要。另一方面，如果您使用基于树的分类器或回归器，这不会产生太大的差异。”</p><p id="aa6f" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">作者:<a class="ae kz" href="https://www.kaggle.com/getting-started/217186" rel="noopener ugc nofollow" target="_blank">卡格尔</a></p></blockquote><p id="207f" class="pw-post-body-paragraph jk jl in jm b jn oe jp jq jr of jt ju jv og jx jy jz oh kb kc kd oi kf kg kh ig bi translated">实施nya？Rumus标准化是一个很好的例子:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ol"><img src="../Images/820fbd4d42523e84bf0f60c805818d7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1fzx9DzTOeWGTHb7RzDBAA.jpeg"/></div></div></figure><p id="f948" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">Python库scikit learn提供了统计标准化模块，因此我们可以轻松实现。实现标准化可以让python更好地适应科学研究。</p><p id="d8a2" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">呸。事实上，有几个关键点是通过扩展特性来提高kinerja模型的性能。</p><blockquote class="nu"><p id="10e1" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">“好的特性使<br/>随后的建模步骤变得容易，得到的模型更有能力完成预期的任务。不良特征可能需要复杂得多的模型来实现相同水平的性能。”</p><p id="e7e8" class="nv nw in bd nx ny nz oa ob oc od kh dk translated">作者:爱丽丝·郑丹·阿曼达·卡萨瑞(机器学习书籍的特征工程)</p></blockquote><h1 id="9f2c" class="lv lw in bd lx ly my ma mb mc mz me mf mg om mi mj mk on mm mn mo oo mq mr ms bi translated">参考I</h1><p id="0d2e" class="pw-post-body-paragraph jk jl in jm b jn mt jp jq jr mu jt ju jv mv jx jy jz mw kb kc kd mx kf kg kh ig bi translated">https://howtolearnmachinehlearning . com/articles/feature _ scaling _ machine _ learning/</p><p id="50cf" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">https://www.atoti.io/when-to-perform-a-feature-scaling/</p><p id="be5e" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">https://www . Amazon . com/Feature-Engineering-Machine-Learning-Principles/DP/1491953241</p><p id="4f0d" class="pw-post-body-paragraph jk jl in jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ig bi translated">https://sci kit-learn . org/stable/modules/preprocessing . html #预处理-缩放器</p></div></div>    
</body>
</html>