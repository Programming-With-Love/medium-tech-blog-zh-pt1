<html>
<head>
<title>Active Learning: Select Training Data for Conversational AI Systems NLU from Logs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主动学习:从日志中选择对话式人工智能系统NLU的训练数据</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/active-learning-select-training-data-for-conversational-ai-systems-nlu-from-logs-308c3d2e0f7a?source=collection_archive---------7-----------------------#2019-08-20">https://medium.com/walmartglobaltech/active-learning-select-training-data-for-conversational-ai-systems-nlu-from-logs-308c3d2e0f7a?source=collection_archive---------7-----------------------#2019-08-20</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="78a1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对话AI /虚拟助理系统的响应依赖于在带标签的训练数据上训练的机器学习模型。根据新的现实生活数据不断重新训练虚拟助理模型可以提高它们的性能。然而，注释所有数据或随机样本是昂贵的、缓慢的和低效的。因此，我们希望通过新的主动学习算法(称为Majority-ensemble CRF:Majority-CRF的修改版本(<a class="ae jc" href="https://arxiv.org/abs/1810.03450v2" rel="noopener ugc nofollow" target="_blank"> Peshterliev et al .，2019 </a>)只标注最有信息量的话语，以降低NLU(自然语言理解)系统的错误率。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/2f21a06df5a7aa18b6d8ffc983c6c700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A1Xf7MXBw3ASEWbxQCLgyw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Photo credit: <a class="ae jc" href="https://cdn.pixabay.com/photo/2017/01/04/07/25/bowling-1951472_1280.jpg" rel="noopener ugc nofollow" target="_blank">Active Learning</a></figcaption></figure><h2 id="2ba3" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">什么是主动学习？</h2><p id="0eb3" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">主动学习是一种半监督的机器学习算法，它选择它想要学习的训练数据，以便NLU可以用较少的数据提高其准确性。它迭代地与用户交互，以扩展NLU类训练数据集，从而提高预测精度。这是一种人在回路中的技术。</p><p id="172d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在意图分类模型中，具有低置信度/更多不确定性的文本话语示例是重要的。此外，给这些模型喂食/训练它们应该如何被贴上标签<strong class="ig hi"> </strong>也是有益的。更高不确定性的例子将在模型中给出更多的信息增益。自动选择这些例子称为主动学习。</p><p id="a162" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们来看看主动学习系统对一个NLU类/技能的信息训练数据预测。</p><h2 id="157f" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated">实施细节:</h2><p id="c562" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated"><strong class="ig hi">预训练模型</strong>:首先，你需要使用标记数据训练你的NLU系统，这样它就可以对意图进行分类并提取实体。您的意图分类器模型可以基于模型的集合，包括Sklearn SVC ( <a class="ae jc" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank"> sklearn.svm </a>)、StarSpace:Embed All Things！(<a class="ae jc" href="https://arxiv.org/abs/1709.03856" rel="noopener ugc nofollow" target="_blank"> StarSpace </a>)，BERT ( <a class="ae jc" href="https://github.com/google-research/bert#fine-tuning-with-bert" rel="noopener ugc nofollow" target="_blank"> BERT </a>)，对于实体，seq-seq标注模型条件随机场(CRF)，BERT ( <a class="ae jc" href="https://github.com/google-research/bert#fine-tuning-with-bert" rel="noopener ugc nofollow" target="_blank"> BERT </a>)。</p><p id="0a69" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">预处理</strong>:从未标注的现场话语池中，去除当前NLU系统评分为高置信度(0.8)的话语。现在，剩余的池数据被传递到下一步。</p><p id="3697" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">建模</strong>:训练具有{1，2，3 } gram特征的hinge、squared、logistic等不同损失函数的二元分类器。该模型的训练数据将是NLU技能/类别的正面和负面数据。在Vowpal Wabbit中实现的分类器(<a class="ae jc" href="https://en.wikipedia.org/wiki/Vowpal_Wabbit" rel="noopener ugc nofollow" target="_blank"> Langford等人，2007 </a>)。一旦模型集准备好了，下一步就是过滤。</p><p id="4a43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">过滤</strong>:仅从意图分类器集合中提取多数肯定预测，进入过滤后的话语池。</p><p id="b648" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">评分</strong>:对于过滤池中的每个池话语，分数<em class="kt"> s </em>计算为逻辑分类器概率乘以NER系统概率。对于给定的NLU技能，优先考虑得分最低的话语，作为更具信息性的例子。</p><p id="3dbd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">手动标注</strong>:我们对过滤后的池中话语按照得分最小的<em class="kt"> s </em>进行优先级排序，并在标注前对话语进行去重处理。对于给定的NLU技能的信息训练样本的最小批量(k ): N个信息话语的范围是从(0≤N≤k)。</p><p id="c62e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">重复</strong>:重复这个主动学习的全过程，直到你获得NLU意向的良好转化率。</p><p id="e363" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">影响区域</strong>:通用对话AI系统。</p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><p id="5709" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">参考资料:</p><p id="453e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]佩什泰利耶夫，科尔尼，贾格那塔，基斯，马苏卡斯(2019)。自然语言理解新领域的主动学习。<a class="ae jc" href="https://arxiv.org/abs/1810.03450v2" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1810.03450v2</a></p><p id="25a2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2] Devlin，j .，Chang，m .，Lee，k .，&amp; Toutanova，K. (2018年)。BERT:用于语言理解的深度双向转换器的预训练。<a class="ae jc" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1810.04805</a></p><p id="b5ac" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[3]吴，费希尔，乔普拉，亚当斯，博尔德斯，韦斯顿。2017.星际空间:嵌入所有的东西！。<a class="ae jc" href="https://arxiv.org/abs/1709.03856" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1709.03856</a></p><p id="5f8e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[4]约翰·兰福德、李立宏和亚历克斯·斯特雷尔。2007.沃帕尔·瓦比特</p></div></div>    
</body>
</html>