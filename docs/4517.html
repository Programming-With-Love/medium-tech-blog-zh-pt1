<html>
<head>
<title>Copy data from cloud SQL to BigQuery using apache airflow/cloud composer part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用apache airflow/cloud composer将数据从云SQL复制到BigQuery第2部分</h1>
<blockquote>原文：<a href="https://medium.com/compendium/copy-data-from-cloud-sql-to-bigquery-using-apache-airflow-cloud-composer-part-2-33aa02bf456a?source=collection_archive---------0-----------------------#2019-06-03">https://medium.com/compendium/copy-data-from-cloud-sql-to-bigquery-using-apache-airflow-cloud-composer-part-2-33aa02bf456a?source=collection_archive---------0-----------------------#2019-06-03</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a7a4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在我之前的帖子中，我解释了如何使用命令行工具，如<a class="ae jc" href="https://cloud.google.com/sdk/gcloud/" rel="noopener ugc nofollow" target="_blank"> gcloud </a>和<a class="ae jc" href="https://cloud.google.com/bigquery/docs/bq-command-line-tool" rel="noopener ugc nofollow" target="_blank"> bq </a>，将数据从云SQL加载到bigquery中。在这篇文章中，我将通过一个例子来说明如何使用apache airflow操作符而不是命令行工具来加载数据。这样做有几个优点，比如更干净的代码，更少的黑客攻击，更安全。例如:我们不必担心云sql导出作业的限制，或者导出到csv文件的bug。</p><p id="5893" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我使用的是运行在kubernetes-cloud composer中的gcp管理的气流。本例中运行作业的映像版本是:composer-1.6.1-airflow-1.10.1</p><h1 id="a59d" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">配置云sql代理</h1><p id="6ece" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">我们现在需要做的第一件事是让cloud composer集群能够与云sql实例对话。这可以通过防火墙规则来实现，但我认为使用<a class="ae jc" href="https://cloud.google.com/sql/docs/mysql/sql-proxy" rel="noopener ugc nofollow" target="_blank">云sql代理</a>是更干净的解决方案。</p><p id="5491" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谢天谢地，google为我们提供了构建sql代理所需的一切。我们只需要两个描述部署和服务的yaml文件。</p><p id="56e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">部署文件:用自己的值替换caps lock。也可以指定一个服务帐户，但是在我的例子中，我希望SA是集群SA。(我将在github repo中使用postgreSQL和SA来举例)</p><p id="5835" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">工作部署文件在这里:</p><p id="1b03" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae jc" href="https://github.com/ael-computas/gcp_cloudsql_airflow_bigquery/tree/master/yaml" rel="noopener ugc nofollow" target="_blank">https://github . com/ael-computas/GCP _ cloud SQL _ air flow _ big query/tree/master/YAML</a></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="c75e" class="kp je hh kl b fi kq kr l ks kt"><strong class="kl hi">apiVersion: </strong>extensions/v1beta1<br/><strong class="kl hi">kind: </strong>Deployment<br/><strong class="kl hi">metadata:<br/>  labels:<br/>    run: </strong>mysql-dvh-sqlproxy<br/>  <strong class="kl hi">name: </strong>mysql-dvh-sqlproxy<br/>  <strong class="kl hi">namespace: </strong>default<br/><strong class="kl hi">spec:<br/>  replicas: </strong>1<br/>  <strong class="kl hi">selector:<br/>    matchLabels:<br/>      run: </strong>mysql-dvh-sqlproxy<br/>  <strong class="kl hi">strategy:<br/>    rollingUpdate:<br/>      maxSurge: </strong>1<br/>      <strong class="kl hi">maxUnavailable: </strong>1<br/>    <strong class="kl hi">type: </strong>RollingUpdate<br/>  <strong class="kl hi">template:<br/>    metadata:<br/>      creationTimestamp: </strong>null<br/>      <strong class="kl hi">labels:<br/>        run: </strong>mysql-dvh-sqlproxy<br/>    <strong class="kl hi">spec:<br/>      volumes:<br/>      </strong>- <strong class="kl hi">name: </strong>ssl-certs<br/>        <strong class="kl hi">hostPath:<br/>          path: </strong>/etc/ssl/certs<br/>      <strong class="kl hi">containers:<br/>      </strong>- <strong class="kl hi">image: </strong>gcr.io/cloudsql-docker/gce-proxy:1.11<br/>        <strong class="kl hi">volumeMounts:<br/>        </strong>- <strong class="kl hi">name: </strong>ssl-certs<br/>          <strong class="kl hi">mountPath: </strong>/etc/ssl/certs<br/>        <strong class="kl hi">command: </strong>["/cloud_sql_proxy",<br/>          "-instances=PROJECT_ID:REGION:DATABASE=tcp:0.0.0.0:3306"]<br/>        <strong class="kl hi">imagePullPolicy: </strong>Always<br/>        <strong class="kl hi">livenessProbe:<br/>          exec:<br/>            command:<br/>            </strong>- /bin/sh<br/>            - -c<br/>            - netstat -tlnp | grep -i cloud_sql_proxy<br/>          <strong class="kl hi">failureThreshold: </strong>3<br/>          <strong class="kl hi">periodSeconds: </strong>10<br/>          <strong class="kl hi">successThreshold: </strong>1<br/>          <strong class="kl hi">timeoutSeconds: </strong>1<br/>        <strong class="kl hi">name: </strong>mysql-dvh-sqlproxy<br/>        <strong class="kl hi">ports:<br/>        </strong>- <strong class="kl hi">containerPort: </strong>3306<br/>          <strong class="kl hi">protocol: </strong>TCP<br/>        <strong class="kl hi">resources: </strong>{}<br/>        <strong class="kl hi">terminationMessagePath: </strong>/dev/termination-log<br/>      <strong class="kl hi">dnsPolicy: </strong>ClusterFirst<br/>      <strong class="kl hi">restartPolicy: </strong>Always<br/>      <strong class="kl hi">securityContext: </strong>{}<br/>      <strong class="kl hi">terminationGracePeriodSeconds: </strong>30</span></pre><p id="38fc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这项服务非常简单:</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="cf98" class="kp je hh kl b fi kq kr l ks kt"><strong class="kl hi">kind: </strong>Service<br/><strong class="kl hi">apiVersion: </strong>v1<br/><strong class="kl hi">metadata:<br/>  labels:<br/>    run: </strong>mysql-dvh-sqlproxy<br/>  <strong class="kl hi">name: </strong>mysql-dvh-sqlproxy-service<br/><strong class="kl hi">spec:<br/>  ports:<br/>  </strong>- <strong class="kl hi">port: </strong>3306<br/>    <strong class="kl hi">protocol: </strong>TCP<br/>    <strong class="kl hi">targetPort: </strong>3306<br/>  <strong class="kl hi">selector:<br/>    run: </strong>mysql-dvh-sqlproxy</span></pre><p id="6ecf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这意味着服务将进入默认的名称空间，这很好。</p><p id="bdc2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在两个文件上使用“kubectl apply -f”。</p><p id="e116" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">确保部署和服务在您的kubernetes集群中运行。您可能需要启用API并修复服务帐户权限。日志在这里应该很有帮助。</p><p id="b351" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ku">注意，我见过为你安装代理的运营商，但我自己从未试过..如果你害怕yaml文件，可以查看一下</em><a class="ae jc" href="https://airflow.readthedocs.io/en/latest/howto/connection/gcp_sql.html" rel="noopener ugc nofollow" target="_blank"><em class="ku">https://air flow . readthe docs . io/en/latest/how to/connection/GCP _ SQL . html</em></a><em class="ku">这可能就是你要找的。</em></p><h1 id="0885" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">配置气流连接</h1><p id="7727" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在kubernetes集群报告部署和服务运行之后，您需要在airflow中配置连接字符串。</p><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es kv"><img src="../Images/04475fb22d619d8a66d8434e4ab127dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*i03cUHJ9dCyzWJDHkhcAXw.png"/></div></figure><p id="8f1c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以给conn id取任何名称，只要记住这也是您将从代码中使用的标识！Host和servicename一样，kubernetes会为你打理dns魔法。</p><h1 id="ae5e" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">创建DAG</h1><p id="7a32" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">配置完成后，看看这些操作符:</p><ul class=""><li id="121a" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated"><a class="ae jc" href="https://airflow.apache.org/_modules/airflow/contrib/operators/gcs_to_bq.html" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">Google cloudstoragetobigqueryoperator</strong></a></li><li id="043b" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated"><a class="ae jc" href="https://airflow.apache.org/_modules/airflow/contrib/operators/mysql_to_gcs.html" rel="noopener ugc nofollow" target="_blank">T5】MySqlToGoogleCloudStorageOperatorT7】</a></li></ul><p id="c786" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与我之前的<a class="ae jc" rel="noopener" href="/grensesnittet/copy-data-from-cloud-sql-to-bigquery-using-apache-airflow-b51bdb277463">帖子</a>相比，现在的流程简单多了:</p><ul class=""><li id="d9ce" class="kz la hh ig b ih ii il im ip lb it lc ix ld jb le lf lg lh bi translated">导出到云存储，json中的模式</li><li id="d89e" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">从存储导入到bigquery。</li><li id="3300" class="kz la hh ig b ih li il lj ip lk it ll ix lm jb le lf lg lh bi translated">云存储应该有一个让文件在一段时间后自动删除的策略。</li></ul><p id="e49d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">代码应该如下所示</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="eea8" class="kp je hh kl b fi kq kr l ks kt">class TableConfig:<br/>    STANDARD_EXPORT_QUERY = None<br/>    _STANDARD_EXPORT_QUERY = "SELECT * from {}"<br/><br/>    def __init__(self,<br/>                 cloud_sql_instance,<br/>                 export_bucket,<br/>                 export_database,<br/>                 export_table,<br/>                 export_query,<br/>                 gcp_project,<br/>                 stage_dataset,<br/>                 stage_table,<br/>                 stage_final_query,<br/>                 bq_location<br/>                 ):<br/><br/>        self.params = {<br/>            'export_table': export_table,<br/>            'export_bucket': export_bucket,<br/>            'export_database': export_database,<br/>            'export_query': export_query or self._STANDARD_EXPORT_QUERY.format(export_table),<br/>            'gcp_project': gcp_project,<br/>            'stage_dataset': stage_dataset,<br/>            'stage_table': stage_table or export_table,<br/>            'stage_final_query': stage_final_query,<br/>            'cloud_sql_instance': cloud_sql_instance,<br/>            'bq_location': bq_location or "EU",<br/>        }<br/><br/><br/>def get_tables():<br/><em class="ku">    </em>dim_tables = ["DimAge", "DimPerson"]<br/>    fact_tables = ["FactPerson"]<br/>    export_tables = dim_tables + fact_tables<br/>    tables = []<br/>    for dim in export_tables:<br/>        tables.append(TableConfig(cloud_sql_instance='CLOUD_SQL_INSTANCE_NAME',<br/>                                  export_table=dim,<br/>                                  export_bucket='YOUR_STAGING_BUCKET',<br/>                                  export_database='prod',<br/>                                  export_query=TableConfig.STANDARD_EXPORT_QUERY,<br/>                                  gcp_project="YOUR_PROJECT_ID",<br/>                                  stage_dataset="YOUR_STAGING_DATASET",<br/>                                  stage_table=None,<br/>                                  stage_final_query=None,<br/>                                  bq_location="EU"))<br/>    return tables<br/><br/>def gen_export_table_task(table_config):<br/>    export_task = MySqlToGoogleCloudStorageOperator(task_id='export_{}'.format(table_config.params['export_table']),<br/>                                                    dag=dag,<br/>                                                    sql=table_config.params['export_query'],<br/>                                                    bucket=table_config.params['export_bucket'],<br/>                                                    filename="cloudsql_to_bigquery/{}/{}".format(table_config.params['export_table'],<br/>                                                                                                 table_config.params['export_table']) + "_{}",<br/>                                                    schema_filename="cloudsql_to_bigquery/schema/{}/schema_raw".format(table_config.params['export_table']),<br/>                                                    mysql_conn_id="gcp_dvh_cloudsql")<br/>    return export_task<br/><br/><br/>def gen_import_table_task(table_config):<br/>    import_task = GoogleCloudStorageToBigQueryOperator(<br/>        task_id='{}_to_bigquery'.format(table_config.params['export_table']),<br/>        bucket=table_config.params['export_bucket'],<br/>        source_objects=["cloudsql_to_bigquery/{}/{}*".format(table_config.params['export_table'],<br/>                                                             table_config.params['export_table'])],<br/>        destination_project_dataset_table="{}.{}.{}".format(table_config.params['gcp_project'],<br/>                                                            table_config.params['stage_dataset'],<br/>                                                            table_config.params['stage_table']),<br/>        schema_object="cloudsql_to_bigquery/schema/{}/schema_raw".format(table_config.params['export_table']),<br/>        write_disposition='WRITE_TRUNCATE',<br/>        source_format="NEWLINE_DELIMITED_JSON",<br/>        dag=dag)<br/><br/>    return import_task<br/><br/><br/>"""<br/>The code that follows setups the dependencies between the tasks<br/>"""<br/><br/>for table_config in get_tables():<br/>    export_script = gen_export_table_task(table_config)<br/>    import_script = gen_import_table_task(table_config)<br/><br/>    export_script &gt;&gt; import_script</span></pre><p id="c029" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该作业看起来如下所示:</p><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es ln"><img src="../Images/9b1b4fb2120ca3049cb7dc3357069c40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*hXtKG9a50ha8_RSEccZ9mg.png"/></div></figure><p id="475a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果您使用的是managed airflow实例，可能需要安装额外的依赖项。从GCP控制台转到Composer和pypi包来安装它们。</p><p id="2783" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">与之前的解决方案相比，这是一种更高效的导出方式，因为它将并行运行导出，而不是一次只导出一个。这有点冗长，因为数据现在是json而不是csv，但是临时存储在这里不是问题。</p><p id="3d50" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它还可以更优雅地处理大块数据，因为airflow操作符会拆分比bigquery喜欢的文件更大的文件。</p><p id="8f94" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我的示例github repo已经用包含这个简化工作流的新文件进行了更新:(v2文件)</p><div class="lo lp ez fb lq lr"><a href="https://github.com/ael-computas/gcp_cloudsql_airflow_bigquery" rel="noopener  ugc nofollow" target="_blank"><div class="ls ab dw"><div class="lt ab lu cl cj lv"><h2 class="bd hi fi z dy lw ea eb lx ed ef hg bi translated">ael-computas/GCP _ cloud SQL _ air flow _ big query</h2><div class="ly l"><h3 class="bd b fi z dy lw ea eb lx ed ef dx translated">如何使用air flow-ael-computas/GCP _ cloud sql _ air flow _ bigquery从云SQL导出到big query的示例</h3></div><div class="lz l"><p class="bd b fp z dy lw ea eb lx ed ef dx translated">github.com</p></div></div><div class="ma l"><div class="mb l mc md me ma mf kx lr"/></div></div></a></div><p id="2e93" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ku">注意，如果需要从mssql加载，apache airflow </em>  <em class="ku">中对此也有一个</em> <a class="ae jc" href="https://github.com/apache/airflow/blob/master/airflow/contrib/operators/mssql_to_gcs.py" rel="noopener ugc nofollow" target="_blank"> <em class="ku">运算符。但是，它不能正确地处理模式生成，所以您需要自己(现在)通过从数据库中选择来完成。信息_模式。列，并确保float类型是bigquery float类型。</em></a></p><p id="264e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果你喜欢这个帖子，请鼓掌:)</p><p id="32dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">编辑:<a class="ae jc" rel="noopener" href="/@ael_78866/argo-workflows-as-alternative-to-cloud-composer-db4db2bea1af">请查看这篇帖子，并附上作曲者的相关替代信息</a></p></div></div>    
</body>
</html>