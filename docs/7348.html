<html>
<head>
<title>Inferring Label Hierarchies with hLDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用hLDA推断标签层次结构</h1>
<blockquote>原文：<a href="https://medium.com/square-corner-blog/inferring-label-hierarchies-with-hlda-2093d0413337?source=collection_archive---------1-----------------------#2018-11-14">https://medium.com/square-corner-blog/inferring-label-hierarchies-with-hlda-2093d0413337?source=collection_archive---------1-----------------------#2018-11-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/08be7cabc12606552d63e915e9c311fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6ojP_MwqiT3lAth2"/></div></div></figure><p id="faee" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">注意，我们已经行动了！如果您想继续了解Square的最新技术内容，请访问我们的新家<a class="ae jn" href="https://developer.squareup.com/blog" rel="noopener ugc nofollow" target="_blank">https://developer.squareup.com/blog</a></p><h2 id="fe0c" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">问题:组织广场支持中心文章</h2><p id="ab76" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">在Square，我们让业务快速启动并运行。我们还努力确保卖家的问题得到快速回答。当Square卖家遇到Square产品的问题时，第一道防线是Square的支持中心，该中心有数百篇文章可以帮助卖家找到他们问题的答案。</p><p id="df7d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Square的内容团队定期向支持中心添加文章。这套文章到目前为止一直是有机增长的；它们是根据卖家的需要编写和出版的，因此，它们没有任何特定的层次结构<em class="ko">先验</em>。</p><p id="0751" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们有数百篇文章涵盖了Square产品的所有主题，我们想知道是否有一种快速的方法来按主题组织和标记它们，以便我们可以跟踪文章组如何帮助卖家找到他们问题的答案。比如所有和广场资本有关的文章有多大帮助？如果我们能在小组层面上理解文章的表现，我们就能找出哪些主题的内容应该重点改进。</p><p id="f0fc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">虽然支持中心有一些结构(如Square Support主页上的“<a class="ae jn" href="https://squareup.com/help/us/en/" rel="noopener ugc nofollow" target="_blank">热门话题</a>”下所示)，但组织和标记文章是一个手动过程。考虑到我们的库一直在变化，如果有一种自动化的方式来处理标签就好了。</p><p id="7f03" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们决定给<em class="ko">主题建模</em>一个快速的尝试，看看我们能否找到支持中心文档之间的共性，让我们能够以一种明智的方式对它们进行分组和标记。</p><h2 id="5a06" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">hLDA:一种按主题层次对文档进行分组的模型</h2><p id="cb35" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">主题建模最常用的技术之一是潜在狄利克雷分配(LDA ),它是一种生成模型，将各个文档表示为主题的混合，其中文档中的每个单词都是由某个主题生成的。然而，LDA有一些限制。必须事先选择主题的数量<em class="ko"/>，并且假设所有主题都具有扁平的层次结构。我们可以想象一个场景，让一个无监督的模型推断出主题的层次结构是很有用的。例如，我们可能希望将所有关于Square硬件的文章分组，然后在该组中，根据它们是关于磁条读卡器、芯片/非接触式读卡器、Square寄存器还是其他Square硬件来组织文章。</p><p id="06d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae jn" href="http://www.cs.columbia.edu/~blei/papers/BleiGriffithsJordanTenenbaum2003.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ko">分层LDA </em> </a> (hLDA)模型扩展了LDA，以从文档语料库中推断主题的分层结构。我们有兴趣看看是否可以使用这种技术来自动组织Square的支持中心文章。</p><p id="3613" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">hLDA使我们有可能假设我们的主题以树状结构排列，其中树有<em class="ko"> L </em>级，每个节点是一个主题。对于LDA，我们使用具有<em class="ko"> K </em>随机混合比例的混合模型来选择主题(其中<em class="ko"> K </em>是可能主题的数量)，由K维向量θ表示。使用hLDA，我们在<em class="ko"> L </em>级树中选择从根到叶的路径，从L维的Dirichlet分布中选择主题比例的向量θ，然后使用从根到叶的主题混合来生成组成每个文档的单词，使用混合比例θ。</p><p id="500a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这种方法的一个问题是，即使我们在<em class="ko"> L </em>固定了树级别的数量，也有许多可能的树结构可供我们选择，因为我们没有在树的任何级别固定分支因子(每个节点的子节点数量)。不必先验地确定整个树结构<em class="ko">而代之以从数据中学习结构将是很好的。选择树形结构的一种方法是，在数据输入模型时，使用所谓的嵌套中餐馆流程(nCRP)建立一个层次结构。这是<a class="ae jn" href="https://works.bepress.com/jim_pitman/1/" rel="noopener ugc nofollow" target="_blank"> <em class="ko">中餐厅流程</em> </a> (CRP)的延伸；我们不会在这里详细讨论，但是当混合模型中混合成分的数量不确定时，CRP是有用的。</em></p><h2 id="3424" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">将hLDA应用于小数据集</h2><p id="b734" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">我们用IMDb <a class="ae jn" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">大型电影评论数据集</a>的子集尝试了hLDA，看看它是否给出了我们可能预期的结果。我们收集了1000条积极的电影评论，并使用hLDA的现有<a class="ae jn" href="https://github.com/joewandy/hlda" rel="noopener ugc nofollow" target="_blank"> Python实现</a>对它们进行了分类。</p><p id="dc7a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的原始数据示例如下:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kp"><img src="../Images/1d026508e9146d40017d280858760001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Kc7iPf0bsPXY-JZq7XyFA.png"/></div></div></figure><p id="7ba6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们删除了停用词并进行标记，这样数据就变成了:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ku"><img src="../Images/9bbdca66c3f50cfe703ef57bef3a317c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Sp0cGmatBUKoLx6r_gNiA.png"/></div></div></figure><p id="e5cf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">运行hLDA得到了以下(部分)结果:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/8749642875b2fff0633f557bebe7f8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mF531IoIMEZceDpU"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx">Figure 1. A sample of the representative words for a subset of nodes in a label tree that was learned via hLDA using positive IMDb movie reviews</figcaption></figure><p id="e731" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们的树的根包括像“电影”、“电影”和“伟大”这样的通用词，这似乎是一个合理的结果。通过检查根以下级别的两个节点中的单词，我们看到它们可能表明体裁上的分歧(“经典”和“疯狂”对“母亲”和“女儿”)。当我们重新检查我们现在标记的数据集时，我们看到至少有一篇关于“护士雪莉”(1978年的恐怖电影)、“现代启示录”(关于越战的经典电影)和“驾驶课程”(鲁伯特·格林特主演的成人故事)的评论分别被分配到左边、中间和最右边。我们的模型的结果似乎有直观的意义。</p><h2 id="f5c2" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">hLDA在方形支撑件中的应用</h2><p id="d695" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">然后，我们在Square Support Center文档集上运行该算法。</p><p id="cbe0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">删除停用词和数字，然后进行标记，<a class="ae jn" href="https://squareup.com/help/us/en/article/5440-deposit-options-faqs" rel="noopener ugc nofollow" target="_blank">这篇关于存款选项</a> s的常见问题解答文章变成了一篇文档，其中前几个词看起来像:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kz"><img src="../Images/365f388fdc1db40b97da9eed0691cf26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O8eQMRkEkNYHV69DRO9_6g.png"/></div></div></figure><p id="301a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们获得了一个分层的主题结构。我们举例说明了下面层次结构中的一小部分主题:</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es la"><img src="../Images/e417fd438385a4e312702ec6b469a1e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hkm0Zv_lGc6qLRpk"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx">Figure 2. A sample of the representative words for a subset of nodes in a label tree that was learned via hLDA using the Square Support Center corpus.</figcaption></figure><p id="cc97" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如我们所料，涉及Square一般用途的单词，如<em class="ko">账户、</em>支付、<em class="ko">卡</em>，是我们根话题的代表。该算法已经发现了反映在树中的Square生态系统的一些不同方面。</p><p id="19da" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们还可以看到标签结构的整个从根到叶的路径，这有助于我们理解为什么某些文章会被分组到树上单独的叶子中，即使它们有相似的关键字。我们可以在树中看到两组硬件相关的主题——一组具有代表性的术语<em class="ko"> ipad、usb、</em>和<em class="ko"> android </em>，另一组具有术语<em class="ko">信息、安全、</em>和<em class="ko">服务</em>。在它们的根到叶路径中，这些主题共享类似于<em class="ko">阅读器</em>和<em class="ko">设备</em>的术语，但是我们期望<em class="ko"> ipad、usb、</em>和<em class="ko"> android </em>主题也与<em class="ko">更新</em>相关。<a class="ae jn" href="https://squareup.com/help/us/en/article/5584-devices-compatible-with-square-contactless-and-chip-card-reader" rel="noopener ugc nofollow" target="_blank">兼容方形非接触式和芯片式读卡器的设备</a> <em class="ko"> </em>在此主题之内。本文描述了哪些设备与Square Stand兼容，并建议安装iOS和Square应用程序的更新版本。同时，最右边的主题(如上所述)与硬件有关，但也与安全性有关；<a class="ae jn" href="https://squareup.com/help/us/en/article/3796-privacy-and-security" rel="noopener ugc nofollow" target="_blank">隐私与安全</a>和<a class="ae jn" href="https://squareup.com/help/us/en/article/3797-secure-data-encryption" rel="noopener ugc nofollow" target="_blank">安全数据加密</a>文章可以在这一页中找到。因此，即使一组文章共享几个关键词，拥有一个主题层次结构可以让我们以一种逻辑的方式将文章组彼此分开；在某种程度上，我们甚至可以通过沿着树根到树叶的路径来追踪模型用来分离文章的逻辑。</p><p id="6145" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">虽然上面的主题路径似乎是合理的，但我们也看到了数量惊人的没有多大意义的模型结果。例如，<a class="ae jn" href="https://squareup.com/help/us/en/article/3862-edit-your-shipping-address" rel="noopener ugc nofollow" target="_blank">编辑您的送货地址</a>与<a class="ae jn" href="https://squareup.com/help/us/en/article/5749-best-practices-for-square-invoices" rel="noopener ugc nofollow" target="_blank">方形发票的最佳实践</a>放在一起，而<a class="ae jn" href="https://squareup.com/help/us/en/article/5380-update-your-taxpayer-identification-number" rel="noopener ugc nofollow" target="_blank">更新您的纳税人识别号</a>与<a class="ae jn" href="https://squareup.com/help/us/en/article/5440-deposit-options-faqs" rel="noopener ugc nofollow" target="_blank">存款选项常见问题</a>放在一起。事实上，通过对模型输出的快速手动检查，我们看到该模型有大量错误的分类，并且肯定不能开箱即用。我们还看到，主题树可以从一个模型运行到下一个模型运行发生巨大的变化。</p><p id="1b2c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">那么，hLDA如何对我们有用呢？通过运行模型，我们可以使用它来开始注释我们的文章，然后与内容专家一起调整注释，直到它们符合我们自己对树应该是什么样子的预期。以我们支持中心的规模(几百篇文章)，这不是一个不合理的任务。</p><h2 id="38f4" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">分层主题建模:一些经验教训</h2><p id="72fc" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">从Square的支持中心数据中，我们可以看到hLDA在高层次上对文章进行了分类，但就其本身而言，它并不具备我们实际实现所需的准确性。我们可能会从构建第一轮模型中获得最大的投资回报，然后要求内容专家调整模型结果，直到我们有一个合理的标记文章层次结构。在构建该模型的过程中，我们提出了一些我们认为在考虑使用无监督模型来解决文档分类问题时会有用的一般性问题。</p><p id="96bd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">我们运行这种模式的目的是什么？在使用像hLDA这样的无监督学习技术对语料库中的文本进行分类之前，我们应该事先知道我们打算如何使用这些结果。假设我们想要使用来自hLDA的主题标签来可视化我们的语料库。或者说，我们希望使用主题作为监督学习模型的输入特征。在这些情况下，开箱即用运行hLDA可能就足够了。或者，假设我们想对大量文档进行一次分类，然后在可预见的将来使用该分类。并且说我们对我们的树结构应该是什么样子有一个强烈的先入之见。在这种情况下，我们可以使用hLDA首先在数据集中发现主题，然后请人类内容专家调整结果。</strong></p><p id="2135" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">与手动标记相比，构建模型的投资回报率是多少？我们不会通过开箱即用运行无监督的ML算法来获得完美的结果。但我们也不想让内容专家从零开始手动标记数百篇文章。我们应该如何平衡花费在建模上的时间和花费在手工标记上的时间？这因用例而异，但一个明智的开始是运行一个快速的hLDA来启动我们的语料库组织，然后请内容专家构建这些结果。如果一天的建模可以节省一到两周的手工标记，那么我们已经在建模工作上取得了可观的ROI。</p><p id="4399" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">层次聚类对我的语料库有意义吗？</strong> hLDA很复杂，只有在以下情况下才真正值得做:1)有理由相信数据集中存在潜在的层次结构，2)该层次结构可以以某种方式<em class="ko">使用</em>。即使是开箱即用的模型运行也需要开发人员的工作；有一些超参数需要优化，用户需要事先指定层次结构的深度。还要考虑语料库的结构:在我们的IMDb例子中，描述不同电影的词汇差异很大，我们的模型很容易挑出单部电影。然而，对于Square支持中心的文章，主题树会因运行而异。这表明Square的支持中心语料库不像IMDb电影评论那样模块化，我们可能需要依赖内容专家进行最终注释。</p><h2 id="8421" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">使用标签层次结构</h2><p id="62c1" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">通过将hLDA应用于我们的支持中心数据，我们获得了对支持中心层次结构的估计，如果我们选择，我们可以通过人工重组和注释来改进。</p><p id="6e07" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一旦我们有了高质量的注释集，我们就可以用它们来回答许多关于数据集的有趣问题。我们的支持中心内的某些主题是代表过多还是代表不足(就文章数量而言)？某个主题的页面浏览量是否远远超过其他主题——也许这意味着我们需要我们的内容团队在未来额外关注这个主题？</p><p id="79f9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设我们想要预测某篇特定支持中心文章在下个月的页面浏览量。除了文本特征之外，我们还可以包括分类特征，指示文章属于哪个主题(在层次结构中的每一层)。</p><p id="18ed" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">无论我们是使用hLDA来启动注释流程，还是手动完成所有标记，我们的支持中心文章上的注释都可以支持我们当前跟踪支持中心文章的工作，目标是确保支持中心尽可能快速有效地让Square sellers找到他们的答案。</p><h2 id="fe4f" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">附录:LDA简介</h2><p id="1618" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated"><a class="ae jn" href="https://www-cs.stanford.edu/people/ang/papers/jair03-lda.pdf" rel="noopener ugc nofollow" target="_blank"> LDA </a>的具体细节已经在<a class="ae jn" href="http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/" rel="noopener ugc nofollow" target="_blank">涵盖</a>T4井<a class="ae jn" href="https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/" rel="noopener ugc nofollow" target="_blank">其他地方</a>介绍过了，但是总的想法是存在一个未被注意到的主题集合，每个主题都频繁使用一小组单词。LDA范式假设每个可观察的文档都由这些不可观察的主题组成。</p><p id="bb0f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">LDA将每个文档建模为以一定的概率从每个主题中抽取单词来创建。在学习阶段，它试图确定使用了哪些主题来生成文档。</p><p id="c597" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">更具体地说，我们可以假设我们有一个由文档语料库组成的数据集，其中每个文档是来自一组词汇的一组单词。我们还可以假设我们有一组单词分布(主题)，并且每个文档都是通过具有随机混合比例的混合模型生成的。如这里<a class="ae jn" href="http://www.cs.columbia.edu/~blei/papers/BleiGriffithsJordanTenenbaum2003.pdf" rel="noopener ugc nofollow" target="_blank">所介绍的</a>，任何单个单词的混合分布都可以用以下公式来描述</p><figure class="kq kr ks kt fd ii er es paragraph-image"><div class="er es lb"><img src="../Images/ea7aa3a5bb0413c2d73a043a67179afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*EazEMRykyDv8lfQO31sYeg.png"/></div></figure><p id="cd69" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其中<em class="ko"> p </em>是在给定K维向量θ的情况下画出单词<em class="ko"> w </em>的概率，该向量表示特定于文档的随机混合比例，<em class="ko"> z </em>是对于<em class="ko"> z </em>的每个值具有不同主题分布的多项式变量，β是一个矩阵(现在视为固定的)，其中β <em class="ko"> ij </em>表示特定单词w <em class="ko"> i </em>在特定主题z <em class="ko">内的概率当分布p(θ|⍺)(其中⍺是全局参数)被选择为狄利克雷分布时，那么我们有效地运行LDA模型。</em></p><p id="1a55" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如前所述，LDA有一些约束；主题的数量需要事先选择，我们在组内没有等级观念。对于我们使用IMDb电影评论构建的hLDA示例，可以认为根节点下面的层构成了一组“流派”组，每个叶子可能大致对应于一部特定的电影。LDA可以按电影对评论进行分组，但我们丢失了类型分组。</p><h2 id="c793" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">参考</h2><p id="079f" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">布莱博士，Ng，A. Y .，&amp;乔丹，M. I. (2003年)。潜在狄利克雷分配。<em class="ko">机器学习研究杂志</em>，<em class="ko"> 3 </em>(一月)，993–1022。</p><p id="b7dd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Frigyik，文学学士，Kapila，a .，&amp; Gupta，M. R. (2010年)。狄利克雷分布及相关过程介绍。<em class="ko">华盛顿大学电气工程系，u weetr-2010–0006</em>。</p><p id="c209" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">格里菲斯、T. L .、乔丹、M. I .、特南鲍姆、J. B .、布雷、D. M. (2004年)。层次话题模型和嵌套中餐馆过程。在<em class="ko">神经信息处理系统的进展</em>(第17-24页)。</p><p id="f72b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">(2011年6月)。学习用于情感分析的词向量。在<em class="ko">计算语言学协会第49届年会会议录:人类语言技术-第1卷</em>(第142-150页)。计算语言学协会。</p><p id="834c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">皮特曼，J. (2002年)。<em class="ko">组合随机过程</em>。加州大学伯克利分校统计系第621号技术报告，2002年。圣面粉课程讲义。</p><h2 id="cc23" class="jo jp hh bd jq jr js jt ju jv jw jx jy ja jz ka kb je kc kd ke ji kf kg kh ki bi translated">脚注</h2><p id="2c2d" class="pw-post-body-paragraph ip iq hh ir b is kj iu iv iw kk iy iz ja kl jc jd je km jg jh ji kn jk jl jm ha bi translated">(1)CRP与狄利克雷过程相关，因为CRP是一种特定的随机过程，而狄利克雷过程是一种随机过程，其中每个观察值是一个概率分布。CRP可用于<em class="ko">构建</em>一个狄利克雷过程样本。<a class="ae jn" href="http://mayagupta.org/publications/FrigyikKapilaGuptaIntroToDirichlet.pdf" rel="noopener ugc nofollow" target="_blank">狄利克雷分布和相关过程简介</a>的第5.1节对此进行了详细描述。</p></div></div>    
</body>
</html>