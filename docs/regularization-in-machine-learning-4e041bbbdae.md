# 机器学习中如何使用正则化？

> 原文：<https://medium.com/edureka/regularization-in-machine-learning-4e041bbbdae?source=collection_archive---------6----------------------->

![](img/9505fb27227aadc64b2489600ea49204.png)

您是否遇到过这样的情况:您的机器学习模型对训练数据的建模非常好，但对测试数据却表现不佳，即无法预测测试数据？这种情况可以用机器学习中的正则化来处理。

当模型从训练数据中学习到非常特定的模式和噪音，以至于对我们的模型从训练数据归纳到新的(“看不见的”)数据的能力产生负面影响时，就会发生过度拟合。所谓噪声，我们指的是数据集中不相关的信息或随机性。

防止过拟合对于提高我们的机器学习模型的性能是非常必要的。

以下几点将在本文中涉及，并最终帮助我们解决这个问题:

*   什么是正规化
*   正规化是如何运作的
*   正则化技术

继续这篇关于机器学习中的正则化的文章。

# 什么是正规化？

一般来说，规则化就是让事情变得有规律或者可以接受。这正是我们用它来进行应用机器学习的原因。在机器学习的上下文中，正则化是将系数正则化或缩小到零的过程。简而言之，正则化不鼓励学习更复杂或更灵活的模型，以防止过度拟合。

继续这篇关于机器学习中的正则化的文章。

# 正规化是如何运作的？

基本思想是惩罚复杂的模型，即增加一个复杂的项，这会给复杂的模型带来更大的损失。为了理解它，让我们考虑一个简单的线性回归关系。数学上表述如下:
y≈w _ 0+w _ 1x _ 1+w _ 2x _(2 )+⋯+w_p x _ p

其中 Y 是学习的关系，即要预测的值。
X_1，X_，X \\, P，是决定 y 值的特征。
W_1，W_，W \\, P，分别是特征 X_1，X \\, P 的权重。
w0 代表偏差。

现在，为了拟合精确预测 Y 值的模型，我们需要一个损失函数和优化参数，即偏差和权重。

通常用于线性回归的损失函数称为残差平方和(RSS)。根据上述线性回归关系，可以给出:
RSS = ∑_(j=1)^m(y_i-w_0-∑_(i=1)^n w _ I x _ Ji)

我们也可以称 RSS 为没有正则化的线性回归目标。

现在，模型将通过这个损失函数来学习。基于我们的训练数据，它会调整权重(系数)。如果我们的数据集有噪声，它将面临过拟合问题，估计的系数不会在看不见的数据上推广。

这就是正规化发挥作用的地方。它通过惩罚系数的大小将这些学习到的估计值调整到零。

但是它是如何给系数分配惩罚的，让我们来探索一下。

继续这篇关于机器学习中的正则化的文章。

# 正则化技术

有两种主要的正则化技术，即岭回归和套索回归。它们的不同之处在于对系数分配惩罚的方式。

![](img/e8169f4cada4d109678dfb7c6158bfde.png)

**岭回归(L2 正则化)**

这种正则化技术执行 L2 正则化。它通过添加相当于系数大小的平方的惩罚(收缩量)来修改 RSS。
∑_(j=1)^m(y_i-w_0-∑_(i=1)^n w _ I x _ Ji)+α∑_(i=1)^n w _ I = RSS+α∑_(i=1)^n w _ I

现在，使用这个修改的损失函数来估计系数。

在上面的等式中，您可能已经注意到了参数α (alpha)以及收缩量。这就是所谓的调整参数，它决定了我们想要惩罚模型的程度。换句话说，调整参数平衡了对最小化 RSS 和最小化系数平方和的重视程度。

让我们看看αα的值如何影响岭回归产生的估计值。

**当** α=0 时，惩罚项无效。这意味着它返回平方的残差和作为我们最初选择的损失函数，即我们将获得与简单线性回归相同的系数。

**当** α=∞时，岭回归系数将为零，因为修改的损失函数将忽略核心损失函数并最小化系数平方，最终将参数值取为 0。

**当** 0 < α < ∞时，对于简单线性回归，岭回归系数会在 0 到 1 之间。

这就是为什么选择一个好的α值是至关重要的。由岭回归正则化技术产生的系数方法也称为 L2 范数。

**拉索回归(L1 正则化)**

这种正则化技术执行 L1 正则化。它通过添加相当于系数绝对值之和的罚值(收缩量)来修改 RSS。
∑_(j=1)^m(y_i-w_0-∑_(i=1)^n w _ I x _ Ji)+α∑_(i=1)^n | w _ I | = RSS+α∑_(i=1)^n | w _ I |
现在，使用这个修改的损失函数来估计系数。

套索回归不同于岭回归，因为它使用绝对系数值进行归一化。

由于损失函数只考虑绝对系数(权重)，优化算法将惩罚高系数。这就是众所周知的 L1 规范。

这里，α (alpha)也是一个调整参数，工作方式类似于岭回归，并在平衡系数的 RS 幅度之间提供一个折衷。

像岭回归一样，lasso 回归中的α (alpha)可以取如下各种值:

**当** α=0 时，我们将得到与简单线性回归相同的系数。

**当** α=∞时，拉索回归系数为零。

**当** 0 < α < ∞时，对于简单的线性回归，套索回归系数会在 0 到 1 之间。

它看起来与岭回归非常相似，但是让我们从不同的角度来看看这两种技术。

把岭回归想成解一个方程，其中权(系数)的平方和小于等于 s，据此，考虑到给定问题中有 2 个参数，岭回归表示为
w1+w2≤s

这意味着岭回归系数对于位于上述方程给出的圆内的所有点具有最小的损失函数。

同样，把 lasso 回归想成解一个方程，其中权(系数)的模之和小于等于 s，据此，考虑到给定问题中有 2 个参数，lasso 回归表示为
| w1 |+| w2 |≤s

这意味着岭回归系数对于位于上述等式给出的菱形内的所有点具有最小的损失函数。
下图描述了上述等式:

在这幅图中我们可以看到，约束函数(蓝色区域)；左边的是套索，而右边的是山脊，以及损失函数(即 RSS)的等高线(绿色月食)。

在上述情况下，对于两种回归技术，系数估计值由轮廓(日蚀)接触约束(圆形或菱形)区域的第一个点给出。

岭回归系数估计将完全是非零的。为什么？因为岭回归有一个圆形约束，没有尖点，所以日蚀不会与轴上的约束相交。

另一方面，套索约束，因为它的菱形，在每个轴上都有角，因此日蚀通常会在每个轴上相交。因此，至少有一个系数等于零。

上述情况表明，岭回归会将系数缩小到非常接近 0，但永远不会使它们精确到 0，这意味着最终模型将包括所有预测值。这是岭回归的一个缺点，叫做模型可解释性。

然而，当α足够大时，lasso 回归会将某些系数估计值缩小到恰好为 0。这就是 lasso 提供稀疏解的原因。

这就是正规化及其技术，我希望现在你能更好地理解它。你可以利用这一点来提高你的机器学习模型的准确性。

现在，我们到了这篇“机器学习中的正则化”文章的结尾。希望这篇文章是有见地的！

如果你对这个话题有任何疑问，请在下面留下评论，我们会尽快回复你。如果你想查看更多关于 Python、DevOps、Ethical Hacking 等市场最热门技术的文章，你可以参考 Edureka 的官方网站。

请留意本系列中解释数据科学各个方面的其他文章。

> *1。* [*数据科学教程*](/edureka/data-science-tutorial-484da1ff952b)
> 
> *2。* [*数据科学的数学与统计*](/edureka/math-and-statistics-for-data-science-1152e30cee73)
> 
> *3。*[*R 中的线性回归*](/edureka/linear-regression-in-r-da3e42f16dd3)
> 
> *4。* [*数据科学教程*](/edureka/data-science-tutorial-484da1ff952b)
> 
> *5。*[*R 中的逻辑回归*](/edureka/logistic-regression-in-r-2d08ac51cd4f)
> 
> *6。* [*分类算法*](/edureka/classification-algorithms-ba27044f28f1)
> 
> *7。* [*随机森林中的 R*](/edureka/random-forest-classifier-92123fd2b5f9)
> 
> *8。* [*决策树中的 R*](/edureka/a-complete-guide-on-decision-tree-algorithm-3245e269ece)
> 
> *9。* [*机器学习入门*](/edureka/introduction-to-machine-learning-97973c43e776)
> 
> *10。* [*朴素贝叶斯在 R*](/edureka/naive-bayes-in-r-37ca73f3e85c)
> 
> *11。* [*统计与概率*](/edureka/statistics-and-probability-cf736d703703)
> 
> *12。* [*如何创建一个完美的决策树？*](/edureka/decision-trees-b00348e0ac89)
> 
> *13。* [*关于数据科学家角色的 10 大误区*](/edureka/data-scientists-myths-14acade1f6f7)
> 
> *14。*[*5 大机器学习算法*](/edureka/machine-learning-algorithms-29eea8b69a54)
> 
> 15。 [*数据分析师 vs 数据工程师 vs 数据科学家*](/edureka/data-analyst-vs-data-engineer-vs-data-scientist-27aacdcaffa5)
> 
> 16。 [*人工智能的种类*](/edureka/types-of-artificial-intelligence-4c40a35f784)
> 
> *17。*[*R vs Python*](/edureka/r-vs-python-48eb86b7b40f)
> 
> *18。* [*人工智能 vs 机器学习 vs 深度学习*](/edureka/ai-vs-machine-learning-vs-deep-learning-1725e8b30b2e)
> 
> 19。 [*机器学习项目*](/edureka/machine-learning-projects-cb0130d0606f)
> 
> 20。 [*数据分析师面试问答*](/edureka/data-analyst-interview-questions-867756f37e3d)
> 
> *21。* [*面向非程序员的数据科学和机器学习工具*](/edureka/data-science-and-machine-learning-for-non-programmers-c9366f4ac3fb)
> 
> *22。* [*十大机器学习框架*](/edureka/top-10-machine-learning-frameworks-72459e902ebb)
> 
> *23。* [*用于机器学习的统计*](/edureka/statistics-for-machine-learning-c8bc158bb3c8)
> 
> *24。* [*随机森林中的 R*](/edureka/random-forest-classifier-92123fd2b5f9)
> 
> *25。* [*广度优先搜索算法*](/edureka/breadth-first-search-algorithm-17d2c72f0eaa)
> 
> *26。*[*R 中的线性判别分析*](/edureka/linear-discriminant-analysis-88fa8ad59d0f)
> 
> *27。* [*机器学习的先决条件*](/edureka/prerequisites-for-machine-learning-68430f467427)
> 
> *28。* [*互动 WebApps 使用 R 闪亮*](/edureka/r-shiny-tutorial-47b050927bd2)
> 
> *29。* [*机器学习十大书籍*](/edureka/top-10-machine-learning-books-541f011d824e)
> 
> *三十。* [*无监督学习*](/edureka/unsupervised-learning-40a82b0bac64)
> 
> *31.1* [*0 最佳数据科学书籍*](/edureka/10-best-books-data-science-9161f8e82aca)
> 
> 32。 [*监督学习*](/edureka/supervised-learning-5a72987484d0)

*原载于 2019 年 8 月 7 日*[*https://www.edureka.co*](https://www.edureka.co/blog/regularization-in-machine-learning/)*。*