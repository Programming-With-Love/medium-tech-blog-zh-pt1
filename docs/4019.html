<html>
<head>
<title>A Glance at Apache Spark optimization techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark优化技术一瞥</h1>
<blockquote>原文：<a href="https://medium.com/globant/glance-on-spark-optimization-techniques-1d3df3074e41?source=collection_archive---------0-----------------------#2022-10-31">https://medium.com/globant/glance-on-spark-optimization-techniques-1d3df3074e41?source=collection_archive---------0-----------------------#2022-10-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="f934" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">简要介绍广泛使用的Spark性能优化技术</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/1772279ccf3fcb9e8d598c395061265a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rtlRj__LHq0O9Vph"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo by <a class="ae js" href="https://unsplash.com/@marcojodoin?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Marc-Olivier Jodoin</a> on <a class="ae js" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9ff4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">数据只不过是关于事实和数字的信息。数据来源主要有三种:<a class="ae js" href="https://en.wikipedia.org/wiki/Social_data_analysis" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">社会数据</strong></a><a class="ae js" href="https://en.wikipedia.org/wiki/Machine-generated_data" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">机器生成数据</strong></a><a class="ae js" href="https://en.wikipedia.org/wiki/Transaction_data" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">交易数据</strong> </a> <strong class="ig hi">。</strong>数据分析有助于组织改进其内部流程、客户关系管理、增强用户体验、潜在改进等。这浩如烟海的数据无非就是<a class="ae js" href="https://en.wikipedia.org/wiki/Big_data" rel="noopener ugc nofollow" target="_blank">大数据</a>。它需要高速计算技术来更快地处理它，因为传统系统无法处理它。</p><p id="e139" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae js" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"><strong class="ig hi">Spark</strong></a><strong class="ig hi"/>是一个非常强大的处理大数据的框架。您可能已经看到Spark应用程序运行缓慢，即使分配了足够的资源，因为该应用程序可能没有得到足够的优化。Spark有一些内置的优化技术，如<a class="ae js" href="https://www.databricks.com/glossary/catalyst-optimizer" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">催化剂优化器</strong> </a>和<a class="ae js" href="https://www.databricks.com/glossary/tungsten" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi">钨优化器</strong> </a>，但是还有更多优化Spark应用的方法。</p><p id="a0cf" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本文面向使用Spark并希望在Spark应用程序中获得一些性能提升的数据工程师和数据科学家。在本文中，我将简要介绍Spark支持的一些广泛使用的优化技术，以及如何使用它们。让我们开始…</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="67eb" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">Spark中的优化技术</h1><p id="7c32" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">Spark engine有一些内置的优化，但我们仍然需要更多地关注其他优化，这可能需要在集群优化、配置值调整、表优化、代码级优化等方面进行。，它们共同帮助提高了我们应用程序的性能。</p><p id="f08f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以将优化技术大致分为以下三类:</p><ul class=""><li id="d66f" class="ld le hh ig b ih ii il im ip lf it lg ix lh jb li lj lk ll bi translated"><strong class="ig hi">优化火花配置:</strong>这包括改变火花特性。</li><li id="99f7" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">优化Spark程序:</strong>这包括代码级优化。</li><li id="9fe6" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi">优化存储:</strong>这包括文件格式优化。</li></ul><p id="f269" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让我们深入了解每个类别…</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="4114" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">优化Spark C <em class="lr">配置</em></h1><p id="9b89" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">Spark引擎中有许多Spark配置属性，可以在创建<code class="du ls lt lu lv b">SparkSesion</code>对象时设置。这些属性控制着Spark应用程序，可以通过调整来提高应用程序的性能。</p><p id="ee48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你将如何设置火花配置？</p><p id="a446" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">1.您需要导入所需的类。</p><pre class="jd je jf jg fd lw lv lx ly aw lz bi"><span id="317d" class="ma kb hh lv b fi mb mc l md me">from pyspark.sql import SparkSession</span></pre><p id="2c71" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.创建一个对象，并在config()函数中设置配置值。</p><pre class="jd je jf jg fd lw lv lx ly aw lz bi"><span id="0049" class="ma kb hh lv b fi mb mc l md me">spark = (SparkSession.builder<br/>           .master("yarn")<br/>           .appName("test") <br/>           .config("spark.sql.shuffle.partitions", 200)<br/>           .getOrCreate())</span></pre><p id="ad40" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">现在让我们来看看我们可以调整哪些属性来提高Spark应用程序的性能…</p><h2 id="e642" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">调谐火花执行器</strong></h2><p id="2174" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">大多数时候，我们给应用程序分配静态资源。当我们知道应用程序的源数据大小时，这种策略非常有效。但是，如果某一天您从源中获得大量数据，而另一天您获得少量数据，该怎么办呢？在这种情况下，静态分配不是一个实际的选择。我们可以根据Spark应用程序的工作负载动态地添加/删除执行器。Spark有一个动态分配技术来支持执行器伸缩。当我们每天的工作负载不一致时，这种方法是最好的。您需要将<em class="ms">spark . dynamic allocation . enabled</em>属性设置为“true”来启用资源的动态分配。此后，Spark应用程序将根据需要伸缩执行器。</p><p id="2c50" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这需要设置以下两个属性中的任何一个:</p><ul class=""><li id="a06b" class="ld le hh ig b ih ii il im ip lf it lg ix lh jb li lj lk ll bi translated"><em class="ms">spark . shuffle . service . enabled</em></li><li id="acbc" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><em class="ms">火花.动态定位.洗牌跟踪.启用</em></li></ul><p id="523c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">以下属性也与<a class="ae js" href="https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation" rel="noopener ugc nofollow" target="_blank">动态分配配置</a>相关:</p><ul class=""><li id="23ca" class="ld le hh ig b ih ii il im ip lf it lg ix lh jb li lj lk ll bi translated"><em class="ms">spark . dynamic allocation . minexecutors</em></li><li id="d544" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><em class="ms">spark . dynamic allocation . max executors</em></li><li id="4aa2" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><em class="ms">spark . dynamic callocation . initial executors</em></li><li id="524e" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><em class="ms">spark . dynamic allocation . executor allocation ratio</em></li></ul><h2 id="1d0a" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">调谐火花记忆</strong></h2><p id="7237" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">我们经常在驱动程序或执行程序端得到一个<a class="ae js" href="https://www.clairvoyant.ai/blog/apache-spark-out-of-memory-issue" rel="noopener ugc nofollow" target="_blank">内存不足的错误</a>。为了避免这种情况，我们需要一个正确的内存配置。执行器内存分为不同的层，可以通过调整来提高性能。我们需要<a class="ae js" href="https://stackoverflow.com/questions/39660377/spark-2-0-memory-fraction" rel="noopener ugc nofollow" target="_blank">设置<em class="ms">spark . memory . fraction</em></a>属性来调整这些层的值。</p><p id="0466" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以在下图中看到三个内存区域:</p><figure class="jd je jf jg fd jh er es paragraph-image"><div class="er es mt"><img src="../Images/49729ba7b10d7b5be482967811e8d2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*ZLbiteNB9r_NnJm-1ufi6g.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx">Memory management in Spark</figcaption></figure><ul class=""><li id="5e11" class="ld le hh ig b ih ii il im ip lf it lg ix lh jb li lj lk ll bi translated"><strong class="ig hi"> <em class="ms">预留内存:</em> </strong>系统预留的内存，其大小是硬编码的。一般是300 MB。</li><li id="1926" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi"> <em class="ms">用户内存:</em> </strong>用户内存计算为<strong class="ig hi"><em class="ms"/></strong><em class="ms">(“Java堆内存”——“保留内存”)*(1.0——spark . Memory . fraction)。</em>这个内存池在分配了Spark内存之后依然存在，完全由你按照自己喜欢的方式使用。用户内存，你完全可以决定在这个RAM中存储什么，Spark如何完全不考虑你在那里做什么，以及你是否尊重这个界限。在代码中不考虑这一界限可能会导致内存不足的错误。</li><li id="b80e" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated"><strong class="ig hi"> <em class="ms"> Spark内存</em> </strong> : Spark内存计算为<em class="ms">(“Java堆内存”——“保留内存”)* spark.memory.fraction </em>。内存池由Spark管理，进一步分为两个内存区域——<em class="ms">存储内存</em> &amp; <em class="ms">执行内存。</em></li></ul><p id="8dee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">存储内存</strong>用于存储所有的缓存数据，广播变量也存储在这里。Spark将为任何包含“内存”的<em class="ms"> persist() </em>选项在这个段中存储数据。它通过使用下面给出的公式来计算:</p><p id="b034" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ms">存储内存= (Java堆内存—保留内存)* spark . Memory . fraction * spark . Memory . Storage fraction</em></p><p id="d74b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">执行内存</strong>由Spark用于任务执行过程中创建的对象。它通过使用下面给出的公式来计算:</p><p id="3678" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ms">执行内存= (Java堆内存—保留内存)* spark . Memory . fraction *(1.0—spark . Memory . storage fraction)</em></p><h2 id="4f79" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">调混文件缓冲</strong></h2><p id="d85e" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">与内存中的数据访问相比，磁盘访问速度较慢，因为它涉及一个耗费时间和资源的序列化过程。我们可以通过在内存中引入随机读/写文件缓冲区来降低磁盘I/O成本。</p><p id="a786" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">内存缓冲区大小控制着创建中间随机文件时的磁盘寻道和系统调用。我们需要设置<em class="ms">spark . shuffle . file . buffer</em>属性来改变内存缓冲区的大小。该属性的默认值是32k。如果可用内存资源足够，我们可以增加该参数的大小，以减少shuffle write过程中磁盘文件溢出的次数，从而减少磁盘IO次数，提高性能。该属性的建议大小为1 MB。这允许Spark在将最终的地图结果写入磁盘之前做更多的缓冲。我们可以如下设置该属性:<br/> <code class="du ls lt lu lv b">spark.shuffle.file.buffer = 1 MB</code></p><h2 id="86c2" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">调整压缩块大小</strong></h2><p id="b305" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">对于大型数据集，我们可以更改默认的压缩块大小。这些数据块可以通过存储或基于速度进行压缩，如<a class="ae js" href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer" rel="noopener ugc nofollow" target="_blank"> LZO </a>、<a class="ae js" href="https://en.wikipedia.org/wiki/Snappy_(compression)" rel="noopener ugc nofollow" target="_blank">爽快</a>和<a class="ae js" href="https://en.wikipedia.org/wiki/Gzip" rel="noopener ugc nofollow" target="_blank"> GZIP </a>。需要为<a class="ae js" href="https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)" rel="noopener ugc nofollow" target="_blank"> LZ4 </a>压缩中使用的块大小设置以下属性。默认压缩块大小为32 kb，这对于大型数据集来说不是最佳值。通过增加块大小，您可以看到随机/溢出文件大小减少了20%。<br/> <code class="du ls lt lu lv b">spark.io.compression.lz4.blockSize = 512KB</code></p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mu"><img src="../Images/c6d73020dae769cdd8ff30cfc2335994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3VwhXMRUXYjcbhxeJYr9Hg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Compression block size vs. size of shuffle files</figcaption></figure><h2 id="2e7d" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">调整洗牌分区值</strong></h2><p id="7323" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">shuffle-partition是指导致数据混洗的每个转换步骤后生成的分区数量，如<code class="du ls lt lu lv b">join()</code>、<code class="du ls lt lu lv b">agg()</code>、<code class="du ls lt lu lv b">reduce()</code>等。通过设置<em class="ms">spark . SQL . shuffle . partitions</em>属性，您可以决定Spark应用程序中的并行级别。</p><p id="5d6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以<a class="ae js" href="https://www.linkedin.com/pulse/always-more-resources-performance-spark-optimization-gaurav-patil" rel="noopener ugc nofollow" target="_blank">设置Spark属性<em class="ms">Spark . SQL . shuffle . partitions</em></a>来控制默认的shuffle分区。该属性的默认值为200。您应该根据集群的数据大小和可用资源来设置随机分区的数量。分区的数量应该是您拥有的执行器的倍数，这样分区就可以在任务之间平均分配。<br/> <code class="du ls lt lu lv b">spark.sql.shuffle.partitions = &lt;&lt;integer value&gt;&gt;</code></p><h2 id="db87" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">使用堆外内存</strong></h2><p id="0ab3" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">堆外内存位于JVM之外，但是被JVM用于特定的用例(例如字符串的内部化)。Spark可以显式地使用堆外内存来存储序列化的数据帧和rdd。Spark使用堆外内存有两个目的:</p><p id="2994" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">—一部分堆外内存由Java内部使用，用于字符串存储和JVM开销等目的。作为钨计划的一部分，Spark也可以明确地使用堆外内存来存储其数据。</p><p id="8a76" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Spark执行器的总堆外内存由<em class="ms">Spark . executor . memory overhead</em>配置控制。Spark用来存储实际数据帧的堆外内存量由<em class="ms">Spark . memory . off heap . size</em>控制。可以通过将<em class="ms">spark . memory . off heap . use</em>配置设置为true来启用。这两个存储器的区别在这里<a class="ae js" href="https://stackoverflow.com/questions/61263618/difference-between-spark-executor-memoryoverhead-and-spark-memory-offheap-size" rel="noopener ugc nofollow" target="_blank">解释</a>。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="f85d" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">优化Spark程序</h1><p id="fd22" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">只有当我们运行一个优化良好的程序时，才能看到CPU的真正威力。我们不应该仅仅认为增加资源会有助于提高性能，除非我们优化代码。Spark提供了许多我们可以用来优化应用程序的技术。本节将介绍一些广泛使用的程序优化技术。</p><h2 id="487f" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">广播加入</strong></h2><p id="3ca4" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">让我们考虑一个场景，其中您将一个大表与一个小表连接起来。在该加入操作期间，将发生更多的<a class="ae js" href="https://sparkbyexamples.com/spark/spark-shuffle-partitions/#:~:text=Shuffling%20is%20a%20mechanism%20Spark,Disk%20I%2FO" rel="noopener ugc nofollow" target="_blank">洗牌</a>。我们可以通过使用广播连接来避免洗牌。它会将一个小表复制到执行程序运行的每个节点上。然而，在某个阈值之后，广播加入往往比基于混洗的加入具有更少的优势。</p><pre class="jd je jf jg fd lw lv lx ly aw lz bi"><span id="72c3" class="ma kb hh lv b fi mb mc l md me"># Example program to illustrate use of broadcast join<br/>from pyspark.sql.functions import broadcast</span><span id="4794" class="ma kb hh lv b fi mv mc l md me">emp_df = spark.sql("select id, name, dept_id from employee")<br/>dept_df = spark.sql("select dept_id, dept_name from department")<br/>df_joined = emp_df.join(broadcast(dept_df),emp_df.dept_id == dept_df.dept_id, ‘inner’)<br/>df_joined.show(20)</span></pre><h2 id="6d63" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">缓存数据</strong></h2><p id="c4ef" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">每次我们调用Spark程序中的<a class="ae js" href="https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/" rel="noopener ugc nofollow" target="_blank"> <em class="ms">动作</em> </a>，都会触发<a class="ae js" href="https://www.projectpro.io/recipes/what-is-dag-apache-spark" rel="noopener ugc nofollow" target="_blank"> DAG </a>并从头执行。这就是为什么建议不要在Spark程序中使用不必要的<em class="ms">动作</em>的原因。仔细检查代码，删除或注释掉您在Spark程序中为调试/测试而编写的<em class="ms">动作</em>。</p><p id="dcc6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">当同一个<a class="ae js" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank">数据帧</a>在多个地方被引用时，提高性能的一个关键点是缓存该数据帧。Spark有两个函数来缓存数据帧:<code class="du ls lt lu lv b">cache()</code>和<code class="du ls lt lu lv b">persist()</code>。RDD的<code class="du ls lt lu lv b">cache()</code>函数默认将数据帧保存到内存中，而<code class="du ls lt lu lv b">persist()</code>函数用于在用户定义的存储级别存储数据帧。<code class="du ls lt lu lv b">persist()</code>功能支持不同的参数。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mw"><img src="../Images/07840095ee363a2fada94d4681242bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwg2DnghjHx0QsZ1pJ-AoA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Storage Levels comparison chart</figcaption></figure><h2 id="9529" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">重新分区数据</strong></h2><p id="a45c" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">数据帧被分区意味着其中有逻辑记录组。一组记录称为一个分区。每个任务处理每个分区，许多任务在一个执行器内并行运行，并行执行发生在Spark中。如果我们正确地分配数据，并行度可以提高。对于重新分区，Spark有两种方法— <code class="du ls lt lu lv b"><a class="ae js" href="https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce" rel="noopener ugc nofollow" target="_blank">repartition()</a></code>和<code class="du ls lt lu lv b"><a class="ae js" href="https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce" rel="noopener ugc nofollow" target="_blank">coalesce()</a></code>。</p><p id="478c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们检查源代码，我们看到<code class="du ls lt lu lv b">coalesce()</code>在<code class="du ls lt lu lv b">repartition()</code> <em class="ms"> </em>中被调用，其中<code class="du ls lt lu lv b">shuffle</code> <em class="ms"> </em>参数被设置为<code class="du ls lt lu lv b">true</code>。不带<code class="du ls lt lu lv b">shuffle</code> <em class="ms"> </em>参数<em class="ms"> </em>的<code class="du ls lt lu lv b">coalesce()</code>与<code class="du ls lt lu lv b">repartition()</code>的主要区别在于<code class="du ls lt lu lv b">coalesce()</code>主要用于减少分区，<code class="du ls lt lu lv b">repartition()</code>用于增加分区。与<code class="du ls lt lu lv b">repartition()</code>相比，当我们使用<code class="du ls lt lu lv b">coalesce()</code> it时，发生的洗牌更少。<code class="du ls lt lu lv b">repartition()</code>函数也可用于解决数据偏斜问题。</p><p id="b196" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下面是摘自<a class="ae js" href="https://github.com/apache/spark/blob/15dc0eb6c6b5b5da4632eddafaec0ac82fe1043b/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L473" rel="noopener ugc nofollow" target="_blank"> <strong class="ig hi"> RDD源代码</strong> </a>的代码片段。</p><pre class="jd je jf jg fd lw lv lx ly aw lz bi"><span id="63e4" class="ma kb hh lv b fi mb mc l md me">/*<br/>* Return a new RDD that has exactly numPartitions partitions.<br/>* Can increase or decrease the level of parallelism in this RDD. <br/>* Internally, this uses a shuffle to redistribute data.<br/>* If you are decreasing the number of partitions in this RDD, <br/>* consider using `coalesce`, which can avoid performing a shuffle.<br/>*/<br/>def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {<br/>    coalesce(numPartitions, shuffle = true)<br/>}</span></pre><p id="c323" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">因此，为了提高应用程序的性能，您需要增加或减少分区。这完全取决于你的数据。如果您正在处理大量数据，并且想要创建更多的并发任务，那么您可以使用<code class="du ls lt lu lv b">repartition()</code>功能来增加分区。如果您想根据列值重新排列数据，也可以使用<code class="du ls lt lu lv b">repartition()</code>功能。如果你想减少输出文件的数量，那么你可以使用<code class="du ls lt lu lv b">coalesce()</code>功能。应用这些更改后，您可以显著提高应用程序的性能。</p><h2 id="885e" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">过滤前面步骤中的数据</strong></h2><p id="ecfb" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">提高连接和其他处理性能的关键是在前面的步骤中过滤结果集中不需要的数据。假设您只想处理和存储‘Mumbai’的员工。然后，您可以在将表读入数据帧时，先过滤数据帧，而不是读取整个数据和过程，然后对结果应用过滤器。</p><p id="c6ad" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">Spark还有一个更重要的特性。它在内部优化了逻辑计划。它使用<a class="ae js" href="https://docs.datastax.com/en/dse/6.8/dse-dev/datastax_enterprise/spark/sparkPredicatePushdown.html" rel="noopener ugc nofollow" target="_blank">谓词下推</a>来支持文件格式，并在开始时推送过滤器，然后对过滤后的数据进行处理。</p><h2 id="601a" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">使用</strong> <a class="ae js" href="https://towardsdatascience.com/skewed-data-in-spark-add-salt-to-compensate-16d44404088b" rel="noopener" target="_blank"> <strong class="ak">加盐技术</strong> </a> <strong class="ak">消除数据偏斜问题。</strong></h2><p id="50f5" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">我们经常在Spark UI上看到，有些任务需要更长的时间，有些任务很快就完成了。当您的数据有偏差时，就会发生这种情况。这意味着数据不是均匀地分布在各个分区上。因此，在数据量大于其他分区的分区上工作的任务需要更多的时间来完成。这有时也会导致内存不足的问题。跨分区重新洗牌可以消除数据偏斜。我们可以通过使用<code class="du ls lt lu lv b">repartition()</code>函数来实现洗牌。但是这个函数不能保证记录的均匀分布。所以，我们必须在dataframe中的一个新列中添加一些随机值，我们通常称之为<em class="ms">加盐键</em>，然后我们可以在repartition函数中将那个<em class="ms">加盐键</em>作为参数传递。之后，<code class="du ls lt lu lv b">repartition()</code>函数将根据<em class="ms"> salted key </em>列值重新洗牌。</p><pre class="jd je jf jg fd lw lv lx ly aw lz bi"><span id="982f" class="ma kb hh lv b fi mb mc l md me"># Example program to illustrate use of Salting Technique</span><span id="623c" class="ma kb hh lv b fi mv mc l md me">salt_df_new = df.withColumn(“salted_key”, <br/>                                (rand * n).cast(IntegerType))<br/>shuffled_df = salt_df_new.repartition(100, ‘salted_key’)</span></pre><h2 id="06d2" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">将数据读入数据帧时，明确提供模式</strong></h2><p id="2e87" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">预定义的模式使得Spark更容易获得列和数据类型，而无需读取整个文件；如果您正在处理大量数据，这将提高Spark代码的性能。如果我们希望Spark隐式地识别模式，我们可以使用<code class="du ls lt lu lv b">inferSchema=True</code>选项，并且我们可以使用schema选项为dataframe提供一个模式。</p><pre class="jd je jf jg fd lw lv lx ly aw lz bi"><span id="d593" class="ma kb hh lv b fi mb mc l md me"># Example program showing how to provide the schema externally<br/># while reading the file</span><span id="5fc2" class="ma kb hh lv b fi mv mc l md me">from pyspark.sql.types import StructType, IntegerType, DateType</span><span id="bef0" class="ma kb hh lv b fi mv mc l md me">schema = StructType([<br/>         StructField(“col_01”, IntegerType()),<br/>         StructField(“col_02”, DateType()),<br/>         StructField(“col_03”, IntegerType()) ])</span><span id="838c" class="ma kb hh lv b fi mv mc l md me">df = spark.read.csv(filename, header=True, schema=schema)</span></pre><h2 id="d286" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">优化JSON文件处理</strong></h2><p id="1408" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">JSON是一种广泛使用的格式，允许半结构化数据，因为它不需要模式。这为存储和查询不总是遵循固定模式和数据类型的数据提供了更大的灵活性。JSON文件与许多系统兼容。总是建议在读取JSON文件时显式传递schema，以避免不正确的数据类型标识。</p><pre class="jd je jf jg fd lw lv lx ly aw lz bi"><span id="4a36" class="ma kb hh lv b fi mb mc l md me"># Example program showing how to provide the schema externally <br/># while reading JSON file</span><span id="0b0c" class="ma kb hh lv b fi mv mc l md me">from pyspark.sql.types import StructType, StructField, StringType <br/>schema = StructType([ <br/>             StructField(“first_name”, StringType(), True),<br/>             StructField(“last_name”, StringType(), True), <br/>             StructField(“address”, StringType(), True)<br/>         ])</span><span id="ed26" class="ma kb hh lv b fi mv mc l md me">emp_df = sqlContext.read.json(“employee_file.json”, schema)</span></pre><h2 id="7031" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">对GroupByKey使用ReduceByKey函数</strong></h2><p id="5f6b" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated"><code class="du ls lt lu lv b">reduceByKey()</code>和<code class="du ls lt lu lv b">groupByKey()</code> <em class="ms"> </em>都是宽泛的转换，这意味着两者都将在分区间洗牌。<code class="du ls lt lu lv b">reduceByKey()</code>和<code class="du ls lt lu lv b">groupByKey()</code>的关键区别在于<code class="du ls lt lu lv b">reduceByKey()</code> <em class="ms"> </em>做地图边合并而<code class="du ls lt lu lv b">groupByKey()</code>不做。<code class="du ls lt lu lv b">reduceByKey()</code>就像一个微型减速器。所以，如果我们使用<code class="du ls lt lu lv b">reduceByKey()</code>，就可以减少数据的混乱。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mx"><img src="../Images/27332e592c0e80c17f35193a425dff58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pcw6NlwwY9W12iDGUoK-XA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Reduce by key example</figcaption></figure><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es my"><img src="../Images/83c5b17e72c17a08812b10ffab19db6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3yhToDLcAtG2OJ6ArFSYzw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Group by key example</figcaption></figure><h2 id="055e" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">避免使用UDF(用户定义函数)</strong></h2><p id="845b" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">说到性能，UDF很重。Spark没有优化UDF，因为它就像一个黑盒。内置的“<a class="ae js" href="https://www.databricks.com/glossary/what-is-spark-sql" rel="noopener ugc nofollow" target="_blank"> Spark SQL </a>”函数进行了优化，推荐在程序中使用。Spark提供了很多常用的Spark SQL函数进行数据转换。因此，在决定创建UDF之前，请查看Spark SQL文档，看看是否有可以满足您要求的函数。</p><h2 id="c809" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">使用连接列时检查键的唯一性</strong></h2><p id="ca36" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">我们应该总是小心地选择连接两个数据帧的连接列。如果在任一列中存在重复值，那么连接这样的数据帧需要很长时间，因为可能会产生笛卡儿积。这也会导致内存不足的错误。因此，在我们编写代码之前，最好通过执行一些查询来分析这个表。</p></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="5f95" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">优化存储</h1><p id="6f34" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">优化存储对于提高Spark应用程序的性能非常重要。这包括通过创建分区和存储桶来组织数据、使用序列化数据格式、根据应用程序的需求选择合适的文件格式等。让我们一个一个来看…</p><h2 id="51da" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">分桶和分区</strong></h2><p id="ce1d" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">分桶和分区是优化配置单元表的广泛使用的技术。分区通常表示为目录，存储区表示为文件。分区根据分区列，将记录拆分到与分区列值同名的不同目录下的文件中。分桶有助于根据哈希函数将记录进一步拆分到不同的文件中。这提高了我们在Spark中读取数据时的性能。</p><p id="996a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果表被分区和分桶，那么“Spark SQL ”(其中的过滤器列与分区/分桶列相同)可以运行得更快，因为它只扫描特定的目录和文件，而不是扫描所有的目录。因此，查询性能得到了提高。</p><p id="258d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">列的<strong class="ig hi">基数</strong>可以定义为该列中不同值的数量。总是建议对高基数列使用分桶，对低基数列使用分区。</p><pre class="jd je jf jg fd lw lv lx ly aw lz bi"><span id="9722" class="ma kb hh lv b fi mb mc l md me"># Example program showing how to use partitioning and bucketing <br/># while creating a Hive table</span><span id="2b48" class="ma kb hh lv b fi mv mc l md me">CREATE EXTERNAL TABLE zipcodes(<br/>    rec_no int,<br/>    country string,<br/>    city string,<br/>    zip_code int<br/>)<br/>PARTITIONED BY(<br/>    state string<br/>)<br/>CLUSTERED BY zip_code INTO 10 BUCKETS<br/>ROW FORMAT DELIMITED<br/>FIELDS TERMINATED BY ‘,’;</span></pre><h2 id="794d" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated"><strong class="ak">序列化数据格式</strong></h2><p id="5084" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated"><strong class="ig hi">序列化</strong> <em class="ms"> </em>是一种将对象状态转换成字节流的机制。这种机制用于持久化对象。<strong class="ig hi">反序列化</strong> <em class="ms"> </em>是相反的过程，其中字节流用于在内存中重新创建实际的Java对象。Spark使用Java序列化程序。我们可以通过使用<a class="ae js" href="https://blog.knoldus.com/kryo-serialization-in-spark/" rel="noopener ugc nofollow" target="_blank"> Kryo序列化器</a>来显式地提示Spark，这样可以提高性能。</p><p id="3c28" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">不同的文件格式支持序列化，比如<a class="ae js" href="https://avro.apache.org/docs/1.2.0/" rel="noopener ugc nofollow" target="_blank"> Avro </a>、<a class="ae js" href="https://www.databricks.com/glossary/what-is-parquet" rel="noopener ugc nofollow" target="_blank"> Parquet </a>和<a class="ae js" href="https://cwiki.apache.org/confluence/display/hive/languagemanual+orc#LanguageManualORC-ORCFiles" rel="noopener ugc nofollow" target="_blank"> ORC </a>。这些文件格式也支持压缩库，如<a class="ae js" href="https://docs.python.org/3/library/zlib.html" rel="noopener ugc nofollow" target="_blank"> ZLIB </a>、<a class="ae js" href="https://en.wikipedia.org/wiki/Snappy_(compression)" rel="noopener ugc nofollow" target="_blank"> SNAPPY </a>等。</p><h2 id="61b0" class="ma kb hh bd kc mf mg mh kg mi mj mk kk ip ml mm ko it mn mo ks ix mp mq kw mr bi translated">选择合适的文件格式</h2><p id="1d90" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">Spark兼容多种文件格式，如text、Parquet、ORC、Avro、JSON等。每种文件格式都有其优点和缺点。文本格式广泛用于非结构化数据。JSON格式用于半结构化数据。结构化数据可以存储在ORC或Parquet中，因为两者都是列格式。ORC和Parquet是内存高效的，在加速查询方面表现更好。Avro以JSON格式存储schema，与其他系统具有广泛的兼容性。根据需求，我们必须选择正确的文件格式来平衡性能和需求。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mz"><img src="../Images/e38ec1423e2ed493831dc4f214b521f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Wx13r0kvYL1mUKQCvvWhQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">File formats comparison</figcaption></figure></div><div class="ab cl jt ju go jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="ha hb hc hd he"><h1 id="2534" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">结论</h1><p id="eb69" class="pw-post-body-paragraph ie if hh ig b ih ky ij ik il kz in io ip la ir is it lb iv iw ix lc iz ja jb ha bi translated">在本文中，我向您简要介绍了所有广泛使用的优化技术。尽管Spark是一个非常受欢迎和突出的大数据处理引擎，但优化始终是一个具有挑战性的话题。因此，了解您的数据和您的过程中所有可能的火花配置是至关重要的。</p><p id="404f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">没有预定义的方法来优化Spark应用程序。这完全取决于您的数据和代码。在决定程序的优化技术之前，确保执行不同的基准和源数据分析。您可以使用Spark UI来分析您的应用程序，并确定可以改进的地方。谢谢你阅读我的博客。编码快乐！</p><h1 id="08fa" class="ka kb hh bd kc kd na kf kg kh nb kj kk kl nc kn ko kp nd kr ks kt ne kv kw kx bi translated">参考</h1><ul class=""><li id="8ab1" class="ld le hh ig b ih ky il kz ip nf it ng ix nh jb li lj lk ll bi translated">调谐火花—<a class="ae js" href="https://spark.apache.org/docs/2.3.2/tuning.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/2.3.2/tuning.html</a></li><li id="5228" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">性能调优—<a class="ae js" href="https://spark.apache.org/docs/2.4.2/sql-performance-tuning.html" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/2 . 4 . 2/SQL-performance-Tuning . html</a></li><li id="f46b" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">Spark内存管理—<a class="ae js" href="https://community.cloudera.com/t5/Community-Articles/Spark-Memory-Management/ta-p/317794" rel="noopener ugc nofollow" target="_blank">https://Community . cloud era . com/T5/Community-Articles/Spark-Memory-Management/ta-p/317794</a></li><li id="1b8d" class="ld le hh ig b ih lm il ln ip lo it lp ix lq jb li lj lk ll bi translated">大数据文件格式—<a class="ae js" href="https://www.clairvoyant.ai/blog/big-data-file-formats" rel="noopener ugc nofollow" target="_blank">https://www.clairvoyant.ai/blog/big-data-file-formats</a></li></ul></div></div>    
</body>
</html>