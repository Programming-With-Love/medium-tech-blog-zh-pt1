<html>
<head>
<title>Custom Machine Learning Estimators at Scale on Dask &amp; RAPIDS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Dask &amp; RAPIDS上的大规模定制机器学习估算器</h1>
<blockquote>原文：<a href="https://medium.com/capital-one-tech/custom-machine-learning-estimators-at-scale-on-dask-rapids-e2b9519a6d1f?source=collection_archive---------4-----------------------#2021-03-04">https://medium.com/capital-one-tech/custom-machine-learning-estimators-at-scale-on-dask-rapids-e2b9519a6d1f?source=collection_archive---------4-----------------------#2021-03-04</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="580a" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">如何构建与Scikit-learn、Dask和RAPIDS集成的可重用组件</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/0dc53b8b72085f79996018505bcd11df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jSYcd9sXfkao-Byq.jpg"/></div></div></figure><p id="6721" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在构建可重用的数据科学和机器学习代码时，开发人员通常需要围绕现有的开源库(如scikit-learn)添加定制的业务逻辑。这些定制可以执行数据预处理、以特定方式分割数据或者实现专有算法。自定义逻辑导致需要理解和维护更多的代码，这增加了复杂性并带来了风险。这篇博客文章将讨论如何利用<a class="ae ke" href="https://scikit-learn.org/stable/developers/develop.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn库的API </a>来添加这样的定制，以最大限度地减少代码、减少维护、方便重用，并提供利用<a class="ae ke" href="https://dask.org/" rel="noopener ugc nofollow" target="_blank"> Dask </a>和<a class="ae ke" href="https://rapids.ai/" rel="noopener ugc nofollow" target="_blank"> RAPIDS </a>等技术<a class="ae ke" rel="noopener" href="/capital-one-tech/dask-and-rapids-the-next-big-things-for-data-science-and-machine-learning-at-capital-one-d4bba136cc70">扩展</a>的能力。</p><h1 id="466f" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">为什么在建模代码时很难遵循scikit-learn API？</h1><p id="04f4" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">任何机器学习努力的最终目标通常是尽快获得给定数据集的最佳模型。在机器学习中，太多的时间花费在数据处理、模型训练和验证上，以至于在这个过程中创建的代码的设计和可维护性有时会被忽略。在一个资源有限的世界里，也许这种优先排序是有意义的。毕竟，将投入生产的是模型，而不是产生它的代码。在银行和其他金融机构中，模型在投入生产之前必须经过验证过程，其中可重复性至关重要。一旦模型被部署，生成它的代码通常会在git存储库中几周、几个月甚至几年不动。直到有一天，一个模型开发人员，可能是也可能不是最初的作者，会回到代码中来更新模型。他们将大大受益于一个经过良好测试和记录的易于理解的存储库，这样他们可以快速开始工作，从而节省时间和成本。</p><p id="0a0c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在实践中，遵循标准并应用它们可能具有挑战性，因为</p><ul class=""><li id="3305" class="lc ld hh jk b jl jm jo jp jr le jv lf jz lg kd lh li lj lk bi translated">标准在应用之前必须被理解。</li><li id="1a9b" class="lc ld hh jk b jl ll jo lm jr ln jv lo jz lp kd lh li lj lk bi translated">对你的问题应用一个标准需要努力。</li></ul><p id="b6de" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">克服这些挑战需要深思熟虑，需要宝贵的时间，而这些时间在数据科学项目中可能并不充裕。PyData生态系统由许多不断发展的标准和API组成，因此很难跟上这些变化。所有这些都会减慢模型开发过程，因此阻力最小的方法是使用包装在您自己的设计中的最适合您的应用程序的库。然而，这通常会导致一些问题，比如由于用户错误或重复代码导致的数据泄露，因为设计不够模块化，不允许扩展。在后一种情况下，维护开销很高，因为代码库越来越大越来越复杂，即使使用Dask和RAPIDS，修复bug或扩展也变得非常困难。</p><h1 id="dd39" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">sci kit-学习示例</h1><p id="6431" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">我们将使用scikit-learn <a class="ae ke" href="https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html#pipelining" rel="noopener ugc nofollow" target="_blank">文档</a>中的以下示例来说明本文中的观点。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="4da7" class="lv kg hh lr b fi lw lx l ly lz">from sklearn import datasets<br/>from sklearn.decomposition import PCA<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.model_selection import GridSearchCV, train_test_split<br/>import numpy as np<br/><br/>X_digits, y_digits = datasets.load_digits(return_X_y=True)<br/><br/># Define a pipeline to search for the best combination of PCA truncation<br/># and classifier regularization.<br/>pca = PCA()<br/># set the tolerance to a large value to make the example faster<br/>logistic = LogisticRegression(max_iter=10000, tol=0.1)<br/>pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])<br/># Parameters of pipelines can be set using ‘__’ separated parameter names:<br/>param_grid = {<br/>    'pca__n_components': [5, 15, 30, 45, 64],<br/>    'logistic__C': np.logspace(-4, 4, 4),<br/>}<br/><br/>search = GridSearchCV(pipe, param_grid, n_jobs=-1)<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=123)<br/><br/>search.fit(X_train, y_train)<br/><br/>best = search.best_estimator_<br/><br/>print(f"Training set score: {best.score(X_train, y_train)}")<br/>print(f"Test set score: {best.score(X_test, y_test)}")</span></pre><p id="59b4" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">输出:</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="a5fb" class="lv kg hh lr b fi lw lx l ly lz">Training set score: 1.0<br/>Test set score: 0.9666666666666667</span></pre><h1 id="e928" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">定制Scikit-learn示例</h1><p id="2f6b" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">假设我们想修改这个例子，以便在PCA步骤之后改变数据。也许我们有一组总是想从另一个数据源中包含进来的特性，或者需要对数据进行标记。我们可以通过移除管道并添加一个函数来对<code class="du ma mb mc lr b">X</code>执行变异来做到这一点。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="1f17" class="lv kg hh lr b fi lw lx l ly lz">def mutate(X):<br/>    """Mutates X"""<br/>    # ... do something ...<br/>    return X<br/><br/>pca = PCA(n_components=search.best_params_['pca__n_components'])<br/>logistic = LogisticRegression(<br/>    max_iter=10000, tol=0.1, C=search.best_params_['logistic__C'])<br/><br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X_digits, y_digits, random_state=123)<br/><br/>X_train = pca.fit_transform(X_train, y_train)<br/>X_train = mutate(X_train)<br/>logistic = logistic.fit(X_train, y_train)<br/><br/>X_test = pca.transform(X_test) # &lt;- Don't call fit again!<br/>X_test = mutate(X_test) # &lt;-Don’t forget to call mutate on X_test!<br/><br/>print(f"Training set score: {logistic.score(X_train, y_train)}")<br/>print(f"Test set score: {logistic.score(X_test, y_test)}")</span></pre><p id="cb98" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">输出:</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="3aa5" class="lv kg hh lr b fi lw lx l ly lz">Training set score: 1.0<br/>Test set score: 0.9666666666666667</span></pre><p id="d809" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这允许在不要求用户了解scikit-learn管道的情况下进行定制。然而，如果你与其他团队合作或者为其他团队开发一个库，这种模式很快就会出现问题。这段代码可以被封装到一个类中，然后被多次复制。其中一些类甚至可以稍微修改一下，以增加扩展的功能或规模。此外，如果您在其中一个类中发现了一个bug，您可能有很多地方可以修复它。<em class="md">狩猎快乐！</em></p><p id="8bc1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">考虑通过添加包含变异逻辑的定制估计器来扩展scikit-learn API的替代方法。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="08f1" class="lv kg hh lr b fi lw lx l ly lz">from sklearn.base import BaseEstimator, TransformerMixin<br/>from abc import ABCMeta<br/><br/>class Mutate(TransformerMixin, BaseEstimator, metaclass=ABCMeta):<br/>    def fit(self, X, y):<br/>        return self<br/>    <br/>    def transform(self, X):<br/>        """Mutates X"""<br/>        # ... do something ...<br/>        return X<br/><br/>pca = PCA(n_components=search.best_params_['pca__n_components'])<br/>logistic = LogisticRegression(max_iter=10000, tol=0.1, C=search.best_params_['logistic__C'])<br/><br/>pipe = Pipeline(steps=[('pca', pca), ('mutate', Mutate()), ('logistic', logistic)])<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=123)<br/><br/>pipe.fit(X_train, y_train)<br/>print(f"Training set score: {pipe.score(X_train, y_train)}")<br/>print(f"Test set score: {pipe.score(X_test, y_test)}")</span></pre><p id="1453" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">输出:</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="d00c" class="lv kg hh lr b fi lw lx l ly lz">Training set score: 1.0<br/>Test set score: 0.9666666666666667</span></pre><p id="16a8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">通过添加一个定制的估计器，我们以一种允许我们将它插入到流水线的确切步骤中的方式封装了变异。这种方法的另一个好处是，我们只需要维护一个类，它可以随着时间的推移而增长，以包含更多的功能和规模。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="800c" class="lv kg hh lr b fi lw lx l ly lz">def transform(self, X):<br/>    """Mutates X"""<br/>    # ... do something ...<br/>    if is_dask_collection(X):<br/>        # Do Dask things<br/>    elif is_rapids_collection(X):<br/>        # Do RAPIDS things<br/>    return X</span></pre><p id="2388" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">这种方法的主要缺点，也可能是为什么没有被采用的原因，是开发人员需要对“scikit-learn”有足够的理解，以便将问题纳入API。</p><h1 id="3385" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">Dask &amp; RAPIDS中现有的可伸缩自定义估算器</h1><p id="3b05" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">实际上这不是一个新的模式。事实上，在PyData社区中，我们已经有了大量自定义可伸缩估计器的例子。<a class="ae ke" href="https://ml.dask.org/" rel="noopener ugc nofollow" target="_blank"> dask-ml </a>是一个scikit-learn扩展库，它使用dask扩展数据并执行并行计算。它为scikit-learn估算器提供了许多替代产品。</p><p id="1dce" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">下面是dask-ml的玩具示例管道的样子。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="1f9a" class="lv kg hh lr b fi lw lx l ly lz">from dask.distributed import Client, progress<br/>client = Client(processes=False, threads_per_worker=4,<br/>                n_workers=1, memory_limit='3GB')</span><span id="f8b0" class="lv kg hh lr b fi me lx l ly lz">from dask_ml.datasets import make_classification<br/>from dask_ml.decomposition import PCA<br/>from dask_ml.linear_model import LogisticRegression<br/>from dask_ml.model_selection import GridSearchCV, train_test_split<br/>from sklearn.pipeline import Pipeline  # &lt;-- using the sklearn pipeline<br/>import numpy as np<br/><br/>X, y = make_classification(n_samples=1000, n_features=20,<br/>                           chunks=100, n_informative=4,<br/>                           random_state=0)<br/><br/># Define a pipeline to search for the best combination of PCA truncation<br/># and classifier regularization.<br/>pca = PCA()<br/># set the tolerance to a large value to make the example faster<br/>logistic = LogisticRegression(fit_intercept=False, max_iter=10000, tol=0.1)<br/>pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])<br/># Parameters of pipelines can be set using ‘__’ separated parameter names:<br/>param_grid = {<br/>    'pca__n_components': [5, 15, 30, 45, 64],<br/>    'logistic__C': np.logspace(-4, 4, 4),<br/>}<br/><br/>search = GridSearchCV(pipe, param_grid, n_jobs=-1)<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)<br/><br/>search.fit(X_train, y_train)<br/><br/>best = search.best_estimator_<br/><br/>print(f"Training set score: {best.score(X_train, y_train)}")<br/>print(f"Test set score: {best.score(X_test, y_test)}")</span></pre><p id="a073" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">或者，你可以使用<a class="ae ke" href="https://docs.rapids.ai/api/cuml/stable/" rel="noopener ugc nofollow" target="_blank"> cuML </a>的插件来扩展NVIDIA GPU的RAPIDS。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="8053" class="lv kg hh lr b fi lw lx l ly lz">from dask.distributed import Client<br/>from dask_cuda import LocalCUDACluster<br/>from cuml.dask.datasets.classification import make_classification<br/><br/>cluster = LocalCUDACluster()<br/>client = Client(cluster)<br/><br/>X, y = make_classification(n_samples=1000, n_features=20,<br/>                           chunks=100, n_informative=4,<br/>                           random_state=0)</span><span id="4cb4" class="lv kg hh lr b fi me lx l ly lz">from cuml.dask.decomposition import PCA<br/>from cuml.linear_model import LogisticRegression<br/><br/>pca = PCA()<br/># set the tolerance to a large value to make the example faster<br/>logistic = LogisticRegression(max_iter=10000, tol=0.1)<br/>pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])</span></pre><h1 id="ca33" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">设计您自己的scikit-learn评估工具，与Dask &amp; RAPIDS一起扩展</h1><p id="3587" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">至此，我们已经展示了如何以两种方式扩展相同的管道，您可能会看到一种模式的出现。数据结构和建模算法依赖于相同的底层对应库，但是彼此之间是松散耦合的。换句话说，我们已经将数据加载逻辑从计算中分离出来，它依赖于类似于<em class="md">数组的</em>或<em class="md">数据框架</em> API。在引擎盖下，我们看到Dask估计器知道如何处理Dask集合，而cuML估计器知道如何处理RAPIDS集合。如果我们使用与估算器相匹配的库来读取数据，一切都会正常。我们是否可以按照这种模式构建自己的评估器，以一种在Dask和RAPIDS上都可伸缩的方式封装定制的业务逻辑？</p><p id="e587" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">首先，让我们看看内存、分布式和加速框架读取数据的方法。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="ce16" class="lv kg hh lr b fi lw lx l ly lz">import pandas as pd<br/>df = pd.read_csv(...)<br/><br/>import dask.dataframe as dd<br/>df = dd.read_csv(...)<br/><br/>import cudf as cdf<br/>df = cdf.read_csv(...)</span></pre><p id="9264" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">请注意，所有这三个库都提供了可以用作抽象层的替代功能，将读取数据归纳到内存和数据操作中。一旦我们有了一个类似于<em class="md"> dataframe的</em>结构，我们就可以定义一个估算器，对数据API进行仔细的假设。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="10c1" class="lv kg hh lr b fi lw lx l ly lz">from mylib import CustomEstimator<br/>est = CustomEstimator(**params)<br/>est.fit(df)</span></pre><p id="3007" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">或者，如果我们希望在阵列接口上实现标准化，我们估计器的拟合方法可以接受X和y。我们发现，<em class="md">类似数组的</em> API在三个框架中对于scikit-learn任务更加一致，这使得我们的定制估算器代码对于扩展更加通用。下面我们展示了对类似于<em class="md">数组的</em>接口的<code class="du ma mb mc lr b">CustomEstimator</code>用法的一个略微修改的版本。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="a83c" class="lv kg hh lr b fi lw lx l ly lz">from mylib import CustomEstimator<br/>est = CustomEstimator(**params)<br/>X, y = df[features].values, df[target].values<br/>est.fit(X, y)</span></pre><h1 id="10b7" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">定制sci kit-学习估计器</h1><p id="a8b7" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">了解了将数据读取和操作从业务逻辑中分离出来的基本设计之后，让我们看一个例子，看看这在实践中可能是什么样子。下面，我们定义了自己的自定义交叉验证类<code class="du ma mb mc lr b">CustomSearchCV</code>，它实现了遵循scikit-learn API的模型训练逻辑的内存版本。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="1b9c" class="lv kg hh lr b fi lw lx l ly lz">from sklearn.base import BaseEstimator, clone<br/>from sklearn.model_selection import train_test_split<br/>import copy<br/>import pandas as pd<br/>import numpy as np<br/><br/>class CustomSearchCV(BaseEstimator):<br/>    <br/>    def __init__(self, estimator, cv, logger):<br/>        self.estimator = estimator<br/>        self.cv = cv<br/>        self.logger = logger<br/>    <br/>    def fit(self, X, y=None, **fit_kws):<br/>        if isinstance(X, pd.DataFrame):<br/>            X = X.values<br/>        # Insert more guards here!<br/>            <br/>        X_base, X_holdout, y_base, y_holdout = train_test_split(<br/>            X, y, random_state=123)<br/>        <br/>        self.split_scores_ = []<br/>        self.holdout_scores_ = []<br/>        self.estimators_ = []            <br/>        <br/>        for train_idx, test_idx in self.cv.split(X_base, y_base):<br/>            X_test, y_test = X_base[test_idx], y_base[test_idx]<br/>            X_train, y_train = X_base[train_idx], y_base[train_idx]<br/><br/>            estimator_ = clone(self.estimator)<br/>            estimator_.fit(X_train, y_train, **fit_kws)<br/><br/>            self.logger.info("... log things ...")<br/>            self.estimators_.append(estimator_)<br/>            self.split_scores_.append(estimator_.score(X_test, y_test))            <br/>            self.holdout_scores_.append(<br/>                estimator_.score(X_holdout, y_holdout))<br/>    <br/>        self.best_estimator_ = \<br/>                self.estimators_[np.argmax(self.holdout_scores_)]<br/>        return self</span></pre><p id="b33c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><code class="du ma mb mc lr b">CustomSearchCV</code>可以很好地与现有的估计器一起工作，比如<code class="du ma mb mc lr b">sklearn.model_selection.RepeatedKFold</code>和<code class="du ma mb mc lr b">xgboost.XGBRegressor</code>。用户甚至可以定义自己的折叠类，并将其注入到我们的估计器中。下面显示了一个用法示例。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="cd69" class="lv kg hh lr b fi lw lx l ly lz">from sklearn.model_selection import RepeatedKFold<br/>import xgboost as xgb<br/><br/>from mylib import make_classifier_data, logger<br/><br/>X, y = make_classifier_data(<br/>    n_samples=100_000,<br/>    n_features=100,<br/>    response_rate=0.25,<br/>    predictability=0.25,<br/>    random_state=123,<br/>)<br/><br/>cv = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)<br/>clf = CustomSearchCV(xgb.XGBClassifier(n_jobs=-1), cv, logger)<br/><br/>clf.fit(X, y)<br/>clf.best_estimator_</span></pre><h1 id="2c56" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">使用Dask扩展</h1><p id="b187" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated"><code class="du ma mb mc lr b">CustomSearchCV</code>可以使用Dask集合，只需对fit方法稍作修改。首先，创建一个Dask客户机来连接到您的集群。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="e6d3" class="lv kg hh lr b fi lw lx l ly lz">from dask.distributed import Client, progress<br/><br/>client = Client()<br/>client</span></pre><p id="3fb2" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">然后添加以下逻辑来检查输入数据，以确定它是否是Dask集合。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="b7ae" class="lv kg hh lr b fi lw lx l ly lz">from dask.base import is_dask_collection<br/>import dask.dataframe as dd<br/>...<br/>    def fit(self, X, y=None, **fit_kws):<br/>        if isinstance(X, dd.DataFrame):<br/>            X = X.to_dask_array(lengths=True)<br/>        elif isinstance(X, pd.DataFrame):<br/>            X = X.values<br/>        if is_dask_collection(X):<br/>            from dask_ml.model_selection import train_test_split<br/>        else:<br/>            from sklearn.model_selection import train_test_split<br/>        ...</span></pre><p id="0518" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">现在，我们可以使用Dask读取(或生成)数据，并将支持Dask的估计器注入到我们的<code class="du ma mb mc lr b">CustomSearchCV</code>对象中。在这种情况下，我们注入<code class="du ma mb mc lr b">xgb.dask.DaskXGBClassifier</code>。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="6a64" class="lv kg hh lr b fi lw lx l ly lz">from sklearn.model_selection import RepeatedKFold<br/>import xgboost as xgb<br/><br/>from mylib import logger, make_classifier_data<br/><br/>X, y = make_classifier_data(<br/>    n_samples=100_000,<br/>    n_features=1000,<br/>    response_rate=0.25,<br/>    predictability=0.25,<br/>    random_state=123,<br/>    dask_ml=True,<br/>    chunks=1000<br/>)<br/><br/>cv = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)<br/>clf = CustomSearchCV(xgb.dask.DaskXGBClassifier(), cv, logger)<br/><br/>clf.fit(X, y)<br/>clf.best_estimator_</span></pre><h1 id="2351" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">使用GPU扩展</h1><h2 id="d266" class="lv kg hh bd kh mf mg mh kl mi mj mk kp jr ml mm kr jv mn mo kt jz mp mq kv mr bi translated">单个GPU</h2><p id="76ba" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">在对CustomSearchCV进行类似的修改后，我们可以通过用<code class="du ma mb mc lr b">tree_method="gpu_hist"</code>初始化<code class="du ma mb mc lr b">xgb.XGBClassifier</code>来在单个GPU上执行训练。在这种情况下，我们不需要修改数据读取(或生成)，因为XGBoost知道如何将数据移动到GPU上。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="cc35" class="lv kg hh lr b fi lw lx l ly lz">from sklearn.model_selection import RepeatedKFold<br/>from mylib import make_classifier_data, logger<br/>import xgboost as xgb<br/><br/>X, y = make_classifier_data(<br/>    n_samples=100_000,<br/>    n_features=1000,<br/>    response_rate=0.25,<br/>    predictability=0.25,<br/>    random_state=123<br/>)<br/>cv = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)<br/>xgb_clf = xgb.XGBClassifier(n_jobs=-1, tree_method="gpu_hist")<br/>clf = CustomSearchCV(xgb_clf, cv, logger)<br/>clf.fit(X, y)<br/>clf.best_estimator_</span></pre><h2 id="9b57" class="lv kg hh bd kh mf mg mh kl mi mj mk kp jr ml mm kr jv mn mo kt jz mp mq kv mr bi translated">单节点、多GPU</h2><p id="d293" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">许多系统都有多个GPU，可以使用Dask和RAPIDS组合成一个主机集群。下面，我们用<code class="du ma mb mc lr b">tree_method="gpu_hist"</code>初始化<code class="du ma mb mc lr b">xgb.dask.DaskXGBClassifier</code>，并将其连接到一个<code class="du ma mb mc lr b">dask_cuda.LocalCUDACluster</code>。默认情况下，<code class="du ma mb mc lr b">LocalCUDACluster</code>会为主机上的每个GPU添加一个cuda-worker (GPU worker)。如果我们在一个有八个GPU的系统上运行这段代码，我们将有一个有八个工作线程的集群。<a class="ae ke" href="https://www.nvidia.com/en-us/design-visualization/nvlink-bridges/" rel="noopener ugc nofollow" target="_blank"> NVLink </a>和<a class="ae ke" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a>允许在GPU之间进行极其高效的分布式数据访问。</p><p id="e122" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">此外，随着GPU内存的填满，数据将溢出到系统内存，这通常比GPU上可用的内存大得多。这使得单节点、多GPU计算非常适合许多数据科学和机器学习问题。下面的例子显示了没有修改<code class="du ma mb mc lr b">CustomSearchCV</code>类的用法。</p><pre class="ix iy iz ja fd lq lr ls lt aw lu bi"><span id="034e" class="lv kg hh lr b fi lw lx l ly lz">from sklearn.model_selection import RepeatedKFold<br/>from mylib import make_classifier_data, logger<br/><br/>import xgboost as xgb<br/><br/>cv = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)<br/>dclf = xgb.dask.DaskXGBClassifier(tree_method="gpu_hist")<br/>clf = CustomSearchCV(dclf, cv, logger)</span><span id="0761" class="lv kg hh lr b fi me lx l ly lz">from dask.distributed import Client<br/>from dask_cuda import LocalCUDACluster<br/><br/>cluster = LocalCUDACluster()<br/>client = Client(cluster)</span><span id="b2d9" class="lv kg hh lr b fi me lx l ly lz">X, y = make_classifier_data(<br/>    n_samples=100_000,<br/>    n_features=1000,<br/>    response_rate=0.25,<br/>    predictability=0.25,<br/>    random_state=123,<br/>    dask_ml=True,<br/>    chunks=1000<br/>)<br/>X = X.persist()<br/><br/>dclf.client = client<br/>clf.fit(X, y)<br/>clf.best_estimator_</span></pre><p id="4a1c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">运行这段代码时，检查Dask仪表板和GPU利用率。请注意，Dask集群正忙于工作，然后在GPU利用率达到峰值时暂停。这里我们在系统内存和CPU上分割Dask中的数据，然后在GPU上训练<a class="ae ke" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>模型。未来的改进是在GPU上执行训练分割。</p><p id="4c7f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><code class="du ma mb mc lr b">CustomSearchCV</code>估计器可以包含定制逻辑，用于可伸缩的超参数调优，或者执行一些修剪、正则化和早期停止技术，这些技术在之前关于<a class="ae ke" href="https://www.capitalone.com/tech/machine-learning/how-to-control-your-xgboost-model/" rel="noopener ugc nofollow" target="_blank">控制XGBoost模型</a>的帖子中已经讨论过。</p><h1 id="e08c" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">笔记</h1><p id="76d5" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">以下是一些需要考虑的额外注意事项:</p><ul class=""><li id="c900" class="lc ld hh jk b jl jm jo jp jr le jv lf jz lg kd lh li lj lk bi translated">在fit过程的早期对内部数据结构的类数组接口进行标准化，可以降低与Dask和RAPIDS的伸缩相关的复杂性。</li><li id="72b4" class="lc ld hh jk b jl ll jo lm jr ln jv lo jz lp kd lh li lj lk bi translated">确保从数据帧中提取的X值仅包含用于训练的特征，并将标签分离为1d数组或pd.Series。</li><li id="f3bb" class="lc ld hh jk b jl ll jo lm jr ln jv lo jz lp kd lh li lj lk bi translated">确保X值不包含用于分段的列，如日期。</li><li id="aa80" class="lc ld hh jk b jl ll jo lm jr ln jv lo jz lp kd lh li lj lk bi translated">尽量减少数据帧到数组的转换，以避免性能瓶颈。</li><li id="0570" class="lc ld hh jk b jl ll jo lm jr ln jv lo jz lp kd lh li lj lk bi translated">方法应该返回与输入类型匹配的集合。</li><li id="577c" class="lc ld hh jk b jl ll jo lm jr ln jv lo jz lp kd lh li lj lk bi translated">开发人员应该使用<code class="du ma mb mc lr b">sklearn.utils.estimator_checks</code>中的check_estimator助手函数来验证他们的定制估算器是否符合API。</li></ul><h1 id="5f2e" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">结论</h1><p id="9221" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">在本文中，我们讨论了向scikit-learn建模代码添加自定义功能的模式。我们发现，通过遵循scikit-learn API，我们可以最小化定制并将缩放逻辑封装在一个地方。随着时间的推移，这降低了维护成本，并为开发人员提供了如何将他们的代码集成到生态系统中的示例。遵循一个标准的API允许我们共享估算器，并组合它们来服务于许多用例。</p></div><div class="ab cl ms mt go mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ha hb hc hd he"><p id="83cc" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="md">原载于</em><a class="ae ke" href="https://www.capitalone.com/tech/machine-learning/custom-machine-learning-estimators-on-dask-and-rapids/" rel="noopener ugc nofollow" target="_blank"><em class="md">https://www.capitalone.com</em></a><em class="md">。</em></p><p id="1b71" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="md">披露声明:2021资本一。观点是作者个人的观点。除非本帖中另有说明，否则Capital One不隶属于所提及的任何公司，也不被这些公司认可。使用或展示的所有商标和其他知识产权是其各自所有者的财产。</em></p></div></div>    
</body>
</html>