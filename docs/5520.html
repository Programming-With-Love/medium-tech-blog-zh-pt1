<html>
<head>
<title>Yolo Object Detectors: Final Layers and Loss Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Yolo物体探测器:最终层和损失函数</h1>
<blockquote>原文：<a href="https://medium.com/oracledevs/final-layers-and-loss-functions-of-single-stage-detectors-part-1-4abbfa9aa71c?source=collection_archive---------0-----------------------#2018-11-10">https://medium.com/oracledevs/final-layers-and-loss-functions-of-single-stage-detectors-part-1-4abbfa9aa71c?source=collection_archive---------0-----------------------#2018-11-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="29e7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">1.1动机</h1><p id="05f9" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">大多数深层物体探测器由一个特征提取CNN(通常在Imagenet上预先训练，并针对探测进行微调)组成，该特征提取CNN连接到一个最终层，该层将特征整形为探测器特定的输出张量。切换CNN功能会导致速度和精度发生变化，如[1]、[2]、[3]所示。但是在许多情况下，从内存和计算能力的角度来看，从头开始训练一个Imagenet CNN是不切实际的。通常，我们使用开源的预建模型，调整最后的层和损失函数来完成我们的任务。一级对象检测器的损失函数(其中一个CNN产生边界框和类别预测)可能有些不寻常，因为预测张量用于构造真实张量。</p><p id="82a2" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">作为Oracle机器学习团队的一员，我们一直在阅读关于这种对象检测器的文献，并以我们喜欢的数学语言进行解释，此外，我们还创建了图表、伪代码和数学公式来解释作者的意思，但忽略了这些内容。这是我们在Oracle ML阅读小组上发表的演讲的书面材料。我们最初是用LaTex写的，但是已经把我们的图形和方程转换成图像，在媒体上发布。</p><h1 id="93d7" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">1.2物体检测和PascalVOC</h1><p id="2f90" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">给定一幅图像，对象检测器的任务是返回图像中我们关心的对象的边界框坐标和名称(类)。由于没有具体的输入很难谈论算法，我们以PascalVOC数据集[4]为例。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kf"><img src="../Images/73148ef6f5354bc7ec5dc0533a2412c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QHClDj6wJMNV8awaz4QlHQ.png"/></div></div></figure><p id="174a" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">如图<strong class="je hi">图1 </strong>所示，对于每一幅图像，PascalVOC都提供了一个注释文件，其中包含了20个类中的一个类的对象的边界框坐标。PascalVOC通过左上角的<em class="kr"> (x_min，y_min) </em>和右下角的<em class="kr"> (x_max，y_max) </em>角坐标对边界框进行编码，但是一些对象检测算法使用具有宽度和高度的中心xy坐标对框进行编码。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ks"><img src="../Images/851bb3dd987a0d416a535a8c1a151b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E_18qqHfepo8x8JkL9GpwA.png"/></div></div></figure><p id="737b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了将图像输入卷积神经网络，图像被调整大小为正方形。由于PascalVOC边界框坐标取决于图像宽度<strong class="je hi"> <em class="kr"> W </em> </strong>和高度<strong class="je hi"> <em class="kr"> H </em> </strong>，我们将框坐标归一化。由于PascalVOC提供的框编码不是编码边界框的唯一方式，角样式和中心样式编码的标准化如<strong class="je hi">等式1 </strong>所示。</p><p id="4f5b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">优化需要数字，所以每个边界的类名变成了整数<strong class="je hi"> 𝕔 ∈ [1，20] </strong>，因为在PascalVOC中有<strong class="je hi"> 20 </strong>个类。出于我们的目的，从一个类名称到<strong class="je hi"> 𝕔 </strong>，我们按照这个顺序在PascalVOC类列表中找到名称的索引:[飞机、自行车、鸟、船、瓶子、公共汽车、汽车、猫、椅子、奶牛、餐桌、狗、马、摩托车、人、盆栽、羊、沙发、火车、电视监视器]。</p><p id="46cb" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">因此，对于每个图像，标签是由一些标准化的<strong class="je hi"> 4 </strong>维边界框<strong class="je hi"> b </strong>和整数类id <strong class="je hi"> 𝕔 </strong>表示的对象列表。</p><h1 id="9aa6" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.1 Yolo v1</h1><p id="8580" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Yolo是首批深度单级探测器之一，自从第一篇论文在2016年CVPR发表以来，每年都会带来新的Yolo论文或技术报告。我们从Yolo v1 [1]开始，但由于我们主要对分析损失函数感兴趣，我们真正需要了解的是Yolo v1 CNN <strong class="je hi">(图2a) </strong>，它获取一个RGB图像(<strong class="je hi"> 448×448×3 </strong>)并返回一个立方体(<strong class="je hi"> 7×7×30 </strong>)，在<strong class="je hi">(图2b) </strong>中解释。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kt"><img src="../Images/a42684f3c3dc8b690dcec1cfd0d969fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fahR8jDZxKqArfYRPCnDjw.png"/></div></div></figure><h1 id="74e2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.2 Yolo v1包围盒编码</h1><p id="d3c8" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">为了开始理解对7×7×30 输出的解释，我们需要构造Yolo样式的标签。回想一下，一个图像的PascalVOC标签是一个对象列表，每个对象由一个边界框和一个分类来表示。现在的目标是将一幅图像的PascalVOC标签转换成与张量Yolo输出的<strong class="je hi"> 7×7×30 </strong>等价的形式。首先，我们需要从中心标准化的PascalVOC包围盒编码转换到Yolo包围盒编码。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ku"><img src="../Images/dc8dfc90c23020988586c5f65cfd7826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q9bCuUkKTTsd_VXKAmsFow.png"/></div></div></figure><p id="b0ec" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Yolo没有直接预测宽度和高度，而是预测平方根，以说明小数字的偏差比大数字的偏差更大。平方根映射用于扩展较小的数字，例如，<strong class="je hi"> [0，0.25] </strong>中的任何数字都会映射到<strong class="je hi"> [0，0.5] </strong>。</p><p id="ab5d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Yolo预测相对于<strong class="je hi"> 7×7 </strong>网格中的单元格的xy偏移，而不是预测由图像的宽度和高度标准化的边界框的中心。一旦图像被分成一个<strong class="je hi"> 7×7 </strong>网格，对于每个对象，我们定位包含对象中心的网格单元<strong class="je hi"> ( <em class="kr"> g </em> x，gy) </strong>。将预测对象的“职责”分配给网格单元后，我们将边界框的中心描述为从单元的偏移，如图<strong class="je hi">图3 </strong>所示，从而完成Yolo风格边界框的构建。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kv"><img src="../Images/cff240ac01bca2fa853ce81df0914c05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ikdqAz4CFBOlOcPjpwd9w.png"/></div></div></figure><h1 id="e703" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.3通过最大IoU将真实框分配给预测框</h1><p id="66df" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">将对象分配到网格单元后，我们现在可以构造真值向量<strong class="je hi"> y_(gx，gy)<em class="kr">∈</em>【0，1】⁰</strong>，这需要预测<strong class="je hi"> <em class="kr"> ŷ </em> _(gx，gy) </strong>位于从Yolo CNN输出的<strong class="je hi"> 7×7×30 </strong>张量中的网格单元。如<strong class="je hi">图2 </strong>所示，每个网格单元预测两个边界框，它们具有各自的对象存在概率<strong class="je hi"><em class="kr">【P(Object)】</em></strong>和一个类别概率分布，因此每个单元仅预测一个对象，并且在预测时，我们选择具有最高值<strong class="je hi"><em class="kr">【P(Object)</em></strong>的边界框，这是该框包含对象的概率。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kw"><img src="../Images/05cde683f4d4a28a41410bb1318aed20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*49teGcnS_uFEKTm57NPdEg.png"/></div></div></figure><p id="c830" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">为了解释得更清楚，我们将<strong class="je hi"> <em class="kr"> b </em> </strong>表示为真实物体包围盒，将<strong class="je hi"><em class="kr">b̂₁</em></strong><em class="kr"/>和<strong class="je hi"> <em class="kr"> b̂₂ </em> </strong>表示为预测包围盒，它们都是<strong class="je hi">等式2 </strong>中描述的Yolo编码样式。我们用对象类<strong class="je hi"> 𝕔 </strong>来构造真类概率向量<strong class="je hi"><em class="kr">p</em></strong><strong class="je hi"><em class="kr">∈</em></strong><strong class="je hi">【0，1】<em class="kr"/>⁰</strong>，其中除了在索引处<strong class="je hi"> 𝕔 </strong>外所有元素都为零，所以<strong class="je hi"><em class="kr">p</em></strong><strong class="je hi">𝕔</strong>= 1。我们定义<strong class="je hi"> <em class="kr"> p̂ </em> </strong>为预测类概率向量。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kx"><img src="../Images/bc6229751c90278c9be0b00873af2551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uJlDnR1b9QuDxX4XMzvw2g.png"/></div></div></figure><p id="23ff" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们分别用<strong class="je hi"> <em class="kr"> 𝕔̂₁ </em> </strong>和<strong class="je hi"> <em class="kr"> 𝕔̂₂ </em> </strong>来表示盒子1和盒子2包含一个对象的“置信度”(<strong class="je hi"> <em class="kr"> P(Object) </em> </strong>表示各自的盒子)。我们将<strong class="je hi"> <em class="kr"> b </em> </strong>分配给box1或box2中的一个，基于哪个预测的边界盒在并集上具有最高的交集，也称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank"> Jaccard索引</a>，其中<strong class="je hi"> <em class="kr"> b </em> </strong>。作为参考，我们在<strong class="je hi">算法1 </strong>中定义了计算两个矩形的IoU的程序。我们将c设置为最大IoU，有效地将IoU用作将对象分配给预测框的置信度的代理。这个过程产生真值向量<strong class="je hi"> y_(gx，gy) </strong>，图4 中描述了一个例子。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kz"><img src="../Images/38e44a23c8c902f8e01f7af848713bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jnchF46cYx9Dkmx7cw5YgA.png"/></div></div></figure><h1 id="081a" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.4 Yolo v1损失函数</h1><p id="5518" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">在使用编码为(<strong class="je hi"> b </strong>，<strong class="je hi"> 𝕔 </strong>)的对象和来自网格单元<strong class="je hi"> (gx，gy) </strong>的预测来构造<strong class="je hi"> y_(gx，gy) </strong>之后，我们现在可以公式化负责预测对象的网格单元的损失<strong class="je hi"> <em class="kr"> L </em> _(gx，gy) </strong>。尽管对于分类问题不是最佳的，Yolo v1损失基本上是加权线性回归。</p><p id="4fba" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在[1]之后，我们将包围盒坐标<strong class="je hi"><em class="kr">【b̂】</em></strong>上的权重表示为<strong class="je hi"> <em class="kr"> λ </em> _coord </strong>，其在[1]中被设置为<strong class="je hi"> 5 </strong>，当相应的盒子不包含对象时，<strong class="je hi"><em class="kr"/></strong><strong class="je hi"><em class="kr">ĉ₂</em></strong>上的权重表示为<strong class="je hi"> <em class="kr"> λ </em> _noobj为了使方程更简单，让<strong class="je hi"><em class="kr">λ_ coord</em></strong>为<strong class="je hi"> <em class="kr"> 4×4 </em> </strong>矩阵，除了<strong class="je hi"> <em class="kr"> λ </em> _coord </strong>在对角线上重复外，其余全为零。然后，我们可以为图3 </strong>中<strong class="je hi">的网格单元构建Yolo v1损失，将对象分配给图4 </strong>中<strong class="je hi">的框1。</strong></p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es la"><img src="../Images/e670678ca87f86ac0c86963f1a863a6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TOWe0RyO1cP-Q4ohOeWX2Q.png"/></div></div></figure><p id="c60d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">当网格单元没有指定对象时，我们只有两个边界框没有对象丢失。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lb"><img src="../Images/acae40c33c85b6d2e634395b3621662c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yntjAkzFL43D76yl42N-PA.png"/></div></div></figure><p id="e221" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">利用为所有情况的网格单元定义的<strong class="je hi"> <em class="kr"> L </em> _(gx，gy) </strong>，为了得到整个图像的损失<strong class="je hi"> <em class="kr"> L </em> </strong>，我们对所有网格单元的损失求和。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lc"><img src="../Images/3dab90cfeb327eb92981d039073b4c10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRMMN8zdiBPYFAVjqGgX7Q.png"/></div></div></figure><p id="f24e" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们可以看到，这相当于[1]中给出的一幅图像的损耗公式，如图1所示。</p><h1 id="f540" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">2.4 Yolo v2最终层和损失函数</h1><p id="a9a2" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Yolo v2 [2]中最后一层和损失函数的主要变化是引入了“先验框”和每个网格单元的多对象预测。</p><p id="e96b" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Yolo v2先验盒的灵感来自于更快的RCNN [6](一种多级深度对象探测器)中使用的锚盒，但使用了不同的锚盒编码，这可能就是为什么[2]称它们为先验盒的原因。先验框是[2]通过对来自PascalVOC(和COCO)数据集的所有真值边界框运行k-means聚类而选择的宽度和高度。Yolo v2不是直接预测边界框的宽度和高度，而是预测相对于前一个框的宽度和高度偏移。每个边界框预测的中心坐标保持与Yolo v1中的相同。Yolo v2有<strong class="je hi"> 5 </strong>的先验，但这使得创建图表和符号很痛苦，所以我们在讨论中将其限制在<strong class="je hi"> 2 </strong>，出于同样的原因，我们还使用了<strong class="je hi"> 7×7 </strong>网格。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ld"><img src="../Images/24012978ee375594f93d154ca4667c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOmaacGXPWnKVZ3CW6w81Q.png"/></div></div></figure><p id="b2c8" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">在Yolo v1中，最后一层中的每个网格单元只能预测一个对象，因为虽然每个网格单元让我们在两个边界框之间进行选择，但我们只有一个类概率向量。在Yolo v2中，每个网格单元预测一个边界框和每个先前框的类概率向量。假设我们有两个先验框，那么<strong class="je hi"> <em class="kr"> ŷ </em> _(gx，gy) </strong>将是一个<strong class="je hi"> 50 </strong>维向量，因为对于每个先验，我们预测<strong class="je hi"> 25 </strong>个数字:框包含一个对象的概率，四个数字表示相对于前一个框的边界框坐标，以及PascalVOC的<strong class="je hi"> 20 </strong>维概率向量。</p><p id="371d" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Yolo v2损失函数在[2]中没有明确描述，但我们可以从Yolo v1损失函数中进行推断。虽然现在每个网格单元有多个对象预测，但Yolo v2仍然对预测的边界框执行真值的max-IOU匹配。预计包围盒坐标损失仍然是权重线性回归损失。然而，Yolo v3技术报告提到使用二元交叉熵损失进行分类预测，Yolo v2提到分类损失，我们推断这意味着不是回归损失，因此Yolo v2可能使用二元交叉熵。由于没有对象分配给网格单元时的损失与<strong class="je hi">等式5 </strong>中的损失相同，我们仅在<strong class="je hi">图3 </strong>中显示网格单元的Yolo v2损失，其中<strong class="je hi"><em class="kr"/></strong>与<strong class="je hi">等式4 </strong>中的损失相同，除了新的框编码。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es le"><img src="../Images/40904c9452e45b19699fa74b810f768f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ID8MEj4m98ed7n_SjDRE6g.png"/></div></div></figure><h1 id="73f2" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">Yolo v3最终层</h1><p id="f8af" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">Yolo v3技术报告的主要变化是最后一层，这是受特征金字塔网络(FPNs) [7]的启发。Yolo v3 [8]最终层由三个检测张量组成，每个检测张量都有自己的先验框，并且每个检测张量的分辨率都是前一个的两倍，例如，如果每个检测张量都有两个先验框，并且数据集是PascalVOC，那么第一个可能的张量大小是<strong class="je hi"> 7×7×50 </strong>，这意味着第二个是<strong class="je hi"> 14×14×50 </strong>，第三个是<strong class="je hi">28×28×50</strong>——这也是作者称之为“跨尺度预测”的部分原因。每个检测张量都像Yolo v2最终层一样组织，因此网格单元和对象分配的概念适用，虽然没有明确提到，但我们可以推断，像所有以前的Yolo化身一样，每个对象仍然只分配给一个检测张量中的一个网格单元。</p><p id="4e84" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">虽然关于如何从特征提取器到三个检测张量的细节有点缺乏，但我们提出了我们对[8]第2.3节的解释的图表，通过这个图表，我们可以看到Yolo作者称之为“跨尺度预测”的另一个原因。Yolo v3将特征提取器网络中的早期层与后期层(额外的CNN层)合并，这本质上就是fpn所做的。直观上，在高分辨率早期层中比在显著的二次采样的低分辨率后期层中更容易检测到小对象，但是CNN的早期层包含语义弱的特征，因此fpn不是直接使用它们，而是将它们与包含语义强的特征的上采样的后期层合并。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lf"><img src="../Images/e49c5183f79480bfb2e4ed4a52b9c10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ge79qAXIP2E7xQGPwpvF4A.png"/></div></div></figure><p id="3122" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">我们可以从图6 中看到，Yolo v3与FPNs不同，它使用级联而不是求和来合并层，虽然没有提到，但Yolo v3可能采用与FPN相同的方式进行上采样(使用最近邻)。此外，Yolo v3的结构与[7]中的FPN不完全相同，因为Yolo v3不使用先前合并的结果来产生下一个检测张量。为了更清楚地说明差异，我们制作了一个图表，解释如果Yolo v3更接近FPN结构，它会是什么样子。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lg"><img src="../Images/06ee56caf6a2ffcee12aacfd33c717f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9rpvRARq8_D1MbkaeOiblg.png"/></div></div></figure><p id="37c5" class="pw-post-body-paragraph jc jd hh je b jf ka jh ji jj kb jl jm jn kc jp jq jr kd jt ju jv ke jx jy jz ha bi translated">Yolo v3的论文报告了对损失函数的实验，例如使用聚焦损失[9]，当与单次检测器[10](像Yolo这样的一级检测器)和FPN结合时，产生了称为RetinaNet的快速准确的检测器[9]。然而，这些实验都没有改善检测，因此损失基本上与Yolo v2相同。</p><h1 id="47c5" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">参考</h1><ol class=""><li id="c7a6" class="lh li hh je b jf jg jj jk jn lj jr lk jv ll jz lm ln lo lp bi translated">约瑟夫·雷德蒙、桑托什·库马尔·迪夫瓦拉、罗斯·b·格希克和阿里·法尔哈迪。你只看一次:统一的，实时的物体检测。更正，abs/1506.02640，2015年。</li><li id="1c08" class="lh li hh je b jf lq jj lr jn ls jr lt jv lu jz lm ln lo lp bi translated">约瑟夫·雷蒙德和阿里·法尔哈迪。YOLO9000:更好、更快、更强。更正，abs/1612.08242，2016。</li><li id="203f" class="lh li hh je b jf lq jj lr jn ls jr lt jv lu jz lm ln lo lp bi translated">Jonathan Huang、Vivek Rathod、、朱梦龙、Anoop Korattikara、Alireza Fathi、Ian Fis- cher、Zbigniew Wojna、、Sergio和。现代卷积目标探测器的速度/精度权衡更正，abs/1611.10012，2016。</li><li id="07db" class="lh li hh je b jf lq jj lr jn ls jr lt jv lu jz lm ln lo lp bi translated">马克·埃弗林汉姆，吕克·范·古尔，C. K. I .威廉姆斯，j .温，安德鲁·齐泽曼。2010年pascal视觉对象类(voc)挑战赛。</li><li id="e394" class="lh li hh je b jf lq jj lr jn ls jr lt jv lu jz lm ln lo lp bi translated">约瑟夫·雷德蒙。YOLO CVPR 2016演讲和幻灯片。谷歌幻灯片:<a class="ae ky" href="https://docs.google.com/ presentation/d/1kAa7NOamBt4calBU9iHgT8a86RRHz9Yz2oh4-GTdX6M/edit#slide=id.p" rel="noopener ugc nofollow" target="_blank">https://docs.google.com/演示文稿/d/1 ka a7 no ambt 4 cal bu 9 ihgt 8 a 86 rrhz 9 yz 2 oh 4-gtdx 6m/edit # slide = id . p</a>，Youtube:<a class="ae ky" href="https://www.youtube.com/watch?v=NM6lrxy0bxs" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=NM6lrxy0bxs</a>。访问时间:2018–10–12。</li><li id="72ab" class="lh li hh je b jf lq jj lr jn ls jr lt jv lu jz lm ln lo lp bi translated">邵青·任、明凯·何、罗斯·吉斯克和孙健。更快的R-CNN:用区域建议网络实现实时目标检测。在神经信息处理系统(NIPS)中，2015。</li><li id="62d5" class="lh li hh je b jf lq jj lr jn ls jr lt jv lu jz lm ln lo lp bi translated">宗、彼得·多勒、罗斯·吉希克、明凯·何、巴拉思·哈里哈兰和塞尔日·贝隆吉。用于目标检测的特征金字塔网络。2017年在CVPR。</li><li id="62d8" class="lh li hh je b jf lq jj lr jn ls jr lt jv lu jz lm ln lo lp bi translated">约瑟夫·雷蒙德和阿里·法尔哈迪。Yolov3:增量改进。CoRR，abs/1804.02767，2018。</li><li id="9703" class="lh li hh je b jf lq jj lr jn ls jr lt jv lu jz lm ln lo lp bi translated">宗-林逸、普里亚·戈亚尔、罗斯·格希克、明凯·何和彼得·多尔。密集物体探测的聚焦损失。《计算机视觉国际会议论文集》(ICCV)，2017年</li><li id="3d95" class="lh li hh je b jf lq jj lr jn ls jr lt jv lu jz lm ln lo lp bi translated">刘威、Dragomir Anguelov、Dumitru Erhan、Christian Szegedy、Scott E. Reed、傅成阳和Alexander C. Berg。SSD:单次多盒探测器。更正，abs/1512.02325，2015年。</li></ol></div></div>    
</body>
</html>