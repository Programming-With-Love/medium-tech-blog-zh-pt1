# Apache Spark —沃尔玛数据的瑞士军刀

> 原文：<https://medium.com/walmartglobaltech/apache-spark-the-swiss-army-knife-for-data-at-walmart-3c1950da74ea?source=collection_archive---------4----------------------->

![](img/d97baa6da8a0e1628b6640dbc5350755.png)

Pic Credit: [BHPhotoVideo](https://www.bhphotovideo.com/images/images2500x2500/victorinox_53501_swisschamp_pocket_knife_91mm_1149550.jpg)

我们的业务需求很简单:接收、存储、分析数据，并将其作为有意义的信息发布(要么作为报告，要么作为 Kafka 的下游，等等)。)给客户。在沃尔玛，我们有一个新的行话:数据！我很确定这是整个行业的趋势。我们需要一个经济高效的解决方案，它不仅可以扩展到我们正在处理的数据的大小，而且足够灵活，能够以最小的工作量插入额外的数据源或数据接收器。

复杂性在于这样一个事实，即在过去的几年中，微服务架构出现了爆炸式增长。每个团队都可以自由地为他们的微服务选择任何数据库。这导致结合这些数据进行分析的复杂性增加。我们研究了多种选择，从 ETL 和报告工具，到像 Clickhouse、Druid 等开源数据库。虽然它们每个都有自己的优点，但没有一个完全符合我们的用例。进入阿帕奇火花！很快我们发现它可以用于我们所有的三个用例(存储、分析和发布)。它非常灵活，因为它是一个非常通用的框架，可以插入大多数数据源和数据存储，并且它还包含一个相当全面的 API 来分析这些数据。因此，我们开始在 Spark 上开发我们的解决方案，并很快意识到它可以成为我们几乎任何“数据”的首选框架！

这个博客不是教程，也不是对 Apache Spark 的深入研究，互联网上有足够的文档和培训。这是为了分享我们在沃尔玛使用 Spark 的各种使用案例的信息。

![](img/738e8bd9f2ff209e9c43daedd090a70a.png)

从高层次来看，我们的 Spark 用例分为以下几类:

*   **流至存储**
    作为我们审计平台的一部分，我们从各种系统获取审计事件，并将其存储在 HDFS(作为 Parquet 或 CSV)。然后，我们将这些数据用于分析、报告等。
*   **流对流**
    我们从各种系统接收变化事件(主要通过卡夫卡)。我们将这些数据与其他数据源(集群内部的和外部的)结合起来，对其进行分析，并将解决的事件发布给其他下游消费者。例如，当一个项目属性发生变化时，比如价格，我们会收到通知，然后我们会将这种变化与其他项目数据(比如成本、描述等)结合起来。并将其发布到商店系统消费的 Kafka 主题。
*   **存储到流**
    我们有一个 Spark 作业，它将大量 DB2 和 Teradata 表(大约数十亿行)加载到一个 dataframe 中，执行计算和聚合，然后在 Kafka 上发布这些信息。
*   **数据分析**
    我们运行 Spark 作业来分析存储在 HDFS/Hive 中的数据并生成报告。事实证明，Spark SQL 比 Hive QL 快一个数量级，我们能够在几秒钟而不是几分钟内生成报告。由于 Spark 原生支持 Openstack Swift，我们将 csv 报告上传到存储云，企业客户可以轻松下载。我们使用一个名为 [spark-jobserver](https://github.com/spark-jobserver/spark-jobserver) 的令人敬畏的框架，它提供了提交和管理 spark 作业的 API。
*   **数据迁移**
    我们有各种各样的数据库，有时需要在它们之间传输数据，这可能是退役项目或现代化项目的结果。例如 DB2 到 Cassandra，DB2 到 Maria DB，DB2 到 Teradata，Cassandra 到 Solr，等等。Spark 连接如此大量数据库的灵活性及其通用特性帮助我们在短时间内迁移了大量数据。

需要注意的一点是，在沃尔玛的团队中，我们在现有的 Hadoop 集群上部署了 Apache Spark，原因如下:

1.  沃尔玛已经投资建设大规模的 Hadoop 集群。
2.  我们有超过 2000TB 的 HDFS 存储可用。
3.  我们有成千上万的核心可以支配，Spark 在这个集群上的扩展非常漂亮。

**总结**
在处理数据方面，Apache Spark 最终成为了我们的瑞士军刀。Spark 是一个简单、可扩展的通用框架，可以处理几乎所有的事情。我相信我们只是触及了 Spark 的皮毛，我们正在进入一个全新的数据分析和数据科学世界，我们也将使用 Spark。我不认为在不久的将来会远离它！