<html>
<head>
<title>Fundamentals of MapReduce with MapReduce Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MapReduce基础与MapReduce示例</h1>
<blockquote>原文：<a href="https://medium.com/edureka/mapreduce-tutorial-3d9535ddbe7c?source=collection_archive---------0-----------------------#2016-11-15">https://medium.com/edureka/mapreduce-tutorial-3d9535ddbe7c?source=collection_archive---------0-----------------------#2016-11-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/d188d7084a975dd860907690f434250d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*_HFPySvbaF2afneymUT4Nw.png"/></div><figcaption class="il im et er es in io bd b be z dx">MapReduce Tutorial - Edureka</figcaption></figure><p id="253b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇MapReduce教程博客中，我将向您介绍MapReduce，它是<strong class="ir hi"> <em class="jn"> Hadoop框架</em> </strong>中处理的核心构建块之一。在继续之前，我建议你熟悉一下HDFS的概念，我在之前的<em class="jn"> HDFS教程</em>博客中已经提到过。这将有助于您快速轻松地理解MapReduce概念。</p><p id="2428" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Google在2004年12月发布了一篇关于MapReduce技术的论文。这成为了Hadoop处理模型的起源。因此，MapReduce是一种编程模型，它允许我们对巨大的数据集执行并行和分布式处理。我在这篇MapReduce教程博客中涉及的主题如下:</p><ul class=""><li id="b846" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">并行和分布式处理的传统方式</li><li id="1619" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">什么是MapReduce？</li><li id="fc7e" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">MapReduce示例</li><li id="b060" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">MapReduce的优势</li><li id="ff43" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">MapReduce程序</li><li id="aae3" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">MapReduce程序解释</li></ul><h1 id="a065" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">传统方式</h1><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es la"><img src="../Images/78099a7e8f0004fa06f75441a955dd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T71t3ztQ9xESx-x-A2zG_g.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Traditional Way - MapReduce Tutorial</figcaption></figure><p id="a674" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们理解一下，当MapReduce框架还没有出现的时候，并行和分布式处理是如何以传统的方式发生的。让我们举一个例子，我有一个包含2000年到2015年每日平均温度的天气日志。在这里，我想计算每年温度最高的一天。</p><p id="df34" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">所以，就像传统的方式一样，我会将数据分割成更小的部分或块，并将它们存储在不同的机器中。然后，我会找到存储在相应机器中的每个零件的最高温度。最后，我将组合从每台机器收到的结果，得到最终输出。让我们看看与这种传统方法相关的挑战:</p><ol class=""><li id="6ab3" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lj ju jv jw bi translated"><strong class="ir hi">关键路径问题:</strong>它是在不延误下一个里程碑或实际完工日期的情况下，完成工作所花费的时间量。因此，如果任何一台机器延误了工作，整个工作都会延误。</li><li id="b692" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated"><strong class="ir hi">可靠性问题:</strong>如果任何一台处理部分数据的机器出现故障怎么办？这种故障转移的管理成为一项挑战。</li><li id="cde4" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated"><strong class="ir hi">平均分割问题:</strong>我如何将数据分割成更小的块，以便每台机器都可以处理一部分数据。换句话说，如何平均分配数据，使每台机器都不会过载或利用不足。</li><li id="ce3a" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated"><strong class="ir hi">单次分割可能失败:</strong>如果任何一台机器无法提供输出，我将无法计算结果。因此，应该有一种机制来确保系统的这种容错能力。</li><li id="cb90" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated"><strong class="ir hi">聚合结果:</strong>应该有一个机制来聚合每台机器生成的结果，以产生最终输出。</li></ol><p id="0579" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这些都是我在使用传统方法执行大型数据集的并行处理时必须单独注意的问题。</p><p id="5d97" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了克服这些问题，我们有了MapReduce框架，它允许我们执行这样的并行计算，而不必担心可靠性、容错等问题。因此，MapReduce为您提供了编写代码逻辑的灵活性，而无需关心系统的设计问题。</p><h1 id="19a2" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">什么是MapReduce？</h1><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lk"><img src="../Images/75d45b7780c0875f4cb3d3a31ba296bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qSswbjBsU4ciOG1iAVRXfw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">What is MapReduce - MapReduce Tutorial</figcaption></figure><p id="22f3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">MapReduce是一个编程框架，它允许我们在分布式环境中对大型数据集执行分布式和并行处理。</p><ul class=""><li id="d9de" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">MapReduce由两个不同的任务组成—映射和缩减。</li><li id="2252" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">顾名思义，MapReduce阶段发生在mapper阶段完成之后。</li><li id="92a3" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">因此，第一个是映射作业，读取并处理数据块以产生键值对作为中间输出。</li><li id="605a" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">映射器或映射作业(键值对)的输出被输入到缩减器。</li><li id="d229" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">reducer从多个地图作业接收键值对。</li><li id="a043" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">然后，reducer将这些中间数据元组(中间键-值对)聚集成一个更小的元组或键-值对集，这是最终的输出。</li></ul><h1 id="9037" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">MapReduce的一个字数统计例子</h1><p id="5ad2" class="pw-post-body-paragraph ip iq hh ir b is ll iu iv iw lm iy iz ja ln jc jd je lo jg jh ji lp jk jl jm ha bi translated">让我们通过一个例子来理解MapReduce是如何工作的，我有一个名为example.txt的文本文件，其内容如下:</p><p id="b464" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">亲爱的，熊，河，汽车，汽车，河，鹿，汽车和熊</p><p id="d455" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，假设我们必须使用MapReduce对sample.txt进行字数统计。因此，我们将找到独特的词，以及这些独特的词出现的次数。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lk"><img src="../Images/6633aeab058b19e01a85af802e329773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHQOWVAZKjU82sKZkxaKhQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">MapReduce Example - MapReduce Tutorial</figcaption></figure><ul class=""><li id="a0dc" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">首先，我们将输入分成三部分，如图所示。这将在所有地图节点之间分配工作。</li><li id="529f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">然后，我们对每个映射器中的单词进行标记，并给每个标记或单词一个硬编码值(1)。硬编码值等于1的基本原理是，每个单词本身都会出现一次。</li><li id="0c5b" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">现在，将创建一个键-值对列表，其中键只是单个单词，值是1。因此，对于第一行(亲爱的Bear River ),我们有3个键值对——亲爱的，1；熊，1；河，1。所有节点上的映射过程保持不变。</li><li id="0e0a" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">在映射阶段之后，进行一个分区过程，在该过程中进行排序和改组，以便将具有相同关键字的所有元组发送到相应的缩减器。</li><li id="6631" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">因此，在排序和洗牌阶段之后，每个reducer将拥有一个惟一的键和一个与该键对应的值列表。比如熊，[1，1]；汽车，[1，1，1]..等。</li><li id="db8d" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">现在，每个归约器对该值列表中的值进行计数。如图所示，reducer得到一个值列表，对于key Bear是[1，1]。然后，它计算列表中1的数量，并给出最终输出——Bear，2。</li><li id="1208" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">最后，收集所有的输出键/值对，并写入输出文件。</li></ul><h1 id="4908" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">MapReduce的优势</h1><p id="7ceb" class="pw-post-body-paragraph ip iq hh ir b is ll iu iv iw lm iy iz ja ln jc jd je lo jg jh ji lp jk jl jm ha bi translated">MapReduce的两个最大优势是:</p><h2 id="b521" class="lq kd hh bd ke lr ls lt ki lu lv lw km ja lx ly kq je lz ma ku ji mb mc ky md bi translated">1.并行处理:</h2><p id="ac6e" class="pw-post-body-paragraph ip iq hh ir b is ll iu iv iw lm iy iz ja ln jc jd je lo jg jh ji lp jk jl jm ha bi translated">在MapReduce中，我们在多个节点之间划分作业，每个节点同时处理作业的一部分。因此，MapReduce基于分而治之的范式，帮助我们使用不同的机器处理数据。由于数据是由多台机器而不是单台机器并行处理的，处理数据所需的时间大大减少，如下图(2)所示。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es me"><img src="../Images/4537dad8f22b0ae458cf218f4dee1d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ARNbPmjZvHfwr1Mf311gjg.png"/></div></div><figcaption class="il im et er es in io bd b be z dx">Traditional Way Vs. MapReduce Way - MapReduce Tutorial</figcaption></figure><h2 id="145d" class="lq kd hh bd ke lr ls lt ki lu lv lw km ja lx ly kq je lz ma ku ji mb mc ky md bi translated">2.数据位置:</h2><p id="f48e" class="pw-post-body-paragraph ip iq hh ir b is ll iu iv iw lm iy iz ja ln jc jd je lo jg jh ji lp jk jl jm ha bi translated">我们不是将数据移动到处理单元，而是将处理单元移动到MapReduce框架中的数据。在传统系统中，我们通常将数据带到处理单元并对其进行处理。但是，随着数据的增长和变得非常庞大，将如此大量的数据带到处理单元会带来以下问题:</p><ul class=""><li id="04f1" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">移动大量数据进行处理成本高昂，并且会降低网络性能。</li><li id="e372" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">处理需要时间，因为数据由单个单元处理，这成为了瓶颈。</li><li id="bf54" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">主节点可能负担过重并可能出现故障。</li></ul><p id="5750" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，MapReduce允许我们通过将处理单元引入数据来克服上述问题。因此，正如您在上图中看到的，数据分布在多个节点中，每个节点处理驻留在其上的部分数据。这使我们具有以下优势:</p><ul class=""><li id="f63c" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">将处理单元移至数据非常划算。</li><li id="aa89" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">由于所有节点都在并行处理它们的数据部分，因此处理时间减少了。</li><li id="870e" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">每个节点都有一部分数据要处理，因此不会出现节点过载的情况。</li></ul><h1 id="be1d" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">MapReduce示例程序</h1><p id="9828" class="pw-post-body-paragraph ip iq hh ir b is ll iu iv iw lm iy iz ja ln jc jd je lo jg jh ji lp jk jl jm ha bi translated">在进入细节之前，让我们看一下MapReduce示例程序，对MapReduce环境中的实际工作有一个基本的概念。我举了同一个单词计数的例子，我必须找出每个单词出现的次数。不要担心伙计们，如果你们第一次看的时候不理解代码，请耐心听我讲解MapReduce代码的每一部分。</p><h2 id="dbd1" class="lq kd hh bd ke lr ls lt ki lu lv lw km ja lx ly kq je lz ma ku ji mb mc ky md bi translated">源代码:</h2><pre class="lb lc ld le fd mf mg mh mi aw mj bi"><span id="5823" class="lq kd hh mg b fi mk ml l mm mn">package co.edureka.mapreduce;<br/>import java.io.IOException;<br/>import java.util.StringTokenizer;<br/>import org.apache.hadoop.io.IntWritable;<br/>import org.apache.hadoop.io.LongWritable;<br/>import org.apache.hadoop.io.Text;<br/>import org.apache.hadoop.mapreduce.Mapper;<br/>import org.apache.hadoop.mapreduce.Reducer;<br/>import org.apache.hadoop.conf.Configuration;<br/>import org.apache.hadoop.mapreduce.Job;<br/>import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;<br/>import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;<br/>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br/>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br/>import org.apache.hadoop.fs.Path;<br/> <br/>public class WordCount<br/>{<br/>public static class Map extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; {<br/>public void map(LongWritable key, Text value,Context context) throws IOException,InterruptedException{<br/>String line = value.toString();<br/>StringTokenizer tokenizer = new StringTokenizer(line);<br/>while (tokenizer.hasMoreTokens()) {<br/>value.set(tokenizer.nextToken());<br/>context.write(value, new IntWritable(1));<br/>}<br/>}<br/>}<br/> <br/>public static class Reduce extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {<br/>public void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException,InterruptedException {<br/>int sum=0;<br/>for(IntWritable x: values)<br/>{<br/>sum+=x.get();<br/>}<br/>context.write(key, new IntWritable(sum));<br/>}<br/>}<br/> <br/>public static void main(String[] args) throws Exception {<br/> <br/>Configuration conf= new Configuration();<br/>Job job = new Job(conf,"My Word Count Program");<br/>job.setJarByClass(WordCount.class);<br/>job.setMapperClass(Map.class);<br/>job.setReducerClass(Reduce.class);<br/>job.setOutputKeyClass(Text.class);<br/>job.setOutputValueClass(IntWritable.class);<br/>job.setInputFormatClass(TextInputFormat.class);<br/>job.setOutputFormatClass(TextOutputFormat.class);<br/>Path outputPath = new Path(args[1]);<br/>//Configuring the input/output path from the filesystem into the job<br/>FileInputFormat.addInputPath(job, new Path(args[0]));<br/>FileOutputFormat.setOutputPath(job, new Path(args[1]));<br/>//deleting the output path automatically from hdfs so that we don't have to delete it explicitly<br/>outputPath.getFileSystem(conf).delete(outputPath);<br/>//exiting the job only if the flag value becomes false<br/>System.exit(job.waitForCompletion(true) ? 0 : 1);<br/>}<br/>}</span></pre><h1 id="5cbe" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">MapReduce程序的解释</h1><p id="99c2" class="pw-post-body-paragraph ip iq hh ir b is ll iu iv iw lm iy iz ja ln jc jd je lo jg jh ji lp jk jl jm ha bi translated">整个MapReduce程序基本上可以分为三个部分:</p><ul class=""><li id="1e1d" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">映射器相位代码</li><li id="2756" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">减速器相位代码</li><li id="2bd1" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">驱动程序代码</li></ul><p id="bb15" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将依次理解这三个部分的代码。</p><h2 id="5970" class="lq kd hh bd ke lr ls lt ki lu lv lw km ja lx ly kq je lz ma ku ji mb mc ky md bi translated">映射器代码:</h2><pre class="lb lc ld le fd mf mg mh mi aw mj bi"><span id="0769" class="lq kd hh mg b fi mk ml l mm mn">public static class Map extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt; {<br/> <br/>public void map(LongWritable key, Text value, Context context) throws IOException,InterruptedException {<br/> <br/>String line = value.toString();<br/>StringTokenizer tokenizer = new StringTokenizer(line);<br/>while (tokenizer.hasMoreTokens()) {<br/>value.set(tokenizer.nextToken());<br/>context.write(value, new IntWritable(1));<br/>}</span></pre><ul class=""><li id="a5a4" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">我们已经创建了一个类别映射，它扩展了MapReduce框架中已经定义的类别映射器。</li><li id="ca3a" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">我们使用尖括号在类声明之后定义输入和输出键/值对的数据类型。</li></ul><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mo"><img src="../Images/fcad91acfbf8d0d3d0623ac5c89ad2e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*Dy7p8lVfmMOKMgMNvlKHGQ.png"/></div></figure><ul class=""><li id="0a05" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">映射器的输入和输出都是一个键/值对。</li><li id="42f7" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">输入:</li></ul><ol class=""><li id="4afb" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lj ju jv jw bi translated"><em class="jn">键</em>不过是文本文件中每一行的偏移量:<em class="jn"> LongWritable </em></li><li id="b5e5" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated"><em class="jn">值</em>是每一行(如右图所示):<em class="jn">文本</em></li></ol><ul class=""><li id="dd37" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">输出:</li></ul><ol class=""><li id="ea18" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lj ju jv jw bi translated"><em class="jn">键</em>是标记化的单词:<em class="jn">文本</em></li><li id="e8e6" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated">在我们的例子中，硬编码的<em class="jn">值</em>是1: <em class="jn"> IntWritable </em></li><li id="d03f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated">例子—亲爱的1，熊1，等等。</li></ol><ul class=""><li id="5166" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">我们编写了一个java代码，其中我们对每个单词进行了标记化，并给它们分配了一个硬编码值，等于<em class="jn"> 1 </em>。</li></ul><h2 id="a97c" class="lq kd hh bd ke lr ls lt ki lu lv lw km ja lx ly kq je lz ma ku ji mb mc ky md bi translated">减速器代码:</h2><pre class="lb lc ld le fd mf mg mh mi aw mj bi"><span id="9a49" class="lq kd hh mg b fi mk ml l mm mn">public static class Reduce extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {<br/> <br/>public void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context)<br/>throws IOException,InterruptedException {<br/> <br/>int sum=0;<br/>for(IntWritable x: values)<br/>{<br/>sum+=x.get();<br/>}<br/>context.write(key, new IntWritable(sum));<br/>}<br/>}</span></pre><ul class=""><li id="0659" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">我们创建了一个类Reduce，它像Mapper一样扩展了类Reduce。</li><li id="8af8" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">我们在类声明之后使用尖括号定义输入和输出键/值对的数据类型，就像对Mapper所做的那样。</li><li id="2140" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">减速器的输入和输出都是一个键-值对。</li><li id="2952" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">输入:</li></ul><ol class=""><li id="e0e3" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lj ju jv jw bi translated"><em class="jn">键</em>除了在排序和洗牌阶段后生成的那些独特的单词之外什么都没有:<em class="jn">文本</em></li><li id="fbcd" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated"><em class="jn">值</em>是每个键对应的整数列表:<em class="jn"> IntWritable </em></li><li id="caf9" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated">例如——熊、[1，1]等。</li></ol><ul class=""><li id="d24b" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">输出:</li></ul><ol class=""><li id="2f89" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lj ju jv jw bi translated"><em class="jn">键</em>是输入文本文件中出现的所有唯一单词:<em class="jn">文本</em></li><li id="04fe" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated"><em class="jn">值</em>是每个唯一字的出现次数:<em class="jn"> IntWritable </em></li><li id="94b2" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lj ju jv jw bi translated">例如——熊，2；车，3等。</li></ol><ul class=""><li id="0da0" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">我们汇总了与每个键对应的每个列表中的值，并生成了最终答案。</li><li id="08ca" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">通常，为每个唯一的单词创建一个缩减器，但是，您可以在mapred-site.xml中指定缩减器的数量。</li></ul><h2 id="5e9d" class="lq kd hh bd ke lr ls lt ki lu lv lw km ja lx ly kq je lz ma ku ji mb mc ky md bi translated">驱动程序代码:</h2><pre class="lb lc ld le fd mf mg mh mi aw mj bi"><span id="0864" class="lq kd hh mg b fi mk ml l mm mn">Configuration conf= new Configuration();<br/>Job job = new Job(conf,"My Word Count Program");<br/>job.setJarByClass(WordCount.class);<br/>job.setMapperClass(Map.class);<br/>job.setReducerClass(Reduce.class);<br/>job.setOutputKeyClass(Text.class);<br/> <br/>job.setOutputValueClass(IntWritable.class);<br/>job.setInputFormatClass(TextInputFormat.class);<br/>job.setOutputFormatClass(TextOutputFormat.class);<br/>Path outputPath = new Path(args[1]);<br/> <br/>//Configuring the input/output path from the filesystem into the job<br/>FileInputFormat.addInputPath(job, new Path(args[0]));<br/>FileOutputFormat.setOutputPath(job, new Path(args[1]));</span></pre><ul class=""><li id="eb6a" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">在驱动程序类中，我们将MapReduce作业的配置设置为在Hadoop中运行。</li><li id="1b77" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">我们指定作业的名称、映射器和缩减器的输入/输出的数据类型。</li><li id="9ec7" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">我们还指定了映射器和缩减器类的名称。</li><li id="6c6b" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">还指定了输入和输出文件夹的路径。</li><li id="40e4" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">setInputFormatClass()方法用于指定映射器将如何读取输入数据或什么将是工作单元。这里，我们选择了TextInputFormat，这样映射器就可以从输入文本文件中一次读取一行。</li><li id="1ab3" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">main()方法是驱动程序的入口点。在这个方法中，我们为作业实例化一个新的配置对象。</li></ul><h1 id="32ed" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">运行MapReduce代码:</h1><p id="b017" class="pw-post-body-paragraph ip iq hh ir b is ll iu iv iw lm iy iz ja ln jc jd je lo jg jh ji lp jk jl jm ha bi translated">运行MapReduce代码的命令是:</p><p id="f1c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">Hadoop jar Hadoop-MapReduce-example . jar字数/样本/输入/样本/输出</em></p><p id="7edd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，你们已经对MapReduce框架有了基本的了解。你应该已经意识到MapReduce框架是如何帮助我们编写代码来处理HDFS的海量数据的。</p><p id="0851" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、Python、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="5a64" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中解释大数据其他各方面的其他文章。</p><blockquote class="mq mr ms"><p id="b792" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">1.<a class="ae mp" rel="noopener" href="/edureka/hadoop-tutorial-24c48fbf62f6"> Hadoop教程</a></p><p id="a3d1" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">2.<a class="ae mp" rel="noopener" href="/edureka/hive-tutorial-b980dfaae765">蜂巢教程</a></p><p id="8329" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">3.<a class="ae mp" rel="noopener" href="/edureka/pig-tutorial-2baab2f0a5b0">养猪教程</a></p><p id="d4b5" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">4.<a class="ae mp" rel="noopener" href="/edureka/big-data-tutorial-b664da0bb0c8">大数据教程</a></p><p id="1b3a" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">5.<a class="ae mp" rel="noopener" href="/edureka/hbase-tutorial-bdc36ab32dc0"> HBase教程</a></p><p id="a999" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">6.<a class="ae mp" rel="noopener" href="/edureka/hdfs-tutorial-f8c4af1c8fde"> HDFS教程</a></p><p id="9c78" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">7.<a class="ae mp" rel="noopener" href="/edureka/hadoop-3-35e7fec607a"> Hadoop 3 </a></p><p id="487d" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">8.<a class="ae mp" rel="noopener" href="/edureka/apache-sqoop-tutorial-431ed0af69ee"> Sqoop教程</a></p><p id="ee42" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">9.<a class="ae mp" rel="noopener" href="/edureka/apache-flume-tutorial-6f7150210c76">水槽教程</a></p><p id="b44f" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">10.<a class="ae mp" rel="noopener" href="/edureka/apache-oozie-tutorial-d8f7bbbe1591"> Oozie教程</a></p><p id="9811" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">11.<a class="ae mp" rel="noopener" href="/edureka/hadoop-ecosystem-2a5fb6740177"> Hadoop生态系统</a></p><p id="4fc4" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">12.<a class="ae mp" rel="noopener" href="/edureka/hive-commands-b70045a5693a">HQL顶级配置单元命令及示例</a></p><p id="417e" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">13.<a class="ae mp" rel="noopener" href="/edureka/create-hadoop-cluster-with-amazon-emr-f4ce8de30fd"> Hadoop集群搭配亚马逊EMR？</a></p><p id="d68b" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">14.<a class="ae mp" rel="noopener" href="/edureka/big-data-engineer-resume-7bc165fc8d9d">大数据工程师简历</a></p><p id="e57e" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">15.<a class="ae mp" rel="noopener" href="/edureka/hadoop-developer-cc3afc54962c"> Hadoop开发人员-工作趋势和薪水</a></p><p id="3d25" class="ip iq jn ir b is it iu iv iw ix iy iz mt jb jc jd mu jf jg jh mv jj jk jl jm ha bi translated">16.<a class="ae mp" rel="noopener" href="/edureka/hadoop-interview-questions-55b8e547dd5c"> Hadoop面试问题</a></p></blockquote></div><div class="ab cl mw mx go my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ha hb hc hd he"><p id="4227" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">原载于2016年11月15日</em><a class="ae mp" href="https://www.edureka.co/blog/mapreduce-tutorial" rel="noopener ugc nofollow" target="_blank"><em class="jn">www.edureka.co</em></a><em class="jn">。</em></p></div></div>    
</body>
</html>