<html>
<head>
<title>Building Services at Airbnb, Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Airbnb的建筑服务，第3部分</h1>
<blockquote>原文：<a href="https://medium.com/airbnb-engineering/building-services-at-airbnb-part-3-ac6d4972fc2d?source=collection_archive---------1-----------------------#2018-12-11">https://medium.com/airbnb-engineering/building-services-at-airbnb-part-3-ac6d4972fc2d?source=collection_archive---------1-----------------------#2018-12-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="9fdf" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">在我们关于扩展服务开发系列的第三篇文章中，我们将深入探讨构建在标准服务平台中的弹性工程实践，该平台为Airbnb新的面向服务的架构提供支持。</h2></div><p id="aebb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">作者:<a class="ae js" href="https://www.linkedin.com/in/weibohe/" rel="noopener ugc nofollow" target="_blank"> <em class="jt">卫伯何</em></a><em class="jt"/><a class="ae js" href="https://www.linkedin.com/in/liang-guo-04189a/" rel="noopener ugc nofollow" target="_blank"><em class="jt"/></a></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ju"><img src="../Images/e407afdf01102f0f347272a2a81755a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lH5QyWjJ8lLLl-G_POna-Q.jpeg"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">A bright and spacious <a class="ae js" href="https://www.airbnb.com/rooms/plus/11474031" rel="noopener ugc nofollow" target="_blank">Airbnb Plus listing</a> in London</figcaption></figure><p id="a17a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi kk translated"><span class="l kl km kn bm ko kp kq kr ks di"> A </span> irbnb正在将其基础设施转向面向服务的架构。一个可靠的、高性能的、开发人员友好的多语言服务平台是Airbnb建筑发展的基础组件。在我们的<strong class="iy hi"><em class="jt"/></strong>系列的<a class="ae js" rel="noopener" href="/airbnb-engineering/building-services-at-airbnb-part-1-c4c1d8fa811b"><strong class="iy hi"><em class="jt">Part 1</em></strong></a>和<a class="ae js" rel="noopener" href="/airbnb-engineering/building-services-at-airbnb-part-2-142be1c5d506"><strong class="iy hi"><em class="jt">Part 2</em></strong></a>中，我们分享了我们是如何使用Thrift service IDL为中心的服务框架来规模化开发服务的；标准化服务平台如何鼓励和实施基础设施标准；以及如何在不产生额外开发开销的情况下对所有新服务实施最佳实践。</p><p id="220a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">面向服务的架构培养了所有权并提高了开发速度。然而，它带来了一系列新的挑战。分布式服务的系统复杂性比单一应用程序高得多，许多过去在单一体系结构中工作的技术不再适用。在本帖中，我们分享了我们如何将弹性工程构建到服务平台标准中，并帮助服务所有者提高其服务的可用性。</p><h1 id="8b74" class="kt ku hh bd kv kw kx ky kz la lb lc ld in le io lf iq lg ir lh it li iu lj lk bi translated">弹性是一个需求，而不是一个特性</h1><p id="f8e4" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi kk translated">在分布式服务架构中，服务弹性是一个硬性要求。随着服务间通信复杂性的增加，每个服务的响应能力和避免停机的能力都会降低。例如，Airbnb Homes PDP(产品详情页面)需要从20个下游服务获取数据。假设我们没有采取措施来提高我们的弹性，如果这20个依赖服务每个都有99.9%的可用性，我们的PDP页面将只有98.0%的正常运行时间，或每月大约14.5小时的停机时间。</p><p id="a172" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">没有什么比现实世界的生产事故根本原因分析更能清楚地说明弹性的影响了。在这些来自过去生产事故事后分析的样本中，我们可以看到一些常见的可靠性问题。</p><p id="59fe" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">"<em class="jt">我们看到了多波流量增加(外部流量以及因请求失败而重试的流量)。从单轨铁路的角度来看，这导致我们OAuth服务的可用性持续下降。这影响了登录流程中涉及的大多数API，并导致用户出现站点范围的身份验证问题，从而影响了核心业务指标。单轨错误率最初在5分钟内达到25%,然后在36分钟内再次达到5%。在停机期间，API错误大幅增加，增加了500个错误。这表现为P2/P3的印象率、创建的信息和其他业务指标的下降。</em></p><p id="8f0c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">"<em class="jt">当每个服务箱达到其极限时，它开始更频繁地超时，并导致上游P2服务重试。P2服务的积极重试很快使其流量翻倍。因此，该服务试图创建太多代理连接，所有连接都超时。所有服务后端的延迟激增，导致运行状况检查失败。最终，所有P2服务的盒子都被标上了</em></p><p id="5eec" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">“<em class="jt">在此期间，我们看到了10到20倍的QPS峰值，服务请求延迟在P99时增加了10倍以上。因此，上游客户端经历了请求失败。该服务使用计算密集型的哈希算法，我们认为根本原因是请求的峰值导致CPU利用率急剧增加，这使得其他请求的资源匮乏，并导致健康报告将单个服务节点标记为不健康。这种局部化的变化会对机群的其余部分产生级联效应，因为其他节点将不得不承担更多的流量。</em></p><p id="b787" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">处理过类似系统和事件的读者一定注意到了一些常见的可靠性问题:<em class="jt">请求峰值</em>，<em class="jt">系统过载</em>，<em class="jt">服务器资源耗尽</em>，<em class="jt">激进重试</em>，<em class="jt">级联故障</em>。过去，常见的容错措施，如请求超时、指数补偿重试和断路器的实施不一致。我们的标准化服务平台将弹性工程实践一致地构建到所有服务中，以降低任何单个服务成为网站可用性薄弱环节的可能性。</p><h1 id="237b" class="kt ku hh bd kv kw kx ky kz la lb lc ld in le io lf iq lg ir lh it li iu lj lk bi translated">我们建造的东西</h1><p id="091a" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi kk translated">Airbnb核心预订流程上的所有服务必须支持高吞吐量，并对瞬时故障具有容错能力。从事核心业务服务的产品工程师应该只需要专注于构建新功能，而不是自己实现这些需求。我们实施的弹性措施是众所周知的模式，已经防止了核心预订流程中的停机。</p><h2 id="d312" class="lq ku hh bd kv lr ls lt kz lu lv lw ld jf lx ly lf jj lz ma lh jn mb mc lj md bi translated">异步服务器请求处理</h2><p id="cb1e" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">我们大部分的核心预订流程服务都是Java服务。在<a class="ae js" rel="noopener" href="/airbnb-engineering/building-services-at-airbnb-part-1-c4c1d8fa811b"> <strong class="iy hi"> <em class="jt">第1部分</em> </strong> </a>中，我们解释了我们如何为Java服务开发扩展Dropwizard web服务框架。在大多数Airbnb服务中，请求处理涉及到对下游服务或数据库的多次RPC调用。我们使用的Dropwizard版本具有同步请求处理功能，在这种情况下，当工作线程池处理I/O密集型工作时，服务器线程会空闲等待。因此，我们的服务在等待网络I/O时容易受到资源利用不足的影响，无法吸收突发流量，并且容易受到请求过载或重试风暴的影响。</p><p id="ea3e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">利用最新Dropwizard框架中的异步响应特性，我们构建了一个端到端的异步请求处理流程，可以编写高度并发、无阻塞的应用程序。它连接Dropwizard/Jersey 2异步响应、我们内部构建的异步数据加载器框架和异步服务IDL客户端，用于服务间通信。异步请求处理有几个主要好处:</p><ul class=""><li id="d5ea" class="me mf hh iy b iz ja jc jd jf mg jj mh jn mi jr mj mk ml mm bi translated">增加吞吐量:主I/O线程向异步执行器提交请求，然后返回接受新的入站请求。更少的I/O线程可以处理更多的并发请求，并更好地吸收高峰流量。</li><li id="aa44" class="me mf hh iy b iz mn jc mo jf mp jj mq jn mr jr mj mk ml mm bi translated">饥饿预防:一个服务可能有几个I/O或CPU密集型的端点。如果一个节点收到太多对这些昂贵的端点的请求，它将在试图处理那些昂贵的请求时饿死简单和快速的请求。使用异步请求处理，主I/O线程可以接受所有请求，并将它们分派到不同权重的请求队列。昂贵和便宜的请求可以并发执行。</li></ul><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ms"><img src="../Images/0f67b02d0b60d2d44934348ee301cff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Sa3h5yhO6hQyx81E"/></div></div></figure><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mt"><img src="../Images/51025611a91467bb87d9a3f12beae62d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QlGf75x6YXYJObkF"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">With async request processing, a server typically uses fewer threads and has better tail latency when under load.</figcaption></figure><h2 id="b34b" class="lq ku hh bd kv lr ls lt kz lu lv lw ld jf lx ly lf jj lz ma lh jn mb mc lj md bi translated">请求排队</h2><p id="7114" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">每个后端服务器在其定义的延迟服务级别目标内都有处理请求的数量限制。这个限制是有限资源的函数:CPU、内存、网络带宽、文件描述符、磁盘队列等。对于Airbnb的大多数服务，典型的工作负载模式涉及对下游服务和数据库的数据获取调用；应用轻量级逻辑来计算派生数据；以及根据所获取和导出的数据组成响应。换句话说，大多数服务都是网络I/O绑定的，而不是计算绑定的，因此更有可能受到内存的限制。为了理解资源枯竭是如何导致大范围失败的，让我们再来看一个样本生产事故的事后分析:</p><p id="d88e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">"<em class="jt">当每个服务箱达到其极限时，它开始更频繁地超时，并导致上游P2服务重试。P2服务的积极重试很快使其流量翻倍。因此，该服务试图创建太多代理连接，所有连接都超时。所有服务后端的延迟激增，导致运行状况检查失败。最终，P2服务的所有箱子都被标上了</em></p><p id="271b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">请求排队是一种有效的技术，允许服务吸收突发的请求负载，防止服务由于资源耗尽而失败。更具体地说，我们用<em class="jt">自适应LIFO </em> ( <em class="jt">后进先出</em>)实现了一个<em class="jt">控制的延迟</em> ( <em class="jt"> CoDel </em>)队列，如文章<a class="ae js" href="https://queue.acm.org/detail.cfm?id=2839461" rel="noopener ugc nofollow" target="_blank"> <em class="jt">中所介绍的</em> </a>。</p><p id="e6ca" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">受控延迟队列</strong></p><p id="cd9c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在正常操作条件下，服务器能够在请求到来时处理请求，并在N毫秒内清空队列，而<em class="jt"> CoDel </em>请求队列使用N作为正常请求超时值。如果请求队列在N毫秒内没有被清空，它将使用一个更激进的请求超时值。积极的超时有助于防止常设队列的建立，从而保护服务器不被它跟不上的负载绊倒。</p><p id="98e8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">自适应后进先出法</strong></p><p id="8834" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在正常操作条件下，服务器按FIFO顺序处理请求。然而，当队列越来越大时，它就切换到LIFO模式。在排队期间，最后进入的请求比先进入的请求更有可能满足客户端请求的截止日期，因为先进入的请求在队列中等待的时间更长。</p><p id="2a74" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">自适应后进先出法与CoDel queue在Airbnb的核心预订流程中很好地实现了高吞吐量、低延迟的服务，因为它们是有效的延迟预防措施。此外，我们看到定制请求队列实现实际上提高了单个服务主机的吞吐量和错误率。</p><p id="3a38" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们将受控延迟队列(黄色)与之前的处理器(一个有界线程池(红色))进行了基准测试。在一个高吞吐量的Java服务中，切换到受控延迟队列提高了单主机吞吐量并降低了错误率。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mu"><img src="../Images/4e47175befecfbee6847d7d87b550a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Rus5PVbz1puunR3G"/></div></div></figure><p id="24a9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在过载情况下，请求队列确实会导致第95和第99百分位延迟略有增加，如下图所示。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mv"><img src="../Images/187f7328af2fbbbf659acd7ad1770ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4w0VSqb3ciJJnCTL"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">Bounded thread pool is shown in blue, while the controlled delay queue is shown in purple.</figcaption></figure><h2 id="96a8" class="lq ku hh bd kv lr ls lt kz lu lv lw ld jf lx ly lf jj lz ma lh jn mb mc lj md bi translated">甩负荷</h2><p id="1d26" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">事后分析的生产事件也不止一次引用了积极的客户端重试。事实上，这是级联故障最常见的原因，因为重试风暴没有给过载的服务任何恢复的空间。我们的标准内部服务客户端实现超时、重试和断路器模式。它将开发人员从编写自己的重试循环中解放出来，而重试循环通常是重试风暴的来源。然而，一个更有效的弹性特性是让客户端知道何时完全停止发送请求。</p><p id="3651" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">服务背压</strong></p><p id="6fbf" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">服务器端请求队列状态是服务是否过载的最佳信号。当服务主机中的请求队列增加时，它会切换到LIFO模式，并开始快速失败带有背压错误的入站请求，直到请求队列水位线下降到安全阈值。当传播到客户端时，智能客户端将避免重试并快速失败上游请求。服务反压力需要服务器和客户机的协调，并且通过有意识地使一小部分请求失败来实现更快的恢复。更重要的是，它防止了我们在生产事件中看到的由于服务过载和重试风暴的组合而导致的级联故障。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mw"><img src="../Images/9e341e1389ae09f4f79385f15d592f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u5pTATyPh8yoBB8f"/></div></div></figure><p id="3220" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在此标准服务器控制面板图中，我们可以看到一台主机的成功QPS急剧下降，但随后在1分钟内恢复。此实例的请求队列越来越多，服务器开始发送服务回压错误。智能客户端通过不重试任何请求并立即触发客户端断路器来处理服务过载信号。这种协调允许服务器自行恢复，并防止潜在的生产事故。</p><p id="8bd9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi"> API申请截止日期</strong></p><p id="00d1" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">我们使用的另一种减载技术是API请求截止期，这是RPC截止期传播的一种实现，它允许服务优雅地处理请求过载。Airbnb的公共API端点都仔细设置了与延迟SLO相关的请求超时。每个入站API请求都有一个由API网关使用端点级超时设置的截止时间。最后期限随后随着请求向后端服务扩散而传播。服务器框架检查截止日期，并快速拒绝已经到达截止日期的请求。随着我们继续在跨服务间通信栈构建更多的弹性工程实践，我们将更加积极地设置API请求超时和强制执行截止日期。</p><h2 id="ef17" class="lq ku hh bd kv lr ls lt kz lu lv lw ld jf lx ly lf jj lz ma lh jn mb mc lj md bi translated">基于客户配额的速率限制</h2><p id="dfb5" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">服务反压保护服务在过载情况下不会出现故障，但是，它不能区分不同上游调用者之间的流量。例如，我们的列表服务有30多个上游调用者。任何行为不端的调用者都可能发送过多的请求并影响其他客户端。</p><p id="51df" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Airbnb经历了几起核心服务事件，原因是流氓客户的负载过大。我们在由计数器服务和动态配置系统组成的标准服务框架中实现了基于客户端配额的速率限制。服务所有者可以在动态配置系统中定义客户端配额，服务将使用分布式计数器服务来跟踪配额并对任何滥用的客户端进行速率限制。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mx"><img src="../Images/218aa46b07e2b28f76d4a5fdd414cdde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fP49WNyiBCejLUwQ"/></div></div></figure><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es my"><img src="../Images/16022ca5ac820f1112686ec41d72a3df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OI7It1TPJx41QfCP"/></div></div></figure><p id="fd91" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">基于配额的速率限制与生产中的服务背压配合良好。在正常情况下，服务的容量在其上游调用者之间公平分配。当调用者行为不当时，速率限制保护服务免受意外负载，防止其他客户端的服务背压错误。</p><h2 id="3ca5" class="lq ku hh bd kv lr ls lt kz lu lv lw ld jf lx ly lf jj lz ma lh jn mb mc lj md bi translated">依赖隔离和优雅降级</h2><p id="96af" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">在分布式服务架构中，一个服务通常有多个下游依赖项。当依赖项的数量增加时，单个有问题的依赖项导致服务停止的可能性就变得更高。让我们看一个发生在Airbnb的例子:</p><p id="75d7" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">P3服务对内容审核服务有一种软依赖性，因为只有极小一部分请求会调用它。在该事件中，下游内容审核服务变得无响应，并且P3服务异步工作线程没有及时释放。最终，P3服务耗尽了它的线程池，导致所有请求的错误增加。”</p><p id="7d4d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">为我们的请求采用<a class="ae js" href="https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead" rel="noopener ugc nofollow" target="_blank">隔板模式</a>可以减轻这个问题。我们在异步请求执行器框架和服务平台上的服务间IDL客户端中实现了依赖隔离，类似于<a class="ae js" href="https://github.com/Netflix/Hystrix/wiki/How-it-Works#isolation" rel="noopener ugc nofollow" target="_blank">网飞的Hystrix </a>的方法。独立的依赖项有独立的异步工作线程池，因此一个有问题的下游依赖项只会使相应的工作线程池饱和。如果有问题的依赖项恰好是软依赖项，例如产品详细信息页面上的趋势信息，则除了提供部分响应之外，服务的可用性不会受到影响。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mz"><img src="../Images/1fca26de30e281531498f4784af8ba6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZtq_Sk1El8aov4l8J946w.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">Graph of thread pool sizes per dependency for a service.</figcaption></figure><h2 id="0bfb" class="lq ku hh bd kv lr ls lt kz lu lv lw ld jf lx ly lf jj lz ma lh jn mb mc lj md bi translated">异常服务器主机检测</h2><p id="e920" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi translated">正如我们在事后分析中看到的，服务器主机故障是正常运行时间的常见威胁。当您在生产中拥有大量服务器时，一些服务器主机开始出现错误只是时间问题。</p><p id="08f6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="jt">“一个服务实例在PDT下午7:45开始经历异常状态。实例的QPS下降，实例的错误率上升。服务团队的警报直到晚上11:49才被触发。中午12:40，值班工程师发现了有问题的实例，并重启它以解决问题。”</em></p><p id="1847" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这个事件突出了我们需要改进的三个工具:监视、检测和警告离群服务器。故障服务器的QPS下降和错误率上升是一个明显的信号。然而，我们的服务发现堆栈并没有捕获和处理它，而是需要人工干预。在轮换有故障的服务器主机之前，随叫随到的工程师必须在分散的仪表板中搜索，以便将症状与根本原因联系起来。因此，这种情况下的MTTR(平均修复时间)时间比预期的要短得多。</p><p id="4347" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">客户端智能负载平衡是一种自动化的弹性措施，可以完全防止这些情况发生。在客户端智能负载平衡中，客户端维护一个服务器主机列表。它跟踪每个主机的成功率和响应延迟，并避免将请求路由到成功率低、延迟高的异常主机。我们将<a class="ae js" href="https://www.envoyproxy.io/docs/envoy/latest/intro/what_is_envoy" rel="noopener ugc nofollow" target="_blank">特使</a>添加到我们的服务发现堆栈中，它支持<a class="ae js" href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/outlier#arch-overview-outlier-detection" rel="noopener ugc nofollow" target="_blank">异常检测</a>。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es na"><img src="../Images/fc3fa9c714c85efdf28924edb65a3c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HFss0Btjlzn9qm9X"/></div></div></figure><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es nb"><img src="../Images/79314e22e8d70689809a3ff3115491d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wOy5dL-rP_y42nJM"/></div></div></figure><p id="fc3c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在生产中，当服务器主机的错误响应比正常情况高时，异常值检测会快速检测并将其排除，主机的流量会立即减少。在服务器出现故障后不到一分钟，自动恢复措施就会发挥作用，而且电话服务工程师根本不需要被呼叫。</p><h1 id="c3f9" class="kt ku hh bd kv kw kx ky kz la lb lc ld in le io lf iq lg ir lh it li iu lj lk bi translated">结论</h1><p id="23fe" class="pw-post-body-paragraph iw ix hh iy b iz ll ii jb jc lm il je jf ln jh ji jj lo jl jm jn lp jp jq jr ha bi kk translated"><span class="l kl km kn bm ko kp kq kr ks di">在</span>面向服务的架构中，服务间的通信复杂度随着服务数量和堆栈深度的增长呈指数级增长。维护站点可靠性和可用性的技术不同于整体架构。我们分享了我们在标准服务平台中构建的一些弹性工程最佳实践。它们是在服务器框架和客户端库中统一实现的。它们易于配置和使用。我们认为，在分布式服务架构中，弹性是一项需求，而不是可选特性。这项工作有效地防止了越来越多的潜在事故。</p><p id="5991" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在任何情况下，无论是部署、流量激增、短暂的网络故障还是持续的主机故障，单个服务都应该保持其SLO(服务级别目标)。标准服务弹性工程可以帮助服务所有者实现这一目标。弹性工程有助于整个工程，帮助服务所有者配置他们的SLO、服务可用性验证测试、容量规划等。这根柱子遮住了低垂的果实。在我们的下一篇文章中，我们将分享我们为加强SLO而建设和计划建设的东西；失效注射试验；可靠性和可用性验证测试；和混沌工程。继续收听！</p></div><div class="ab cl nc nd go ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ha hb hc hd he"><p id="1615" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">感谢、邢安、关、蔡、穆索姆·达尔·古普塔、索尼娅·斯坦、贾森·简、奥斯汀·朱对我们复原力工程项目中使用的图书馆的贡献。感谢迪伦·赫德的编辑评论。</p></div><div class="ab cl nc nd go ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ha hb hc hd he"><p id="b556" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你喜欢阅读这篇文章，并且觉得你会喜欢在服务基础设施上工作，生产平台团队总是在寻找有才华的工程师来加入团队。</p></div></div>    
</body>
</html>