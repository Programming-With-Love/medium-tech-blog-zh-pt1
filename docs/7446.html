<html>
<head>
<title>Exploring Topic Modelling using Semi-Supervised Learning (Correlation Explanation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用半监督学习探索主题建模(相关性解释)</h1>
<blockquote>原文：<a href="https://medium.com/version-1/exploring-topic-modelling-using-semi-supervised-learning-correlation-explanation-b81d2603c9a2?source=collection_archive---------0-----------------------#2021-01-14">https://medium.com/version-1/exploring-topic-modelling-using-semi-supervised-learning-correlation-explanation-b81d2603c9a2?source=collection_archive---------0-----------------------#2021-01-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d1e17daed245e5cc72563ce1f13022b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1UHZAgitB4CYniTbtx3zWg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://www.pexels.com/@markusspiske?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> from <a class="ae it" href="https://www.pexels.com/photo/crowd-reflection-color-toy-1679618/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="2845" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在当今世界，<a class="ae it" href="https://www.version1.com/it-service/digital-services/" rel="noopener ugc nofollow" target="_blank">数字化转型</a>是每个行业都在追求的东西，而数据驱动的洞察力是它的核心。每秒钟都会以不同的形式生成和收集数据，所有组织都努力确保拥有完整的数据视图，以提供实时见解，并能够采取数据驱动的行动。任何数据驱动流程的挑战部分都是在短时间内获得相关和所需的信息。开发了许多算法和技术来获取人们正在寻找的信息。在本文中，我们将探讨自然语言处理领域中的一种流行技术，即主题建模，以及增强这一领域的一个特定包。</p><h1 id="9f64" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">什么是话题造型？</strong></h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kq"><img src="../Images/6f72d599d760b4c9833024c27f7e0d61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFEG5buQxxHRlr-wi4ll_w.png"/></div></div></figure><p id="9598" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">主题建模是识别文档中主题的过程。随着电子邮件、推文、书籍、期刊、文章等数字化文本的增加，主题建模仍然是识别和自动将这些文档分类为类别或主题的最重要的技术之一。有许多标准方法来处理主题建模，其中一些流行的方法是<a class="ae it" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hi">【潜在狄利克雷分配(LDA) </strong> </a>和<a class="ae it" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hi">非负矩阵分解(NMF) </strong> </a>。</p><p id="d3c3" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">LDA和NMF都是python scikit库中的标准实现，在数据科学社区中被广泛使用。这些方法是无监督的学习算法，生成各种相互关联的主题，并高度依赖于它对数据集做出的假设。然而，这些方法在概括底层细节和围绕数据生成过程的复杂假设时有局限性。在某些情况下，由于人类输入数据的高维性，这些模型最终会得出错误的假设。</p><p id="a6fa" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在本文中，我们将探索CorEx包，它允许对模型生成的主题进行某种程度的控制。</p><h1 id="a003" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">相关性解释(CorEx) </strong></h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/eafeb2a6e6301ae4b0b3f179a198fdfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gcp4Vrgta-b4_jvG1b0yyg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">CorEx uses keywords to anchor topics</figcaption></figure><p id="8504" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><a class="ae it" href="https://ryanjgallagher.github.io/code/corex/overview" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hi">关联解释(CorEx </strong> </a> <strong class="iw hi"> ) </strong>是一个灵活的框架，由Greg Ver Steeg开发，用于主题建模，以识别最大化文本语料库中可用信息的主题。CorEx模型允许通过用户特定的锚词来整合领域知识，锚词将模型导向感兴趣的主题。这使得模型能够表示不自然出现的主题，并提供分离关键字的能力，从而允许识别不同的主题。</p><p id="012d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">例如，下表显示了与新组中的主题相关的一些关键字，以及将用于识别这些主题的锚词。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es kw"><img src="../Images/0dbd05be0034496f108532043d031ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*N6pDSqVN_DBmUs1taeXLig.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Table created from examples in <a class="ae it" href="https://www.aclweb.org/anthology/Q17-1037.pdf" rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/Q17-1037.pdf</a></figcaption></figure><h1 id="d1bc" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">如何使用CorEx？</strong></h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/ae5498c5164e0a5d6edbdacb6f37c0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MKHtCHoVKgVVs7HBVxC_-A.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://www.pexels.com/@anntarazevich?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Anna Tarazevich</a> from <a class="ae it" href="https://www.pexels.com/photo/sign-abstract-typography-business-6230975/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="6ac5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">CorEx的python实现在<strong class="iw hi"> </strong> <a class="ae it" href="https://github.com/gregversteeg/corex_topic" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hi"> Github上有。</strong>T13】</a></p><p id="ddb5" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">您可以使用pip命令在python上安装CorEx。</p><pre class="kr ks kt ku fd kx ky kz la aw lb bi"><span id="04d9" class="lc jt hh ky b fi ld le l lf lg">pip install corextopic</span></pre><p id="6ec2" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以下是使用标准无监督方法(如LDA和NMF)的主题建模示例。</p><p id="df99" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">使用LDA的主题建模</strong></p><pre class="kr ks kt ku fd kx ky kz la aw lb bi"><span id="bc3a" class="lc jt hh ky b fi ld le l lf lg">from sklearn.decomposition LatentDirichletAllocation as LDA<br/><br/>no_of_topics = 4<br/>tfidf = TF-IDF matrix of your documents</span><span id="c56c" class="lc jt hh ky b fi lh le l lf lg"># Run LDA<br/>lda = LDA(n_topics=no_of_topics).fit(tfidf)</span><span id="0e62" class="lc jt hh ky b fi lh le l lf lg"># Display top n words for each topic identified<br/>def display_topics(model, features, words_count):<br/>    for topic_no, topic in enumerate(model.components_):<br/>        print("Topic %d:" % (topic_no))<br/>        print(" ".join([features[i] for i in topic.argsort()[:-words_count - 1:-1]])<br/><br/>words_count = 10</span><span id="9443" class="lc jt hh ky b fi lh le l lf lg"># Display top 10 words for each topic<br/>display_topics(lda, tfidf_feature_names, words_count)</span></pre><p id="e40b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">利用NMF进行话题建模</strong></p><pre class="kr ks kt ku fd kx ky kz la aw lb bi"><span id="277c" class="lc jt hh ky b fi ld le l lf lg">from sklearn.decomposition import NMF<br/><br/>no_of_topics = 4<br/>tfidf = TF-IDF matrix of your documents</span><span id="80db" class="lc jt hh ky b fi lh le l lf lg"># Run NMF<br/>nmf = NMF(n_components=no_of_topics).fit(tfidf)</span><span id="e7df" class="lc jt hh ky b fi lh le l lf lg"># Display top n words for each topic identified<br/>def display_topics(model, features, words_count):<br/>    for topic_no, topic in enumerate(model.components_):<br/>        print("Topic %d:" % (topic_no))<br/>        print(" ".join([features[i] for i in topic.argsort()[:-words_count - 1:-1]])<br/><br/>words_count = 10</span><span id="e9c3" class="lc jt hh ky b fi lh le l lf lg"># Display top 10 words for each topic <br/>display_topics(nmf, tfidf_feature_names, words_count)</span></pre><p id="f370" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">上面的两个例子都没有从用户那里获取任何输入来识别主题。主题纯粹是根据基础概念选择的，可能无法捕捉复杂数据集中跨主题的始终关联或始终用于独立概念的关键字之间的任何关系。</p><p id="ea16" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">使用相关性解释的主题建模通过使用<strong class="iw hi">锚关键词</strong>克服了这一限制，如下例所示:</p><pre class="kr ks kt ku fd kx ky kz la aw lb bi"><span id="14dc" class="lc jt hh ky b fi ld le l lf lg">from corextopic import corextopic as ct<br/><br/>no_of_topics = 4<br/>anchor_strength = 3<br/>tfidf = TF-IDF matrix of your documents</span><span id="ce9a" class="lc jt hh ky b fi lh le l lf lg"># Anchor Keywords<br/>keywords =   [<br/>              ["congress", "clinton", "trump"],<br/>              ["bible", "christian", "muslim", "hindu"],<br/>              ["circuit"],<br/>              ["pitching","goal"]<br/>             ]</span><span id="4c7f" class="lc jt hh ky b fi lh le l lf lg"># Run Anchored CorEx<br/>topic_model = ct.Corex(n_hidden=no_of_topics)<br/>topic_model.fit(tfidf, anchors = keywords, anchor_strength = anchor_strength);</span><span id="af0c" class="lc jt hh ky b fi lh le l lf lg"># Display top n words for each topic identified<br/>def display_topics(model, words_count):<br/>     for i, topic_words in enumerate(model.get_topics(n_words = words_count)):<br/>         topic_words = [words[0] for words in topic_words if words[1] &gt; 0]<br/>         print("Topic #{}: {}".format(i+1, ", ".join(topic_words)))</span><span id="5a78" class="lc jt hh ky b fi lh le l lf lg">words_count = 10</span><span id="5c0e" class="lc jt hh ky b fi lh le l lf lg"># Display top 10 words for each topic <br/>display_topics(topic_model,words_count)</span></pre><p id="43e7" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">锚关键词是分配给每个主题的一组关键词。在上面的例子中，关键字被用来标识诸如政治、宗教、体育和电力(公用事业服务)等主题。CorEx模型还有一个强度参数，它定义了生成的主题对锚关键词的偏向。这个值应该总是大于1，值越大表示越倾向于锚关键词。</p><p id="b79c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">瑞安·加拉格尔的CorEx笔记本示例可点击此<a class="ae it" href="https://github.com/gregversteeg/corex_topic/blob/master/corextopic/example/corex_topic_example.ipynb" rel="noopener ugc nofollow" target="_blank">链接获得。</a></p><h1 id="5833" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">我们在版本1中做了什么？</h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es li"><img src="../Images/ddc5ee0600734d8ac4b50dbc38ff9141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cefImDhdTldvS-_6D9W_EQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Photo by <a class="ae it" href="https://www.pexels.com/@shotkit-3551620?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Shotkit</a> from <a class="ae it" href="https://www.pexels.com/photo/white-and-black-love-print-on-white-snow-5355642/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="5424" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在第1版中，我们通过季度调查了解客户的满意度。这有助于我们了解客户需要什么，这是一个可以改进的领域，也有助于我们发现创新和增值的机会。</p><p id="4176" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Corex主题建模器有助于识别客户在关键领域的主题，使我们能够改进，并确保我们始终提供卓越的服务。</p></div></div>    
</body>
</html>