# Twitter 如何在 Pig 的帮助下分析推文？

> 原文：<https://medium.com/edureka/pig-tutorial-2baab2f0a5b0?source=collection_archive---------0----------------------->

![](img/adce649569c9de67b8e7a0fdd2618815.png)

Pig Tutorial - Edureka

正如我在我的 [***Hadoop 生态系统***](/edureka/hadoop-ecosystem-2a5fb6740177) 文章中提到的，Apache Pig 是我们 Hadoop 生态系统中必不可少的一部分。所以，我想带你看一下这个 Apache Pig 教程，它是我们 Hadoop 教程系列 ***的一部分。*** 在这篇阿帕奇猪教程文章中，我将谈到:

*   阿帕奇猪 vs MapReduce
*   阿帕奇猪简介
*   阿帕奇猪用在哪里？
*   Twitter 案例研究
*   阿帕奇猪建筑
*   Pig 拉丁数据模型
*   Apache Pig 模式

在开始学习 Apache Pig 教程之前，我想让您问自己一个问题—“***虽然 MapReduce 是为了大数据分析而存在的，但为什么 Apache Pig 会出现在画面中呢？***

对此，甜蜜而简单的回答是:

***大约 10 行 Pig 代码等于 200 行 MapReduce 代码*** 。

用 Java 编写 MapReduce 作业对每个人来说都不是一件容易的事情。因此，Apache Pig 成为不擅长 Java 或 Python 的程序员的福音。即使有人懂 Java 并且擅长 MapReduce，他们也会更喜欢 Apache Pig，因为使用 Pig 很容易。现在让我们来看看。

# 阿帕奇猪 vs MapReduce

程序员在编写 MapReduce 任务时面临困难，因为它需要 Java 或 Python 编程知识。对他们来说，阿帕奇猪是救星。

*   Pig Latin 是一种高级数据流语言，而 MapReduce 是一种低级数据处理范式。
*   不用在 MapReduce 中编写复杂的 Java 实现，程序员可以使用 Pig Latin 非常容易地实现相同的实现。
*   Apache Pig 使用多查询方法(即使用 Pig Latin 的单个查询我们可以完成多个 MapReduce 任务)，这将代码长度减少了 20 倍。因此，这将开发周期缩短了近 16 倍。
*   Pig 提供了许多内置的操作符来支持数据操作，如连接、过滤、排序、分类等。而在 MapReduce 中执行相同的功能是一项巨大的任务。
*   在 Apache Pig 中执行连接操作很简单。而在 MapReduce 中很难执行数据集之间的连接操作，因为它需要顺序执行多个 MapReduce 任务来完成工作。
*   此外，它还提供了 MapReduce 中没有的嵌套数据类型，如元组、包和地图。一会儿我会给你解释这些数据类型。

现在我们知道了为什么阿帕奇猪会出现，你会很好奇什么是阿帕奇猪？让我们在本文中继续前进，浏览 Apache Pig 的介绍和特性。

# 阿帕奇猪简介

Apache Pig 是一个平台，用于分析将它们表示为数据流的大型数据集。它旨在提供 MapReduce 的抽象，降低编写 MapReduce 程序的复杂性。我们可以使用 Apache Pig 在 Hadoop 中非常容易地执行数据操纵操作。

阿帕奇猪的特点是:

*   Pig 使程序员能够在不了解 Java 的情况下编写复杂的数据转换。
*   Apache Pig 有两个主要组件——***Pig 拉丁语言*** 和 ***Pig 运行时环境*** *，*，Pig 拉丁程序在其中执行。
*   对于大数据分析，Pig 给出了一个简单的数据流语言，称为 ***Pig Latin*** ，它具有类似于 SQL 的功能，如连接、过滤、限制等。
*   使用脚本语言和 SQL 的开发人员利用了猪拉丁语。这使得开发人员 ***很容易用 Apache Pig 进行*** 编程。Pig Latin 提供了各种内置的操作符，如 join、sort、filter 等，用于读取、写入和处理大型数据集。由此可见，猪头有一套 ***富套符*** 。
*   程序员使用 Pig Latin 编写脚本来分析数据，这些脚本由 Pig MapReduce 引擎在内部转换为 Map 和 Reduce 任务。在 Pig 之前，编写 MapReduce 任务是处理存储在 HDFS 的数据的唯一方法。
*   如果程序员想编写 Pig 中没有的自定义函数，Pig 允许他们用自己选择的任何语言编写用户定义的函数(***【UDF】***)，如 Java、Python、Ruby、Jython、JRuby 等。并把它们嵌入到猪文字中。这就给阿帕奇猪提供了**的扩展性。**
*   **Pig 可以处理任何类型的数据，即来自不同来源的结构化、半结构化或非结构化数据。阿帕奇猪 ***处理各种数据*** 。**
*   **大约，10 行 pig 代码等于 200 行 MapReduce 代码。**
*   **它可以处理不一致的模式(对于非结构化数据)。**
*   **Apache Pig 提取数据，对数据执行操作，并以所需格式将数据转储到*HDFS 中，即 **ETL(提取转换负载)**。***
*   ***Apache Pig 在执行前自动优化任务，即 ***自动优化*** 。***
*   **它允许程序员和开发人员专注于整个操作，而不用分别创建映射器和缩减器函数。**

**在了解了什么是 Apache Pig 之后，现在让我们了解一下在哪里可以使用 Apache Pig，最适合 Apache Pig 的用例有哪些？**

# **阿帕奇猪用在哪里？**

**Apache Pig 用于分析和执行涉及特定处理的任务。使用阿帕奇猪:**

*   **在我们需要处理的地方，像我们的博客，在线数据流等巨大的数据集。**
*   **我们需要搜索平台的数据处理(需要处理不同类型的数据)，如 ***雅虎使用 Pig 完成 40%的工作，包括新闻提要和搜索引擎*** 。**
*   **我们需要处理时间敏感的数据负载。在这里，需要快速提取和分析数据。例如，机器学习算法需要时间敏感的数据负载，如 twitter，需要快速提取客户活动的数据(即推文、转发推文和喜欢)，并分析数据以发现客户行为的模式，并像趋势推文一样立即提出建议。**

**现在，在我们的 Apache Pig 教程中，让我们浏览一下 **Twitter 案例研究**，以更好地了解 Apache Pig 如何帮助分析数据并使业务理解变得更容易。**

# **Twitter 案例研究**

**我将带你看一个 Twitter 的案例，在这个案例中，Twitter 采用了 Apache Pig。**

**Twitter 的数据正在加速增长(即 10 TB 数据/天)。因此，Twitter 决定将存档数据转移到 HDFS，并采用 Hadoop 来从中提取商业价值。**

**他们的主要目标是分析存储在 Hadoop 中的数据，以得出以下每日、每周或每月的见解。**

## **计数操作:**

*   **twitter 一天服务多少请求？**
*   **请求的平均延迟是多少？**
*   **Twitter 上每天有多少搜索？**
*   **收到多少个独特的查询？**
*   **有多少独立用户来访问？**
*   **用户的地理分布如何？**

## **关联大数据:**

*   **移动用户的使用有何不同？**
*   **群组分析:根据用户的行为，通过对用户进行分类来分析数据。**
*   **当站点出现问题时，哪里出错了？**
*   **用户经常使用哪些功能？**
*   **搜索更正和搜索建议。**

## **研究大数据并产生更好的结果，如:**

*   **Twitter 可以从用户的推文中分析出什么信息？**
*   **谁跟随谁，在什么基础上？**
*   **跟随者与跟随者的比例是多少？**
*   **用户口碑如何？**

****还有更多……****

**所以，为了分析数据，Twitter 最初使用 MapReduce，这是一种基于 HDFS 的并行计算(即 Hadoop 分布式文件系统)。**

**例如，他们想分析在给定的 tweet 表中每个用户存储了多少 tweet？**

**使用 MapReduce，该问题将依次得到解决，如下图所示:**

**![](img/1d04196ecd70acaf864142d0bf673a73.png)**

**Twitter MapReduce Example - Pig Tutorial**

**MapReduce 程序首先将键作为行输入，并将 tweet 表信息发送给 mapper 函数。然后，映射器功能将选择用户 id 并将单位值(即 1)与每个用户 id 相关联。Shuffle 函数会将相同的用户 id 排序在一起。最后，Reduce 函数会将属于同一用户的所有 tweets 的数量加在一起。输出将是用户 id，结合用户名和每个用户的 tweets 数量。**

**但是在使用 MapReduce 时，他们面临一些限制:**

*   **分析通常需要用 Java 来完成。**
*   **执行的连接需要用 Java 编写，这使得它更长并且更容易出错。**
*   **对于投影和过滤器，需要编写自定义代码，这使得整个过程变慢。**
*   **使用 MapReduce 时，作业被分成许多阶段，这使得管理起来很困难。**

**于是，Twitter 转移到阿帕奇猪进行分析。现在，连接数据集、分组、排序和检索数据变得越来越简单。你可以在下图中看到 twitter 如何使用 Apache Pig 来分析他们的大型数据集。**

**![](img/99136defabcd2139bd512cdfcb683809.png)**

**Twitter 既有半结构化数据，如 Twitter Apache 日志、Twitter 搜索日志、Twitter MySQL 查询日志、应用程序日志，也有结构化数据，如推文、用户、阻止通知、电话、收藏夹、保存的搜索、转发推文、认证、短信使用、用户关注等。这很容易被阿帕奇猪处理。**

**Twitter 将其所有存档数据都转储到 HDFS。它有两个表格，即用户数据和推文数据。用户数据包含关于用户的信息，如用户名，追随者，追随者，推文数量等。而 tweet 数据包含 Tweet、其所有者、转发次数、点赞次数等。现在，Twitter 使用这些数据来分析他们客户的行为，并改善他们过去的体验。**

**我们将看到 Apache Pig 如何解决 MapReduce 所解决的相同问题:**

****问题** : ***在给定的 tweet 表中，分析每个用户存储了多少 tweet？*****

**下图显示了 Apache Pig 解决问题的方法:**

**![](img/56f6faba7eaaf3e0ca7b01ebf50032fe.png)**

**Twitter Solution - Pig Tutorial**

**上图显示了这个问题的逐步解决方案。**

> ****步骤****1**–首先，twitter 将 twitter 表(即用户表和 tweet 表)导入 HDFS。**
> 
> ****步骤****2**–然后 Apache Pig 将表加载( **LOAD** )到 Apache Pig 框架中。**
> 
> ****步骤****3**–然后使用 **COGROUP** 命令将 tweet 表和用户表连接并分组，如上图所示。**
> 
> **这导致了内袋数据类型，我们将在本文后面讨论。**
> 
> **生产的内袋示例(参考上图)–**
> 
> **(1， **{(1，Jay，xyz)，(1，Jay，pqr)，(1，Jay，lmn)}** )**
> 
> **(2， **{(2，艾莉，abc)，(2，艾莉，vxy)}** )**
> 
> **(3， **{(3，山姆，斯图)}** )**
> 
> ****步骤****4**–然后根据使用**计数**命令的用户对推文进行计数。因此，每个用户的推文总数可以很容易地计算出来。**
> 
> **产生的元组示例为(id，tweet count)(参考上图)–**
> 
> **(1、 **3** )**
> 
> **(2， **2****
> 
> **(3、 **1****
> 
> ****步骤****5**–最后将结果与用户表连接，提取出产生结果的用户名。**
> 
> **产生的元组示例为(id，name，tweet count)(参考上图)–**
> 
> **(1、**杰、** 3)**
> 
> **(2，**艾莉，** 2)**
> 
> **(3，**山姆**，1)**
> 
> ****步骤****6**–最后，这个结果被存储回 HDFS。**

**Pig 不仅仅限于此操作。它可以执行我之前在这个用例中提到的各种其他操作。**

**这些见解有助于 Twitter 进行情感分析，并基于用户行为和模式开发机器学习算法。**

**现在，在了解了 Twitter 案例研究之后，在这篇 Apache Pig 教程中，让我们深入了解一下 Apache Pig 和 Pig Latin 的数据模型的架构。这将有助于我们了解 pig 的内部工作原理。阿帕奇猪从它的建筑中汲取力量。**

# **体系结构**

**为了编写 Pig 脚本，我们需要 Pig 拉丁语，为了执行它们，我们需要一个执行环境。Apache Pig 的架构如下图所示。**

**![](img/744ead1e58d834e01e56daf123c3759f.png)**

**Architecture of Pig - Pig Tutorial**

## **猪拉丁文字**

**最初如上图所示，我们向 Apache Pig 执行环境提交 Pig 脚本，可以使用内置操作符用 Pig 拉丁语编写。**

**有三种方法可以执行 Pig 脚本:**

> *****Grunt Shell* :** 这是为执行所有 Pig 脚本而提供的 Pig 交互 Shell。**
> 
> *****脚本文件* :** 将所有 Pig 命令写入一个脚本文件，并执行 Pig 脚本文件。这是由 Pig 服务器执行的。**
> 
> *****嵌入式脚本* :** 如果一些函数在内置操作符中不可用，我们可以通过编程创建用户定义的函数，使用其他语言如 Java、Python、Ruby 等来实现这些功能。并嵌入猪拉丁脚本文件**。**然后，执行脚本文件。**

## **句法分析程序**

**从上图可以看到，在通过 Grunt 或 Pig 服务器后，Pig 脚本被传递给解析器。解析器进行类型检查并检查脚本的语法。解析器输出 DAG(有向无环图)。DAG 表示 Pig 拉丁语句和逻辑运算符。逻辑运算符表示为节点，数据流表示为边。**

## **【计算机】优化程序**

**然后 DAG 被提交给优化器。优化器执行优化活动，如拆分、合并、转换和重新排序操作符等。这个优化器为 Apache Pig 提供了自动优化功能。优化器的基本目标是在处理提取的数据时减少管道中任何时刻的数据量，为此，它执行以下功能:**

*   *****上推过滤器* :** 如果过滤器中有多个条件，且过滤器可以拆分，Pig 拆分条件，分别上推每个条件。尽早选择这些条件有助于减少管道中剩余的记录数量。**
*   *****PushDownForEachFlatten*:**在计划中尽可能晚地应用 flattens，这将在复杂类型(如 tuple 或 bag)和记录中的其他字段之间产生叉积。这使得管道中的记录数量很少。**
*   *****ColumnPruner* :** 省略从不使用或不再需要的列，减小记录的大小。这可以在每个操作符之后应用，以便可以尽可能积极地修剪字段。**
*   *****MapKeyPruner*:**省略从不使用的映射键，减小记录的大小。**
*   *****limit optimizer*:**如果在加载或排序操作符之后立即应用 limit 操作符，Pig 会将加载或排序操作符转换为对限制敏感的实现，这不需要处理整个数据集。较早应用限制会减少记录的数量。**

**这只是优化过程的一部分。此外，它还通过执行**加入**、**命令，通过**功能执行**分组。****

**要关机，自动优化，可以执行这个命令:**

```
**pig -optimizer_off [opt_rule | all ]**
```

## **编译程序**

**在优化过程之后，编译器将优化后的代码编译成一系列 MapReduce 作业。编译器负责将 Pig 作业自动转换为 MapReduce 作业。**

## **执行引擎**

**最后，如图所示，这些 MapReduce 作业被提交给执行引擎执行。然后执行 MapReduce 作业并给出所需的结果。使用“ **DUMP** ”语句可将结果显示在屏幕上，并使用“ **STORE** ”语句可将结果存储在 HDFS 中。**

**在理解了架构之后，现在在这个 Apache Pig 教程中，我将向您解释 Pig Latins 的数据模型。**

# **Pig 拉丁数据模型**

**Pig Latin 的数据模型使 Pig 能够处理所有类型的数据。Pig Latin 可以处理两种原子数据类型，如 int、float、long、double 等。以及复杂的数据类型，如 tuple、bag 和 map。我将单独解释它们。下图显示了数据类型及其相应的类，我们可以使用这些类来实现它们:**

**![](img/54948ed7840dc436df1fbbd8add6928e.png)**

**Pig Data Types - Pig Tutorial**

## **原子/标量数据类型**

**原子或标量数据类型是所有语言中使用的基本数据类型，如 string、int、float、long、double、char[]、byte[]。这些也被称为原始数据类型。字段(列)中每个单元格的值都是原子数据类型，如下图所示。**

**对于字段，位置索引由系统自动生成(也称为**位置符号**)，用' $ '表示，从$0 开始，增长$1，$2，依此类推…与下图相比，$0 =序号，$1 =波段，$2 =成员，$3 =原点。**

**标量数据类型有“1”、“Linkin Park”、“7”、“California”等。**

**![](img/60c290c83b2bba90cde552e917e8a3a5.png)**

**Pig Latin Data Model - Pig Tutorial**

**现在我们将讨论猪拉丁语中的 ***复杂数据类型*** 即 Tuple、Bag 和 Map。**

## **元组**

**元组是一组有序的字段，每个字段可以包含不同的数据类型。您可以将它理解为存储在关系数据库的一行中的记录。一个元组是一行中的一组单元格，如上图所示。元组中的元素不一定需要附加模式。**

**元组由“()”符号表示。**

**元组示例(加利福尼亚州林肯公园 7 号 1 号)**

**由于元组是有序的，我们可以使用字段的索引来访问每个元组中的字段，比如元组上面的$1 form 将返回值‘Linkin Park’。您可以注意到，上面的元组没有附加任何模式。**

## **包**

**包是一组元组的集合，这些元组是表的行的子集或整行。一个包可以包含重复的元组，并且不强制要求它们必须是唯一的。**

**该包具有灵活的模式，即包内的元组可以具有不同数量的字段。一个包也可以有不同数据类型的元组。**

**一个包用“{}”符号表示。**

**一个包的例子 **{(加州林肯公园 7 号)，(金属乐队 8 号)，(洛杉矶超级死亡)}****

**但是为了让 Apache Pig 有效地处理包，这些字段和它们各自的数据类型需要处于相同的序列中。**

**一套包**

****{(林肯公园，7，加州)，(金属乐队，8)，(超级死亡，洛杉矶)}，****

****{(金属乐队，8 岁，洛杉矶)，(超级死神，8 岁)，(林肯公园，加州)}****

**袋子有两种类型，即**外袋**或关系袋和**内袋。****

**外部包或关系只不过是一个元组包。这里的关系类似于关系数据库中的关系。为了更好地理解它，让我们举一个例子:**

****{(加州林肯公园)，(洛杉矶金属乐队)，(洛杉矶超级死亡)}****

**上面的袋子解释了*乐队*和他们的原产地*T21 的关系。***

**另一方面，内包包含元组内的包。例如，如果我们根据*条带的来源*对*条带*元组进行排序，我们将得到:**

**(洛杉矶， **{(金属乐队，洛杉矶)，(超级死神，洛杉矶)}** )**

**(加州， **{(加州林肯公园)}** )**

**这里，第一个字段类型是字符串，而第二个字段类型是包，它是元组中的内部包。**

## **地图**

**![](img/d7c72e1035b3877c4a273a03401df44e.png)**

**Map Example - Pig Tutorial**

**映射是用于表示数据元素的键值对。该键必须是一个 char 数组[]，并且应该像列名一样是唯一的，这样就可以对其进行索引，并且可以根据键来访问与其相关联的值。该值可以是任何数据类型。**

**地图由“[]”符号表示，键值由“#”符号分隔，如上图所示。**

**地图示例-[乐队#Linkin Park，成员#7 ]，[乐队#Metallica，成员#8 ]**

**现在我们学习了 Pig Latin 的数据模型。我们将理解 Apache Pig 如何处理模式以及如何处理无模式数据。**

# **(计划或理论的)纲要**

**Schema 为字段指定一个名称，并声明字段的数据类型。该模式在 Pig Latin 中是可选的，但是 Pig 鼓励您尽可能使用它们，因为在解析脚本时，错误检查变得有效，从而导致程序的有效执行。模式可以声明为简单和复杂数据类型。在 LOAD function 过程中，如果声明了模式，它也会附加到数据上。**

**Pig 中关于图式的几点:**

*   **如果模式只包含字段名称，则字段的数据类型被视为字节数组。**
*   **如果为字段指定了名称，则可以通过字段名和位置符号来访问该字段。而如果缺少字段名，我们只能通过位置符号来访问它，即$后跟索引号。**
*   **如果您执行任何关系组合的操作(如 JOIN、COGROUP 等)。)并且如果任何关系缺少模式，则产生的关系将具有空模式。**
*   **如果模式为空，Pig 会将其视为一个字节数组，字段的实际数据类型将动态确定。**

**我希望这篇 Apache Pig 教程文章内容丰富，并且您喜欢它。在本文中，您了解了 Apache Pig 的基础知识、它的数据模型和它的架构。Twitter 案例研究可以帮助你更好地沟通。**

**到此，我们结束这篇关于猪的文章。**

**如果你想查看更多关于人工智能、Python、道德黑客等市场最热门技术的文章，你可以参考 Edureka 的官方网站。**

**请留意本系列中解释大数据其他各方面的其他文章。**

> **1. [Hadoop 教程](/edureka/hadoop-tutorial-24c48fbf62f6)**
> 
> **2.[蜂巢教程](/edureka/hive-tutorial-b980dfaae765)**
> 
> **3.[大数据教程](/edureka/big-data-tutorial-b664da0bb0c8)**
> 
> **4.[地图缩小教程](/edureka/mapreduce-tutorial-3d9535ddbe7c)**
> 
> **5. [HBase 教程](/edureka/hbase-tutorial-bdc36ab32dc0)**
> 
> **6. [HDFS 教程](/edureka/hdfs-tutorial-f8c4af1c8fde)**
> 
> **7. [Hadoop 3](/edureka/hadoop-3-35e7fec607a)**
> 
> **8. [Sqoop 教程](/edureka/apache-sqoop-tutorial-431ed0af69ee)**
> 
> **9.[水槽教程](/edureka/apache-flume-tutorial-6f7150210c76)**
> 
> **10. [Oozie 教程](/edureka/apache-oozie-tutorial-d8f7bbbe1591)**
> 
> **11. [Hadoop 生态系统](/edureka/hadoop-ecosystem-2a5fb6740177)**
> 
> **12.[HQL 顶级配置单元命令及示例](/edureka/hive-commands-b70045a5693a)**
> 
> **13. [Hadoop 集群搭配亚马逊 EMR？](/edureka/create-hadoop-cluster-with-amazon-emr-f4ce8de30fd)**
> 
> **14.[大数据工程师简历](/edureka/big-data-engineer-resume-7bc165fc8d9d)**
> 
> **15. [Hadoop 开发人员-工作趋势和薪水](/edureka/hadoop-developer-cc3afc54962c)**
> 
> **16. [Hadoop 面试问题](/edureka/hadoop-interview-questions-55b8e547dd5c)**

***原载于 2016 年 11 月 28 日 www.edureka.co**T21*[。](https://www.edureka.co/blog/pig-tutorial/)**