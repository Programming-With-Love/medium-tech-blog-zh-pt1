<html>
<head>
<title>Detecting image similarity using Spark, LSH and TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用火花、LSH和张量流检测图像相似性</h1>
<blockquote>原文：<a href="https://medium.com/pinterest-engineering/detecting-image-similarity-using-spark-lsh-and-tensorflow-618636afc939?source=collection_archive---------0-----------------------#2018-06-15">https://medium.com/pinterest-engineering/detecting-image-similarity-using-spark-lsh-and-tensorflow-618636afc939?source=collection_archive---------0-----------------------#2018-06-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="5009" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">安德烈·古塞夫，Pinterest工程师，内容质量</p><p id="be06" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为一个视觉平台，从图像中学习以理解我们的内容的能力很重要。为了检测近似重复的图像，我们使用NearDup系统，一个基于火花和张量流的管道。流水线的核心是批量<a class="ae jc" href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" rel="noopener ugc nofollow" target="_blank"> LSH </a>(位置敏感散列)搜索的Spark实现和基于TensorFlow的分类器。每天，管道都会比较数十亿个项目，并增量更新集群。在本帖中，我们将解释我们如何使用这项技术来更好地理解图像，并提高推荐和搜索结果的准确性和密度。</p><h1 id="4126" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">概观</h1><p id="9bf3" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">我们将我们的图像世界划分为几乎相同的图像类别(如人类观察者所感知的)。虽然这个概念有些主观，但下面的一组图像对会让你知道什么属于接近阈值。请注意，图像可能不一定来自相同的源照片(见右下方的示例图像)或具有相同的背景(见左下方)。它可能会有明显的几何扭曲(见左上)，或者可能是旋转、裁剪或翻转变体(见中间和右上)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/1ad540ea0d99439eb74134e3348e6ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rwAwck7JXEF0KxRO"/></div></div></figure><p id="6204" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在图像的宇宙中寻找一个理想的划分在数学上是不明确的，因为邻近关系不是传递的，因此不是等价关系。为了说明这一点，想象一下经过1000次迭代，慢慢地将一只猫的图像变形为一只狗的图像。预计每一次迭代都将落在NearDup阈值内，但如何划分序列尚不清楚:是有一群猫、一群狗还是一群猫狗？我们使用所选候选对象上的传递闭包和贪婪k-切割的组合来找到最小化图中k-切割的划分的近似。在图中，边代表图像的相似性，节点代表图像。</p><h1 id="12c4" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">使用批处理LSH的候选生成</h1><h2 id="1f83" class="ks je hh bd jf kt ku kv jj kw kx ky jn ip kz la jr it lb lc jv ix ld le jz lf bi translated">嵌入和LSH术语</h2><p id="bd7c" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">为了理解图像的内容，我们将图像映射到嵌入的向量空间中。视觉嵌入是捕捉视觉和语义相似性的图像的高维向量表示。它们通常通过神经网络架构产生，如VGG16和Inception。为了通过NearDup关系对图像进行聚类，我们每天都要将数千万张新图像与数十亿个现有聚类进行比较。在没有优化的情况下进行这样的最近邻搜索将产生二次时间复杂度和与超过10万亿次成比例的运行时间(即16个零！)图片对比。取而代之的是，我们使用LSH项来简化图像的嵌入表示，以极大地改善这类问题的易处理性。</p><p id="3718" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">LSH是一种现代技术，用于降低高维数据的维数，同时保留各个点之间的成对距离。通过基于<a class="ae jc" href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Random_projection" rel="noopener ugc nofollow" target="_blank">随机投影</a>和<a class="ae jc" href="http://mlwiki.org/index.php/Bit_Sampling_LSH" rel="noopener ugc nofollow" target="_blank">比特采样LSH </a>的过程，我们首先降低原始空间的维度。接下来，在从根本上权衡检测概率和运行时间的过程中，导出的比特被分组为LSH项。分组越小，运行最近邻搜索的计算开销就越大，但这增加了准确检测的概率。这个过程使用LSH项和它们的Jaccard重叠作为原始嵌入空间中向量之间的余弦相似性的近似。</p><h2 id="bf82" class="ks je hh bd jf kt ku kv jj kw kx ky jn ip kz la jr it lb lc jv ix ld le jz lf bi translated">批量LSH搜索</h2><p id="f117" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">每个图像都由一组LSH术语表示，我们继续建立倒排索引，并实现面向批量的图像搜索。在高层次上，我们使用函数转换、压缩的倒排索引和连接的组合来一次计算所有查询图像的结果集。流水线是在Spark中实现的，需要一系列的优化来确保我们能够处理数据量，即使是在计算上更易处理的LSH项空间中。一些优化包括:</p><ul class=""><li id="ba1b" class="lg lh hh ig b ih ii il im ip li it lj ix lk jb ll lm ln lo bi translated"><strong class="ig hi">字典编码</strong>这样一切都通过尽可能最小宽度的数字原语进行编码</li><li id="1597" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb ll lm ln lo bi translated"><strong class="ig hi">可变字节编码</strong>用于所有倒排索引</li><li id="9d43" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb ll lm ln lo bi translated"><strong class="ig hi">索引分区</strong>改善了倒排索引的平衡</li><li id="6361" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb ll lm ln lo bi translated"><strong class="ig hi">基于成本的优化器</strong>检测嵌入式空间的密度，并确定最佳运行时参数</li><li id="2a43" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb ll lm ln lo bi translated"><strong class="ig hi">原始数据打包</strong>进一步提高内存利用率</li><li id="dad3" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb ll lm ln lo bi translated"><strong class="ig hi"> Jaccard重叠计数</strong>是通过低级别、高性能的收集完成的</li><li id="d64e" class="lg lh hh ig b ih lp il lq ip lr it ls ix lt jb ll lm ln lo bi translated"><strong class="ig hi">关闭堆积</strong>减少GC开销</li></ul><h1 id="a403" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">使用迁移学习的候选人选择</h1><p id="b15c" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">批量LSH是一种在最小化计算成本的同时产生高召回率的有效方法。但是，它通常不会产生最佳的精确度和候选人排名。我们通过监督分类器对生成的候选项进行传递，以选择足够相似的候选项，从而被认为是接近的。分类器是基于视觉嵌入的迁移学习的一个例子。它使用TensorFlow前馈网络和AdamOptimizer。我们已经在包含超过10亿个不同对的集合上训练了分类器。训练集是从具有几何验证的SURF视觉特征上的决策树分类器的输出中导出的，其被用于NearDup系统的先前迭代中。为了改进每对图像的学习和收敛，汉明比特(从视觉嵌入中得到)被异或并被馈送到输入层。该分类器被调整为高精度，并在人类标记的数据上实现了超过99%的精度。</p><p id="c2fb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">经过训练的网络上的推断也发生在Spark上下文中。使用mapPartitions和分组范式，我们可以使用大批量的预定义大小来有效地向量化和减少开销。借助一个拥有近1000万个参数的网络，我们在一个r 3.8 x大型机器集群上实现了平均2毫秒/次的预测。</p><h1 id="d4c0" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结论</h1><p id="4c59" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">接近检测需要计算成本很高的成对比较。通过利用Spark中的批量LSH实现，我们跳过了不可能相似的图像对，从而大大降低了计算复杂度。基于Spark的实现结合了高效的工作负载分配和低级优化，以最大限度地减少内存和CPU占用。随后的微调步骤使用受监督的前馈网络来选择和排列高于NearDup相似性阈值的图像对。Spark和TensorFlow推理的结合使用了最好的分布式计算和每个内核的矢量化，以实现高吞吐量和低预测延迟。这两个步骤的结果然后被用于聚类图像，这有助于每天在Pinterest上提供数百亿个搜索结果和推荐。</p><p id="40b2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lu">关于这个话题的更多信息，请查看我在</em> <a class="ae jc" href="https://databricks.com/session/image-similarity-detection-at-scale-using-lsh-and-tensorflow" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> Spark+AI峰会2018 </em> </a> <em class="lu">下面的</em></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="98ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"/></p></div></div>    
</body>
</html>