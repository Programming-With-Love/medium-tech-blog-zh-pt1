<html>
<head>
<title>A Comprehensive Guide To Spark SQL With Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用示例激发SQL的全面指南</h1>
<blockquote>原文：<a href="https://medium.com/edureka/spark-sql-tutorial-6de1e241bf76?source=collection_archive---------1-----------------------#2017-01-02">https://medium.com/edureka/spark-sql-tutorial-6de1e241bf76?source=collection_archive---------1-----------------------#2017-01-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/13bdf098e434f2b0812c93312a4e15fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*HKbaeZmCjXt4_7jkaQjFrA.png"/></div><figcaption class="il im et er es in io bd b be z dx">Spark SQL Tutorial — Edureka</figcaption></figure><p id="db9c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Apache Spark是一个为快速计算而设计的快如闪电的集群计算框架。这是Apache软件基金会最成功的项目之一。随着大数据生态系统中实时处理框架的出现，公司正在他们的解决方案中严格使用Apache Spark，因此这增加了对拥有<strong class="ir hi"> <em class="jn"> Apache Spark培训</em> </strong>的专业人员的需求。Spark SQL是Spark中的一个新模块，它将关系处理与Spark的函数式编程API集成在一起。它支持通过SQL或Hive查询语言查询数据。</p><p id="cfcb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于那些熟悉RDBMS的人来说，Spark SQL将是从早期工具的简单过渡，在早期工具中，您可以扩展传统关系数据处理的边界。通过这篇文章，我将向您介绍Spark SQL这个令人兴奋的新领域，我们将一起装备自己，带领我们的组织利用关系处理的优势，并在Spark中调用复杂的分析库。</p><p id="df23" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以下提供了博客的故事情节:</p><ol class=""><li id="d443" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">为什么Spark SQL会出现？</li><li id="6cd6" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Spark SQL概述</li><li id="f71c" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Spark SQL库</li><li id="794f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Spark SQL的特性</li><li id="b75c" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">使用Spark SQL进行查询</li><li id="4c71" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">将模式添加到rdd</li><li id="c99c" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">RDDs作为关系</li><li id="762b" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">在内存中缓存表</li></ol><h1 id="4280" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">为什么Spark SQL会出现？</h1><p id="4f2c" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark SQL起源于Apache Hive，运行在Spark之上，现在与Spark堆栈集成在一起。Apache Hive有一些限制，如下所述。Spark SQL的建立就是为了克服这些缺点，取代Apache Hive。</p><h2 id="7165" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">Hive的限制:</h2><ul class=""><li id="97ad" class="jo jp hh ir b is la iw lb ja lt je lu ji lv jm lw ju jv jw bi translated">配置单元在内部启动MapReduce作业以执行即席查询。在分析中等大小的数据集(10到200 GB)时，MapReduce的性能会有所滞后。</li><li id="66d6" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">配置单元没有恢复功能。这意味着，如果处理在工作流中终止，您将无法从它停滞的地方恢复。</li><li id="90c9" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">启用回收站时，配置单元无法在级联中删除加密的数据库，这将导致执行错误。为了克服这一点，用户必须使用清除选项来跳过垃圾而不是丢弃。</li></ul><p id="5f7f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这些缺点让位于Spark SQL的诞生。</p><h1 id="fa46" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Spark SQL概述</h1><p id="61a6" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark SQL将关系处理与Spark的函数式编程集成在一起。它提供了对各种数据源的支持，并使得将SQL查询与代码转换结合起来成为可能，从而产生了一个非常强大的工具。</p><p id="2146" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们探索一下，Spark SQL提供了什么。Spark SQL模糊了RDD和关系表之间的界限。通过与Spark代码集成的声明性数据框架API，它在关系处理和过程处理之间提供了更紧密的集成。它还提供了更高的优化。DataFrame API和Datasets API是与Spark SQL交互的方式。</p><p id="0197" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有了Spark SQL，Apache Spark可供更多用户访问，并改进了对现有用户的优化。Spark SQL提供了DataFrame APIs，可以对外部数据源和Spark内置的分布式集合执行关系操作。它引入了一个名为Catalyst的可扩展优化器，因为它有助于支持大数据中的各种数据源和算法。</p><p id="9312" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark可以在Windows和类UNIX系统(例如Linux、微软、Mac OS)上运行。在一台机器上本地运行很容易——您所需要的就是在系统路径上安装java，或者JAVA_HOME环境变量指向JAVA安装。</p><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es lx"><img src="../Images/eff78000aa4ac6535fca7110e82bd7e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1bmqXoGDlLIXN1ixuea1zw.png"/></div></div></figure><h1 id="0490" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Spark SQL库</h1><p id="f158" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark SQL有以下四个库，用于与关系和过程处理进行交互:</p><h2 id="b4c9" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">数据源API(应用程序编程接口):</h2><p id="a81d" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">这是一个加载和存储结构化数据的通用API。</p><ul class=""><li id="aef3" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lw ju jv jw bi translated">它内置了对Hive、Avro、JSON、JDBC、Parquet等的支持。</li><li id="b773" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">通过Spark包支持第三方集成。</li><li id="797d" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">支持智能源。</li><li id="256a" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">它是一种数据抽象和领域特定语言(DSL ),适用于结构化和半结构化数据。</li></ul><h2 id="36a8" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">数据框架API:</h2><ul class=""><li id="7349" class="jo jp hh ir b is la iw lb ja lt je lu ji lv jm lw ju jv jw bi translated">DataFrame API是以命名的列和行的形式的分布式数据集合。</li><li id="6edd" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">它像Apache Spark转换一样被延迟评估，并且可以通过SQL上下文和Hive上下文访问。</li><li id="d488" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">它处理单节点集群到多节点集群上千字节到千兆字节大小的数据。</li><li id="6785" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">支持不同的数据格式(Avro，CSV，Elastic Search，和Cassandra)和存储系统(HDFS，HIVE Tables，MySQL等。).</li><li id="2851" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">可以通过Spark-Core轻松与所有大数据工具和框架集成。</li><li id="2b89" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">为Python、Java、Scala和R编程提供API。</li><li id="4d95" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">数据帧是组织到命名列中的数据的分布式集合。它相当于SQL中用于将数据存储到表中的关系表。</li></ul><h2 id="e306" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">SQL解释器和优化器:</h2><p id="8594" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">SQL解释器和优化器基于Scala中构造的函数式编程。</p><ul class=""><li id="7f4c" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lw ju jv jw bi translated">它是SparkSQL最新的、技术上最先进的组件。</li><li id="f24e" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">它为转换树提供了一个通用框架，用于执行分析/评估、优化、规划和运行时代码生成。</li><li id="6844" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">这支持基于成本的优化(运行时间和资源利用率称为成本)和基于规则的优化，使查询运行速度比RDD(弹性分布式数据集)快得多。</li></ul><p id="92db" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">例如，Catalyst是一个模块化库，它是一个基于规则的系统。框架中的每条规则都侧重于不同的优化。</p><h2 id="28a0" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">SQL服务:</h2><p id="47b8" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">SQL服务是Spark中处理结构化数据的入口点。它允许创建DataFrame对象以及执行SQL查询。</p><h1 id="93df" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Spark SQL的特性</h1><p id="3980" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">以下是Spark SQL的特性:</p><h2 id="d385" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">与Spark集成</h2><p id="1387" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark SQL查询与Spark程序集成在一起。Spark SQL允许我们使用SQL或可在Java、Scala、Python和r中使用的DataFrame API来查询Spark程序中的结构化数据。要运行流计算，开发人员只需针对DataFrame / Dataset API编写一个批处理计算，Spark就会自动增加计算，以流方式运行它。这种强大的设计意味着开发人员不必手动管理状态、故障或保持应用程序与批处理作业同步。相反，流式作业总是对相同的数据给出与批处理作业相同的答案。</p><h2 id="7492" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">统一数据访问</h2><p id="dc4a" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">DataFrames和SQL支持访问各种数据源的通用方法，如Hive、Avro、Parquet、ORC、JSON和JDBC。这将连接这些来源的数据。这对于将所有现有用户纳入Spark SQL非常有帮助。</p><h2 id="ed02" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">蜂巢兼容性</h2><p id="7875" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark SQL对当前数据运行未修改的配置单元查询。它重写了配置单元前端和元存储，允许与当前配置单元数据、查询和UDF完全兼容。</p><h2 id="fa32" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">标准连接</h2><p id="89c9" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">这种连接是通过JDBC或ODBC实现的。JDBC和ODBC是商业智能工具连接的行业规范。</p><h2 id="f4fe" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">性能和可扩展性</h2><p id="66b6" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark SQL整合了基于成本的优化器、代码生成和列存储，使查询更加灵活，同时使用Spark引擎计算数千个节点，提供了完整的中间查询容错。Spark SQL提供的接口为Spark提供了更多关于数据结构和正在执行的计算的信息。在内部，Spark SQL使用这些额外的信息来执行额外的优化。Spark SQL可以直接从多个源读取(文件、HDFS、JSON/Parquet文件、现有rdd、Hive等。).它确保现有配置单元查询的快速执行。</p><p id="c5b3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下图描述了Spark SQL与Hadoop相比的性能。Spark SQL的执行速度比Hadoop快100倍。</p><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/55e927ef578c70816a9d41035b9c83dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6lkr07xrNoA2zeLM5p7yKw.png"/></div></div></figure><h2 id="025d" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">用户定义的函数</h2><p id="3b97" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark SQL有语言集成的用户定义函数(UDF)。UDF是Spark SQL的一个特性，它定义了新的基于列的函数，扩展了Spark SQL用于转换数据集的DSL词汇。UDF在执行时是黑盒。</p><p id="274e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面的例子定义了一个将给定文本转换成大写字母的UDF。</p><p id="b31b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代号解释:</strong> <br/> 1。创建数据集“hello world”<br/>2。定义一个将字符串转换成大写字母的函数“upper”。<br/> 3。我们现在将“udf”包导入Spark。<br/> 4。定义我们的UDF，“upperUDF”并导入我们的函数“upper”。<br/> 5。在新列“upper”中显示用户定义函数的结果。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="0312" class="lf kd hh mi b fi mm mn l mo mp">val dataset = Seq((0, "hello"),(1, "world")).toDF("id","text")<br/>val upper: String =&amp;amp;amp;amp;amp;gt; String =_.toUpperCase<br/>import org.apache.spark.sql.functions.udf<br/>val upperUDF = udf(upper)<br/>dataset.withColumn("upper", upperUDF('text)).show</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/b5443cf1dfb13f8d5652103a4226d552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_kj9w4y31pTSPQDVOo-0A.png"/></div></div></figure><p id="91f7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。我们现在将我们的函数注册为‘my upper’<br/>2。在其他功能中编目我们的UDF。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="6732" class="lf kd hh mi b fi mm mn l mo mp">spark.udf.register("myUpper", (input:String) =&amp;amp;amp;amp;amp;gt; input.toUpperCase)<br/>spark.catalog.listFunctions.filter('name like "%upper%").show(false)</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/9a8e96e01e04e4731621fb91d33bf053.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w1_bya6HiVJp0qkGp1Wy6g.png"/></div></div></figure><h1 id="116b" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">使用Spark SQL进行查询</h1><p id="a3de" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">我们现在将开始使用Spark SQL进行查询。注意，实际的SQL查询类似于流行的SQL客户机中使用的查询。</p><p id="c331" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">启动火花外壳。转到Spark目录并执行。将终端中的/bin/spark-shell设置为Spark Shell。</p><p id="54b8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">对于博客中显示的查询示例，我们将使用两个文件，“employee.txt”和“employee.json”。下图显示了这两个文件的内容。这两个文件都存储在包含spark安装的文件夹(~/Downloads/Spark-2 . 0 . 2-bin-Hadoop 2.7)内的“examples/src/main/Scala/org/Apache/Spark/examples/SQL/sparksqlexample . Scala”中。所以，所有执行查询的人，把它们放在这个目录中，或者在下面的代码行中设置文件的路径。</p><figure class="ly lz ma mb fd ii er es paragraph-image"><div class="er es mq"><img src="../Images/f8de61cd211f8135d6408292501ee56d.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*gkxIe_m5Hg-hO1Lvx1e1-Q.png"/></div></figure><figure class="ly lz ma mb fd ii er es paragraph-image"><div class="er es mq"><img src="../Images/8b0171f46338fe9bd73e50701e8daf8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*HTK-qMOYJnGG9hQh6TnWBg.png"/></div></figure><p id="c994" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代号解释:</strong> <br/> 1。我们首先将Spark会话导入Apache Spark。<br/> 2。使用“builder()”函数创建spark会话“Spark”。<br/> 3。将Implicts类导入我们的“spark”会话。<br/> 4。我们现在创建一个数据框架‘df’并从‘employee . JSON’文件导入数据。<br/> 5。显示数据帧“df”。结果是来自我们的“employee.json”文件的5行年龄和姓名的表格。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="91de" class="lf kd hh mi b fi mm mn l mo mp">import org.apache.spark.sql.SparkSession<br/>val spark = SparkSession.builder().appName("Spark SQL basic example").config("spark.some.config.option", "some-value").getOrCreate()<br/>import spark.implicits._<br/>val df = spark.read.json("examples/src/main/resources/employee.json")<br/>df.show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/fe14e435df0716a7f4f6731b9d2d7a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W6q5GTYgckxSR2-GXhPo2A.png"/></div></div></figure><p id="6804" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。将Implicts类导入我们的“spark”会话。<br/> 2。打印我们的“df”数据帧的模式。<br/> 3。显示“df”数据帧中所有记录的名称。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="629e" class="lf kd hh mi b fi mm mn l mo mp">import spark.implicits._<br/>df.printSchema()<br/>df.select("name").show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/73feaa27f83d396c31500f2c185c8fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LqiBtSKQai4AnRygOU1HCA.png"/></div></div></figure><p id="69a1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。在每个人的年龄增加两岁后显示数据帧。<br/> 2。我们筛选所有30岁以上的雇员并显示结果。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="cbe5" class="lf kd hh mi b fi mm mn l mo mp">df.select($"name", $"age" + 2).show()<br/>df.filter($"age" &amp;amp;amp;amp;amp;gt; 30).show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/5a96e2df850a2817c36ce417e3b3da0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mhj_d7ZEZT7AjzNddhpxaQ.png"/></div></div></figure><p id="7ba8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。计算年龄相同的人的数量。同样，我们使用“分组”函数。<br/> 2。为我们的“df”数据框架创建临时视图“employee”。<br/> 3。在我们的“雇员”视图上执行“选择”操作，将表显示到“sqlDF”中。<br/> 4。显示“sqlDF”的结果。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="3c06" class="lf kd hh mi b fi mm mn l mo mp">df.groupBy("age").count().show()<br/>df.createOrReplaceTempView("employee")<br/>val sqlDF = spark.sql("SELECT * FROM employee")<br/>sqlDF.show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/d61c413c505591ac234bf14dc52d0c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-E8EJ24soKXV18Ek3nO3zg.png"/></div></div></figure><h2 id="bb02" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">创建数据集</h2><p id="3602" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在理解了数据帧之后，现在让我们转到数据集API。以下代码在SparkSQL中创建了一个数据集类。</p><p id="ec07" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。创建一个“Employee”类来存储雇员的姓名和年龄。<br/> 2。分配数据集“caseClassDS”来存储Andrew的记录。<br/> 3。显示数据集“caseClassDS”。<br/> 4。创建一个原始数据集来演示数据帧到数据集的映射。<br/> 5。将上述序列赋给一个数组。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="f85d" class="lf kd hh mi b fi mm mn l mo mp">case class Employee(name: String, age: Long)<br/>val caseClassDS = Seq(Employee("Andrew", 55)).toDS()<br/>caseClassDS.show()<br/>val primitiveDS = Seq(1, 2, 3).toDS<br/>()primitiveDS.map(_ + 1).collect()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/3a5d86d63574e8d4f77dd86647187d6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QVEY7QyaKqQqjlAb7ogYiw.png"/></div></div></figure><p id="200d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。设置JSON文件“employee.json”的路径。<br/> 2。从文件创建数据集。<br/> 3。显示“employeeDS”数据集的内容。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="25a2" class="lf kd hh mi b fi mm mn l mo mp">val path = "examples/src/main/resources/employee.json"<br/>val employeeDS = spark.read.json(path).as[Employee]<br/>employeeDS.show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/5ded9c0ea020af4e02662cd5e8a1c697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9D4SFrNdP4wuxKfmNQcIw.png"/></div></div></figure><h1 id="acb2" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">将模式添加到rdd</h1><p id="9773" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark引入了RDD(Resilient Distributed Dataset)的概念，这是一个不可变的容错分布式对象集合，可以并行操作。RDD可包含任何类型的对象，通过加载外部数据集或从驱动程序分发集合来创建。</p><p id="eb73" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">模式RDD是一个RDD，您可以在其中运行SQL。它不仅仅是SQL。它是结构化数据的统一接口。</p><p id="f7df" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代号解释:</strong> <br/> 1。正在为rdd导入表达式编码器。rdd类似于数据集，但是使用编码器进行序列化。<br/> 2。将编码器库导入外壳。<br/> 3。将Implicts类导入我们的“spark”会话。<br/> 4。从“employee.txt”创建“employeeDF”数据帧，并将基于分隔符逗号“，”的列映射到临时视图“employee”中。<br/> 5。正在创建临时视图“员工”。<br/> 6。定义一个数据框架“youngstersDF ”,它将包含年龄在18到30岁之间的所有雇员。<br/> 7。将RDD中的姓名映射到“youngstersDF”中，以显示年轻人的姓名。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="03c9" class="lf kd hh mi b fi mm mn l mo mp">import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder<br/>import org.apache.spark.sql.Encoder<br/>import spark.implicits._<br/>val employeeDF = spark.sparkContext.textFile("examples/src/main/resources/employee.txt").map(_.split(",")).map(attributes =&amp;amp;amp;amp;amp;gt; Employee(attributes(0), attributes(1).trim.toInt)).toDF()<br/>employeeDF.createOrReplaceTempView("employee")<br/>val youngstersDF = spark.sql("SELECT name, age FROM employee WHERE age BETWEEN 18 AND 30")<br/>youngstersDF.map(youngster =&amp;amp;amp;amp;amp;gt; "Name: " + youngster(0)).show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/6aa1ca0dfd986591f39703bb507655cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9de8Ac8cksHCd87UDaI3yw.png"/></div></div></figure><p id="17d9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。将映射的名称转换为用于转换的字符串。<br/> 2。使用Implicits类中的mapEncoder将姓名映射到年龄。<br/> 3。将名字映射到我们的“youngstersDF”数据框架的年龄。结果是一个数组，其名称映射到各自的年龄。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="def5" class="lf kd hh mi b fi mm mn l mo mp">youngstersDF.map(youngster =&amp;amp;amp;amp;amp;gt; “Name: “ + youngster.getAs[String](“name”)).show()<br/>implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]<br/>youngstersDF.map(youngster =&amp;amp;amp;amp;amp;gt; youngster.getValuesMap[Any](List(“name”, “age”))).collect()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/a1a9e7bd6a8cfcac4f8a75ff672ffdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Foj2vQh4lwWrngg_r4nBXA.png"/></div></div></figure><p id="86cb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">rdd支持两种类型的操作:</p><ul class=""><li id="0669" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm lw ju jv jw bi translated">变换:这些是对RDD执行的操作(如映射、过滤、连接、联合等),可生成包含结果的新RDD。</li><li id="3a2f" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm lw ju jv jw bi translated">操作:这些操作(如reduce、count、first等)在对RDD运行计算后返回值。</li></ul><p id="9a36" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Spark中的转换是“懒惰的”，这意味着它们不会立即计算出结果。相反，它们只是“记住”要执行的操作和要对其执行操作的数据集(例如，文件)。仅当调用动作时才计算转换，并且结果被返回到驱动程序并存储为有向非循环图(DAG)。这种设计使得Spark的运行效率更高。例如，如果一个大文件以各种方式转换并传递给第一个动作，Spark将只处理并返回第一行的结果，而不是处理整个文件。</p><figure class="ly lz ma mb fd ii er es paragraph-image"><div class="er es mr"><img src="../Images/ae9c28b3a5513c4b5490fd23dd5bb0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*wJOYOCCrZ0smzkbLonJGwg.png"/></div></figure><p id="8a45" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">默认情况下，每次对变换后的RDD执行操作时，都会对其进行重新计算。但是，您也可以使用persist或cache方法将RDD持久化到内存中，在这种情况下，Spark会将元素保留在集群中，以便下次查询时可以更快地访问。</p><h1 id="4323" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">RDDs作为关系</h1><p id="9e60" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">弹性分布式数据集(rdd)是分布式内存抽象，它允许程序员以容错方式在大型集群上执行内存计算。rdd可以从任何数据源创建。例如:Scala集合，本地文件系统，Hadoop，亚马逊S3，HBase表等。</p><h2 id="74e1" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">指定模式</h2><p id="1c1b" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated"><strong class="ir hi">代号解释:</strong> <br/> 1。将“类型”类导入Spark Shell。<br/> 2。将“Row”类导入Spark Shell。行用于映射RDD方案。<br/> 3。从文本文件“employee.txt”创建RDD“employeeRDD”。<br/> 4。将模式定义为“名称年龄”。这用于绘制RDD的列。<br/> 5。定义“字段”RDD，它将是将“employeeRDD”映射到架构“schemaString”后的输出。<br/> 6。正在将“字段”的类型RDD获取到“架构”中。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="e739" class="lf kd hh mi b fi mm mn l mo mp">import org.apache.spark.sql.types._<br/>import org.apache.spark.sql.Row<br/>val employeeRDD = spark.sparkContext.textFile(“examples/src/main/resources/employee.txt”)<br/>val schemaString = “name age”<br/>val fields = schemaString.split(“ “).map(fieldName =&amp;amp;amp;amp;amp;gt; StructField(fieldName, StringType, nullable = true))<br/>val schema = StructType(fields)</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/f84f0e30c29b32e927b85ff1e81f2687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FtnwQAvD-DJc1vkoC3ZNUg.png"/></div></div></figure><p id="9f32" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。我们现在创建一个名为“rowRDD”的RDD，并使用“map”函数将“employeeRDD”转换为“rowRDD”。<br/> 2。我们定义了一个数据帧‘employeeDF ’,并将RDD模式存储在其中。<br/> 3。将“employeeDF”的临时视图创建为“employee”。<br/> 4。对“雇员”执行SQL操作以显示雇员的内容。<br/> 5。从“员工”视图显示上一道工序的名称。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="0b1f" class="lf kd hh mi b fi mm mn l mo mp">val rowRDD = employeeRDD.map(_.split(",")).map(attributes =&amp;amp;amp;amp;amp;gt; Row(attributes(0), attributes(1).trim))<br/>val employeeDF = spark.createDataFrame(rowRDD, schema)<br/>employeeDF.createOrReplaceTempView("employee")<br/>val results = spark.sql("SELECT name FROM employee")<br/>results.map(attributes =&amp;amp;amp;amp;amp;gt; "Name: " + attributes(0)).show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/ce23ae8934753fe5c325b2520ec65e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sl54v0F_mOajggoNrVQhSQ.png"/></div></div></figure><p id="d90b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">即使定义了rdd，它们也不包含任何数据。在RDD中创建数据的计算仅在引用数据时进行。例如缓存结果或写出RDD。</p><h1 id="3f6d" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">在内存中缓存表</h1><p id="4159" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">Spark SQL使用内存中的列格式缓存表:</p><ol class=""><li id="f677" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">仅扫描必需的列</li><li id="44f6" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">更少的分配对象</li><li id="92ac" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">自动选择最佳比较</li></ol><h2 id="a213" class="lf kd hh bd ke lg lh li ki lj lk ll km ja lm ln kq je lo lp ku ji lq lr ky ls bi translated">以编程方式加载数据</h2><p id="71a4" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">下面的代码将读取employee.json文件并创建一个DataFrame。然后，我们将使用它来创建一个拼花文件。</p><p id="2e00" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。将隐式类导入外壳。<br/> 2。从我们的“employee.json”文件创建“employeeDF”数据框架。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="4575" class="lf kd hh mi b fi mm mn l mo mp">import spark.implicits._<br/>val employeeDF = spark.read.json(“examples/src/main/resources/employee.json”)</span></pre><p id="9d69" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代号解释:</strong> <br/> 1。为我们的数据帧创建一个“parquetFile”临时视图。<br/> 2。从我们的拼花文件中选择年龄在18到30岁之间的人的名字。<br/> 3。显示Spark SQL操作的结果。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="f5a4" class="lf kd hh mi b fi mm mn l mo mp">employeeDF.write.parquet("employee.parquet")<br/>val parquetFileDF = spark.read.parquet("employee.parquet")<br/>parquetFileDF.createOrReplaceTempView("parquetFile")<br/>val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 18 AND 30")<br/>namesDF.map(attributes =&amp;amp;amp;amp;amp;gt; "Name: " + attributes(0)).show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/12b0b7e3113337e28abc43330ed9cd8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUljhdHLP2j42c4aqiAhOw.png"/></div></div></figure><h1 id="0705" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">JSON数据集</h1><p id="c86e" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">我们现在将处理JSON数据。由于Spark SQL支持JSON数据集，我们创建了employee.json的数据帧。然后，我们定义一个年轻人数据框架，并添加所有年龄在18到30岁之间的员工。</p><p id="36c7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。将设置为“employee.json”文件的路径。<br/> 2。从我们的JSON文件创建一个DataFrame 'employeeDF'。<br/> 3。正在打印“employeeDF”的架构。<br/> 4。将数据框架的临时视图创建为“employee”。<br/> 5。定义一个数据帧“youngsterNamesDF ”,它存储“employee”中所有年龄在18到30岁之间的雇员的姓名。<br/> 6。显示数据框的内容。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="4562" class="lf kd hh mi b fi mm mn l mo mp">val path = "examples/src/main/resources/employee.json"<br/>val employeeDF = spark.read.json(path)<br/>employeeDF.printSchema()<br/>employeeDF.createOrReplaceTempView("employee")<br/>val youngsterNamesDF = spark.sql("SELECT name FROM employee WHERE age BETWEEN 18 AND 30")<br/>youngsterNamesDF.show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/3af3ddd8d365f937ae94153222c1baa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mCHiJ1eGOOSfd7v1ZoRO6A.png"/></div></div></figure><p id="f237" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。创建一个RDD“otherEmployeeRDD ”,它将存储来自新德里的雇员George的内容。<br/> 2。将“otherEmployeeRDD”的内容分配给“otherEmployee”。<br/> 3。显示“其他员工”的内容。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="1dcb" class="lf kd hh mi b fi mm mn l mo mp">val otherEmployeeRDD = spark.sparkContext.makeRDD(“””{“name”:”George”,”address”:{“city”:”New Delhi”,”state”:”Delhi”}}””” :: Nil)<br/>val otherEmployee = spark.read.json(otherEmployeeRDD)<br/>otherEmployee.show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/af3af0f6045af5f9ba9cc94f4e99aeb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGTrzlaUBoGvh1ycZHjCxw.png"/></div></div></figure><h1 id="ce97" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">蜂巢餐桌</h1><p id="036f" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">我们使用Hive表执行一个Spark示例。</p><p id="7b75" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代号解释:</strong> <br/> 1。将“Row”类导入Spark Shell。行用于映射RDD方案。<br/> 2。将Spark会话导入shell。<br/> 3。创建属性为Int和String的类“Record”。<br/> 4。将“仓库位置”的位置设置为Spark warehouse。<br/> 5。我们现在构建一个Spark会话‘Spark’来演示Spark SQL中的Hive示例。<br/> 6。将隐式类导入外壳。<br/> 7。将SQL库导入Spark Shell。<br/> 8。创建包含存储键和值的列的表“src”。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="ca1e" class="lf kd hh mi b fi mm mn l mo mp">import org.apache.spark.sql.Row<br/>import org.apache.spark.sql.SparkSession<br/>case class Record(key: Int, value: String)<br/>val warehouseLocation = “spark-warehouse”<br/>val spark = SparkSession.builder().appName(“Spark Hive Example”).config(“spark.sql.warehouse.dir”, warehouseLocation).enableHiveSupport().getOrCreate()<br/>import spark.implicits._<br/>import spark.sql<br/>sql(“CREATE TABLE IF NOT EXISTS src (key INT, value STRING)”)</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/bb1f9f9fecdbb8be9ba08616b337e8a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QaVe4yMoqu0-2Hexxol8SQ.png"/></div></div></figure><p id="33b3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。现在，我们将Spark目录中的示例数据加载到我们的表“src”中。<br/> 2。“src”的内容显示如下。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="20aa" class="lf kd hh mi b fi mm mn l mo mp">sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")<br/>sql("SELECT * FROM src").show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/1e8025cdf539581de10d0d1c83691635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7uX-RPYg-V7rYzfOhtrp1Q.png"/></div></div></figure><p id="cad9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代码解释:</strong> <br/> 1。我们执行“计数”操作来选择“src”表中的键的数量。<br/> 2。我们现在选择“key”值小于10的所有记录，并将其存储在“sqlDF”数据帧中。<br/> 3。正在从“sqlDF”创建数据集“stringDS”。<br/> 4。显示“stringDS”数据集的内容。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="9a1b" class="lf kd hh mi b fi mm mn l mo mp">sql(“SELECT COUNT(*) FROM src”).show()<br/>val sqlDF = sql(“SELECT key, value FROM src WHERE key &amp;amp;amp;amp;amp;lt; 10 ORDER BY key”) val stringsDS = sqlDF.map {case Row(key: Int, value: String) =&amp;amp;amp;amp;amp;gt; s”Key: $key, Value: $value”}<br/>stringsDS.show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/49f1a63da03b9b9117e8d19f351728fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DfInYJmiGPbrdyH-RrcFw.png"/></div></div></figure><p id="8f74" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">代号解释:</strong> <br/> 1。我们创建一个数据帧“recordsDF ”,存储键值为1到100的所有记录。<br/> 2。创建“记录定义”数据帧的临时视图“记录”。<br/> 3。显示以“key”作为主键的表“records”和“src”的连接内容。</p><pre class="ly lz ma mb fd mh mi mj mk aw ml bi"><span id="a844" class="lf kd hh mi b fi mm mn l mo mp">val recordsDF = spark.createDataFrame((1 to 100).map(i =&amp;amp;amp;amp;amp;gt; Record(i, s”val_$i”)))<br/>recordsDF.createOrReplaceTempView(“records”)<br/>sql(“SELECT * FROM records r JOIN src s ON r.key = s.key”).show()</span></pre><figure class="ly lz ma mb fd ii er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/4aacc359ae40c0000c1b7fa881e4afbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPSmi3y7O9krSJIteOValw.png"/></div></div></figure><p id="52fd" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">各位，这篇关于Spark SQL教程的文章到此结束。</p><p id="6741" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">原来就是这样！我希望这篇博客能给你提供信息，增加你的知识。如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="fc19" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释Spark的各个方面。</p><blockquote class="mt mu mv"><p id="3f34" class="ip iq jn ir b is it iu iv iw ix iy iz mw jb jc jd mx jf jg jh my jj jk jl jm ha bi translated"><em class="hh"> 1。</em> <a class="ae ms" rel="noopener" href="/edureka/spark-architecture-4f06dcf27387"> <em class="hh">阿帕奇Spark架构</em> </a></p><p id="117c" class="ip iq jn ir b is it iu iv iw ix iy iz mw jb jc jd mx jf jg jh my jj jk jl jm ha bi translated"><em class="hh"> 2。</em> <a class="ae ms" rel="noopener" href="/edureka/spark-streaming-92bdcb1d94c4"> <em class="hh">火花串流教程</em> </a></p><p id="b851" class="ip iq jn ir b is it iu iv iw ix iy iz mw jb jc jd mx jf jg jh my jj jk jl jm ha bi translated"><em class="hh"> 3。</em><a class="ae ms" rel="noopener" href="/edureka/spark-mllib-e87546ac268"><em class="hh">Spark ml lib</em></a></p><p id="63da" class="ip iq jn ir b is it iu iv iw ix iy iz mw jb jc jd mx jf jg jh my jj jk jl jm ha bi translated"><em class="hh"> 4。</em> <a class="ae ms" rel="noopener" href="/edureka/spark-tutorial-2a036075a572"> <em class="hh">阿帕奇火花教程</em> </a></p><p id="31e7" class="ip iq jn ir b is it iu iv iw ix iy iz mw jb jc jd mx jf jg jh my jj jk jl jm ha bi translated"><em class="hh"> 5。</em> <a class="ae ms" rel="noopener" href="/edureka/spark-graphx-f9bd805ac429"> <em class="hh"> Spark GraphX教程</em> </a></p><p id="79f8" class="ip iq jn ir b is it iu iv iw ix iy iz mw jb jc jd mx jf jg jh my jj jk jl jm ha bi translated"><em class="hh"> 6。</em> <a class="ae ms" rel="noopener" href="/edureka/spark-java-tutorial-cb2f54991c2b"> <em class="hh"> Spark Java教程</em> </a></p></blockquote></div></div>    
</body>
</html>