<html>
<head>
<title>Neural Network Tutorial — Know How To Model A Multi-Layer Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络教程—了解如何建立多层神经网络模型</h1>
<blockquote>原文：<a href="https://medium.com/edureka/neural-network-tutorial-2a46b22394c9?source=collection_archive---------0-----------------------#2017-12-08">https://medium.com/edureka/neural-network-tutorial-2a46b22394c9?source=collection_archive---------0-----------------------#2017-12-08</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/f8bcfb426bd9c97c150c3cddbdfcdc47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*edfHRASKIU8o8T9Tdup6xQ.gif"/></div><figcaption class="il im et er es in io bd b be z dx">Neural Network Tutorial — Edureka</figcaption></figure><p id="c880" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这个神经网络教程中，我们将讨论称为多层感知器(人工神经网络)的感知器网络。</p><p id="c646" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在本神经网络教程中，我们将讨论以下主题:</p><ul class=""><li id="914b" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">单层感知器的局限性</li><li id="f75a" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">什么是多层感知器(人工神经网络)？</li><li id="fa0f" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">人工神经网络是如何工作的？</li><li id="cedf" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">使用案例</li></ul><p id="77eb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这篇关于神经网络教程的文章将在最后包含一个用例。为了实现这个用例，我们将使用TensorFlow。</p><p id="d09e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我将从讨论单层感知器的局限性开始。</p><h1 id="f8ff" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">单层感知器的局限性；</h1><p id="9448" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">嗯，有两个主要问题:</p><ul class=""><li id="03f8" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">单层Percpetrons无法对非线性可分离数据点进行分类。</li><li id="7950" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">单层感知器无法解决涉及大量参数的复杂问题。</li></ul><h2 id="d872" class="le kc hh bd kd lf lg lh kh li lj lk kl ja ll lm kp je ln lo kt ji lp lq kx lr bi translated">单层Percpetrons无法对非线性可分离数据点进行分类</h2><p id="f55c" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">让我们以异或门为例来理解这一点。考虑下图:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es ls"><img src="../Images/fdfbe11ea7795e56496dc30048285d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rmr3edOfuKaR-012J5hBvg.png"/></div></div></figure><p id="e06c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这里，你不能用一条直线来区分高点和低点。但是，我们可以用两条直线把它分开。考虑下图:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/4df502ea8f80e46fb8104ef9da73efc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*jX29s_g7n-Gt8ehPelfMNw.png"/></div></figure><h2 id="5834" class="le kc hh bd kd lf lg lh kh li lj lk kl ja ll lm kp je ln lo kt ji lp lq kx lr bi translated">单层感知器无法解决涉及大量参数的复杂问题:</h2><p id="d592" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">这里我也用一个例子来说明。</p><p id="b04e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">作为一家电子商务公司，你注意到你的销售额在下降。现在，你试着组建一个营销团队，销售产品以增加销售额。</p><p id="29a0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">营销团队可以通过各种方式营销您的产品，例如:</p><ul class=""><li id="e934" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">谷歌广告</li><li id="2394" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">个人电子邮件</li><li id="4c13" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">相关网站上的销售广告</li><li id="535d" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">引用程序</li><li id="1123" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">博客等等。。。</li></ul><p id="3997" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑到所有可用的因素和选项，营销团队必须决定一个策略来进行最佳和有效的营销，但这项任务太复杂，人类无法分析，因为参数数量相当高。这个问题将不得不使用深度学习来解决。考虑下图:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mc"><img src="../Images/740bc22a6aeb0c3753a00f46e153066c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pmu1jBihTYaNC7ftJbVgUg.png"/></div></div></figure><p id="a95b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">他们可以只使用一种方式来推销他们的产品，也可以使用多种方式。</p><p id="616a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">每种方式都有不同的优点和缺点，他们必须关注各种因素和选项，例如:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mc"><img src="../Images/2597b1b3dacda0239579c473df8b6254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yfXjrIw_Atpv4hSndfcvVw.png"/></div></div></figure><p id="15ef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将发生的销售数量将取决于不同的分类输入、它们的子类别和它们的参数。然而，仅仅通过一个神经元(感知器)不可能从如此多的输入及其子参数进行计算。</p><p id="175d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是为什么要用一个以上的神经元来解决这个问题。考虑下图:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es md"><img src="../Images/7d5893aa04066763034b22839004bfe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZlHwbFJLDooN1ZD3gk19Q.png"/></div></div></figure><p id="0faf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">由于所有这些原因，单层感知器不能用于复杂的非线性问题。</p><p id="0aef" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">接下来，在这个神经网络教程中，我将重点介绍多层感知器(MLP)。</p><h1 id="f1c9" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">什么是多层感知器？</h1><p id="96df" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">正如你所知，我们的大脑是由数百万个神经元组成的，所以神经网络实际上只是感知器的组合，以不同的方式连接，并对不同的激活功能进行操作。</p><p id="9ab5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑下图:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es me"><img src="../Images/b2f422888b1a54436d452f94c9e688a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jt5DUzww-fFb8-1T1JwC1g.png"/></div></div></figure><ul class=""><li id="eb26" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated"><strong class="ir hi">输入节点</strong> —输入节点向网络提供来自外界的信息，统称为“输入层”。任何输入节点都不执行任何计算，它们只是将信息传递给隐藏节点。</li><li id="377c" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">隐藏节点</strong> —隐藏节点与外界没有直接联系(因此得名“隐藏”)。它们执行计算并将信息从输入节点传输到输出节点。隐藏节点的集合形成了“隐藏层”。虽然网络只有一个输入图层和一个输出图层，但它可以有零个或多个隐藏图层。多层感知器有一个或多个隐藏层。</li><li id="35a0" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated"><strong class="ir hi">输出节点</strong> —输出节点统称为“输出层”，负责计算和从网络向外界传输信息。</li></ul><p id="6267" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">是的，你猜对了，我举个例子来解释一下——一个<em class="mf">人工神经网络</em>是如何工作的。</p><p id="42d8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设我们有一个足球队的数据，切尔西。该数据包含三列。最后一栏显示了切尔西是赢了比赛还是输了比赛。另外两栏大概是，上半场进球领先，下半场控球。控球时间是球队控球时间的百分比。所以，如果我说一个队在半场(45分钟)有50%的控球率，这意味着，这个队在45分钟里有22.5分钟有球。</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mg"><img src="../Images/d3e2595d9abea79f1e8f0879deb97c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I-Rmd3S0Y0mkOGu0eIzkbg.png"/></div></div></figure><p id="dd21" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最终的结果列可以有两个值1或0，表示切尔西是否赢得了比赛。例如，我们可以看到，如果在上半场有0球领先，而在下半场切尔西有80%的控球率，那么切尔西就赢了这场比赛。</p><p id="9f9b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，假设，我们想预测切尔西是否会赢得比赛，如果上半场的进球领先优势是2，下半场的控球率是32%。</p><p id="bd89" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是一个二元分类问题，其中多层感知器可以从给定的示例(训练数据)中学习，并在给定新数据点的情况下做出明智的预测。我们将在下面看到多层感知器如何学习这样的关系。</p><p id="5f5d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">多层感知器学习的过程被称为反向传播算法，我会推荐你阅读<a class="ae mh" rel="noopener" href="/edureka/backpropagation-bd2cf8fdde81"> <strong class="ir hi">反向传播</strong> </a>文章。</p><p id="eb8a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑下图:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mi"><img src="../Images/4cf2b1857798cefeb30cc5aef5b72a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K7XvpxkgiJnGLXpQSJSjDg.png"/></div></div></figure><h1 id="d0e6" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">正向传播:</h1><p id="3f3c" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">这里，我们将向前传播，即计算输入的加权和并添加偏差。在输出层，我们将使用softmax函数来获得切尔西输赢的概率。</p><p id="7f04" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你注意到这个图表，赢的概率是0.4，输的概率是0.6。但是，根据我们的数据，我们知道当上半场的进球领先是1，下半场的控球率是42%时，切尔西会赢。我们的网络做出了错误的预测。</p><p id="f2b8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们看到误差(比较网络输出和目标)，它是0.6和-0.6。</p><h1 id="2db2" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">反向传播和权重更新；</h1><p id="667d" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">我会推荐你参考<a class="ae mh" rel="noopener" href="/edureka/backpropagation-bd2cf8fdde81"> <em class="mf">反向传播文章。</em>T3】</a></p><p id="4b10" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们计算输出节点的总误差，并使用反向传播通过网络传播这些误差，以计算<em class="mf">梯度</em>。然后，我们使用优化方法，如<em class="mf">梯度下降</em>来“调整”<strong class="ir hi">网络中的所有</strong>权重，目的是减少输出层的误差。</p><p id="8439" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我解释一下梯度下降优化器的工作原理:</p><p id="855e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">步骤— 1: </strong>首先我们计算误差，考虑下面的等式:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/9c4525e4d48fa25eb16cd5635ae24084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*FUfmukqcxuwEQaXOc_zjYQ.png"/></div></figure><p id="94bf" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">步骤— 2: </strong>根据我们得到的误差，它将计算误差随重量变化的变化率。</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div class="er es mk"><img src="../Images/12eb9727fc1dde5ad168859f0ef06f00.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*kOtTOmP9ZZ5JYvfE-DFi7w.png"/></div></figure><p id="fcf5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">第3步:</strong>现在，根据重量的变化，我们将计算新的重量值。</p><p id="73fe" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们现在再次向网络输入相同的例子，网络应该比以前表现得更好，因为权重现在已经被调整以最小化预测中的误差。考虑下面的例子，如图所示，与之前的[0.6，-0.4]相比，输出节点的误差现在减少到[0.2，-0.2]。这意味着我们的网络已经学会正确分类我们的第一个训练样本。</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mi"><img src="../Images/2edfda9d3b43065c5af568a92bc57323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wW-rsrt6IPZnGe_zTDKufA.png"/></div></div></figure><p id="a879" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们对数据集中的所有其他训练示例重复此过程。然后，我们的网络据说已经<em class="mf">学会了</em>那些例子。</p><p id="c32f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我可以向我们的网络输入信息。如果我把上半场的进球领先数输入为2，下半场的控球率输入为32%，我们的网络将预测切尔西是否会赢得这场比赛。</p><p id="d12b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇文章中，我们将体验动手的乐趣。我将使用张量流来模拟一个多层神经网络。</p><h1 id="f75b" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">使用案例:</h1><p id="e037" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">让我们看看我们的问题陈述:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es ml"><img src="../Images/8ec57022f0a680cf3e436bb27d42482d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tbaYayQ4s4ES4xhZSWfmNQ.png"/></div></div></figure><p id="1030" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们看看数据集，我们将使用它来训练我们的网络。</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mm"><img src="../Images/03ecc5dee556ea330c0964ff6f838c29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wMX-OYcU7VNdvU7jQP9c4g.png"/></div></div></figure><p id="84d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">前四列是要素，最后一列是标注。</p><p id="53e6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数据是从图像中提取的，这些图像取自真的和伪造的类似钞票的样本。最终图像的像素为400×400。由于物镜和到所研究物体灰度的距离，获得了分辨率约为660 dpi的照片。小波变换工具用于从图像中提取特征。</p><p id="a78b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了实现这个用例，我们将使用下面的流程图:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mn"><img src="../Images/72d4335c282e60e8ce25a07a040883fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1t5LkMWr178b_Vjuf1dy6g.png"/></div></div></figure><p id="51cb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们现在执行它:</p><pre class="lt lu lv lw fd mo mp mq mr aw ms bi"><span id="88e4" class="le kc hh mp b fi mt mu l mv mw">import matplotlib.pyplot as plt<br/>import tensorflow as tf<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.utils import shuffle<br/>from sklearn.model_selection import train_test_split<br/> <br/># Reading the dataset<br/>def read_dataset():<br/>    df = pd.read_csv("C:\\Users\\Saurabh\\PycharmProjects\\Neural Network Tutorial\\banknote.csv")<br/>    # print(len(df.columns))<br/>    X = df[df.columns[0:4]].values<br/>    y = df[df.columns[4]]<br/> <br/>    # Encode the dependent variable<br/>    Y = one_hot_encode(y)<br/>    print(X.shape)<br/>    return (X, Y)<br/> <br/> <br/># Define the encoder function.<br/>def one_hot_encode(labels):<br/>    n_labels = len(labels)<br/>    n_unique_labels = len(np.unique(labels))<br/>    one_hot_encode = np.zeros((n_labels, n_unique_labels))<br/>    one_hot_encode[np.arange(n_labels), labels] = 1<br/>    return one_hot_encode<br/> <br/> <br/># Read the dataset<br/>X, Y = read_dataset()<br/> <br/># Shuffle the dataset to mix up the rows.<br/>X, Y = shuffle(X, Y, random_state=1)<br/> <br/># Convert the dataset into train and test part<br/>train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.20, random_state=415)<br/> <br/># Inpect the shape of the training and testing.<br/>print(train_x.shape)<br/>print(train_y.shape)<br/>print(test_x.shape)<br/> <br/># Define the important parameters and variable to work with the tensors<br/>learning_rate = 0.3<br/>training_epochs = 100<br/>cost_history = np.empty(shape=[1], dtype=float)<br/>n_dim = X.shape[1]<br/>print("n_dim", n_dim)<br/>n_class = 2<br/>model_path = "C:\\Users\\Saurabh\\PycharmProjects\\Neural Network Tutorial\\BankNotes"<br/> <br/># Define the number of hidden layers and number of neurons for each layer<br/>n_hidden_1 = 4<br/>n_hidden_2 = 4<br/>n_hidden_3 = 4<br/>n_hidden_4 = 4<br/> <br/>x = tf.placeholder(tf.float32, [None, n_dim])<br/>W = tf.Variable(tf.zeros([n_dim, n_class]))<br/>b = tf.Variable(tf.zeros([n_class]))<br/>y_ = tf.placeholder(tf.float32, [None, n_class])<br/> <br/> <br/># Define the model<br/>def multilayer_perceptron(x, weights, biases):<br/> <br/>    # Hidden layer with RELU activationsd<br/>    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])<br/>    layer_1 = tf.nn.relu(layer_1)<br/> <br/>    # Hidden layer with sigmoid activation<br/>    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])<br/>    layer_2 = tf.nn.relu(layer_2)<br/> <br/>    # Hidden layer with sigmoid activation<br/>    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])<br/>    layer_3 = tf.nn.relu(layer_3)<br/> <br/>    # Hidden layer with RELU activation<br/>    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])<br/>    layer_4 = tf.nn.sigmoid(layer_4)<br/> <br/>    # Output layer with linear activation<br/>    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']<br/>    return out_layer<br/> <br/> <br/># Define the weights and the biases for each layer<br/> <br/>weights = {<br/>    'h1': tf.Variable(tf.truncated_normal([n_dim, n_hidden_1])),<br/>    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),<br/>    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),<br/>    'h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),<br/>    'out': tf.Variable(tf.truncated_normal([n_hidden_4, n_class]))<br/>}<br/>biases = {<br/>    'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),<br/>    'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),<br/>    'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),<br/>    'b4': tf.Variable(tf.truncated_normal([n_hidden_4])),<br/>    'out': tf.Variable(tf.truncated_normal([n_class]))<br/>}<br/> <br/># Initialize all the variables<br/> <br/>init = tf.global_variables_initializer()<br/> <br/>saver = tf.train.Saver()<br/> <br/># Call your model defined<br/>y = multilayer_perceptron(x, weights, biases)<br/> <br/># Define the cost function and optimizer<br/>cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))<br/>training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)<br/> <br/>sess = tf.Session()<br/>sess.run(init)<br/> <br/># Calculate the cost and the accuracy for each epoch<br/> <br/>mse_history = []<br/>accuracy_history = []<br/> <br/>for epoch in range(training_epochs):<br/>    sess.run(training_step, feed_dict={x: train_x, y_: train_y})<br/>    cost = sess.run(cost_function, feed_dict={x: train_x, y_: train_y})<br/>    cost_history = np.append(cost_history, cost)<br/>    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))<br/>    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<br/>    # print("Accuracy: ", (sess.run(accuracy, feed_dict={x: test_x, y_: test_y})))<br/>    pred_y = sess.run(y, feed_dict={x: test_x})<br/>    mse = tf.reduce_mean(tf.square(pred_y - test_y))<br/>    mse_ = sess.run(mse)<br/>    mse_history.append(mse_)<br/>    accuracy = (sess.run(accuracy, feed_dict={x: train_x, y_: train_y}))<br/>    accuracy_history.append(accuracy)<br/> <br/>    print('epoch : ', epoch, ' - ', 'cost: ', cost, " - MSE: ", mse_, "- Train Accuracy: ", accuracy)<br/> <br/>save_path = saver.save(sess, model_path)<br/>print("Model saved in file: %s" % save_path)<br/> <br/>#Plot Accuracy Graph<br/>plt.plot(accuracy_history)<br/>plt.xlabel('Epoch')<br/>plt.ylabel('Accuracy')<br/>plt.show()<br/> <br/># Print the final accuracy<br/> <br/>correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))<br/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))<br/>print("Test Accuracy: ", (sess.run(accuracy, feed_dict={x: test_x, y_: test_y})))<br/> <br/># Print the final mean square error<br/> <br/>pred_y = sess.run(y, feed_dict={x: test_x})<br/>mse = tf.reduce_mean(tf.square(pred_y - test_y))<br/>print("MSE: %.4f" % sess.run(mse))</span></pre><p id="52a8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一旦您运行此代码，您将获得以下输出:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mx"><img src="../Images/b45781a2b39b824d55fe992ac0e9278c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fn5YIjnemA7jjVBPC8qAgQ.png"/></div></div></figure><p id="eaca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以注意到，最终精度为99.6364%，均方误差为2.2198。我们实际上可以通过增加纪元的数量来使它变得更好。</p><p id="d8d4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面是历元与精确度的关系图:</p><figure class="lt lu lv lw fd ii er es paragraph-image"><div class="er es ie"><img src="../Images/393c20bb6e307489eabd675b6ed61a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*fovyKCSGR8iM70JSAMeuHQ.png"/></div></figure><p id="e9ac" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可以看到，每次迭代之后，精确度都在增加。</p><p id="2cea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就把我们带到了“神经网络”这篇文章的结尾。我希望这篇文章对你有所帮助，并增加了你的知识价值。</p><p id="6864" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，那么你可以参考<a class="ae mh" href="https://www.edureka.co/blog/?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=neural-network-tutorial" rel="noopener ugc nofollow" target="_blank"> Edureka的官方网站。</a></p><p id="27e8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释深度学习的各个其他方面。</p><blockquote class="my mz na"><p id="1554" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">1.<a class="ae mh" rel="noopener" href="/edureka/tensorflow-tutorial-ba142ae96bca"> TensorFlow教程</a></p><p id="3d67" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">2.<a class="ae mh" rel="noopener" href="/edureka/pytorch-tutorial-9971d66f6893"> PyTorch教程</a></p><p id="fdfc" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">3.<a class="ae mh" rel="noopener" href="/edureka/perceptron-learning-algorithm-d30e8b99b156">感知器学习算法</a></p><p id="6c08" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">4.<a class="ae mh" rel="noopener" href="/edureka/tensorflow-object-detection-tutorial-8d6942e73adc">tensor flow中的对象检测</a></p><p id="02f9" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">5.<a class="ae mh" rel="noopener" href="/edureka/backpropagation-bd2cf8fdde81">什么是反向传播？</a></p><p id="cfab" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">6.<a class="ae mh" rel="noopener" href="/edureka/convolutional-neural-network-3f2c5b9c4778">卷积神经网络</a></p><p id="9a49" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">7.<a class="ae mh" rel="noopener" href="/edureka/capsule-networks-d7acd437c9e">胶囊神经网络</a></p><p id="351c" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">8.<a class="ae mh" rel="noopener" href="/edureka/recurrent-neural-networks-df945afd7441">递归神经网络</a></p><p id="7acc" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">9.<a class="ae mh" rel="noopener" href="/edureka/autoencoders-tutorial-cfdcebdefe37">自动编码器教程</a></p><p id="38f4" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">10.<a class="ae mh" rel="noopener" href="/edureka/restricted-boltzmann-machine-tutorial-991ae688c154">受限玻尔兹曼机教程</a></p><p id="e012" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">11.<a class="ae mh" rel="noopener" href="/edureka/pytorch-vs-tensorflow-252fc6675dd7"> PyTorch vs TensorFlow </a></p><p id="a50b" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">12.<a class="ae mh" rel="noopener" href="/edureka/deep-learning-with-python-2adbf6e9437d">用Python进行深度学习</a></p><p id="612a" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">13.<a class="ae mh" rel="noopener" href="/edureka/artificial-intelligence-tutorial-4257c66f5bb1">人工智能教程</a></p><p id="53eb" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">14.<a class="ae mh" rel="noopener" href="/edureka/tensorflow-image-classification-19b63b7bfd95">张量流图像分类</a></p><p id="be05" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">15.<a class="ae mh" rel="noopener" href="/edureka/artificial-intelligence-applications-7b93b91150e3">人工智能应用</a></p><p id="fdf5" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">16.<a class="ae mh" rel="noopener" href="/edureka/become-artificial-intelligence-engineer-5ac2ede99907">如何成为一名人工智能工程师？</a></p><p id="3c52" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">17.<a class="ae mh" rel="noopener" href="/edureka/q-learning-592524c3ecfc">问学习</a></p><p id="c6dd" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">18.<a class="ae mh" rel="noopener" href="/edureka/apriori-algorithm-d7cc648d4f1e"> Apriori算法</a></p><p id="e6c1" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">19.<a class="ae mh" rel="noopener" href="/edureka/introduction-to-markov-chains-c6cb4bcd5723">用Python实现马尔可夫链</a></p><p id="c054" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">20.<a class="ae mh" rel="noopener" href="/edureka/artificial-intelligence-algorithms-fad283a0d8e2">人工智能算法</a></p><p id="ef1e" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">21.<a class="ae mh" rel="noopener" href="/edureka/best-laptop-for-machine-learning-a4a5f8ba5b">机器学习的最佳笔记本电脑</a></p><p id="f5f8" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">22.<a class="ae mh" rel="noopener" href="/edureka/top-artificial-intelligence-tools-36418e47bf2a">12大人工智能工具</a></p><p id="04df" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">23.<a class="ae mh" rel="noopener" href="/edureka/artificial-intelligence-interview-questions-872d85387b19">人工智能(AI)面试问题</a></p><p id="7545" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">24.<a class="ae mh" rel="noopener" href="/edureka/theano-vs-tensorflow-15f30216b3bc"> Theano vs TensorFlow </a></p><p id="d211" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">25.<a class="ae mh" rel="noopener" href="/edureka/what-is-a-neural-network-56ae7338b92d">什么是神经网络？</a></p><p id="0967" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">26.<a class="ae mh" rel="noopener" href="/edureka/pattern-recognition-5e2d30ab68b9">模式识别</a></p><p id="8f17" class="ip iq mf ir b is it iu iv iw ix iy iz nb jb jc jd nc jf jg jh nd jj jk jl jm ha bi translated">27.<a class="ae mh" rel="noopener" href="/edureka/alpha-beta-pruning-in-ai-b47ee5500f9a">人工智能中的阿尔法贝塔剪枝</a></p></blockquote></div><div class="ab cl ne nf go ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ha hb hc hd he"><p id="06c2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="mf">原载于2017年12月8日www.edureka.co</em><em class="mf">的</em> <a class="ae mh" href="https://www.edureka.co/blog/neural-network-tutorial/" rel="noopener ugc nofollow" target="_blank"> <em class="mf">。</em></a></p></div></div>    
</body>
</html>