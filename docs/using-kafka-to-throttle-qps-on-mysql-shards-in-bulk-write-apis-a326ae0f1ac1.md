# 在批量编写 API 中使用 Kafka 抑制 MySQL 碎片上的 QPS

> 原文：<https://medium.com/pinterest-engineering/using-kafka-to-throttle-qps-on-mysql-shards-in-bulk-write-apis-a326ae0f1ac1?source=collection_archive---------0----------------------->

黎齐|实时分析软件工程师

在 Pinterest，后端核心服务负责来自 Pinners 和内部服务的 pin、boards 和用户的各种操作。由于实时响应，Pinners 的操作被识别为在线流量，而内部流量被识别为离线流量，因为处理是异步的，不需要实时响应。

服务的读写 API 在这些情况的流量之间共享。Pinners 对单个对象的大多数操作(如创建电路板、保存 Pin 或通过 web 或移动设备编辑用户设置)都被路由到其中一个 API，以获取和更新数据存储中的数据。同时，内部服务使用这些 API 代表用户对大量对象采取行动(例如停用垃圾邮件帐户、删除垃圾邮件 pin)。

为了从 API 中卸载内部离线流量，以便在线流量可以以更好的可靠性和性能被专门处理，编写 API 应该支持批处理对象。提出并实现了一个基于 Kafka 的批量写平台。这也确保了像 QPS 这样的内部服务得到更有效的支持，而不会被限制为保证高吞吐量。在本帖中，我们将介绍内部离线流量的特征、我们面临的挑战以及我们如何通过在后端核心服务中构建批量写入平台来应对这些挑战。

# 数据存储和编写 API

在 Pinterest，MySQL 是存储用户创建内容的主要数据库。为了为数亿个 Pinners 存储数十亿个 pin、电路板和其他数据，许多 MySQL 数据库实例形成了一个 MySQL 集群，该集群被分割成逻辑碎片，以便更有效地管理和服务数据。所有数据都分散在这些碎片上。

为了让一个用户高效地读写数据，数据存储在同一个分片中，这样 API 只需要从一个分片中获取数据，而不需要对各个分片进行扇出查询。为了防止任何单个请求长时间占用 MySQL 数据库资源，每个查询都配置了超时。

核心服务的所有写 API 在开始时都是为来自 pinner 的在线流量构建的，并且工作良好，因为只接受单个对象，因为 pinner 大部分时间在单个对象上操作)并且操作是轻量级的。即使当 Pinners 进行批量操作时，例如将许多引脚移动到 section one 板上，性能仍然很好，因为对象的数量不是很大，编写 API 可以逐个处理它们。

# 挑战

随着越来越多的内部服务使用现有的写 API 进行各种批量操作(例如在短时间内为垃圾邮件用户删除许多 pin，或者为大量现有 pin 回填新字段)，情况发生了变化。由于写 API 一次只能处理一个对象，因此在这些 API 中会出现高得多的流量峰值。

为了处理更多的流量，可以应用服务的自动伸缩，但不一定能完全解决问题，因为系统的容量受到 MySQL 集群的限制。在现有的 MySQL 集群架构下，很难实现 MySQL 集群的自动伸缩。

为了保护服务和 MySQL 集群，对编写 API 应用了速率限制。

虽然节流在某种程度上有所帮助，但它有几个缺点，会妨碍后端核心服务变得更加可靠和可伸缩。

1.  API 的在线和离线流量相互影响。如果内部离线流量出现峰值，同一 API 的在线流量会受到更高延迟和性能降级的影响，从而影响用户体验。
2.  随着越来越多的内部流量被发送到 API，速率限制需要小心地保持颠簸，以便 API 可以在不影响现有流量的情况下服务更多的流量。
3.  限速并不能阻止热碎片。当内部服务为特定用户写入数据时，例如为合作伙伴接收大量 feed pins，所有请求都指向同一个 shard。由于短时间内的请求高峰，热碎片是意料之中的。当 MySQL 中的更新操作非常昂贵时，情况会变得更糟。

由于内部服务需要在短时间内处理大量的对象，并且不需要实时响应，因此可以将指向同一个 shard 的请求组合在一起，用一个对 MySQL 的共享查询异步处理，以提高效率并节省 MySQL 的连接资源带宽。所有合并的批处理请求都应该以受控的速率处理，以避免热碎片。

# **批量写入架构**

批量写入平台旨在通过高吞吐量和零热碎片来支持内部服务的高 QPS。此外，移植到平台应该很简单，只需调用新的 API。

![](img/0ce97c8b3b312e5e1a4a6a656f7f75aa.png)

## 批量编写 API 和代理

为了支持对一批对象的写(更新、删除和创建)操作，为内部服务提供了一组批量写 API，它可以接受对象列表，而不是单个对象。与常规的编写 API 相比，这有助于显著降低 API 的 QPS。

Proxy 是一个欺骗服务，它将传入的请求映射到不同的批处理模块，这些模块根据对象的类型将请求组合到同一个 shard 中。

## 批处理模块

![](img/ae5ab58f912a995e8abd76df74b8d797.png)

批处理模块是根据操作类型和对象类型将一个批处理请求分成小批，这样一批对象可以在 MySQL 中高效地处理，MySQL 为每个查询配置了超时。

这是出于两个主要考虑而设计的:

*   首先，应对每个碎片的写入速率进行配置，以避免热碎片，因为碎片可能包含不同数量的记录，性能也各不相同。来自代理的一个批处理请求包含不同分片上的对象。为了在碎片上精确地控制 QPS，批处理请求基于目标碎片被分成多个批处理。“分片批处理”模块根据受影响的 MySQL 分片来拆分请求
*   其次，每个写操作都有自己的批量大小。不同对象类型上的操作具有不同的性能，因为它们更新不同数量的各种表。例如，创建一个新的 Pin 可能会更改四到五个不同的表，而更新一个现有的 Pin 可能只会更改两个表。此外，对表的更新查询可能需要不同长度的时间。因此，一个对象类型的批量更新对于不同的批量大小可能会经历不同的延迟。为了提高批量更新的效率，针对不同的写操作对批量大小进行了不同的配置。“操作批处理”根据操作类型进一步拆分这些请求。

## 卡夫卡限速器

来自批处理模块的批处理请求中的所有对象都在同一个片上。如果有太多的请求命中一个特定的碎片，那么就需要热碎片。热碎片会影响所有其他对同一碎片的查询，并降低系统的性能。为了避免这个问题，对一个分片的所有请求都应该以受控的速率发送，这样分片就不会不堪重负，可以有效地处理请求。为了实现这个目标，每个分片都需要一个速率限制器，它控制分片的所有请求。

为了同时支持来自内部客户端的高 QPS，来自它们的所有请求都应该临时存储在平台中，并以受控的速度进行处理。这就是卡夫卡很适合这些目的的地方。

1.  Kafka 可以处理非常高的 qps 读写。
2.  Kafka 是一个可靠的分布式消息存储系统，用于缓冲批量请求，以便以受控的速率处理请求。
3.  Kafka 可以利用负载的重新平衡，自动管理消费者。
4.  每个分区被专门分配给一个消费者(在同一个消费者组中),消费者可以以良好的速率限制来处理请求。
5.  所有分区中的请求由不同的消费者处理器同时处理，因此吞吐量非常高。

![](img/a9334318630ea7f35b03f6bbceb20674.png)

P: partition C: consumer processor

## 卡夫卡式结构

首先，MySQL 集群中的每个分片在 Kafka 中都有一个匹配的分区，因此对该分片的所有请求都将被发布到相应的分区，并由一个具有精确 QPS 的专用消费处理器进行处理。第二，大量消费者处理器正在运行，因此最多将一个或两个分区分配给一个消费者处理器，以实现最大吞吐量。

## 消费处理器

![](img/3d1288b4ed85fe0e8936e61c452aa802.png)

消费类处理器分两步在一个碎片上对 QPS 进行速率限制:

*   首先，配置消费者一次可以从其分区中提取多少个请求。
*   其次，消费者咨询分片的配置，以获得一个分片可以处理的批量请求的精确数量，并使用 Guava Ratelimiter 进行速率控制。例如，对于某些碎片，它可能处理较低的流量，因为热用户存储在这些碎片中。

消费类处理器可以通过适当的操作来处理不同的故障。为了处理线程池中的拥塞，如果线程池已满且忙于现有任务，消费者处理器将使用配置的退避时间重试任务。为了处理 MySQL shards 中的故障，它将检查来自 MySQL cluster 的响应，以捕捉错误和异常，并对不同的故障采取适当的措施。例如，当它发现连续两次超时失败时，它将向系统管理员发送警报，并在配置的等待时间内停止提取和处理请求。通过这些机制，请求处理的成功率很高。

# **结果**

内部团队的几个用例已经发布到批量编写平台，性能良好。例如，合作伙伴的饲料摄取正在使用该平台。在花费的时间和过程的成功率方面观察到许多改进。摄取大约 **4.3** 百万针的结果如下。

![](img/e71399109a969a6f7ba651b2753a9c47.png)

此外，在摄食过程中再也看不到热碎片，这在以前引起过许多类似的问题。

# 下一步是什么

随着越来越多的内部流量从现有的写 API 分离到新的批量写 API，在线流量 API 的性能得到了改善，如更少的停机时间和更低的延迟。这有助于提高系统的可靠性和效率。

新平台的下一步是通过在更多对象类型上扩展现有操作来支持更多情况。

# 感谢

*感谢 Kapil Bajaj、Carlo De Guzman、Chen 和 Pinterest 核心服务团队的其他成员！还要特别感谢购物基础设施团队的 Brian Pin 和 Sam Meder 提供的支持。*