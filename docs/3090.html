<html>
<head>
<title>How To Implement Linear Discriminant Analysis in R?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在R中实现线性判别分析？</h1>
<blockquote>原文：<a href="https://medium.com/edureka/linear-discriminant-analysis-88fa8ad59d0f?source=collection_archive---------2-----------------------#2019-07-24">https://medium.com/edureka/linear-discriminant-analysis-88fa8ad59d0f?source=collection_archive---------2-----------------------#2019-07-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/720bfc9b7083948ba2dd2edcf6bb4b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*V0XNax1o7Ce-TdU0dedoOw.png"/></div><figcaption class="il im et er es in io bd b be z dx">Linear Discriminant Analysis — Edureka</figcaption></figure><p id="9a95" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">线性判别分析是一种非常流行的用于解决分类问题的机器学习技术。在本文中，我们将尝试理解这种技术背后的直觉和数学。还提供了一个用R语言实现LDA的例子。</p><ul class=""><li id="7539" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">线性判别分析假设</li><li id="5cf7" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">直觉</li><li id="c27e" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">LDA的数学描述</li><li id="d113" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">学习模型参数</li><li id="5677" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">R中的示例</li></ul><p id="8f04" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">那么让我们开始吧</p><h1 id="3721" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">线性判别分析假设</h1><p id="a6b2" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">线性判别分析基于以下假设:</p><ul class=""><li id="4909" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">因变量<em class="le"> Y </em>是离散的。在本文中，我们将假设因变量是二元的，并且取类值<em class="le"> {+1，-1} </em>。样本属于类别<em class="le"> +1 </em>的概率，即<em class="le"> P(Y = +1) = p </em>。因此，样本属于类别<em class="le"> -1 </em>的概率为<em class="le"> 1-p </em>。</li><li id="e6a6" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">自变量<em class="le"> X </em>来自高斯分布。高斯分布的平均值取决于类别标签<em class="le"> Y </em>。即如果<em class="le"> Yi = +1 </em>，那么<em class="le"> Xi </em>的平均值就是<em class="le"> 𝜇+1 </em>，否则就是<em class="le"> 𝜇-1 </em>。两个类的方差<em class="le"> 𝜎2 </em>是相同的。数学上讲，<em class="le"> X|(Y = +1) ~ N(𝜇+1，𝜎2) </em>和<em class="le"> X|(Y = -1) ~ N(𝜇-1，𝜎2) </em>，其中<em class="le"> N </em>表示正态分布。</li></ul><p id="a8dc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有了这些信息，就有可能为自变量和因变量构建一个联合分布<em class="le"> P(X，Y) </em>。因此，LDA属于<strong class="ir hi">生成分类器模型</strong>的一类。一个密切相关的生成分类器是二次判别分析(QDA)。它基于LDA所有相同的假设，除了类方差不同。</p><p id="e9de" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们继续用线性判别分析文章看看</p><h1 id="5155" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">直觉</h1><p id="83b6" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">考虑给定类别<em class="le"> Y </em>的<em class="le"> X </em>的类别条件高斯分布。下图显示了分布的密度函数。在该图中，如果<em class="le"> Y = +1 </em>，则<em class="le"> X </em>的平均值为10，如果<em class="le"> Y = -1 </em>，则平均值为2。两种情况下的方差都是2。</p><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/885945c898629913ed8af468edd1a2d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XXDbM64yiv8dKu6B1LMXQ.png"/></div></div></figure><p id="7aaa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在假设给我们一个新的值<em class="le"> X </em>。我们姑且称之为<em class="le"> xi </em>。任务是确定这个<em class="le"> xi </em>最可能的阶级标签，即<em class="le">伊</em>。为简单起见，假设样本属于类别<em class="le"> +1 </em>的概率<em class="le"> p </em>与属于类别<em class="le"> -1 </em>的概率相同，即<em class="le"> p=0.5 </em>。</p><p id="c2f4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">直觉上讲，如果<em class="le"/>比<em class="le"/>更靠近<em class="le">【𝜇+1】</em>，那么<em class="le">易= +1 </em>的可能性更大。更正式的说法是，<em class="le"> yi = +1 </em> if:</p><p id="ad44" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">| 𝜇+1| Xi&lt;| 𝜇-1| Xi</em></p><p id="209e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通过标准偏差归一化两边:</p><p id="cb82" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">|𝜇+1|/𝜎Xi&lt;|𝜇-1|/𝜎Xi</em></p><p id="95e3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">两边成直角:</p><p id="2d49" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">(𝜇+1)2/𝜎2—Xi&lt;)(𝜇-1)2/𝜎2—Xi</em></p><p id="1fce" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">xi2/𝜎2+𝜇+12/𝜎2–2 xi𝜇+1/𝜎2&lt;xi2/𝜎2+𝜇-12/𝜎2–2 xi𝜇-1/𝜎2</em></p><p id="dc0b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">2(𝜇-1—𝜇+1)/𝜎2—(𝜇-12/𝜎2—𝜇+12/𝜎2)&lt;0</em></p><p id="ce2f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">-2(𝜇-1—𝜇+1)/𝜎2)+(𝜇-12/𝜎2—𝜇+12/𝜎2)&gt;0</em></p><p id="54c7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上述表达式的形式为<em class="le"> bxi + c &gt; 0 </em>其中<em class="le"> b = -2(𝜇-1 — 𝜇+1)/𝜎2 </em>和<em class="le"> c = (𝜇-12/𝜎2 — 𝜇+12/𝜎2) </em>。</p><p id="919e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">显而易见，方程的形式是<strong class="ir hi">线性</strong>，因此得名线性判别分析。</p><p id="773d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们继续看线性判别分析文章，</p><h1 id="f647" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">LDA的数学描述</h1><p id="992b" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">LDA表达式的数学推导基于贝叶斯规则和<strong class="ir hi">贝叶斯最优分类器</strong>等概念。</p><p id="cf99" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将直接针对<em class="le"> Y </em>取两个类<em class="le"> {+1，-1} </em>的具体情况提供表达式。我们还将把上一节展示的直觉扩展到一般情况，其中<em class="le"> X </em>可以是多维的。假设有<em class="le"> k </em>个自变量。在这种情况下，类均值<em class="le"> 𝜇-1 </em>和<em class="le"> 𝜇+1 </em>将是维度<em class="le"> k*1 </em>的向量，方差-协方差矩阵<em class="le"> 𝜮 </em>将是维度<em class="le"> k*k </em>的矩阵。</p><p id="1898" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">分类器函数如下所示</p><p id="10b5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le"> Y = h(X) = sign(bTX + c) </em></p><p id="611a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在哪里，</p><p id="7ec7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le"> b = -2 𝜮 -1(𝜇-1 — 𝜇+1) </em></p><p id="39b8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">c =𝜇-1t𝜮-1𝜇-1—𝜇-1t𝜮-1𝜇-1-2ln {(1-p)/p }</em></p><p id="49d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">sign函数如果表达式<em class="le"> bTx + c &gt; 0 </em>则返回<em class="le"> +1 </em>，否则返回<em class="le"> -1 </em>。<em class="le"> c </em>中的自然对数项用于调整两个类别的类别概率不需要相等的事实，即<em class="le"> p </em>可以是(0，1)之间的任何值，而不仅仅是0.5。</p><h1 id="fbc0" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">学习模型参数</h1><p id="586b" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">给定一个有<em class="le"> N </em>个数据点<em class="le"> (x1，y1)，(x2，y2)，… (xn，yn) </em>的数据集，我们需要估计<em class="le"> p，𝜇-1，</em>和<em class="le"> 𝜮 </em>。一种称为最大似然估计的统计估计技术用于估计这些参数。上述参数的表达式如下所示。</p><p id="cd9a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le"> 𝜇+1= (1/N+1) * </em> 𝚺 <em class="le">我:易=+1 xi </em></p><p id="649e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">=(1/n-1)*</em>𝚺<em class="le">我:易=-1 xi </em></p><p id="8ee5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le"> p = N+1/N </em></p><p id="55d6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">𝜮=(1/n)*</em>𝚺i<em class="le">= 1:n(𝜇i)(xi——𝜇i)t——Xi</em></p><p id="b87e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">其中<em class="le"> N+1 =样本数，yi = +1 </em>，N-1 =样本数，yi = -1 。</p><p id="f596" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">有了上面的表达式，LDA模型就完成了。可以使用上述表达式来估计模型参数，并在分类器函数中使用它们来获得自变量<em class="le"> X </em>的任何新输入值的类别标签。</p><p id="f8ee" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们继续用线性判别分析文章看看</p><h1 id="c334" class="kb kc hh bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">R中的示例</h1><p id="b6c8" class="pw-post-body-paragraph ip iq hh ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm ha bi translated">以下代码生成一个虚拟数据集，其中包含两个独立变量<em class="le"> X1 </em>和<em class="le"> X2 </em>以及一个因变量<em class="le"> Y </em>。对于<em class="le"> X1 </em>和<em class="le"> X2 </em>，我们将使用均值<em class="le"> 𝜇-1= (2，2) </em>和<em class="le"> 𝜇+1= (6，6) </em>从两个多元高斯分布中生成一个样本。40%的样本属于类别<em class="le"> +1 </em>，60%属于类别<em class="le"> -1 </em>，因此<em class="le"> p = 0.4 </em>。</p><pre class="lg lh li lj fd lo lp lq lr aw ls bi"><span id="d5ce" class="lt kc hh lp b fi lu lv l lw lx">library(ggplot2)<br/>library(MASS)<br/>library(mvtnorm)<br/>#Variance Covariance matrix for random bivariate gaussian sample<br/>var_covar = matrix(data = c(1.5, 0.3, 0.3, 1.5), nrow=2)<br/>#Random bivariate gaussian samples for class +1<br/>Xplus1 &lt;- rmvnorm(400, mean = c(6, 6), sigma = var_covar)<br/># Random bivariate gaussian samples for class -1<br/>Xminus1 &lt;- rmvnorm(600, mean = c(2, 2), sigma = var_covar)<br/>#Samples for the dependent variable<br/>Y_samples &lt;- c(rep(1, 400), rep(-1, 600))<br/>#Combining the independent and dependent variables into a dataframe<br/>dataset &lt;- as.data.frame(cbind(rbind(Xplus1, Xminus1), Y_samples))<br/>colnames(dataset) &lt;- c("X1", "X2", "Y")<br/>dataset$Y &lt;- as.character(dataset$Y)<br/>#Plot the above samples and color by class labels<br/>ggplot(data = dataset)+<br/>geom_point(aes(X1, X2, color = Y))</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/627d6153a11be7325ca378bda1233cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WOoUO-dJ6QJpAcKOuzLHmQ.png"/></div></div></figure><p id="e293" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，蓝点代表<em class="le"> +1 </em>类的样品，红点代表<em class="le"> -1 </em>类的样品。样本之间有一些重叠，即不能用一条简单的线将类别完全分开。换句话说，它们不是完美的线性可分的。</p><p id="4314" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们现在将使用上述数据训练LDA模型。</p><pre class="lg lh li lj fd lo lp lq lr aw ls bi"><span id="0272" class="lt kc hh lp b fi lu lv l lw lx">#Train the LDA model using the above dataset<br/>lda_model &lt;- lda(Y ~ X1 + X2, data = dataset)<br/>#Print the LDA model<br/>lda_model</span></pre><p id="8ef5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">输出:</strong></p><pre class="lg lh li lj fd lo lp lq lr aw ls bi"><span id="f9fd" class="lt kc hh lp b fi lu lv l lw lx">Prior probabilities of groups:<br/>-1 1<br/>0.6 0.4<br/>Group means:<br/>X1 X2<br/>-1 1.928108 2.010226<br/>1 5.961004 6.015438<br/>Coefficients of linear discriminants:<br/>LD1<br/>X1 0.5646116<br/>X2 0.5004175</span></pre><p id="bea1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">可以看出，模型学习的类均值对于类<em class="le"> -1 </em>为(1.928108，2.010226)，对于类<em class="le"> +1 </em>为(5.961004，6.015438)。这些平均值非常接近我们用来生成这些随机样本的类别平均值。组<em class="le"> +1 </em>的先验概率是参数<em class="le"> p </em>的估计。<em class="le"> b </em>向量是线性判别系数。</p><p id="8e52" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们现在将使用上述模型来预测相同数据的类别标签。</p><pre class="lg lh li lj fd lo lp lq lr aw ls bi"><span id="c722" class="lt kc hh lp b fi lu lv l lw lx">#Predicting the class for each sample in the above dataset using the LDA model<br/>y_pred &lt;- predict(lda_model, newdata = dataset)$class<br/>#Adding the predictions as another column in the dataframe<br/>dataset$Y_lda_prediction &lt;- as.character(y_pred)<br/>#Plot the above samples and color by actual and predicted class labels<br/>dataset$Y_actual_pred &lt;- paste(dataset$Y, dataset$Y_lda_prediction, sep=",")<br/>ggplot(data = dataset)+<br/>geom_point(aes(X1, X2, color = Y_actual_pred))</span></pre><figure class="lg lh li lj fd ii er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/65afec74cbb1ca5db703663902e8b3e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-15Y1dH8l5aR3iV-shyUA.png"/></div></div></figure><p id="d36d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上图中，紫色样本来自被LDA模型正确分类的类别<em class="le"> +1 </em>。同样，红色样品来自正确分类的类别<em class="le"> -1 </em>。蓝色的是来自<em class="le"> +1 </em>级，但是被错误地归类为<em class="le"> -1 </em>。绿色的是来自<em class="le"> -1 </em>级的，被误划为<em class="le"> +1 </em>。发生错误分类是因为这些样本比它们的实际类均值更接近另一个类均值(中心)。</p><p id="0fc2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于Python、DevOps、Ethical Hacking等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="2902" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释数据科学的各个方面。</p><blockquote class="lz ma mb"><p id="557d" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 1。</em> <a class="ae ly" rel="noopener" href="/edureka/data-science-tutorial-484da1ff952b"> <em class="hh">数据科学教程</em> </a></p><p id="8353" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 2。</em> <a class="ae ly" rel="noopener" href="/edureka/math-and-statistics-for-data-science-1152e30cee73"> <em class="hh">数据科学的数学与统计</em> </a></p><p id="30d2" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 3。</em><a class="ae ly" rel="noopener" href="/edureka/linear-regression-in-r-da3e42f16dd3"><em class="hh">R中的线性回归</em> </a></p><p id="fcc3" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 4。</em> <a class="ae ly" rel="noopener" href="/edureka/machine-learning-algorithms-29eea8b69a54"> <em class="hh">机器学习算法</em> </a></p><p id="a156" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 5。</em><a class="ae ly" rel="noopener" href="/edureka/logistic-regression-in-r-2d08ac51cd4f"><em class="hh">R中的逻辑回归</em> </a></p><p id="97e7" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 6。</em> <a class="ae ly" rel="noopener" href="/edureka/classification-algorithms-ba27044f28f1"> <em class="hh">分类算法</em> </a></p><p id="4c71" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 7。</em> <a class="ae ly" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="a0cf" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 8。</em> <a class="ae ly" rel="noopener" href="/edureka/a-complete-guide-on-decision-tree-algorithm-3245e269ece"> <em class="hh">决策树中的R </em> </a></p><p id="945b" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 9。</em> <a class="ae ly" rel="noopener" href="/edureka/introduction-to-machine-learning-97973c43e776"> <em class="hh">机器学习入门</em> </a></p><p id="5460" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 10。</em> <a class="ae ly" rel="noopener" href="/edureka/naive-bayes-in-r-37ca73f3e85c"> <em class="hh">朴素贝叶斯在R </em> </a></p><p id="b3b9" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 11。</em> <a class="ae ly" rel="noopener" href="/edureka/statistics-and-probability-cf736d703703"> <em class="hh">统计与概率</em> </a></p><p id="3c39" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 12。</em> <a class="ae ly" rel="noopener" href="/edureka/decision-trees-b00348e0ac89"> <em class="hh">如何创建一个完美的决策树？</em>T71】</a></p><p id="e70b" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 13。</em> <a class="ae ly" rel="noopener" href="/edureka/data-scientists-myths-14acade1f6f7"> <em class="hh">关于数据科学家角色的十大神话</em> </a></p><p id="6668" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 14。</em> <a class="ae ly" rel="noopener" href="/edureka/data-science-projects-b32f1328eed8"> <em class="hh">顶级数据科学项目</em> </a></p><p id="c16d" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 15。</em> <a class="ae ly" rel="noopener" href="/edureka/data-analyst-vs-data-engineer-vs-data-scientist-27aacdcaffa5"> <em class="hh">数据分析师vs数据工程师vs数据科学家</em> </a></p><p id="5402" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 16。</em> <a class="ae ly" rel="noopener" href="/edureka/types-of-artificial-intelligence-4c40a35f784"> <em class="hh">人工智能的种类</em> </a></p><p id="2c54" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 17。</em><a class="ae ly" rel="noopener" href="/edureka/r-vs-python-48eb86b7b40f"><em class="hh">R vs Python</em></a></p><p id="128a" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">18。 <a class="ae ly" rel="noopener" href="/edureka/ai-vs-machine-learning-vs-deep-learning-1725e8b30b2e"> <em class="hh">人工智能vs机器学习vs深度学习</em> </a></p><p id="71b4" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">19。 <a class="ae ly" rel="noopener" href="/edureka/machine-learning-projects-cb0130d0606f"> <em class="hh">机器学习项目</em> </a></p><p id="1f13" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh">二十。</em> <a class="ae ly" rel="noopener" href="/edureka/data-analyst-interview-questions-867756f37e3d"> <em class="hh">数据分析师面试问答</em> </a></p><p id="bb31" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 21。</em> <a class="ae ly" rel="noopener" href="/edureka/data-science-and-machine-learning-for-non-programmers-c9366f4ac3fb"> <em class="hh">面向非程序员的数据科学和机器学习工具</em> </a></p><p id="3883" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated">22。 <a class="ae ly" rel="noopener" href="/edureka/top-10-machine-learning-frameworks-72459e902ebb"> <em class="hh">十大机器学习框架</em> </a></p><p id="29f4" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 23。</em> <a class="ae ly" rel="noopener" href="/edureka/statistics-for-machine-learning-c8bc158bb3c8"> <em class="hh">用于机器学习的统计</em> </a></p><p id="52ef" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh">二十四。</em> <a class="ae ly" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="3cf9" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 25。</em> <a class="ae ly" rel="noopener" href="/edureka/breadth-first-search-algorithm-17d2c72f0eaa"> <em class="hh">广度优先搜索算法</em> </a></p><p id="9e5a" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 26。</em> <a class="ae ly" rel="noopener" href="/edureka/supervised-learning-5a72987484d0"> <em class="hh">监督学习</em> </a></p><p id="03e0" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 27。</em> <a class="ae ly" rel="noopener" href="/edureka/prerequisites-for-machine-learning-68430f467427"> <em class="hh">机器学习的先决条件</em> </a></p><p id="74fc" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 28。</em> <a class="ae ly" rel="noopener" href="/edureka/r-shiny-tutorial-47b050927bd2"> <em class="hh">互动WebApps使用R闪亮</em> </a></p><p id="34c5" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 29。</em> <a class="ae ly" rel="noopener" href="/edureka/top-10-machine-learning-books-541f011d824e"> <em class="hh">机器学习十大书籍</em> </a></p><p id="06a3" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh">三十。</em> <a class="ae ly" rel="noopener" href="/edureka/unsupervised-learning-40a82b0bac64"> <em class="hh">无监督学习</em> </a></p><p id="8f10" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 31。</em> <a class="ae ly" rel="noopener" href="/edureka/10-best-books-data-science-9161f8e82aca"> <em class="hh"> 10本最好的数据科学书籍</em> </a></p><p id="418b" class="ip iq le ir b is it iu iv iw ix iy iz mc jb jc jd md jf jg jh me jj jk jl jm ha bi translated"><em class="hh"> 32。</em> <a class="ae ly" rel="noopener" href="/edureka/machine-learning-with-r-c7d3edf1f7b"> <em class="hh">机器学习使用R </em> </a></p></blockquote></div><div class="ab cl mf mg go mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ha hb hc hd he"><p id="1788" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="le">原载于2019年7月24日</em><a class="ae ly" href="https://www.edureka.co/blog/linear-discriminant-analysis/" rel="noopener ugc nofollow" target="_blank"><em class="le">www.edureka.co</em></a><em class="le">。</em></p></div></div>    
</body>
</html>