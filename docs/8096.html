<html>
<head>
<title>ACID transformations on Distributed file system</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分布式文件系统上的ACID转换</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/acid-transformations-on-distributed-file-system-fdec5301c1b1?source=collection_archive---------6-----------------------#2022-02-10">https://medium.com/walmartglobaltech/acid-transformations-on-distributed-file-system-fdec5301c1b1?source=collection_archive---------6-----------------------#2022-02-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/b1b1618fa54771c608a16737ffa00760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*LSqmQXyaRI6WsNwZ.png"/></div><figcaption class="il im et er es in io bd b be z dx"><strong class="bd ip">Apache Hudi</strong></figcaption></figure><p id="8502" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">上图来源:<a class="ae jo" href="https://hudi.apache.org/" rel="noopener ugc nofollow" target="_blank">https://hudi.apache.org/</a></p><p id="3487" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">合著者</strong>:绍拉布·佩特里亚</p><p id="00e5" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">同是挑战:</strong></p><p id="1589" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi jp translated"><span class="l jq jr js bm jt ju jv jw jx di">在</span>大数据世界中，在分布式文件系统上执行ACID转换总是具有挑战性，而专有数据库通常更适合执行ACID转换，这通常会对存储和计算收费</p><p id="b527" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">酸是原子性、一致性、隔离性和持久性的缩写。这四种品质中的每一种都有助于事务确保数据完整性的能力</p><h1 id="7787" class="jy jz hh bd ip ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">阿帕奇胡迪简介:</h1><blockquote class="kv kw kx"><p id="ff96" class="iq ir ky is b it iu iv iw ix iy iz ja kz jc jd je la jg jh ji lb jk jl jm jn ha bi translated">Apache胡迪是一种开源文件格式，它提供了ACID转换，而胡迪意味着Hadoop可以进行插入、删除和增量。为了降低成本，我们可以将数据存储在分布式文件系统中，并在其上创建胡迪表。所以我们只付仓储费</p></blockquote><p id="ca26" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">胡迪以parquet格式存储数据，并将事务日志存储在<strong class="is hi">中。帽衫文件夹</strong>。对于每次提交，都会创建一个增量日志，日志以<a class="ae jo" href="http://avro.apache.org/" rel="noopener ugc nofollow" target="_blank"> Avro </a>格式对数据进行编码，以加快日志记录速度</p><p id="dcfe" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">胡迪是围绕基本文件和增量日志文件的概念设计的，这些日志文件存储对给定基本文件(称为文件片)的更新/增量。它们的格式是可插拔的，Parquet(列访问)和HFile(索引访问)是目前支持的基本文件格式。增量日志以<a class="ae jo" href="http://avro.apache.org/" rel="noopener ugc nofollow" target="_blank"> Avro </a>(面向行)格式对数据进行编码，以加快日志记录速度(就像Kafka主题一样)。胡迪独特的文件布局方案将对给定基本文件的所有更改编码为一系列块(数据块、删除块、回滚块),这些块被合并以导出更新的基本文件。本质上，这构成了一个自包含的重做日志，让我们在其上实现有趣的特性。</p><figure class="ld le lf lg fd ii er es paragraph-image"><div class="er es lc"><img src="../Images/7934b9e6a83953c119fb624f752a4f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*t2fF5urM2JXqzRaA.png"/></div></figure><p id="ae02" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">上图来源:<a class="ae jo" href="https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform/" rel="noopener ugc nofollow" target="_blank">https://hudi . Apache . org/blog/2021/07/21/streaming-data-lake-platform/</a></p><p id="96e9" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在胡迪有两种不同类型的桌子，如下所述</p><ol class=""><li id="6f9c" class="lh li hh is b it iu ix iy jb lj jf lk jj ll jn lm ln lo lp bi translated"><strong class="is hi">写入时复制(CoW): </strong>数据以列格式存储(Parquet)，在写入期间更新会创建文件的新版本。这种存储类型最适用于读取量大的工作负载，因为最新版本的数据集总是以高效的列文件形式提供</li><li id="1017" class="lh li hh is b it lq ix lr jb ls jf lt jj lu jn lm ln lo lp bi translated"><strong class="is hi">读时合并(MoR): </strong>数据以列(Parquet)和基于行(Avro)格式的组合存储；更新被记录到基于行的“增量文件”中，并在以后被压缩，以创建列文件的新版本。这种存储类型最适用于写负载繁重的工作负载，因为新提交会作为增量文件快速写入，但是读取数据集需要将压缩的列文件与增量文件合并。</li></ol><p id="62e9" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">写时复制表支持两种查询。</p><p id="4710" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">快照查询:</strong> <br/>查询最近一次快照的表数据，直到最后一次提交成功。</p><p id="f753" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><strong class="is hi">增量查询:</strong> <br/>查询对表所做的增量修改。这些查询有助于下游作业处理更改的数据。<br/>例如:在时间段t1和时间段t2之间将数据提交到表中。(更新+插入)</p><p id="9cb6" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">从胡迪0.9版本开始，它与Spark SQL兼容，因此我们可以编写本地SQL来执行ACID转换，如删除、更新和合并命令</p><h1 id="c823" class="jy jz hh bd ip ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">让我们深入研究代码。</h1><blockquote class="kv kw kx"><p id="fcf8" class="iq ir ky is b it iu iv iw ix iy iz ja kz jc jd je la jg jh ji lb jk jl jm jn ha bi translated"><strong class="is hi">环境设置<br/> Spark版本:2.4 <br/> Apache胡迪版本:0.10 <br/>文件系统:HDFS/GCS等..</strong></p></blockquote><p id="2552" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">下面的代码将启动一个包含所有胡迪包的spark shell，并创建一个胡迪表，然后根据逻辑执行ACID转换</p><pre class="ld le lf lg fd lv lw lx ly aw lz bi"><span id="3777" class="ma jz hh lw b fi mb mc l md me">spark-shell \<br/>--class org.apache.hudi.utilities.HoodieClusteringJob \<br/>- packages org.apache.hudi:hudi-spark-bundle_2.12:0.10.0,org.apache.spark:spark-avro_2.12:2.4.4 \<br/>- conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'</span><span id="f248" class="ma jz hh lw b fi mf mc l md me">import scala.collection.JavaConversions._<br/>import org.apache.spark.sql.SaveMode._<br/>import org.apache.hudi.DataSourceReadOptions._<br/>import org.apache.hudi.DataSourceWriteOptions._<br/>import org.apache.hudi.config.HoodieWriteConfig._</span><span id="6c20" class="ma jz hh lw b fi mf mc l md me">spark.sql(“create table if not exists Hudi_Table ( col1 string, col2 string, col3 string,col4 string,col5 string,col6 int,col7 date,col8 string) using hudi options ( type = ‘cow’, primaryKey = ‘col1’ ) partitioned by (col7,col8)”)</span><span id="845b" class="ma jz hh lw b fi mf mc l md me">spark.sql(“merge into Hudi_Table X using ( select col1,col2,col3,col4,col5,col6,col7,col8 from stage_Table ) source on X.col1 = source.col1 WHEN MATCHED THEN update set X.col2 = source.col2 when NOT matched then INSERT * ")</span></pre><p id="7f2e" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">默认情况下，胡迪表创建下面提到的列以及我们在逻辑中指定的列</p><ul class=""><li id="dfdd" class="lh li hh is b it iu ix iy jb lj jf lk jj ll jn mg ln lo lp bi translated"><strong class="is hi"> _hoodie_commit_time </strong>:最后一次触及该记录的提交。基于此列，将检索记录的最新版本</li><li id="9d40" class="lh li hh is b it lq ix lr jb ls jf lt jj lu jn mg ln lo lp bi translated"><strong class="is hi"> _hoodie_record_key </strong>:被视为每个DFS分区内的主键，所有更新/插入的基础</li><li id="aedb" class="lh li hh is b it lq ix lr jb ls jf lt jj lu jn mg ln lo lp bi translated"><strong class="is hi"> _hoodie_file_name </strong>:包含记录的实际文件名(对筛选重复非常有用)</li><li id="5042" class="lh li hh is b it lq ix lr jb ls jf lt jj lu jn mg ln lo lp bi translated"><strong class="is hi">_ hoodie _ partition _ Path</strong>:basePath中标识包含该记录的分区的路径</li></ul><p id="630a" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">将超过2 GB的数据加载到具有upsert数据的胡迪表中大约需要9分钟</p><figure class="ld le lf lg fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/fdc934f469482f5feacf37e292543c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/0*w6mKMH3WD0KBnd3s.jpeg"/></div></figure><h1 id="eb55" class="jy jz hh bd ip ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">性能调整:</h1><h1 id="84f7" class="jy jz hh bd ip ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">1.聚类:</h1><p id="3f2e" class="pw-post-body-paragraph iq ir hh is b it mi iv iw ix mj iz ja jb mk jd je jf ml jh ji jj mm jl jm jn ha bi translated">文件大小和摄取速度之间的权衡由<strong class="is hi">hoodie . parquet . small . file . limit</strong>属性提供，以配置文件大小。有两种类型的集群调度和执行集群。</p><p id="eadc" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">下面是聚类的例子</p><figure class="ld le lf lg fd ii er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es lc"><img src="../Images/c1d1eb44fc69e5daa865c32e1430910b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sEYtJFAf0RMThxdq.png"/></div></div></figure><p id="1b73" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">上图来源:</p><p id="238b" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated"><a class="ae jo" href="https://hudi.apache.org/blog/2021/01/27/hudi-clustering-intro/" rel="noopener ugc nofollow" target="_blank">https://hudi . Apache . org/blog/2021/01/27/胡迪-聚类-intro/ </a></p><h1 id="3df9" class="jy jz hh bd ip ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">2.批量插入:</h1><p id="dc74" class="pw-post-body-paragraph iq ir hh is b it mi iv iw ix mj iz ja jb mk jd je jf ml jh ji jj mm jl jm jn ha bi translated">通过设置以下两种配置，胡迪支持将bulk_insert作为写操作类型:</p><p id="0bdb" class="pw-post-body-paragraph iq ir hh is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">设置hoodie . SQL . bulk . insert . enable = true；<br/>设置hoodie . SQL . insert . mode =非严格；</p><h1 id="41e8" class="jy jz hh bd ip ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">3.索引</h1><p id="5a6e" class="pw-post-body-paragraph iq ir hh is b it mi iv iw ix mj iz ja jb mk jd je jf ml jh ji jj mm jl jm jn ha bi translated">当您将记录键建模为单调递增(例如时间戳前缀)时，胡迪提供了最佳的索引性能，从而导致范围修剪过滤掉大量文件进行比较。即使对于基于UUID的按键，也有已知的技术来实现这一点。例如，在一个包含80B个关键字/3个分区/11416个文件/10TB数据的事件表上，使用100M个带时间戳前缀的关键字(5%的更新，95%的插入)，胡迪索引实现了比普通spark join快7倍(2880秒对440秒)的速度。</p><h1 id="1a3c" class="jy jz hh bd ip ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">局限性:</h1><ol class=""><li id="7d12" class="lh li hh is b it mi ix mj jb mr jf ms jj mt jn lm ln lo lp bi translated">目前，Apache胡迪0.10只提供基于主键列更新记录的功能。不支持基于非主键列或主键列的更新</li><li id="cc30" class="lh li hh is b it lq ix lr jb ls jf lt jj lu jn lm ln lo lp bi translated">如果定义了复合主键，则需要在ON条件中指定所有列</li><li id="0873" class="lh li hh is b it lq ix lr jb ls jf lt jj lu jn lm ln lo lp bi translated">胡迪表上不支持varchar、时间戳等数据类型</li></ol><h1 id="6432" class="jy jz hh bd ip ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">可能的解决方案:</h1><ol class=""><li id="0f44" class="lh li hh is b it mi ix mj jb mr jf ms jj mt jn lm ln lo lp bi translated"><strong class="is hi"> Delta : </strong>它是另一种文件格式，在分布式文件系统之上提供ACID转换功能。在这里，我们也可以基于非主键列更新记录。</li><li id="6170" class="lh li hh is b it lq ix lr jb ls jf lt jj lu jn lm ln lo lp bi translated"><strong class="is hi"> Iceberg : </strong>这是一种针对大型分析数据集的开放式表格格式。Iceberg为Spark、PrestoDB、Hive等计算引擎添加了表格..它还提供了基于非主键更新记录的功能</li></ol><h1 id="80b3" class="jy jz hh bd ip ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">参考资料:</h1><p id="0dc3" class="pw-post-body-paragraph iq ir hh is b it mi iv iw ix mj iz ja jb mk jd je jf ml jh ji jj mm jl jm jn ha bi translated">【https://hudi . Apache . org/blog/2021/07/21/streaming-data-lake-platform/<br/><a class="ae jo" href="https://hudi.apache.org/docs/quick-start-guide" rel="noopener ugc nofollow" target="_blank">https://hudi.apache.org/docs/quick-start-guide</a><br/><a class="ae jo" href="https://github.com/apache/hudi/tree/master/hudi-examples" rel="noopener ugc nofollow" target="_blank">https://github.com/apache/hudi/tree/master/hudi-examples</a><br/><a class="ae jo" href="https://cwiki.apache.org/confluence/display/HUDI/Tuning+Guide" rel="noopener ugc nofollow" target="_blank">https://CWI ki . Apache . org/confluence/display/胡迪/Tuning+Guide </a></p></div></div>    
</body>
</html>