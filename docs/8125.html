<html>
<head>
<title>DataBathing — A Framework for Transferring the Query to Spark Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据打包——一个将查询转移到Spark代码的框架</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/databathing-a-framework-for-transferring-the-query-to-spark-code-484957a7e049?source=collection_archive---------0-----------------------#2022-06-24">https://medium.com/walmartglobaltech/databathing-a-framework-for-transferring-the-query-to-spark-code-484957a7e049?source=collection_archive---------0-----------------------#2022-06-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="b886" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">关于查询配置驱动编码的迷你指南</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/01739903dc00c7710451ef7b5264203d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ndNqVw_3gT4Q0qsA2aoK9Q.jpeg"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Photo credit: Pixabay</figcaption></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><p id="6fbc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们的团队已经成功地从Hive SQL驱动转变为代码驱动的数据工程。我们每天都在用Spark (Scala或者Python)，计算性能大幅提升。(我们已经将平均运行时间减少了10–80%。)</p><p id="6d67" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">然而，编码会花费更多的时间，不同的开发人员会有不同版本的编码风格，这会影响spark作业的性能。</p><p id="970c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么如何才能解决上述问题呢？我们能否有一些标准化的方法来将SQL用于复杂的管道？— <em class="jz">我会在下面的博客中解释为什么我们不想直接使用Spark SQL。</em></p><p id="2235" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> <em class="jz">是的，这就是为什么DataBathing要来了！！！</em> </strong></p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="49d6" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">议程</h1><ul class=""><li id="7e83" class="ky kz hh ig b ih la il lb ip lc it ld ix le jb lf lg lh li bi translated">数据打包即将到来</li><li id="ddf6" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">Spark数据框架与Spark SQL</li><li id="ea3c" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">英雄:mo _ sql _解析</li><li id="b253" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">小演示</li><li id="64ca" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">当前支持的功能</li><li id="c64e" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">下一个路线图</li><li id="431b" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">贡献</li><li id="c442" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">谢谢</li><li id="7184" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">摘要</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="08b6" class="ka kb hh bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">数据交换即将到来</strong></h1><p id="619c" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">DataBathing是一个可以将SQL转换为Spark Dataframe计算流程代码的库。我们可以在图1中找到高级示例。DataBathing可以解析SQL查询，输出将是PySpark或Scala Spark代码。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es lr"><img src="../Images/6f8aba25295af0f0cca020c70bc8c503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CblX0wqENA_UEgRvC7Zj6A.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Figure 1 — What DataBathing can do !!!</figcaption></figure><p id="113c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在图1中，我们将逻辑保存为数据帧，以备将来在数据谱系的每个阶段使用。我们将在下一节讨论这种设计的原因。简而言之，我们使用<em class="jz">“with statement”</em>来链接我们管道中的逻辑。</p><p id="a7dc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>第一阶段，我们使用数据打包来提取SQL的逻辑和数据血统。通常，在使用<em class="jz"> "with statement" </em>创建逻辑之后，我们可以轻松地将数据帧计算复制并粘贴到我们的实际管道中。图2显示了流程。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es mb"><img src="../Images/ecdcc7067bfdfd719dd8dbe768032e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJp03Zk30n-tBWh2A6GYXQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Figure 2 — Phase I for DataBathing Usage</figcaption></figure><h1 id="4556" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated">Spark数据框架与Spark SQL</h1><p id="a291" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">如果您以前使用过Spark，您可能会奇怪为什么我们需要将SQL转换为Spark Dataframe计算。我会解释的。</p><h2 id="8dee" class="mh kb hh bd kc mi mj mk kg ml mm mn kk ip mo mp ko it mq mr ks ix ms mt kw mu bi translated"><strong class="ak">技术问题</strong></h2><ul class=""><li id="997f" class="ky kz hh ig b ih la il lb ip lc it ld ix le jb lf lg lh li bi translated">直到运行时我们才知道语法错误——尤其是Scala部分——当使用Spark SQL时。这将是巨大的发展成本。然而，对于Dataframe计算流，我们没有这个问题，因为错误将在编译时被捕获。</li><li id="6ebb" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">如果我们将一个大型SQL查询放入Spark SQL运行任务中，并且该查询是低效的，那么这将是ETL作业<em class="jz">的一个问题。</em>另一方面，紧凑的火花数据帧计算可以提高性能并提供清晰的数据谱系。这是我们从数据打包中得到的好处之一:将复杂的逻辑分割成更小的部分。 </li></ul><h2 id="f6e1" class="mh kb hh bd kc mi mj mk kg ml mm mn kk ip mo mp ko it mq mr ks ix ms mt kw mu bi translated"><strong class="ak">下一步计划</strong></h2><p id="f8e1" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">DataBathing不仅是一个可以帮助开发人员将SQL解析成Spark Dataframe计算流的库；这也是我们下一代<em class="jz">自动生成管道框架</em>使用的重要基础。</p><blockquote class="mv mw mx"><p id="216f" class="ie if jz ig b ih ii ij ik il im in io my iq ir is mz iu iv iw na iy iz ja jb ha bi translated">自动生成的管道框架:简而言之，用了这个框架之后，就不需要写spark代码了；它可以帮助我们用框架生成基于逻辑本身的管道。(我会在以后的博客里解释。)</p></blockquote><p id="1392" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>数据打包的第二阶段，所有的SQL查询都在一个管道中被解析和连接；如果我们想利用一些中间件逻辑，框架可以帮助我们。使用DataBathing的<em class="jz">“with statement”</em>设计，我们可以轻松实现我们的目标。在图3中，我们的下一代<em class="jz">自动生成的管道</em>在内部顺序调用我们的DataBathing服务，以获取Spark数据帧计算流并组合它们。我们将在下一篇<a class="ae nb" rel="noopener" href="/walmartglobaltech/modularization-using-auto-generated-pipeline-with-databathing-8f78b94dad08">博客</a>中详述这种用法。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nc"><img src="../Images/42fd5ec50de174831dc5088448059aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fHVjJsP1XXiM1eRMTsX4Dw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx">Figure 3— Phase II for DataBathing Usage</figcaption></figure><h1 id="2f15" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated"><strong class="ak">英雄:mo _ sql _解析</strong></h1><p id="6818" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">如何将SQL解析成结构化格式？用mo_sql_parsing！</p><blockquote class="nd"><p id="8b8c" class="ne nf hh bd ng nh ni nj nk nl nm jb dx translated">mo_sql_parsing是一个将sql解析成JSON的库。</p></blockquote><p id="e94c" class="pw-post-body-paragraph ie if hh ig b ih nn ij ik il no in io ip np ir is it nq iv iw ix nr iz ja jb ha bi translated">请参考下面的例子和Github链接:</p><blockquote class="mv mw mx"><p id="79a5" class="ie if jz ig b ih ii ij ik il im in io my iq ir is mz iu iv iw na iy iz ja jb ha bi translated">&gt; &gt; &gt; parse("select a as hello，b as world from jobs ")<br/>{ ' select ':[{ ' value ':' a '，' name': 'hello'}，{'value': 'b '，' name': 'world'}]，' from': 'jobs'}</p></blockquote><div class="ns nt ez fb nu nv"><a href="https://github.com/klahnakoski/mo-sql-parsing" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab dw"><div class="nx ab ny cl cj nz"><h2 class="bd hi fi z dy oa ea eb ob ed ef hg bi translated">GitHub-klahnakoski/mo-sql-parsing:让我们制作一个SQL解析器，这样我们就可以提供一个熟悉的接口…</h2><div class="oc l"><h3 class="bd b fi z dy oa ea eb ob ed ef dx translated">将SQL解析为JSON，这样我们就可以将其转换为其他数据存储！查看更改SQL是一种熟悉的语言，用于访问…</h3></div><div class="od l"><p class="bd b fp z dy oa ea eb ob ed ef dx translated">github.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj jm nv"/></div></div></a></div><h1 id="89ba" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated"><strong class="ak">小演示</strong></h1><p id="c635" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">您还可以从给定的SQL查询中生成PySpark代码。请安装PyPI的<a class="ae nb" href="https://pypi.org/project/databathing/" rel="noopener ugc nofollow" target="_blank">数据打包</a>包。</p><blockquote class="mv mw mx"><p id="9491" class="ie if jz ig b ih ii ij ik il im in io my iq ir is mz iu iv iw na iy iz ja jb ha bi translated"><code class="du ok ol om on b"><em class="hh">pip install databathing</em></code></p><p id="db5e" class="ie if jz ig b ih ii ij ik il im in io my iq ir is mz iu iv iw na iy iz ja jb ha bi translated"><code class="du ok ol om on b">&gt;&gt;&gt; from databathing import Pipeline<br/>&gt;&gt;&gt; pipeline = Pipeline("SELECT * FROM Test WHERE info = 1")<br/>'final_df = Test\\\n.filter("info = 1")\\\n.selectExpr("a","b","c")\n\n'</code></p></blockquote><h1 id="e5b8" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated">当前支持的功能</h1><p id="6b07" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">特性和功能— PySpark版本</p><ul class=""><li id="7b9b" class="ky kz hh ig b ih ii il im ip oo it op ix oq jb lf lg lh li bi translated"><code class="du ok ol om on b">SELECT</code>特写</li><li id="968b" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">FROM</code>功能</li><li id="738c" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">INNER</code> <code class="du ok ol om on b">JOIN</code>和<code class="du ok ol om on b">LEFT</code> <code class="du ok ol om on b">JOIN</code>功能</li><li id="913e" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">ON</code>特色</li><li id="f544" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">WHERE</code>特性</li><li id="549b" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">GROUP BY</code>功能</li><li id="891a" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">HAVING</code>功能</li><li id="6d3c" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">ORDER BY</code>功能</li><li id="be1d" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">AGG</code>功能</li><li id="9f4d" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">WINDOWS功能特性(<code class="du ok ol om on b">SUM</code>、<code class="du ok ol om on b">AVG</code>、<code class="du ok ol om on b">MAX</code>、<code class="du ok ol om on b">MIN</code>、<code class="du ok ol om on b">MEAN</code>、<code class="du ok ol om on b">COUNT</code>、<code class="du ok ol om on b">COLLECT_LIST</code>、<code class="du ok ol om on b">COLLECT_SET</code>)</li><li id="8acd" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">别名功能</li><li id="1b10" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">WITH</code>报表功能</li><li id="5189" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><code class="du ok ol om on b">SPLIT</code>特性</li></ul><h1 id="f7d3" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated">下一个路线图</h1><p id="6395" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">在下一个路线图中，我将重点关注:</p><ul class=""><li id="b9a3" class="ky kz hh ig b ih ii il im ip oo it op ix oq jb lf lg lh li bi translated">更多测试案例</li><li id="bdee" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">PySpark中12个特性的Scala版本</li><li id="7cec" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">更多加入功能，如<code class="du ok ol om on b">OUTER, FULL, FULLOUTER, LEFTSEMI, LEFTANTI</code></li></ul><h1 id="1ad4" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated">贡献</h1><p id="a99a" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">如果您需要来自数据打包的更多特性，您可以提出一个问题并显示错误，或者您可以创建一个带有新特性和测试的PR。如果你也提交了一个补丁，那么我会感谢你的。</p><p id="3dcc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">请按照以下说明在提交PR时更新版本，因为我们将cicleci用于CICD流程:</p><div class="ns nt ez fb nu nv"><a href="https://circleci.com/blog/publishing-a-python-package/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab dw"><div class="nx ab ny cl cj nz"><h2 class="bd hi fi z dy oa ea eb ob ed ef hg bi translated">发布Python包</h2><div class="oc l"><h3 class="bd b fi z dy oa ea eb ob ed ef dx translated">对于许多软件工程师和开发人员来说，使用标准库或内置对象是不够的。为了拯救…</h3></div><div class="od l"><p class="bd b fp z dy oa ea eb ob ed ef dx translated">circleci.com</p></div></div><div class="oe l"><div class="or l og oh oi oe oj jm nv"/></div></div></a></div><p id="a261" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">您可以在GitHub主页上找到DataBathing:</p><div class="ns nt ez fb nu nv"><a href="https://github.com/jason-jz-zhu/databathing" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab dw"><div class="nx ab ny cl cj nz"><h2 class="bd hi fi z dy oa ea eb ob ed ef hg bi translated">GitHub-Jason-JZ-Zhu/data bathing</h2><div class="oc l"><h3 class="bd b fi z dy oa ea eb ob ed ef dx translated">将SQL解析为JSON，这样我们就可以将其转换为其他数据存储！查看从sql转换到spark后的变化，数据…</h3></div><div class="od l"><p class="bd b fp z dy oa ea eb ob ed ef dx translated">github.com</p></div></div><div class="oe l"><div class="os l og oh oi oe oj jm nv"/></div></div></a></div><h1 id="d891" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated">谢谢</h1><p id="ea99" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lo ir is it lp iv iw ix lq iz ja jb ha bi translated">当我开始设计和实现数据打包时，我发现这篇文章很有帮助。我要感谢作者，尽管我不知道他们是谁。</p><div class="ns nt ez fb nu nv"><a href="https://sqlandhadoop.com/online-sql-to-pyspark-converter/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab dw"><div class="nx ab ny cl cj nz"><h2 class="bd hi fi z dy oa ea eb ob ed ef hg bi translated">在线SQL到PySpark转换器</h2><div class="oc l"><h3 class="bd b fi z dy oa ea eb ob ed ef dx translated">最近很多人向我寻求帮助，问我是否可以帮助他们学习PySpark，我想到了…</h3></div><div class="od l"><p class="bd b fp z dy oa ea eb ob ed ef dx translated">sqlandhadoop.com</p></div></div><div class="oe l"><div class="ot l og oh oi oe oj jm nv"/></div></div></a></div><h1 id="c33b" class="ka kb hh bd kc kd mc kf kg kh md kj kk kl me kn ko kp mf kr ks kt mg kv kw kx bi translated">摘要</h1><ul class=""><li id="ad8c" class="ky kz hh ig b ih la il lb ip lc it ld ix le jb lf lg lh li bi translated">我们可以通过使用SQL风格<em class="jz">“with statement”</em>和简洁明了的代码<em class="jz">，使用数据打包来减少我们的开发时间。</em></li><li id="0cc9" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">在此期间，我们可以使用它来欣赏Spark的性能计算。</li><li id="352f" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated">最重要的是，当与<em class="jz">自动生成的管道框架</em>结合时，将会有巨大的不同。</li><li id="85c9" class="ky kz hh ig b ih lj il lk ip ll it lm ix ln jb lf lg lh li bi translated"><strong class="ig hi">下期见</strong> <a class="ae nb" rel="noopener" href="/walmartglobaltech/modularization-using-auto-generated-pipeline-with-databathing-8f78b94dad08"> <strong class="ig hi"> <em class="jz">自动生成管道框架</em>博客</strong> </a> <strong class="ig hi">！</strong></li></ul></div></div>    
</body>
</html>