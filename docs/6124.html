<html>
<head>
<title>How machine learning significantly improves engagement abroad</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习如何显著提高国外的参与度</h1>
<blockquote>原文：<a href="https://medium.com/pinterest-engineering/how-machine-learning-significantly-improves-engagement-abroad-98c6ca937f9f?source=collection_archive---------3-----------------------#2017-03-23">https://medium.com/pinterest-engineering/how-machine-learning-significantly-improves-engagement-abroad-98c6ca937f9f?source=collection_archive---------3-----------------------#2017-03-23</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="8b9d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作者:郭| Pinterest工程师，home feed</p><p id="8f4e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">去年，Pinterest成为一家真正的全球性公司，超过一半的人在美国以外使用我们的应用程序。我们投入了大量资金，让Pinterest对所有人都变得个性化、快速和可靠，无论语言、位置或设备如何，这在全球范围内都带来了巨大的保留率和参与度。例如，使用机器学习模型，我们在过去一年将美国以外国家的本地化pin数量增加了250%。现在，每月访问Pinterest的1.5亿多人中的每一个人都会看到与他们的国家和语言最相关的大头针以及其他信号。最终，机器学习使我们能够扩展我们的系统，以更好地理解国际内容和Pinners，并将体验本地化以加速增长。</p><p id="e7e9" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">虽然有许多团队和项目为我们的国际化努力做出了贡献，但在这篇文章中，我们将介绍机器学习如何改善国外Pinners的home feed体验，并导致去年home feed参与度的最大收益。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/e5a2d3edbde4c4936f32203cff5ac974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJ85A-uJrHDAycCfmVpf4g.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Pinterest home feed</figcaption></figure><h2 id="2aea" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated"><strong class="ak">Pinterest首页提要</strong></h2><p id="5ce0" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">如果我们不首先从流量最高的服务之一home feed开始，我们将看不到任何收益。我们1.5亿活跃Pinners中的每个人每次访问时都会获得一个个性化的home feed，这需要机器学习应用程序来不断提高想法的相关性。(关于机器学习问题设置、训练实例生成、早期功能和我们在home feed排名中使用的线性模型的更多细节，请查看<a class="ae jc" rel="noopener" href="/@Pinterest_Engineering/pinnability-machine-learning-in-the-home-feed-64be2074bf60#.8sljzadrn">之前的这篇博文</a>。)多年来，我们已经将我们的排名模型从逻辑回归发展到支持向量机(SVM)，现在又发展到梯度推进决策树(GBDT)。在接下来的章节中，我们将介绍建模和特征工程管道的三个主要改进。</p><h2 id="9663" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated"><strong class="ak">从线性模型到梯度推进决策树(GBDT) </strong></h2><p id="c007" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">我们面临的一个中心问题是为Pinners提供源源不断的相关和吸引人的内容，根据他们的品味和兴趣进行个性化。Home feed通过使用机器学习模型(也称为Pinnability)来解决这个问题，根据预测的相关性对一大组候选pin进行排序，并呈现最相关的pin。Pinnability模型的相关性预测的准确性在一定程度上是Pinterest成功的关键。以前，我们只使用线性模型，逻辑回归或具有线性核函数的SVM排名，来建立相关性预测模型。当相对少量的特征(&lt;100) were used, the linear models were sufficient to explore the feature space and generate a good prediction outcome. However, as we added more features to Pinnability, the linear models were quickly reaching their limits in exploring the ever-complex feature space, so we applied more complex modeling techniques.</p><p id="a36a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">From our A/B experiments, we found <a class="ae jc" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">梯度增强决策树</a> (GBDT)提供最佳参与结果时，所以我们在去年年初在home feed中完全生产了它们。GBDT模型帮助Pinners在他们的主页中发现更多相关的、个性化的想法。因此，每天从家庭饲料中节省针的人数增加了10%以上，这是去年家庭饲料参与量增加最多的一次。GBDT模型特别提高了美国以外的人发现和保存的相关pin的数量，根据国家的不同，最多可达18%。我们使用<a class="ae jc" href="https://cwiki.apache.org/confluence/display/Hive/Home#Home-ApacheHive" rel="noopener ugc nofollow" target="_blank"> Apache Hive </a>和<a class="ae jc" href="http://www.cascading.org/" rel="noopener ugc nofollow" target="_blank"> Cascading </a>生成训练数据，并使用<a class="ae jc" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> xgboost </a>在单个AWS r3.8xlarge实例上离线训练我们的GBDT模型。对于更大的训练数据集，我们开发了在我们内部的Spark集群上分布式训练GBDT模型的能力。现在，我们在由350棵深度为7的树组成的生产模型中使用了700多个密集特征和一些稀疏特征。</p><h2 id="8b7d" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated"><strong class="ak">语言和国家匹配特性</strong></h2><p id="8c81" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">起初，我们的模型不包含美国以外的Pinners的具体特征，因此来自不同国家的具有相似兴趣的人过去常常得到相同的相关性预测分数。为了改善这种体验，我们启动了一个本地化home feed的项目。正如你所料，根据我们的研究，美国以外的Pinners通常更喜欢以他们自己的语言显示文本的pin。我们将Pin的语言和国家定义为保存Pin的外部网页的主要语言和国家。我们基于外部页面的文本开发了准确的语言检测模型，并添加了数十种国家/语言匹配功能，例如<em class="kt">Pin ner的语言和国家是否与Pin的语言和国家相同、</em>以及<em class="kt">Pin ner的语言是否是保存Pin的其他人使用的前X种语言。</em>我们总共定义了大约40种基于语言和国家的匹配特性，并将它们添加到我们的GBDT模型中。凭借这些模型特性，我们将每个国际Pinner节省的pin数量提高了10%到20 %,具体取决于国家/地区。</p><p id="204c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们测量的另一个重要指标是本地源本地性，即本地源中使用Pinner第一语言的引脚的百分比。由于我们同时测量本地性和保存pin的人数，我们有时会观察到更多的人保存pin和更少的本地化pin之间的权衡，特别是对于排序的候选pin相对较少的语言。例如，一个生产排名模型将节省图钉的人数提高了4 %,而本地性降低了8%到10%。我们通过为GBDT开发一个适应性培训流程来解决这个问题，这将在下一节中介绍。</p><h2 id="6dac" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ip ke kf kg it kh ki kj ix kk kl km kn bi translated"><strong class="ak">GBDT车型的适应性训练</strong></h2><p id="9e22" class="pw-post-body-paragraph ie if hh ig b ih ko ij ik il kp in io ip kq ir is it kr iv iw ix ks iz ja jb ha bi translated">在我们的训练数据集中，保存的pin和点击的pin都是正面例子，而没有导致保存或点击的印象是负面例子。对于逻辑损失函数，我们使用xgboost的加权GBDT训练。我们开发了一个自适应训练过程，在训练过程中为不同的树指定不同的实例加权方案。例如，在训练中，我们可以指定保存的Pin的每个实例的权重是前100棵树的点进实例的两倍。然后，对于接下来的100棵树，全局保存的Pin的每个实例的权重是在美国保存的Pin的3倍。我们将为不同的树设置不同的实例权重的过程称为“自适应GBDT训练”(如图1所示)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ku"><img src="../Images/3468f68f866c4e53e2afeb6c2e54063a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uko8a52_6s73bVYH."/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Fig 1: Illustration of an adaptively trained GBDT model</figcaption></figure><p id="fd55" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在实践中，我们应用三种不同的加权方案在我们的prod GBDT模型中生成350棵二叉树。我们首先训练150棵具有高pin保存权重的树，然后使用这150棵树模型作为基础，训练另外100棵具有针对pin保存和点击率的精确权重设置的树。最后，我们添加了100棵正面国际实例权重明显较高的树，其中包括美国以外任何Pinner保存的pin和点击率。每棵树都是由80%随机抽样的训练实例生成的。为了获得最佳参与度，我们逐渐降低了三个适应性培训阶段的学习率。</p><p id="ff34" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们发现，在适应性训练过程中适当地重新加权实例显著地改善了Pinners的体验。与我们之前的GBDT模型相比，仅这种适应性训练过程就使国外每天从家中节省别针的人数增加了10%以上。</p><p id="e357" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们不断尝试新的方法来提高我们每天向Pinners提供的超过100亿条建议的相关性，机器学习模型是核心。除了GBDT，我们已经开始开发基于TensorFlow的深度神经网络(DNN)模型，用于home feed相关性排名。如果你对解决这些机器学习挑战感兴趣，<a class="ae jc" href="https://careers.pinterest.com/" rel="noopener ugc nofollow" target="_blank">加入我们</a>！</p><p id="4c88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="kt">鸣谢:这是与德米特里·切奇克、刘鑫和穆昆德·纳拉辛汗的合作。</em></p><p id="f956" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">【】我们也感谢陈晓芳、Derek Cheng、、Stephanie DeWet、Chris Pinchak、Fu、、、和Dan Frankowski富有成效的讨论和建议。</p></div></div>    
</body>
</html>