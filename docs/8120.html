<html>
<head>
<title>Voice Reorder Experience: add Multiple Product Items to your shopping cart</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语音订购体验:将多个产品添加到您的购物车</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/voice-reorder-experience-add-multiple-product-items-to-your-shopping-cart-59d20fc61797?source=collection_archive---------7-----------------------#2022-05-17">https://medium.com/walmartglobaltech/voice-reorder-experience-add-multiple-product-items-to-your-shopping-cart-59d20fc61797?source=collection_archive---------7-----------------------#2022-05-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/ce3c40eb5504a08cb35bd7f7479584b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qI9VT-XeNqSC4K62-_maow.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Credit: <a class="ae it" href="https://pixabay.com/illustrations/grocery-shopping-groceries-app-5987164/" rel="noopener ugc nofollow" target="_blank">Voice reorder your grocery shopping</a></figcaption></figure><p id="3560" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">电子商务语音订购系统需要从订购话语中识别多个产品名称实体。现有的语音订购系统，如亚马逊Alexa，只能捕获单个产品名称实体。这限制了用户用一句话订购多个项目。近年来，预训练的语言模型，如BERT和GPT-2，在超级胶水等NLP基准上显示出有希望的结果。然而，由于语音订购话语的模糊性，他们不能完美地概括这种多产品名称实体识别(MPNER)任务。为了填补这一研究空白，我们提出了实体转换器(ET)神经网络结构，它可以识别一个话语中多达10个项目。在我们的评估中，与非神经模型相比，最佳ET模型(conveRT + ngram + ET)在测试集上的性能提高了12%,并且在ET方面也优于BERT。这有助于客户通过语音对话完成购物车，从而提高购物效率和体验。</p><h1 id="033a" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">问题定义</h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kq"><img src="../Images/1fd4faa40306e7108e3eb43b5425ce14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ZAmx_b3CEhk_QMNQWrqQA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">A Sample Utterance</figcaption></figure><p id="004f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">以一个示例话语为例，如“在一加仑牛奶中加入七个苹果，两袋葵花籽、新鲜大蒜和一次性湿巾”。产品名称实体识别的任务是识别诸如“苹果”、“牛奶”、“葵花籽”、“新鲜大蒜”、“一次性湿巾”等产品实体。产品名称的数量和多样性对于分割和提取它们是一个挑战。通常，自动语音识别(ASR)系统在将语音翻译成文本时，由于其局限性，不会产生标点和符号。这大大增加了区分相邻产品名称的难度。由于这些障碍(例如，实体数量、缺失标点和产品多样性)对于传统的NER任务来说是不常见的，现有的方法可能不能很好地推广到MPNER。</p><h1 id="70d0" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">模型架构</h1><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kv"><img src="../Images/f4127ce5a633d18001bad1c223664095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjTGgMg-BhGMs3TA7CWxUA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Entity Transformer architecture</figcaption></figure><p id="a969" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">一种称为实体变换器(ET)的编码器-解码器架构是针对受(<a class="ae it" href="https://arxiv.org/abs/2004.09936" rel="noopener ugc nofollow" target="_blank"> Bunk等人，2020 </a>)启发而提出的模型而开发的。在这种设计中，输入序列由变压器层的堆叠进行编码(<a class="ae it" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Vaswani等人，2017 </a>)。编码器映射输入序列(x₁，…，xₙ)到一系列连续的表现E =(e₁，…，eₙ).从e，条件随机场(CRF)解码器产生输出实体标签预测序列(l₁，…，lₙ).</p><h2 id="02a8" class="kw jt hh bd ju kx ky kz jy la lb lc kc jf ld le kg jj lf lg kk jn lh li ko lj bi translated">特征</h2><p id="09b1" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">对于密集特征，我们从各种预先训练好的语言模型(<a class="ae it" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">抱脸</a>)中提取输入序列特征，如来自PolyAI的conveRT(【Henderson et al .，2020 )、BERT(【Devlin et al .，2019 )、Roberta ( <a class="ae it" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> Liu et al .，2019 </a>)。侧面稀疏特性是一个热令牌级编码和多个热字符n元语法(n≤4)特性。稀疏特征被传递到前馈层，其权重通过输入序列共享。前馈神经网络(FNN)层输出和密集序列特征在传递到变换器编码器层之前被连接。</p><h2 id="49b8" class="kw jt hh bd ju kx ky kz jy la lb lc kc jf ld le kg jj lf lg kk jn lh li ko lj bi translated">变形金刚(电影名)</h2><p id="ad86" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">为了对输入序列进行编码，我们使用了具有相对位置注意的堆叠变压器层(N ≤6)。每个变换器编码器层由多头注意层和逐点前馈层组成。这些子层产生维度dₘₒ𝒹ₑₗ = 256的输出。注意力头的数量:Nₕₑₐ𝒹ₛ = 4。变压器的单元数:S = 256。</p><h2 id="87d5" class="kw jt hh bd ju kx ky kz jy la lb lc kc jf ld le kg jj lf lg kk jn lh li ko lj bi translated">条件随机场</h2><p id="0680" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">命名实体识别任务的解码器是条件随机场(CRF) ( <a class="ae it" href="https://arxiv.org/abs/1603.01360" rel="noopener ugc nofollow" target="_blank"> Lample et al .，2016 </a>)，它联合对输入序列的标记决策序列进行建模。我们使用了BILOU标记方案。</p><h2 id="5c4c" class="kw jt hh bd ju kx ky kz jy la lb lc kc jf ld le kg jj lf lg kk jn lh li ko lj bi translated">模型训练和推理</h2><p id="7851" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">模型训练和推理的细节在发表于WeCNLP(西海岸NLP)2021峰会(<a class="ae it" href="https://arxiv.org/pdf/2110.14843.pdf" rel="noopener ugc nofollow" target="_blank"> Gubbala和<br/> Zhang，2021 </a>)的论文中给出</p><h1 id="9579" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">估价</h1><p id="c268" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">我们创建了一个MPNER数据集，由来自语音订单购物话语的大约100万个产品名称实体组成。该数据集具有500k训练子集和65k测试子集话语。每个数据示例中的话语具有随机数(在1和10之间)的乘积。该数据是从70个种子语音顺序话语变体中生成的，使用了来自各个部门的最受欢迎的40k产品的基于同义词的数据扩充。测试数据是由来自相同类别的5k个未知产品创建的。表1显示了MPNER数据集上的模型评估结果。我们的最佳模型配置是具有单词计数向量、char n-gram计数向量(n≤4)的稀疏特征，并且转换预先训练的密集嵌入。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lp"><img src="../Images/aceadf4962115a16a2a1aaa1ed872c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jphzRkmc0NqUQLbCboOAqA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Table 1: Model Performance on MPNER task</figcaption></figure><p id="06e9" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在未来，我们计划研究为MPNER任务定制的预训练模型。</p></div><div class="ab cl lq lr go ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="ha hb hc hd he"><h1 id="0433" class="js jt hh bd ju jv lx jx jy jz ly kb kc kd lz kf kg kh ma kj kk kl mb kn ko kp bi translated">参考资料:</h1><p id="9466" class="pw-post-body-paragraph iu iv hh iw b ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr ha bi translated">[1]普拉尼思·古巴拉和张璇。2021.<a class="ae it" href="https://arxiv.org/abs/2110.14843" rel="noopener ugc nofollow" target="_blank">从对话框</a>中提取多个产品<br/>名称实体的序列<br/>到序列模型。</p><p id="4418" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[2]塔尼娅·邦克、达克什·瓦什涅亚、弗拉迪米尔·弗拉索夫和<br/>艾伦·尼科尔。2020.<a class="ae it" href="https://arxiv.org/abs/2004.09936" rel="noopener ugc nofollow" target="_blank"> Diet:轻量级语言un- <br/>理解对话系统</a>。</p><p id="e250" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[3]马修·亨德森，I·̃nigo·卡萨努埃瓦，尼古拉·姆尔克西奇，<br/>佩-苏浩，温宗贤，和伊万·武利奇. 2020。<br/> <a class="ae it" href="https://arxiv.org/abs/1911.03688" rel="noopener ugc nofollow" target="_blank">转换:来自变形金刚</a>的高效、准确的对话表达。</p><p id="ddac" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[4] Devlin，j .，Chang，m .，Lee，k .，&amp; Toutanova，K. (2018年)。<a class="ae it" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a>。</p><p id="08cc" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[5]刘、Myle Ott、Naman Goyal、杜、Man-dar Joshi、陈、Omer Levy、、Luke Zettlemoyer和Veselin Stoyanov。2019.<a class="ae it" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> Roberta:一种稳健优化的BERT预训练<br/>方法</a>。更正，abs/1907.11692</p><p id="dd19" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">【6】抱脸。<a class="ae it" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/</a></p></div></div>    
</body>
</html>