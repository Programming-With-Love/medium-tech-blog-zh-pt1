<html>
<head>
<title>All You Need To Know About Principal Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于主成分分析(PCA)你需要知道的</h1>
<blockquote>原文：<a href="https://medium.com/edureka/principal-component-analysis-69d7a4babc96?source=collection_archive---------1-----------------------#2019-08-28">https://medium.com/edureka/principal-component-analysis-69d7a4babc96?source=collection_archive---------1-----------------------#2019-08-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/297339b2c7213c2f31d36e8139b4db40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*7rCP8CTfX_AlGIMqtOgB6w.png"/></div><figcaption class="il im et er es in io bd b be z dx">Principal Component Analysis — Edureka</figcaption></figure><p id="ed72" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">随着机器学习和人工智能领域的进步，理解这些技术背后的基础变得至关重要。本文将帮助您理解降维背后的概念，以及如何使用它来处理高维数据。</p><p id="c4c4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以下是本文将涉及的主题列表:</p><ol class=""><li id="d10c" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">需要主成分分析</li><li id="2f97" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">主成分分析的逐步计算</li><li id="66ac" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">标准化</li></ol><ul class=""><li id="0410" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm kb jt ju jv bi translated">计算协方差矩阵</li><li id="ac07" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm kb jt ju jv bi translated">计算特征向量和特征值</li><li id="c259" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm kb jt ju jv bi translated">计算主成分</li><li id="48d2" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm kb jt ju jv bi translated">减少数据维度</li></ul><p id="f267" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">4.用Python实现主成分分析</p><h1 id="0ac6" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">需要主成分分析</h1><p id="166b" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">当为训练机器提供的数据集庞大而简洁时，机器学习通常会产生奇迹。通常拥有大量的数据可以让我们建立更好的预测模型，因为我们有更多的数据来训练机器。然而，使用大型数据集有其自身的缺陷。最大的陷阱是维数灾难。</p><p id="f07b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">事实证明，在大规模数据集中，可能存在大量不一致的特征或数据集中的大量冗余特征，这只会增加计算时间，使数据处理和EDA更加复杂。</p><p id="c4f5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了摆脱维数灾难，引入了一种叫做降维的过程。降维技术可以用来过滤训练所需的有限数量的重要特征，这就是PCA的用武之地。</p><h1 id="92ec" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">什么是主成分分析？</h1><p id="6a56" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated"><em class="lf">主成分分析(PCA)是一种降维技术，使您能够识别数据集中的相关性和模式，以便将其转换为维度显著降低的数据集，而不会丢失任何重要信息。</em></p><p id="df35" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">PCA背后的主要思想是找出数据集中各种特征之间的模式和相关性。在发现不同变量之间的强相关性时，最终决定以仍然保留重要数据的方式减少数据的维度。</p><p id="3839" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在解决涉及使用高维数据集的复杂数据驱动问题时，这样的过程是非常重要的。PCA可以通过一系列步骤实现。让我们讨论整个端到端的过程。</p><h1 id="6d8e" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">主成分分析的逐步计算</h1><p id="708a" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">使用PCA进行降维需要遵循以下步骤:</p><ol class=""><li id="d2b4" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">数据的标准化</li><li id="a389" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">计算协方差矩阵</li><li id="69ad" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">计算特征向量和特征值</li><li id="4e8a" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">计算主成分</li><li id="440e" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">减少数据集的维数</li></ol><p id="ef43" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们详细讨论每个步骤:</p><h2 id="cb4b" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated">步骤1:数据的标准化</h2><p id="8558" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">如果你熟悉数据分析和处理，你就会知道错过标准化可能会导致有偏见的结果。标准化就是对数据进行缩放，使所有变量及其值都在一个相似的范围内。</p><p id="139b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">举个例子，假设我们的数据集中有两个变量，一个值在10-100之间，另一个值在1000-5000之间。在这种情况下，使用这些预测变量计算的输出显然会有偏差，因为范围较大的变量对结果的影响更明显。</p><p id="efc6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，将数据标准化到可比较的范围内是非常重要的。标准化是通过从平均值中减去数据中的每个值，然后除以数据集中的总偏差来实现的。</p><p id="eb30" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">可以这样计算:</p><figure class="lv lw lx ly fd ii er es paragraph-image"><div class="er es lu"><img src="../Images/b038513397016370100dc1226d3da8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*8dmOpJlCQw8vp-8rPgj_Yw.png"/></div></figure><p id="28ac" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这一步之后，数据中的所有变量都在一个标准和可比较的范围内进行缩放。</p><h2 id="a4b8" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated">步骤2:计算协方差矩阵</h2><p id="ee7e" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">如前所述，PCA有助于识别数据集中特征之间的相关性和依赖性。协方差矩阵表示数据集中不同变量之间的相关性。识别高度相关的变量至关重要，因为它们包含有偏差的和冗余的信息，这会降低模型的整体性能。</p><p id="2c03" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">数学上，协方差矩阵是一个p × p矩阵，其中p代表数据集的维数。矩阵中的每一项代表相应变量的协方差。</p><p id="6900" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑这样一种情况，我们有一个包含变量a和b的二维数据集，协方差矩阵是一个2×2矩阵，如下所示:</p><figure class="lv lw lx ly fd ii er es paragraph-image"><div class="er es lz"><img src="../Images/9b36490f0ef321b57399bb6006739c16.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*zDJr4srLz8F2AaS4xiqybg.png"/></div></figure><p id="3286" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的矩阵中:</p><ul class=""><li id="448d" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm kb jt ju jv bi translated">Cov(a，a)表示变量与其自身的协方差，它只不过是变量‘a’的方差</li><li id="a4ae" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm kb jt ju jv bi translated">Cov(a，b)表示变量‘a’相对于变量‘b’的协方差。因为协方差是可交换的，Cov(a，b) = Cov(b，a)</li></ul><p id="bff7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">以下是协方差矩阵的要点:</p><ul class=""><li id="d1a2" class="jn jo hh ir b is it iw ix ja jp je jq ji jr jm kb jt ju jv bi translated">协方差值表示两个变量相互之间的相关程度</li><li id="83a4" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm kb jt ju jv bi translated">如果协方差值为负，则表示各个变量彼此成间接比例</li><li id="8956" class="jn jo hh ir b is jw iw jx ja jy je jz ji ka jm kb jt ju jv bi translated">正协方差表示各个变量彼此成正比</li></ul><p id="d4d2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">简单的数学，不是吗？现在让我们继续，看看PCA的下一步。</p><h2 id="e117" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated">步骤3:计算特征向量和特征值</h2><p id="c99f" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">特征向量和特征值是数学构造，必须从协方差矩阵计算，以确定数据集的主要成分。</p><p id="c2c2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是首先，让我们了解更多关于主成分的知识</p><h2 id="105f" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated">什么是主成分？</h2><p id="ced8" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">简单地说，主成分是从初始变量集中获得的新变量集。主成分是以这样一种方式计算的，即新获得的变量是高度重要的，并且是相互独立的。主成分压缩并拥有分散在初始变量中的大部分有用信息。</p><p id="9e4e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你的数据集是5维的，那么计算5个主成分，这样，第一个主成分存储最大可能的信息，第二个存储剩余的最大信息，以此类推，你明白了。</p><p id="8111" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">那么，特征向量在整个过程中处于什么位置呢？</p><p id="a40a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">假设你们对特征向量和特征值都有基本的了解，我们知道这两个代数公式总是成对计算的，也就是说，每个特征向量都有一个特征值。数据中的维数决定了需要计算的特征向量的数量。</p><p id="2f74" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">考虑一个2维数据集，为其计算2个特征向量(及其各自的特征值)。特征向量背后的思想是使用协方差矩阵来理解数据中哪里的方差最大。由于数据中的方差越大，表示关于数据的信息越多，因此特征向量用于识别和计算主成分。</p><p id="8fdc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="lf">另一方面，特征值简单地表示各自特征向量的标量。因此，特征向量和特征值将计算数据集的主成分。</em></p><h2 id="8b92" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated">步骤4:计算主成分</h2><p id="7e3a" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">一旦我们计算了特征向量和特征值，我们要做的就是按降序排列它们，其中具有最高特征值的特征向量是最重要的，因此形成了第一个主分量。因此，为了减少数据的维数，可以去除较不重要的主要成分。</p><p id="9ade" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计算主成分的最后一步是形成一个称为特征矩阵的矩阵，该矩阵包含所有重要的数据变量，这些变量拥有关于数据的最大信息。</p><h2 id="367e" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated">步骤5:减少数据集的维数</h2><p id="fea6" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">执行PCA的最后一步是用代表数据集最大和最重要信息的最终主成分重新排列原始数据。为了用新形成的主成分替换原始数据轴，只需将原始数据集的转置与获得的特征向量的转置相乘。</p><p id="cc4a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这就是整个PCA过程背后的理论。是时候动手了，通过使用真实的数据集来执行所有这些步骤。</p><h1 id="95fe" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">使用Python进行主成分分析</h1><p id="812b" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在本节中，我们将使用Python执行PCA。</p><p id="f40d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">问题陈述:</strong>进行逐步主成分分析，以降低数据集的维数。</p><p id="d0d4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">数据集描述:</strong>电影评级数据集，包含来自700多个用户的大约9000部电影(特征)的评级。</p><p id="05eb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">逻辑:</strong>通过寻找数据中最显著的特征来执行PCA。PCA将按照上面定义的步骤进行。</p><p id="db5c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们开始吧！</p><h2 id="f270" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated"><strong class="ak">第一步:导入需要的包</strong></h2><pre class="lv lw lx ly fd ma mb mc md aw me bi"><span id="d773" class="lg kd hh mb b fi mf mg l mh mi">import pandas as pd<br/>import numpy as np<br/>from sklearn.preprocessing import StandardScaler<br/>from matplotlib import*<br/>import matplotlib.pyplot as plt<br/>from matplotlib.cm import register_cmap<br/>from scipy import stats<br/>from sklearn.decomposition import PCA as sklearnPCA<br/>import seaborn</span></pre><h2 id="1fd5" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated"><strong class="ak">第二步:导入数据集</strong></h2><pre class="lv lw lx ly fd ma mb mc md aw me bi"><span id="22b6" class="lg kd hh mb b fi mf mg l mh mi">#Load movie names and movie ratings<br/>movies = pd.read_csv('C:UsersNeelTempDesktopPCA DATAmovies.csv')<br/>ratings = pd.read_csv('C:UsersNeelTempDesktopPCA DATAratings.csv')<br/>ratings.drop(['timestamp'], axis=1, inplace=True)</span></pre><h2 id="c8d3" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated"><strong class="ak">第三步:格式化数据</strong></h2><pre class="lv lw lx ly fd ma mb mc md aw me bi"><span id="f371" class="lg kd hh mb b fi mf mg l mh mi">def replace_name(x):<br/>return movies[movies['movieId']==x].title.values[0]<br/>ratings.movieId = ratings.movieId.map(replace_name)<br/>M = ratings.pivot_table(index=['userId'], columns=['movieId'], values='rating')<br/>m = M.shape<br/>df1 = M.replace(np.nan, 0, regex=True)</span></pre><h2 id="f846" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated"><strong class="ak">第四步:标准化</strong></h2><p id="d20b" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在下面的代码行中，我们使用sklearn包提供的StandardScalar()函数，以便在可比较的范围内缩放数据集。如前所述，需要标准化来防止最终结果中的偏差。</p><pre class="lv lw lx ly fd ma mb mc md aw me bi"><span id="16c6" class="lg kd hh mb b fi mf mg l mh mi">X_std = StandardScaler().fit_transform(df1)</span></pre><h2 id="ec5b" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated"><strong class="ak">步骤5:计算协方差矩阵</strong></h2><p id="bb2c" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">如前所述，协方差矩阵表示数据集中不同要素之间的相关性。识别高度相关的变量至关重要，因为它们包含有偏差的和冗余的信息，这会降低模型的整体性能。以下代码片段计算数据的协方差矩阵:</p><pre class="lv lw lx ly fd ma mb mc md aw me bi"><span id="6588" class="lg kd hh mb b fi mf mg l mh mi">mean_vec = np.mean(X_std, axis=0)<br/>cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)<br/>print('Covariance matrix n%s' %cov_mat)<br/> <br/>Covariance matrix<br/>[[ 1.0013947 -0.00276421 -0.00195661 ... -0.00858289 -0.00321221<br/>-0.01055463]<br/>[-0.00276421 1.0013947 -0.00197311 ... 0.14004611 -0.0032393<br/>-0.01064364]<br/>[-0.00195661 -0.00197311 1.0013947 ... -0.00612653 -0.0022929<br/>-0.00753398]<br/>...<br/>[-0.00858289 0.14004611 -0.00612653 ... 1.0013947 0.02888777<br/>0.14005644]<br/>[-0.00321221 -0.0032393 -0.0022929 ... 0.02888777 1.0013947<br/>0.01676203]<br/>[-0.01055463 -0.01064364 -0.00753398 ... 0.14005644 0.01676203<br/>1.0013947 ]]</span></pre><h2 id="96ea" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated"><strong class="ak">步骤6:计算特征向量和特征值</strong></h2><p id="10f2" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在这个步骤中，计算特征向量和特征值，它们基本上计算数据集的主分量。</p><pre class="lv lw lx ly fd ma mb mc md aw me bi"><span id="0b36" class="lg kd hh mb b fi mf mg l mh mi">#Calculating eigenvectors and eigenvalues on covariance matrix<br/>cov_mat = np.cov(X_std.T)<br/>eig_vals, eig_vecs = np.linalg.eig(cov_mat)<br/>print('Eigenvectors n%s' %eig_vecs)<br/>print('nEigenvalues n%s' %eig_vals)<br/> <br/>Eigenvectors<br/>[[-1.34830861e-04+0.j 5.76715196e-04+0.j 4.83014783e-05+0.j ...<br/>5.02355418e-14+0.j 6.48472777e-12+0.j 6.90776605e-13+0.j]<br/>[ 5.61303451e-04+0.j -1.11493526e-02+0.j 8.85798170e-03+0.j ...<br/>-2.38204858e-11+0.j -6.11345049e-11+0.j -1.39454110e-12+0.j]<br/>[ 4.58686517e-04+0.j -2.39083484e-03+0.j 6.58309436e-04+0.j ...<br/>-7.00290160e-12+0.j -5.53245120e-12+0.j 3.35918400e-13+0.j]<br/>...<br/>[ 5.22202072e-03+0.j -5.49944367e-03+0.j 5.16164779e-03+0.j ...<br/>2.53271844e-10+0.j 9.69246536e-10+0.j 5.86126443e-11+0.j]<br/>[ 8.97514078e-04+0.j -1.14918748e-02+0.j 9.41277803e-03+0.j ...<br/>-3.90405498e-10+0.j -7.88691586e-10+0.j -2.80604702e-11+0.j]<br/>[ 4.36362199e-03+0.j -7.90241494e-03+0.j -7.48537922e-03+0.j ...<br/>-6.38353830e-10+0.j -6.47370973e-10+0.j 1.41147483e-13+0.j]]<br/> <br/>Eigenvalues<br/>[ 1.54166656e+03+0.j 4.23920460e+02+0.j 3.19074475e+02+0.j ...<br/>8.84301723e-64+0.j 1.48644623e-64+0.j -3.46531190e-65+0.j]</span></pre><h2 id="ad40" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated"><strong class="ak">步骤7:计算特征向量</strong></h2><p id="1578" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">在这一步，我们按照降序重新排列特征值。这以降序表示主要成分的重要性:</p><pre class="lv lw lx ly fd ma mb mc md aw me bi"><span id="0eb9" class="lg kd hh mb b fi mf mg l mh mi"># Visually confirm that the list is correctly sorted by decreasing eigenvalues<br/>eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]<br/>print('Eigenvalues in descending order:')<br/>for i in eig_pairs:<br/>print(i[0])<br/> <br/>Eigenvalues in descending order:<br/>1541.6665576008295<br/>423.92045998539083<br/>319.07447507743535<br/>279.33035758081536<br/>251.63844082955103<br/>218.62439973216058<br/>154.61586911307694<br/>138.60396745179094<br/>137.6669785626203<br/>119.37014654115806<br/>115.2795566625854<br/>105.40594030056845<br/>97.84201186745533<br/>96.72012660587329<br/>93.39647211318346<br/>87.7491937345281<br/>87.54664687999116<br/>85.93371257360843<br/>72.85051428001277<br/>70.37154679336622<br/>64.45310203297905<br/>63.78603164551922<br/>62.11260590665646<br/>60.080661628776205<br/>57.67255079811343<br/>56.490104252992744<br/>55.48183563193681<br/>53.78161965096411<br/> <br/>....</span></pre><h2 id="664d" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated"><strong class="ak">步骤8:使用PCA()函数降低数据集的维数</strong></h2><p id="c043" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">下面的代码片段使用sklearn包提供的预定义PCA()函数来转换数据。n_components参数表示用于拟合数据的主成分的数量:</p><pre class="lv lw lx ly fd ma mb mc md aw me bi"><span id="387f" class="lg kd hh mb b fi mf mg l mh mi">pca = PCA(n_components=2)<br/>pca.fit_transform(df1)<br/>print(pca.explained_variance_ratio_)<br/> <br/>[0.13379809 0.03977444]</span></pre><p id="f641" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">输出显示，PC1和PC2约占数据集差异的14%。</p><h2 id="7cf1" class="lg kd hh bd ke lh li lj ki lk ll lm km ja ln lo kq je lp lq ku ji lr ls ky lt bi translated"><strong class="ak">第九步:根据主成分预测方差</strong></h2><p id="8c9b" class="pw-post-body-paragraph ip iq hh ir b is la iu iv iw lb iy iz ja lc jc jd je ld jg jh ji le jk jl jm ha bi translated">为了深入了解不同数量的主成分的数据方差，让我们绘制一个scree图。在统计学中，scree图表示与每个主成分相关的方差:</p><pre class="lv lw lx ly fd ma mb mc md aw me bi"><span id="dace" class="lg kd hh mb b fi mf mg l mh mi">pca = PCA().fit(X_std)<br/>plt.plot(np.cumsum(pca.explained_variance_ratio_))<br/>plt.xlabel('number of components')<br/>plt.ylabel('cumulative explained variance')<br/>plt.show()</span></pre><figure class="lv lw lx ly fd ii er es paragraph-image"><div class="er es mj"><img src="../Images/4b5fb7382178d3c680f0f18a30f1e24c.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*qa4AiXAnAnSttdLb9iRy8A.png"/></div></figure><p id="86b6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然您已经知道了主成分分析背后的数学原理，我相信您一定很想了解更多。</p><p id="a664" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">就这样，我们到了这篇文章的结尾。如果你对这个话题有任何疑问，请在下面留下评论，我们会尽快回复你。</p><p id="eb16" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、DevOps、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="27e6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中的其他文章，它们将解释Python和数据科学的各个方面。</p><blockquote class="ml mm mn"><p id="9330" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">1.<a class="ae mk" rel="noopener" href="/edureka/machine-learning-classifier-c02fbd8400c9">Python中的机器学习分类器</a></p><p id="68d7" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">2.<a class="ae mk" rel="noopener" href="/edureka/python-scikit-learn-cheat-sheet-9786382be9f5">Python Scikit-Learn Cheat Sheet</a></p><p id="f728" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">3.<a class="ae mk" rel="noopener" href="/edureka/python-libraries-for-data-science-and-machine-learning-1c502744f277">机器学习工具</a></p><p id="cccc" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">4.<a class="ae mk" rel="noopener" href="/edureka/python-libraries-for-data-science-and-machine-learning-1c502744f277">用于数据科学和机器学习的Python库</a></p><p id="6a78" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">5.<a class="ae mk" rel="noopener" href="/edureka/how-to-make-a-chatbot-in-python-b68fd390b219">Python中的聊天机器人</a></p><p id="7ef0" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">6.<a class="ae mk" rel="noopener" href="/edureka/collections-in-python-d0bc0ed8d938"> Python集合</a></p><p id="67e0" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">7.<a class="ae mk" rel="noopener" href="/edureka/python-modules-abb0145a5963"> Python模块</a></p><p id="685c" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">8.<a class="ae mk" rel="noopener" href="/edureka/python-developer-skills-371583a69be1"> Python开发者技能</a></p><p id="8d30" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">9.<a class="ae mk" rel="noopener" href="/edureka/oops-interview-questions-621fc922cdf4">哎呀面试问答</a></p><p id="626b" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">10.一个Python开发者的简历</p><p id="df67" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">11.<a class="ae mk" rel="noopener" href="/edureka/exploratory-data-analysis-in-python-3ee69362a46e">Python中的探索性数据分析</a></p><p id="72f0" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">12.<a class="ae mk" rel="noopener" href="/edureka/python-turtle-module-361816449390">带Python的乌龟模块的贪吃蛇游戏</a></p><p id="1e24" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">13.<a class="ae mk" rel="noopener" href="/edureka/python-developer-salary-ba2eff6a502e"> Python开发者工资</a></p><p id="5e4b" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">14.<a class="ae mk" rel="noopener" href="/edureka/web-scraping-with-python-d9e6506007bf">用Python进行网页抓取</a></p><p id="e241" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">15.<a class="ae mk" rel="noopener" href="/edureka/python-vs-cpp-c3ffbea01eec"> Python vs C++ </a></p><p id="a2de" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">16.<a class="ae mk" rel="noopener" href="/edureka/scrapy-tutorial-5584517658fb">刺儿头教程</a></p><p id="0d0e" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">17.<a class="ae mk" rel="noopener" href="/edureka/scipy-tutorial-38723361ba4b"> Python SciPy </a></p><p id="5a96" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">18.<a class="ae mk" rel="noopener" href="/edureka/least-square-regression-40b59cca8ea7">最小二乘回归法</a></p><p id="ff92" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">19.<a class="ae mk" rel="noopener" href="/edureka/jupyter-notebook-cheat-sheet-88f60d1aca7"> Jupyter笔记本小抄</a></p><p id="6d9a" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">20.<a class="ae mk" rel="noopener" href="/edureka/python-basics-f371d7fc0054"> Python基础知识</a></p><p id="8c4f" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">21.<a class="ae mk" rel="noopener" href="/edureka/python-pattern-programs-75e1e764a42f"> Python模式程序</a></p><p id="af3e" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">22.<a class="ae mk" rel="noopener" href="/edureka/generators-in-python-258f21e3d3ff">Python中的发电机</a></p><p id="127d" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">23.<a class="ae mk" rel="noopener" href="/edureka/python-decorator-tutorial-bf7b21278564"> Python装饰师</a></p><p id="0b43" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">24.<a class="ae mk" rel="noopener" href="/edureka/spyder-ide-2a91caac4e46"> Python Spyder IDE </a></p><p id="3097" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">25.<a class="ae mk" rel="noopener" href="/edureka/kivy-tutorial-9a0f02fe53f5">在Python中使用Kivy的移动应用</a></p><p id="ed07" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">26.<a class="ae mk" rel="noopener" href="/edureka/best-books-for-python-11137561beb7">十大最佳学习书籍&amp;练习Python </a></p><p id="b090" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">27.<a class="ae mk" rel="noopener" href="/edureka/robot-framework-tutorial-f8a75ab23cfd">使用Python的机器人框架</a></p><p id="d531" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">28.<a class="ae mk" rel="noopener" href="/edureka/snake-game-with-pygame-497f1683eeaa">使用PyGame的Python中的贪吃蛇游戏</a></p><p id="d916" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">29.<a class="ae mk" rel="noopener" href="/edureka/django-interview-questions-a4df7bfeb7e8"> Django面试问答</a></p><p id="b58e" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">30.<a class="ae mk" rel="noopener" href="/edureka/python-applications-18b780d64f3b">十大Python应用</a></p><p id="897e" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">31.<a class="ae mk" rel="noopener" href="/edureka/hash-tables-and-hashmaps-in-python-3bd7fc1b00b4">Python中的哈希表和哈希表</a></p><p id="248c" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">32.<a class="ae mk" rel="noopener" href="/edureka/whats-new-python-3-8-7d52cda747b"> Python 3.8 </a></p><p id="f8c4" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">33.<a class="ae mk" rel="noopener" href="/edureka/support-vector-machine-in-python-539dca55c26a">支持向量机</a></p><p id="522a" class="ip iq lf ir b is it iu iv iw ix iy iz mo jb jc jd mp jf jg jh mq jj jk jl jm ha bi translated">34.<a class="ae mk" rel="noopener" href="/edureka/python-tutorial-be1b3d015745"> Python教程</a></p></blockquote></div><div class="ab cl mr ms go mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ha hb hc hd he"><p id="2ef0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="lf">原载于2019年8月28日</em><a class="ae mk" href="https://www.edureka.co/blog/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank"><em class="lf">https://www.edureka.co</em></a><em class="lf">。</em></p></div></div>    
</body>
</html>