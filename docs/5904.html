<html>
<head>
<title>Working with Data in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在PyTorch中使用数据</h1>
<blockquote>原文：<a href="https://medium.com/oracledevs/working-with-data-in-pytorch-fa2641e37d17?source=collection_archive---------0-----------------------#2022-05-09">https://medium.com/oracledevs/working-with-data-in-pytorch-fa2641e37d17?source=collection_archive---------0-----------------------#2022-05-09</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="e8d1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">欢迎阅读本系列的第二篇文章，在这篇文章中，我们将探讨PyTorch和TensorFlow之间的异同，以及如何使用这两个库处理数据。</p><p id="a068" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将深入研究PyTorch。</p><p id="9874" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">值得注意的是，PyTorch被认为是一个Python库，这意味着它与Python中已经存在的数据科学堆栈集成得非常好。它比TensorFlow有一些优势:</p><ul class=""><li id="9b91" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">它比TensorFlow更新</li><li id="e468" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">它与Torch框架(一个Lua生态系统的社区维护包，用于机器学习、并行处理、数据操作和计算机视觉)相关，Python实现更新了PyTorch的用户群</li><li id="3404" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">这是脸书工程师用的</li><li id="0790" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">它使用<strong class="ig hi">张量</strong>，这可以被认为是NumPy数组(或计算机优化矩阵)的GPU等价物。</li></ul><figure class="jr js jt ju fd jv er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es jq"><img src="../Images/96a8f054dc520c98d038a46ca6a4e1e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SJM6GBjmMv8YO3-V"/></div></div></figure><h1 id="9074" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">PyTorch基础—神经网络</h1><p id="6b13" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated">神经网络(NN)的实现就像人脑中的神经元一样工作:</p><ul class=""><li id="3d12" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">我们有称为感知器的人工神经元</li><li id="f013" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">感知器就像神经元一样，通过轴突(在神经网络中称为<strong class="ig hi">配置</strong>)与其他神经元连接，以双向传输数据</li></ul><p id="9064" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在神经网络中，感知器由一系列输入组成，产生一个输出。所以我们总是有一个输入层和一个输出层；由我们程序员来决定这些层如何通信以及以何种顺序通信。</p><p id="4458" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有两种类型的神经网络:</p><ul class=""><li id="398c" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">前馈NNs:数据从输入层移动到输出层(是否通过隐藏层，视问题而定)；当数据到达输出层时，神经网络已经完成了它的工作。</li><li id="2ec1" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">递归NNs:数据不会在输出层停止。而不是这样做，它再次循环进入先前遍历的层，执行指定数量的循环。需要注意的是，计算梯度是基于<a class="ae lf" href="https://tutorial.math.lamar.edu/classes/calcI/ChainRule.aspx" rel="noopener ugc nofollow" target="_blank">链规则</a>，这需要一点高等数学背景。然而，PyTorch已经善良地实现了他们自己的“自动梯度计算器”，称为<strong class="ig hi">亲笔签名的</strong>，它自动完成大部分数学工作。我们稍后会详细讨论这种叫做<a class="ae lf" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/" rel="noopener ugc nofollow" target="_blank">自动微分</a>的技术。</li></ul><p id="b63a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个前馈神经网络的图像，我们只能看到从输入(下图)到输出(上图)的正向步骤:</p><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es lg"><img src="../Images/fc95c82f65adac0ec618e84749f9b6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/0*hazTqXjFxdnvjhZm"/></div></figure><p id="4b17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个递归神经网络的图像。请注意，如果我们有一个以上的隐藏层，我们可以将NN称为<strong class="ig hi">深NN </strong>。</p><figure class="jr js jt ju fd jv er es paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="er es lh"><img src="../Images/940e4e39b8e9618125038b83397d1508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nqs-kWFK33-pWqoR"/></div></div></figure><h1 id="3577" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">张量</h1><p id="5ce2" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated">在PyTorch中，我们有<strong class="ig hi">张量</strong>。如前一篇文章所述，PyTorch张量与NumPy数组完全等价。就像numpy库一样，操作张量将允许我们对现有数据执行优化操作。遇到由GPU执行PyTorch张量是相当常见的(因为GPU比CPU本身有更多的处理单元)，尽管认为它不能在CPU中执行是一种常见的误解。</p><h1 id="c478" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">加载数据</h1><p id="e647" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated">要加载数据，我们可以遵循几个步骤:</p><ul class=""><li id="942e" class="jc jd hh ig b ih ii il im ip je it jf ix jg jb jh ji jj jk bi translated">从Python兼容的标准库中加载数据集，就像使用<em class="li">熊猫</em>从CSV文件中读取，然后将数据集转换成张量。</li><li id="31dd" class="jc jd hh ig b ih jl il jm ip jn it jo ix jp jb jh ji jj jk bi translated">使用<strong class="ig hi"> torchvision </strong>包，它允许我们使用多处理实现来加载和准备数据集。</li></ul><p id="9271" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">本着展示PyTorch如何工作的精神，我们将从可预加载的内置数据集列表中加载一个<a class="ae lf" href="https://image-net.org/" rel="noopener ugc nofollow" target="_blank">著名数据集</a>:</p><h1 id="da63" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">内置数据集</h1><pre class="jr js jt ju fd lj lk ll lm aw ln bi"><span id="2344" class="lo kd hh lk b fi lp lq l lr ls">import argparse<br/># we get the number of threads we want<br/>parser = argparse.ArgumentParser()<br/>parser.add_argument('-t', '--threads', help='Number of threads to use', required=True)<br/>args = parser.parse_args() </span><span id="3d02" class="lo kd hh lk b fi lt lq l lr ls"># we load the image net dataset<br/>imagenet_data = torchvision.datasets.ImageNet('./imagenet/')</span><span id="3352" class="lo kd hh lk b fi lt lq l lr ls"># By using the DataLoader function, we can load the dataset in n batch sizes (by default 1) <br/># shuffle is used to randomize the dataset's row order<br/>data_loader = torch.utils.data.DataLoader(imagenet_data, batch_size=4, shuffle=True, num_workers=args.threads)</span></pre><p id="a996" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">你可以在这里找到内置数据集列表<a class="ae lf" href="https://pytorch.org/vision/stable/datasets.html" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="6bac" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">PyTorch中的自动微分</h1><p id="312b" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated">关于神经网络的复杂之处在于它们是如何构建的。如果我们开始学习，为了理解神经网络，学习微分方程和积分的强化课程会很复杂。</p><p id="4609" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">自动微分是PyTorch实现和创造的一种技术。这使得神经网络成为一个数学上非常复杂的话题，迫使人们计算递归神经网络的梯度。</p><h1 id="614e" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">执行线性回归</h1><p id="ba96" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated">我们将使用PyTorch执行ML中最简单的回归任务之一:线性回归。</p><p id="7898" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">首先，我们导入生成人工数据所需的所有包。为此，我们将使用numpy的随机函数:</p><pre class="jr js jt ju fd lj lk ll lm aw ln bi"><span id="5e6a" class="lo kd hh lk b fi lp lq l lr ls">import numpy as np<br/>import pandas as pd</span><span id="63dd" class="lo kd hh lk b fi lt lq l lr ls"># we define the 3 components of linear regression<br/># y = mx + b (m = slope, b = intercept)<br/># y is what we need to predict (dependent variable)<br/>m = 1.5<br/>b = 3</span><span id="be0b" class="lo kd hh lk b fi lt lq l lr ls">x = np.random.rand(1024)<br/>randomness = np.random.rand(1024) / 2 # adding some randomness.<br/>y = m*x + b + randomness # we have our linear regression formula with the added randomness</span><span id="dcee" class="lo kd hh lk b fi lt lq l lr ls"># create an empty dataframe and populate it.<br/>df = pd.DataFrame()<br/>df['x'] = x<br/>df['y'] = y</span></pre><p id="805c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们绘制当前随机数据:</p><pre class="jr js jt ju fd lj lk ll lm aw ln bi"><span id="f25a" class="lo kd hh lk b fi lp lq l lr ls">import seaborn as sns</span><span id="7269" class="lo kd hh lk b fi lt lq l lr ls">sns.lmplot(x = 'x', y = 'y',<br/>    data = df)</span></pre><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es lu"><img src="../Images/d77c83767e273e04e310d6fe9490eb0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/0*RVviZ-MXSum9wiGv"/></div></figure><p id="bb07" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们继续创建一个能够预测目标变量“y”的神经网络。我们已经在<strong class="ig hi"> df </strong>数据框架中获得了这些数据。注意，如前所述，我们能够在一个<strong class="ig hi">熊猫</strong>对象和一个PyTorch对象之间进行转换。</p><pre class="jr js jt ju fd lj lk ll lm aw ln bi"><span id="5b84" class="lo kd hh lk b fi lp lq l lr ls">import torch<br/>import torch.nn as nn<br/>from torch.autograd import Variable # we import autograd for automatic differentiation<br/># as mentioned in the last article, we create our object with an init constructor,<br/># and the forward function to define the NN's configuration. In this case, we'll just have one step from the <br/># input layer to the output layer (linear).<br/>class LinearRegressorNN(nn.Module):</span><span id="76bc" class="lo kd hh lk b fi lt lq l lr ls">    def __init__(self, input_dim, output_dim):<br/>      super(LinearRegressionModel, self).__init__()<br/>      # linear equation is of the form Wx = B where W is a weight, x is the input and B is the output.<br/>      # it's the simplest form of PyTorch<br/>      self.linear = nn.Linear(input_dim, output_dim) # nn.Linear is required for linear regression</span><span id="ea4b" class="lo kd hh lk b fi lt lq l lr ls">   def forward(self, x):<br/>      out = self.linear(x)<br/>      return out</span></pre><p id="a164" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为提醒，下面是我们存储在dataframe <strong class="ig hi"> df </strong>中的数据格式的描述:</p><figure class="jr js jt ju fd jv er es paragraph-image"><div class="er es lv"><img src="../Images/8de0eab3810a61132a7acc544519a996.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/0*M4Yi5sqzdnMF8y8b"/></div></figure><p id="5861" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于我们只有一个特征和一个目标变量(特征是自变量x，目标是因变量y)，我们将维度定义为:</p><pre class="jr js jt ju fd lj lk ll lm aw ln bi"><span id="3f88" class="lo kd hh lk b fi lp lq l lr ls">input_dim = x_train.shape[1]<br/>output_dim = y_train.shape[1]<br/># we create an object of our above class<br/>model = LinearRegressorNN(input_dim, output_dim)</span><span id="77c4" class="lo kd hh lk b fi lt lq l lr ls"># the loss function will ultimately determine how gradients are calculated.<br/># in recurrent NNs, gradients are computed by applying the chain rule from the loss function backwards.<br/>criterion = nn.MSELoss() # we define our loss function as the mean squared error<br/>[w, b] = model.parameters()<br/>print(w, b)</span></pre><p id="957c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这将被归类为<strong class="ig hi">前馈线性回归NN </strong>，因为我们没有计算任何梯度或使用自动签名。</p><p id="499b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">正如我们所见，处理数据(即使这是一个非常简单的线性回归问题)并不难，即使对于初学者来说也是如此。我们只需要让NNs的主要概念深入人心。在下一篇文章中，我们将做一些类似的事情，但是使用TensorFlow，之后我们将能够通过在文章系列结束时执行基准测试来比较两个库的性能。</p><p id="df6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">敬请期待！</p><h1 id="688d" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">我如何开始学习OCI？</h1><p id="4960" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated">请记住，你可以随时免费注册OCI！您的Oracle Cloud帐户提供多项始终免费的服务和300美元免费积分的免费试用，可用于所有符合条件的OCI服务，最长30天。这些总是免费的服务是无限期的。免费试用服务可能会一直使用到您的300美元免费点数用完或30天到期，以先到者为准。你可以<a class="ae lf" href="https://signup.cloud.oracle.com/?language=en&amp;sourceType=:ow:de:te::::&amp;intcmp=:ow:de:te::::" rel="noopener ugc nofollow" target="_blank">在这里免费报名</a>。</p><h1 id="4cc8" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">加入对话！</h1><p id="cc9c" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated">如果你对甲骨文开发人员在他们的自然栖息地发生的事情感到好奇，来<a class="ae lf" href="https://join.slack.com/t/oracledevrel/shared_invite/zt-uffjmwh3-ksmv2ii9YxSkc6IpbokL1g?customTrackingParam=:ow:de:te::::RC_WWMK220210P00062:Medium_nachoLoL5" rel="noopener ugc nofollow" target="_blank">加入我们的公共休闲频道</a>！我们不介意成为你的鱼缸🐠</p><h1 id="b50b" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">许可证</h1><p id="6044" class="pw-post-body-paragraph ie if hh ig b ih la ij ik il lb in io ip lc ir is it ld iv iw ix le iz ja jb ha bi translated">由<a class="ae lf" href="https://www.linkedin.com/in/ignacio-g-martinez/" rel="noopener ugc nofollow" target="_blank">伊格纳西奥·吉尔勒莫·马丁内兹</a><a class="ae lf" href="https://github.com/jasperan" rel="noopener ugc nofollow" target="_blank">@贾斯珀兰</a>撰写，由<a class="ae lf" href="https://www.linkedin.com/in/dawsontech/" rel="noopener ugc nofollow" target="_blank">艾琳·道森</a>编辑</p><p id="4989" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">版权所有2022 Oracle和/或其附属公司。</p><p id="fa21" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">根据通用许可许可证(UPL)1.0版进行许可。</p><p id="d7ee" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">详见<a class="ae lf" href="https://github.com/oracle-devrel/leagueoflegends-optimizer/blob/main/LICENSE" rel="noopener ugc nofollow" target="_blank">许可证</a>。</p></div></div>    
</body>
</html>