<html>
<head>
<title>STL and Holt from R to SparkR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从R到SparkR的STL和Holt</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/stl-and-holt-from-r-to-sparkr-1815bacfe1cc?source=collection_archive---------2-----------------------#2020-11-10">https://medium.com/walmartglobaltech/stl-and-holt-from-r-to-sparkr-1815bacfe1cc?source=collection_archive---------2-----------------------#2020-11-10</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="1201" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">用SparkR扩展R中的机器学习算法</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es jc"><img src="../Images/1d28fdc24e1b6c54e9eff8524b20aae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0dHPKDps1FLWLf5GahRuAQ.png"/></div></div></figure><p id="2d36" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了扩展我们目前在R中运行的机器学习算法，我们最近在SparkR中进行了重写整个数据预处理和机器学习管道的活动。</p><p id="1210" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们使用时间序列模型STL和Holt进行预测。对数据进行预处理，以生成具有日期数据类型、插补、标准化的日期列，并最终对w.r.t .相关列进行排序，为ML算法准备输入。由于我不熟悉R，主要的挑战是理解当前设置中执行的数据操作，并在SparkR中找到相应的转换。这篇文章旨在分享我在这个过程中的学习。</p><h2 id="b488" class="jo jp hh bd jq jr js jt ju jv jw jx jy ip jz ka kb it kc kd ke ix kf kg kh ki bi translated"><strong class="ak">入门</strong></h2><ol class=""><li id="45c6" class="kj kk hh ig b ih kl il km ip kn it ko ix kp jb kq kr ks kt bi translated">加载spark库并初始化spark会话</li></ol><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="172d" class="jo jp hh kv b fi kz la l lb lc"><em class="ld">library</em>(SparkR, lib.loc = "/usr/local/spark/R/lib")<br/>sparkEnvir &lt;- <em class="ld">list</em>(spark.num.executors='5',spark.executor.cores='5')</span><span id="bdad" class="jo jp hh kv b fi le la l lb lc"><em class="ld"># initializing Spark context<br/></em>sc &lt;- <em class="ld">sparkR.init</em>(sparkHome = "/usr/local/spark",<br/>                  sparkEnvir = sparkEnvir)</span><span id="8baa" class="jo jp hh kv b fi le la l lb lc"><em class="ld"># initializing SQL context<br/></em>sqlContext &lt;- <em class="ld">sparkRSQL.init</em>(sc)</span></pre><p id="2c9f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">2.读取CSV文件</p><p id="5867" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们可以使用推断的schema = "true "(即我们允许Spark推断DF中每一列的数据类型)来读取CSV文件，因为这减少了程序员将列显式转换为其数据类型的工作量。如果没有推断出模式，则每一列都被读取为字符串数据类型。我们还可以提供一个定制的模式。</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="9c9f" class="jo jp hh kv b fi kz la l lb lc"># sales_training_file : location of the file </span><span id="7cd7" class="jo jp hh kv b fi le la l lb lc">sales_df &lt;- <em class="ld">read.df</em>(sales_training_file, "csv", header = "true", inferSchema = "true", na.strings = "NA", integer64 = "numeric")</span><span id="6412" class="jo jp hh kv b fi le la l lb lc"><em class="ld"># number of rows in the dataframe<br/>nrow</em>(sales_df)</span><span id="648d" class="jo jp hh kv b fi le la l lb lc">#list the columns of a df <br/>columns(sales_df)</span><span id="88d7" class="jo jp hh kv b fi le la l lb lc"># show the first 5 rows <br/>head(sales_df, num = 5)</span><span id="78db" class="jo jp hh kv b fi le la l lb lc"># schema of the dataframe<br/>schema(sales_df)</span><span id="4800" class="jo jp hh kv b fi le la l lb lc"># visualisation of starting data in the spark dataframe <br/><em class="ld">str</em>(sales_df)</span><span id="132d" class="jo jp hh kv b fi le la l lb lc"><em class="ld"># 'SparkDataFrame': 4 variables:<br/># $ _c0            : int 1 1 4 7 5 6<br/># $ audit_date: POSIXct 2017-02-07 2017-03-07 2017-04-07 2017-05-07 2017-06-07 2017-07-07<br/># $ sales      : int 17 20 21 32 20 72<br/># $ price          : num 15.02 9.04 15.10 12.61 13.17 21.11</em></span><span id="f464" class="jo jp hh kv b fi le la l lb lc"># rename a column <br/>sales_df &lt;- <em class="ld">withColumnRenamed</em>(sales_df, "_c0", "product_id")</span><span id="75a0" class="jo jp hh kv b fi le la l lb lc"># add a new column "count" with 0 as default value<br/>sales_df_with_count &lt;- withColumn(sales_df, "count", 0)</span><span id="8b8e" class="jo jp hh kv b fi le la l lb lc"># create custom schema, and pass to load call</span><span id="8a3a" class="jo jp hh kv b fi le la l lb lc">customSchema &lt;- structType(<br/> structField("product_id", type = "integer"),<br/> structField("audit_date", type = "string"),<br/> structField("sales", type = "integer"),<br/> structField("price", type = "double"))</span><span id="b14b" class="jo jp hh kv b fi le la l lb lc">sales_df_custom_schema &lt;- read.df(price_training_file, "csv", header = "true", customSchema, na.strings = "NA", integer64 = "numeric")</span></pre><p id="ddda" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">3.数据操作</p><p id="7b1b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">R中的data.table和data.frame为我们提供了丰富的功能，包括选择、使用数据类型转换列、过滤、填充空值、排序、聚合，甚至跨数据框的连接。有了spark，我们可以利用spark-sql来执行所有这些操作。</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="1402" class="jo jp hh kv b fi kz la l lb lc"># cast column <em class="ld">audit_date to data type string</em><br/>sales_df$<em class="ld">audit_date</em> &lt;- <em class="ld">cast</em>(sales_df$<em class="ld">audit_date</em>, dataType = "string")</span><span id="c0d8" class="jo jp hh kv b fi le la l lb lc"><em class="ld"># create a view on price_df<br/>createOrReplaceTempView</em>(sales_df, "sales_table")</span><span id="9f99" class="jo jp hh kv b fi le la l lb lc">sales_select &lt;- sql("select product_id, audit_date, price from sales_table where price &gt;= 15 order by audit_date")</span><span id="0202" class="jo jp hh kv b fi le la l lb lc">sales_grouped &lt;- sql("select sum(sales),product_id from sales_table group by product_id")</span></pre><p id="fa16" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">除此之外，我们还可以使用spark数据框方法进行上述变换。</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="464b" class="jo jp hh kv b fi kz la l lb lc"># fill all null entries of audit_date with "2016-02-07"<br/>sales_df_cleaned &lt;- <em class="ld">fillna</em>(sales_df, <em class="ld">list</em>("audit_date" = "<em class="ld">2016-02-07</em>"))</span><span id="ef79" class="jo jp hh kv b fi le la l lb lc"># update price to 0, if price is less than 0<br/>sales_df_cleaned$price &lt;- <em class="ld">ifelse</em>(sales_df_cleaned$price &lt; 0, 0, sales_df_cleaned$price)</span><span id="d9ed" class="jo jp hh kv b fi le la l lb lc"># filter all rows with price greater than 10<br/>sales_df_filtered &lt;- filter(sales_df, sales_df$price &gt; 10)</span><span id="2e44" class="jo jp hh kv b fi le la l lb lc"># group by audit_date and sum <br/>sales_df_grouped &lt;- <em class="ld">groupBy</em>(sales_df, sales_df$audit_date)<br/><br/>sales_df_agg &lt;- <em class="ld">agg</em>(sales_df_grouped, tot_sales=<em class="ld">sum</em>(sales_df$sales))</span></pre><p id="f746" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">4.SparkR数据帧到R数据帧，反之亦然</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="7ab7" class="jo jp hh kv b fi kz la l lb lc"># SparkR data frame to R<br/>sales_R_df &lt;- collect(sales_df)</span><span id="9e63" class="jo jp hh kv b fi le la l lb lc"># R to sparkR dataframe<br/>sales_spark_df &lt;- createDataFrame(sales_R_df)</span></pre><p id="547f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="ld">注意:我们应该小心使用collect()，因为整个数据集都是在驱动程序中收集的。在大量数据的情况下，如果Spark驱动程序没有足够的内存，我们可能会面临OOM。</em></p><h2 id="7c8d" class="jo jp hh bd jq jr js jt ju jv jw jx jy ip jz ka kb it kc kd ke ix kf kg kh ki bi translated"><strong class="ak">使用带火花的R包</strong></h2><p id="eb7a" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lf ir is it lg iv iw ix lh iz ja jb ha bi translated">除了对数据帧的基本操作之外，R还有一个丰富的包库，用于数据的统计分析。这些函数需要一个R数据帧或数据表，而<strong class="ig hi">不支持Spark数据帧</strong>。</p><p id="9a99" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><a class="ae li" href="http://spark.apache.org/docs/latest/api/R/dapply.html" rel="noopener ugc nofollow" target="_blank"> SparkR用户定义函数</a> (UDF) API为运行在<a class="ae li" href="https://databricks.com/spark/about" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>上的大数据工作负载提供了拥抱R丰富的软件包生态系统的机会。</p><p id="04e1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SparkR UDF API在Spark JVM和R进程之间来回传输数据。在UDF函数中，我们可以使用R数据框架来访问整个R生态系统。</p><p id="5de3" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SparkR提供了四个API，它们在R中运行一个用户定义的函数到SparkDataFrame</p><ul class=""><li id="2f23" class="kj kk hh ig b ih ii il im ip lj it lk ix ll jb lm kr ks kt bi translated"><em class="ld"> dapply() </em></li><li id="c8e2" class="kj kk hh ig b ih ln il lo ip lp it lq ix lr jb lm kr ks kt bi translated"><em class="ld"> dapplyCollect() </em></li><li id="fe11" class="kj kk hh ig b ih ln il lo ip lp it lq ix lr jb lm kr ks kt bi translated"><em class="ld"> gapply() </em></li><li id="a102" class="kj kk hh ig b ih ln il lo ip lp it lq ix lr jb lm kr ks kt bi translated"><em class="ld"> gapplyCollect() </em></li></ul><p id="982f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图阐释了在UDF执行期间执行的序列化和反序列化。数据被序列化两次，总共被反序列化两次。</p><figure class="jd je jf jg fd jh er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ls"><img src="../Images/5f8b83cde497d7ca8c0f45790e2116a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jU1zLcEPC_DuAHiPENYRgw.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx"><a class="ae li" href="https://databricks.com/blog/2018/08/15/100x-faster-bridge-between-spark-and-r-with-user-defined-functions-on-databricks.html" rel="noopener ugc nofollow" target="_blank">https://databricks.com/blog/2018/08/15/100x-faster-bridge-between-spark-and-r-with-user-defined-functions-on-databricks.html</a></figcaption></figure><p id="ea3d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi"> gapply() </strong></p><p id="e39d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该函数将应用于<code class="du lx ly lz kv b">SparkDataFrame</code>的每个组，并且应该只有两个参数:分组键和对应于该键的R <code class="du lx ly lz kv b">data.frame</code>。这些组是从<code class="du lx ly lz kv b">SparkDataFrames</code>栏中选择的。函数的输出应该是一个<code class="du lx ly lz kv b">data.frame</code>。模式指定了结果<code class="du lx ly lz kv b">SparkDataFrame</code>的行格式。它必须代表基于Spark <a class="ae li" href="https://spark.apache.org/docs/latest/sparkr.html#data-type-mapping-between-r-and-spark" rel="noopener ugc nofollow" target="_blank">数据类型</a>的R函数的输出模式。</p><p id="9050" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们想在结果上调用<em class="ld"> collect() </em>，那么<code class="du lx ly lz kv b">gapplyCollect</code>是一个快捷方式。但是，不要求传递模式。请注意，如果在所有分区上运行的UDF的输出不能被拉至驱动程序并适合驱动程序内存，则<code class="du lx ly lz kv b">gapplyCollect</code>可能会失败。</p><p id="59a6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">r代码</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="0c98" class="jo jp hh kv b fi kz la l lb lc"># <em class="ld">library</em>(energy)<br/>sales_df[, p_value := <em class="ld">poisson.mtest</em>(sales,<br/>                                                 R = 100)$p.value, by = <em class="ld">.</em>(product_id)]</span></pre><p id="bac6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">等效火花码</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="ec0a" class="jo jp hh kv b fi kz la l lb lc">sales_result_schema &lt;- <em class="ld">structType</em>(<br/>  <em class="ld">structField</em>("product_id", "integer"),<br/>  <em class="ld">structField</em>("p_value", "double"))<br/><br/>poisson_sales &lt;- <em class="ld">gapply</em>(sales_df, <em class="ld">c</em>("product_id"), function(key, x) {<br/>  <em class="ld">library</em>(energy)<br/>  y &lt;- <em class="ld">data.frame</em>(key, <em class="ld">poisson.mtest</em>(x$sales, R = 100)$p.value)<br/>}, sales_result_schema)</span></pre><p id="c4d6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">dappy()</strong></p><p id="eb48" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对一个<code class="du lx ly lz kv b">SparkDataFrame</code>的每个分区应用一个函数。应用于<code class="du lx ly lz kv b">SparkDataFrame</code>的每个分区的函数应该只有一个参数，每个分区对应的<code class="du lx ly lz kv b">data.frame</code>将被传递给该参数。函数的输出应该是一个<code class="du lx ly lz kv b">data.frame</code>。Schema指定了结果<code class="du lx ly lz kv b">SparkDataFrame</code>的行格式。它必须与返回值的<a class="ae li" href="https://spark.apache.org/docs/latest/sparkr.html#data-type-mapping-between-r-and-spark" rel="noopener ugc nofollow" target="_blank">数据类型</a>匹配。</p><p id="b6a8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">像<code class="du lx ly lz kv b">dapply</code>一样，我们可以使用<code class="du lx ly lz kv b">dapplyCollect</code>将一个函数应用到<code class="du lx ly lz kv b">SparkDataFrame</code>的每个分区，并将结果收集回来。该函数的输出应该是一个<code class="du lx ly lz kv b">data.frame</code>。但是，不要求传递模式。<code class="du lx ly lz kv b">dapplyCollect</code>如果在所有分区上运行的UDF的输出不能被拉至驱动程序并适合驱动程序内存，则可能失败。</p><h2 id="7f9d" class="jo jp hh bd jq jr js jt ju jv jw jx jy ip jz ka kb it kc kd ke ix kf kg kh ki bi translated">用SparkR运行STL和Holt</h2><p id="db2e" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lf ir is it lg iv iw ix lh iz ja jb ha bi translated"><strong class="ig hi">将STL转换为SparkR </strong></p><p id="4f43" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">执行自定义函数<em class="ld"> stl_forecast() </em>的r代码，该函数在内部使用来自<em class="ld">库(forecast)的函数<em class="ld"> stlf() </em>。</em>对<em class="ld"> stl_forecast() </em>的参数是R data.table。</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="2ceb" class="jo jp hh kv b fi kz la l lb lc">sales_R_df %&gt;%<br/>  <em class="ld">group_by</em>(product_id) %&gt;%<br/>  <em class="ld">do</em>(<em class="ld">stl_forecast</em>(<em class="ld">data.table</em>(.))) %&gt;%<br/>  <em class="ld">data.table</em>(.) -&gt; dt_stl</span></pre><p id="5942" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于SparkR，我们首先需要创建一个将被映射到结果数据框的方案。</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="0b09" class="jo jp hh kv b fi kz la l lb lc">df_stl_schema &lt;- <em class="ld">structType</em>(<br/>  <em class="ld">structField</em>("product_id", "integer"),<br/>  <em class="ld">structField</em>("audit_date", "date"),<br/>  <em class="ld">structField</em>("stl_forecast", "double"),<br/>  <em class="ld">structField</em>("stl_forecast_std", "double")<br/>)</span></pre><p id="5bae" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用gapply对SparkR数据框执行<em class="ld"> stl_forecast() </em>。</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="75ce" class="jo jp hh kv b fi kz la l lb lc">df_stl &lt;- <em class="ld">gapply</em>(sales_df, <em class="ld">c</em>("product_id"), function(key, x) {<br/>  <em class="ld">library</em>(data.table)<br/>  <em class="ld">library</em>(lubridate)<br/>  <em class="ld">library</em>(dplyr)<br/>  <em class="ld">library</em>(forecast)<br/>  <em class="ld">library</em>(TTR)</span><span id="43b8" class="jo jp hh kv b fi le la l lb lc">  # gapply passes R data frame, change to data table and pass as arg to stl_forecast()</span><span id="56ff" class="jo jp hh kv b fi le la l lb lc">  sales1 &lt;- <em class="ld">data.table</em>(x)<br/>  y &lt;- <em class="ld">data.frame</em>(key,<em class="ld">stl_forecast</em>(sales1))<br/>}, df_stl_schema)</span></pre><p id="2495" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">将霍尔特转换为火花</strong></p><p id="da0a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">类似地，我们有一个自定义方法holt_forecast()，它需要一个R数据表。</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="c2cf" class="jo jp hh kv b fi kz la l lb lc">sales_R_df %&gt;%<br/>  <em class="ld">group_by</em>(product_id) %&gt;%<br/>  <em class="ld">do</em>(<em class="ld">holt_forecast</em>(<em class="ld">data.table</em>(.))) %&gt;%<br/>  <em class="ld">data.table</em>(.) -&gt; dt_holt</span></pre><p id="8239" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SparkR中的实现</p><pre class="jd je jf jg fd ku kv kw kx aw ky bi"><span id="2870" class="jo jp hh kv b fi kz la l lb lc">dt_holt_schema &lt;- <em class="ld">structType</em>(<br/>  <em class="ld">structField</em>("product_id", "integer"),<br/>  <em class="ld">structField</em>("audit_date", "date"),<br/>  <em class="ld">structField</em>("holt_unit_forecast", "double"),<br/>  <em class="ld">structField</em>("holt_unit_forecast_std", "double")<br/>)<br/><br/>dt_holt &lt;- <em class="ld">gapply</em>(sales_df, <em class="ld">c</em>("product_id"), function(key, x) {<br/>  <em class="ld">library</em>(data.table)<br/>  <em class="ld">library</em>(lubridate)<br/>  <em class="ld">library</em>(dplyr)<br/>  <em class="ld">library</em>(forecast)<br/>  sales &lt;- <em class="ld">data.table</em>(x)<br/>  y &lt;- <em class="ld">data.frame</em>(key,<em class="ld">holt_forecast</em>(sales))<br/>}, dt_holt_schema)</span></pre><h2 id="d90f" class="jo jp hh bd jq jr js jt ju jv jw jx jy ip jz ka kb it kc kd ke ix kf kg kh ki bi translated"><strong class="ak">观察结果</strong></h2><p id="af9e" class="pw-post-body-paragraph ie if hh ig b ih kl ij ik il km in io ip lf ir is it lg iv iw ix lh iz ja jb ha bi translated">我们预计精度会有一些差异，这可能是由于Spark JVM和R环境之间的多次数据传输造成的。但是在我们对生成的数据集进行验证期间，<strong class="ig hi">我们观察到</strong> <strong class="ig hi">使用SparkR UDFs的预测与之前在R. </strong>中生成的结果完全匹配</p></div><div class="ab cl ma mb go mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ha hb hc hd he"><p id="66c8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[1]:通过Databricks上的用户定义函数将Apache Spark和R之间的桥接速度提高100倍<a class="ae li" href="https://databricks.com/blog/2018/08/15/100x-faster-bridge-between-spark-and-r-with-user-defined-functions-on-databricks.html" rel="noopener ugc nofollow" target="_blank">https://data bricks . com/blog/2018/08/15/100 x-fast-Bridge-Spark-and-R-with User-Defined-Functions-on-data bricks . html</a></p><p id="33d2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">[2]: SparkR文档<a class="ae li" href="https://spark.apache.org/docs/latest/sparkr.html#sparkdataframe" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/latest/SparkR . html # spark data frame</a></p></div></div>    
</body>
</html>