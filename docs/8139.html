<html>
<head>
<title>Implementation of SCD-2 (Slowly Changing Dimension) with Apache Hudi &amp; Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Apache胡迪和Spark实现SCD-2(渐变维度)</h1>
<blockquote>原文：<a href="https://medium.com/walmartglobaltech/implementation-of-scd-2-slowly-changing-dimension-with-apache-hudi-465e0eb94a5?source=collection_archive---------1-----------------------#2022-08-24">https://medium.com/walmartglobaltech/implementation-of-scd-2-slowly-changing-dimension-with-apache-hudi-465e0eb94a5?source=collection_archive---------1-----------------------#2022-08-24</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><blockquote class="ie if ig"><p id="11e0" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">作者及供稿:<a class="jg jh ge" href="https://medium.com/u/7b59b9130f45?source=post_page-----465e0eb94a5--------------------------------" rel="noopener" target="_blank"> <em class="hh">贾亚西埃尔·卡尔加尔</em> </a> <em class="hh">，</em> <a class="jg jh ge" href="https://medium.com/u/48093e2b1014?source=post_page-----465e0eb94a5--------------------------------" rel="noopener" target="_blank"> <em class="hh">艾莎·丁</em> </a> <em class="hh">，</em> <a class="jg jh ge" href="https://medium.com/u/cbfd10c97864?source=post_page-----465e0eb94a5--------------------------------" rel="noopener" target="_blank"> <em class="hh">普拉尚特·米什拉</em> </a></p></blockquote><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es ji"><img src="../Images/aa7138e5c0ae54f0be6f3f7d2241ffe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yJ2aiphe35lzgZw2I_O20w.jpeg"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx">Image source : <a class="ae jy" href="https://pixabay.com/photos/files-paper-office-paperwork-stack-1614223/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/photos/files-paper-office-paperwork-stack-1614223/</a></figcaption></figure><h2 id="f741" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">我们要解决什么？</h2><p id="dff6" class="pw-post-body-paragraph ih ii hh ik b il kx in io ip ky ir is kk kz iv iw ko la iz ja ks lb jd je jf ha bi lc translated"><span class="l ld le lf bm lg lh li lj lk di"> D </span>在当今的分析世界中，ATA是一笔无价的财富。在向最终用户提供数据时，跟踪数据在一段时间内的变化非常重要。渐变维度(SCD)是一种存储和管理当前和历史数据的维度。在SCD的类型中，我们将特别关注类型2 (SCD 2)，它保留了值的完整历史。每个记录都包含有效时间和到期时间，以标识记录处于活动状态的时间段。这可以通过几个审计列来实现。<strong class="ik hi">例如</strong> : <em class="ij">有效开始日期、有效结束日期和有效记录指示器</em>。</p><p id="ae01" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">让我们学习如何通过使用— <strong class="ik hi">阿帕奇胡迪&amp; Spark </strong>来实现SCD-2表格设计</p><blockquote class="ie if ig"><p id="16b0" class="ih ii ij ik b il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf ha bi translated">Apache胡迪是下一代<a class="ae jy" href="https://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform" rel="noopener ugc nofollow" target="_blank">流数据湖平台</a>。Apache胡迪将核心仓库和数据库功能直接引入数据湖。胡迪提供了<a class="ae jy" href="https://hudi.apache.org/docs/next/table_management" rel="noopener ugc nofollow" target="_blank">表</a>、<a class="ae jy" href="https://hudi.apache.org/docs/next/timeline" rel="noopener ugc nofollow" target="_blank">事务</a>、<a class="ae jy" href="https://hudi.apache.org/docs/next/write_operations" rel="noopener ugc nofollow" target="_blank">高效的更新/删除</a>、<a class="ae jy" href="https://hudi.apache.org/docs/next/indexing" rel="noopener ugc nofollow" target="_blank">高级索引</a>、<a class="ae jy" href="https://hudi.apache.org/docs/next/hoodie_deltastreamer" rel="noopener ugc nofollow" target="_blank">流摄取服务</a>、数据<a class="ae jy" href="https://hudi.apache.org/docs/next/clustering" rel="noopener ugc nofollow" target="_blank">集群</a> / <a class="ae jy" href="https://hudi.apache.org/docs/next/compaction" rel="noopener ugc nofollow" target="_blank">压缩</a>优化以及<a class="ae jy" href="https://hudi.apache.org/docs/next/concurrency_control" rel="noopener ugc nofollow" target="_blank">并发</a>，同时将您的数据保持为开源文件格式。</p></blockquote><p id="fec6" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">Apache胡迪总是显示表中的快照数据，即最近提交的最新数据。如果我们想要跟踪历史变化，我们需要利用胡迪的<code class="du ll lm ln lo b"><strong class="ik hi">Point in Time Query</strong></code>(<a class="ae jy" href="https://hudi.apache.org/docs/quick-start-guide#point-in-time-query" rel="noopener ugc nofollow" target="_blank">https://hudi . Apache . org/docs/quick-start-guide # point-in-time-query</a>)</p><p id="fb72" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">胡迪允许最新的数据读取和时间旅行与旧版本的时间点查询。利用时间点查询遍历历史数据变化是乏味的，并且需要对给定数据进行多个时间间隔的分析。</p><p id="32a5" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">让我们看看如何用一种传统的方法来解决这个问题。</p></div><div class="ab cl lp lq go lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ha hb hc hd he"><p id="833c" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">让我们考虑一个包含产品细节和卖家折扣的表格。</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lw lx l"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Hive Table (product_dtl)</figcaption></figure><h2 id="4362" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">步骤</strong></h2><ol class=""><li id="fecd" class="ly lz hh ik b il kx ip ky kk ma ko mb ks mc jf md me mf mg bi translated">让我们使用Spark将这些数据放入胡迪表中。</li></ol><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lw lx l"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Launching Spark Shell with Hudi dependencies</figcaption></figure><p id="f736" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">一旦spark shell启动，我们就可以导入库，并创建我们的胡迪表，如下所示。</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lw lx l"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Importing Hudi libraries &amp; creating Hudi target table</figcaption></figure><p id="a89f" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">在将数据添加到桶中之后，这就是我们的<strong class="ik hi">胡迪</strong>目标表的样子。</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lw lx l"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Hudi target table (product_dtl)</figcaption></figure><p id="b7a0" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">2.让我们假设我们的增量数据存储在下表中(非胡迪格式，可以是hive)。</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lw lx l"/></div></figure><p id="ab20" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">3.现在让我们通过在目标表上做一个<a class="ae jy" href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html#anti-join" rel="noopener ugc nofollow" target="_blank"><strong class="ik hi"/></a><strong class="ik hi"/>左反连接来过滤掉delta表中所有的<strong class="ik hi"> Insert </strong> only记录。</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lw lx l"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Filter Insert only records</figcaption></figure><p id="a2e1" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">4.我们有一个只插入记录的<strong class="ik hi">数据帧。接下来，让我们创建一个dataframe，它将包含来自增量表和目标表的属性，在目标表上有一个<strong class="ik hi">内连接</strong>，它将获取需要更新的记录。</strong></p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lw lx l"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Update records , in both delta and target tables</figcaption></figure><p id="8c07" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">5.现在我们有了一个在单个记录中包含新数据和旧数据的数据帧，让我们将更新记录的<strong class="ik hi">活动</strong>和<strong class="ik hi">非活动</strong>实例分别放在单独的数据帧中。</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es mh"><img src="../Images/c55e0a074820861b9a207c063ad2bc55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9snA3l3R6_ghr1OCvh8Whw.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx">Split update records into active and inactive</figcaption></figure><p id="c0aa" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">在进行上述练习时，我们将通过将活动(新)记录的<code class="du ll lm ln lo b">eff_end_ts</code>更改为<code class="du ll lm ln lo b">eff_start_ts -1 </code>并更新<code class="du ll lm ln lo b">actv_ind = 0</code>来废弃非活动记录</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lw lx l"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Create active and inactive updates dataframes</figcaption></figure><p id="9afb" class="pw-post-body-paragraph ih ii hh ik b il im in io ip iq ir is kk iu iv iw ko iy iz ja ks jc jd je jf ha bi translated">6.现在我们将用union操作符将<strong class="ik hi">插入</strong>、<strong class="ik hi">活动更新</strong>和<strong class="ik hi">非活动更新</strong>到单个数据帧中。将该数据帧作为最终胡迪写逻辑的增量源。</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lw lx l"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Hudi upsert logic &amp; final data of the target table after upsert operation</figcaption></figure></div><div class="ab cl lp lq go lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ha hb hc hd he"><h2 id="0657" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">实施过程中需要考虑的几点</h2><ul class=""><li id="18d1" class="ly lz hh ik b il kx ip ky kk ma ko mb ks mc jf mi me mf mg bi translated">对于现有记录的每次更新，parquet文件将在存储中被重写/移动，这可能会影响写入时的性能</li><li id="46e7" class="ly lz hh ik b il mj ip mk kk ml ko mm ks mn jf mi me mf mg bi translated">在数据查询过程中，根据代表主要过滤器的属性对目标表进行分区总是一个更好的主意。例如:销售日期在销售表格的情况下，销售者为产品目录登记。在我们的例子中，我们选择了<code class="du ll lm ln lo b">actv_ind</code>,因为我们希望保持简单的解释，并将所有活动记录保存在一个分区中。</li></ul></div><div class="ab cl lp lq go lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ha hb hc hd he"><h2 id="48d3" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论</h2><p id="7c6e" class="pw-post-body-paragraph ih ii hh ik b il kx in io ip ky ir is kk kz iv iw ko la iz ja ks lb jd je jf ha bi translated">随着我们将Apache胡迪用于Spark应用程序，我们将继续改进加载数据的策略。上述尝试仅仅是用胡迪实现SCD-2功能的开始。</p></div><div class="ab cl lp lq go lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ha hb hc hd he"><h2 id="9e1d" class="jz ka hh bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">参考</h2><p id="62c6" class="pw-post-body-paragraph ih ii hh ik b il kx in io ip ky ir is kk kz iv iw ko la iz ja ks lb jd je jf ha bi translated"><a class="ae jy" href="https://hudi.apache.org/docs/overview" rel="noopener ugc nofollow" target="_blank">https://hudi.apache.org/docs/overview</a></p></div></div>    
</body>
</html>