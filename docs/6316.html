<html>
<head>
<title>Pinterest’s Analytics as a Platform on Druid (Part 3 of 3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">作为德鲁伊平台的Pinterest分析(第3部分，共3部分)</h1>
<blockquote>原文：<a href="https://medium.com/pinterest-engineering/pinterests-analytics-as-a-platform-on-druid-part-3-of-3-579406ffa374?source=collection_archive---------3-----------------------#2021-09-07">https://medium.com/pinterest-engineering/pinterests-analytics-as-a-platform-on-druid-part-3-of-3-579406ffa374?source=collection_archive---------3-----------------------#2021-09-07</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="4291" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">、齐家·古、、伊莎贝尔·塔拉姆、拉克什米·纳拉亚纳·纳马拉、卡比尔·巴贾杰|实时分析团队</p><p id="0a88" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个由三部分组成的博客系列。点击阅读<a class="ae jc" rel="noopener" href="/pinterest-engineering/pinterests-analytics-as-a-platform-on-druid-part-1-of-3-9043776b7b76">第一部分</a>和<a class="ae jc" rel="noopener" href="/pinterest-engineering/pinterests-analytics-as-a-platform-on-druid-part-2-of-3-e63d5280a1a9">第二部分</a>。</p><p id="c186" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这个博客系列中，我们将讨论作为德鲁伊平台的Pinterest的分析，并分享一些使用德鲁伊的心得。这是博客文章系列的第三篇，将讨论针对实时用例优化Druid的学习。</p><h1 id="9601" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">关于为实时用例优化Druid的学习</h1><p id="fb95" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">当我们第一次将Druid引入Pinterest时，它主要用于查询批量获取的数据。随着时间的推移，我们已经转向基于实时的报告系统，以使指标在到达后几分钟内准备好供查询。用例越来越多地采用lambda架构，除了真理批处理管道之外，Flink上还有流管道。这给Druid上的报告层带来了一个巨大的挑战:我们搭载的最大的实时用例有以下需求:上游流ETL管道产生的kafka主题Druid使用超过500k QPS，并期望Druid上的摄取延迟在一分钟之内。预期的查询QPS约为1，000，P99延迟约为250毫秒</p><h2 id="76c5" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">内存位图</h2><p id="9e06" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在使用用例的过程中，我们首先遇到了为运行在中层管理人员peon流程上的实时片段提供服务的瓶颈。最初，为了满足SLA的要求，增加了大量的主机，但是基础架构成本很快变得与收益成非线性关系。此外，由于有如此多的任务计数和副本，在处理细分元数据时，霸王在处理来自peons的请求方面压力很大。需要大量主机来提供足够的处理线程，除非每个处理线程的工作(扫描数据段)变得更加高效，否则无法减少主机数量。我们进行了分析，发现实时段的查询逻辑效率很低:当数据第一次被接收到实时层时，它首先被放入内存映射中，该映射没有批量接收的druid段所具有的倒排索引，这意味着对实时段的每个查询都将对每一行进行完全扫描，以获得要聚合的候选行。对于我们的大多数用例来说，每个查询要聚合的候选行只占细分行总数的很小一部分。然后，当查询未持久化的实时段时，我们实现了内存位图。这导致我们所有实时用例的中层管理人员能力减少了近70%,同时提供了更好的延迟。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/2890207ef07d0f32ae8a5701760400da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ggCMIi1xdpLeVWvY"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Figure 1: Use case 1 P99 latency reduction on Middle Managers</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/b196a33f83645870acab8793bc281f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b2AldMY3hhp0AM4x"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Figure 2: Use case 2 P90 latency (ms) and infrastructure cost reduction on Middle Managers</figcaption></figure><h2 id="668e" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">分区实时段</h2><p id="b8fe" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">在添加内存位图后，我们在减少处理单个段的CPU方面取得了很好的改进。然而，我们发现我们仍然无法处理线程，因为由于数据段未被分区，需要处理大量的数据段。当实时片段由中层管理人员提供服务时，以及当片段在压缩作业几个小时后开始之前由历史片段完成并提供服务时，这是一个问题。我们的用例具有许多延迟事件的性质，时间戳跨越过去48小时。这给德鲁伊造成了很大的负担，因为每小时创建的段的数量与我们接受的延迟消息时间窗口的数量成正比。经过一些基准测试后，客户同意使用3小时的延迟消息接受窗口来捕捉大多数事件。这比之前的48小时延迟消息窗口要好，但仍然对系统提出了很大的挑战。当QPS很高时，常数乘数很重要。假设我们需要250个任务来将接收延迟保持在1分钟以内，延迟3小时的消息窗口，当前小时的段数至少为250 * 3 = 750。每个单个查询需要扫描750个段，对于1000 QPS，每秒扫描的段数约为750 * 1000 = 750000。Druid中的每个段都由单个处理线程处理，处理线程的典型数量被设置为等于主机中可用CPU的数量。从理论上讲，我们需要750000 / 32 = 23437台32核主机，以避免任何数据段排队，但基础架构成本将无法控制。</p><p id="82f6" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">实际上，效率在于事件分散在所有片段中。对于Pinterest中的大多数实时用例，每个查询都带有一个id来过滤，例如pin_id，因此如果我们可以根据id划分段，这将为我们提供一个机会来修剪段并限制查询扇出。在批量摄取中，分区是可能的，因为我们可以在摄取过程中进行任意混合，而在实时摄取中，摄取到每个部分的数据基于peon过程中每个消费者被分配到的kafka分区。所以我们让上游流处理器使用定制的kafka密钥分割器。分区是在基于哈希的机制中完成的，实时段采用了相应的自定义碎片规范，该规范还包含创建该段的kafka分区的元数据。在查询期间，代理可以在过滤器中重新散列id，以确定给定的段是否可能包含该id的数据。id的散列产生一个kafka分区，从而产生一个段。有了这一点，每个段要扫描的段数减少了250倍，从最初的要扫描的段数/任务数(750000)降至750000 / 250 = 3000段，这需要3000 / 32 = 93台32核主机，比以前更易于管理。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/029ccf26135c1f5db29936e76923bec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TBRrg423xE-zEokd.png"/></div></div></figure><p id="3205" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">作为后续工作，我们还扩展了自定义碎片规范，增加了另一个字段fanOutSize，用于将给定的分区维度值散列到多个段，以解决未来用例中潜在的数据偏斜问题。</p><h2 id="9b85" class="kg je hh bd jf kh ki kj jj kk kl km jn ip kn ko jr it kp kq jv ix kr ks jz kt bi translated">实时段上的布隆过滤器索引</h2><p id="b24c" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">对于分区的实时段，查询段扇出显著下降；但是所需的CPU数量仍然很高。上面计算中要查询的3000个片段仅针对一个小时。我们的实时用例的典型查询要求从现在到24小时前的数据。由于各种原因，在压缩开始之前需要8小时来压缩8小时前的实时数据段。到那时，要查询的数据段数量将减少3倍，因为由延迟事件引入的3个数据段已被压缩，因此8小时之前的数据段所需的CPU数量将大大减少。另一方面，对于最近的8小时部分，3000 * 8 = 24000需要24000 / 32 = 750台32核主机。</p><p id="725f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着对查询模式进行更多的分析，我们发现尽管过滤器中的id基数很大，但并不是所有的id在给定的时间内都有新数据。基于哈希的实时段分区没有足够的元数据来进行单个id级别的存在性检查，因为哈希将导致至少一个段可能包含数据，而实际上该段在大多数情况下没有任何数据要返回。对于高QPS和晚事件，在一个扫描段上的假阳性很重要。</p><p id="f5d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们没有找到一个好的方法来解决上述问题的实时段服务于中层管理人员。他们不断有新的数据流入，我们不知道具体id的数据是否会到达，直到片段最终完成并发布到historicals。同时，因为基于片段的碎片规范中的元数据的修剪逻辑在代理上被调用，而关于片段中的数据的最新信息在peons上是已知的，所以如果元数据不正确，就没有办法在两个不同的组件之间实时同步片段中的特定id。由于这只有一个小时，我们没有触及未完成的部分。</p><p id="7cc5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">另一方面，在最终确定peons的实时细分市场时，我们对细分市场中的数据有充分的了解。因此，我们在shard规范中添加了元数据——当段由中层管理人员提供服务时，代理最初会忽略这些数据，但后来当historicals加载段的最终版本时，这些数据会被提取出来。对于特定的元数据，我们使用了一个bloom过滤器来存储id，这是一个很好的平衡大小和准确性的概率结构。对于段中一百万基数的id，只需要几个MB就可以获得3%的预期误报率。通过这一改变，我们能够在过去的8小时内将扫描的段数减少5倍。上面计算出的所需CPU数量变成了750 / 5 = 150台32核主机，这比以前更具成本效益。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/8821ce089e4d3dabf3959b699cc17ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_MRGSPuouEkSROw9"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Figure 3: Use case 1 number of Segments to Scan</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/a7c07aebab2db19a71453d8d6d21980d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UXOZ5-gQ07D3nZ6V"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Figure 4: Use case 1 processing thread usage on historicals</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/600c13ce82ee5a14e71af2ad13ca95a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Jpj7Ez8x8FuZVTtO.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Table 1: Use case 2 prune rate of number of segments to scan</figcaption></figure><h1 id="bbd0" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">未来的工作</h1><p id="79a6" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">仍有许多方面需要改进，包括但不限于持续压缩、Kafka主题缩放事件兼容实时分区、大任务计数和晚期消息窗口可伸缩性改进等。与此同时，我们也将更加积极地开源我们迄今为止开发的内容。</p><h1 id="3cd2" class="jd je hh bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">承认</h1><p id="9f9a" class="pw-post-body-paragraph ie if hh ig b ih kb ij ik il kc in io ip kd ir is it ke iv iw ix kf iz ja jb ha bi translated">当我们开始反馈我们的工作时，我们从德鲁伊公会与Ads数据团队的讨论以及来自开源社区的反馈中学到了很多。我们还要感谢所有与我们合作将其使用案例纳入统一分析平台的团队:洞察团队、核心产品数据团队、测量团队、信任与安全团队、广告数据团队、信号平台团队、广告服务团队等。每个用例都是不同的，该平台自诞生以来已经发展了很多。</p><p id="b335" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><em class="lk">要在Pinterest了解更多工程知识，请查看我们的</em> <a class="ae jc" href="https://medium.com/pinterest-engineering" rel="noopener"> <em class="lk">工程博客</em> </a> <em class="lk">，并访问我们的</em><a class="ae jc" href="https://www.pinterestlabs.com/?utm_source=medium&amp;utm_medium=blog-article&amp;utm_campaign=want-et-al-september-2-2021" rel="noopener ugc nofollow" target="_blank"><em class="lk">Pinterest Labs</em></a><em class="lk">网站。要查看和申请空缺职位，请访问我们的</em> <a class="ae jc" href="https://www.pinterestcareers.com/?utm_source=medium&amp;utm_medium=blog-article&amp;utm_campaign=want-et-al-september-2-2021" rel="noopener ugc nofollow" target="_blank"> <em class="lk">职业</em> </a> <em class="lk">页面。</em></p></div></div>    
</body>
</html>