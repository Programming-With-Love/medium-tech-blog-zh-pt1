<html>
<head>
<title>How to implement Recursive Queries in Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Spark中实现递归查询</h1>
<blockquote>原文：<a href="https://medium.com/globant/how-to-implement-recursive-queries-in-spark-3d26f7ed3bc9?source=collection_archive---------0-----------------------#2022-07-15">https://medium.com/globant/how-to-implement-recursive-queries-in-spark-3d26f7ed3bc9?source=collection_archive---------0-----------------------#2022-07-15</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/285a7e706cd537d5b5d584a13dbf919f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Vu_T7E1-ahuAo1DEOcjvQ.png"/></div></div></figure><p id="e502" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">为了使用另一列来标识一列的顶级层次结构，我们使用递归公共表表达式，在关系数据库中通常称为<a class="ae jn" href="https://docs.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql?view=sql-server-ver16#guidelines-for-defining-and-using-recursive-common-table-expressions" rel="noopener ugc nofollow" target="_blank"> <em class="jo">递归CTE </em> </a>。<a class="ae jn" href="https://docs.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql?view=sql-server-ver16#guidelines-for-defining-and-using-recursive-common-table-expressions" rel="noopener ugc nofollow" target="_blank">递归CTE </a>是许多传统关系数据库如SQL Server、Oracle、Teradata、Snowflake等的重要特性之一。使用<a class="ae jn" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank"> Dataframe </a>操作时，Spark SQL不支持递归CTE。在本文中，我们将检查如何使用<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a>实现Spark SQL递归<a class="ae jn" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank"> Dataframe </a>。</p><p id="2cd4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在实现这个解决方案之前，我研究了许多选项，而<a class="ae jn" href="https://spark.apache.org/docs/latest/graphx-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> SparkGraphX </a> API有可能实现这个目标。然而，我找不到任何可以满足项目需求的可持续解决方案，我试图实现一个更像SQL的解决方案，并且与<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a>兼容。因此我想出了使用<a class="ae jn" href="https://docs.python.org/3/tutorial/datastructures.html#tut-listcomps" rel="noopener ugc nofollow" target="_blank"> <em class="jo">列表理解</em> </a>和<em class="jo">迭代映射</em>函数在<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a>中实现递归的解决方案。</p></div><div class="ab cl jp jq go jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="ha hb hc hd he"><h2 id="a7a2" class="jw jx hh bd jy jz ka kb kc kd ke kf kg ja kh ki kj je kk kl km ji kn ko kp kq bi translated">前言</h2><p id="782b" class="pw-post-body-paragraph ip iq hh ir b is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji kv jk jl jm ha bi translated">在递归查询中，有一个子元素，或者我们可以说是种子元素，它位于层次结构的最低级别。带有种子元素的查询是生成结果集的第一个查询。在下一步中，seed元素生成的任何结果集都将与另一列连接，以生成结果集。这个步骤一直持续到顶层层次结构。一旦没有检索到新行，迭代就结束。</p><p id="5811" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我已经尝试使用<a class="ae jn" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank">数据帧</a>、<a class="ae jn" href="https://docs.python.org/3/tutorial/datastructures.html#tut-listcomps" rel="noopener ugc nofollow" target="_blank">列表理解</a>和迭代映射函数在<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a>中复制相同的步骤，以达到相同的结果。让我们从实时实现开始，在进入<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a> Dataframe操作之前，让我们检查一下关系数据库中的递归查询。</p></div><div class="ab cl jp jq go jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="ha hb hc hd he"><h2 id="81cc" class="jw jx hh bd jy jz ka kb kc kd ke kf kg ja kh ki kj je kk kl km ji kn ko kp kq bi translated">关系数据库递归查询示例</h2><p id="7dac" class="pw-post-body-paragraph ip iq hh ir b is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji kv jk jl jm ha bi translated">要在本地创建数据集，可以使用下面的命令。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="6889" class="jw jx hh lb b fi lf lg l lh li">CREATE TABLE employee_record (employee_number INT ,manager_employee_number INT); <br/>insert into employee_record values( 404,NULL);<br/>insert into employee_record values( 1014,1019);<br/>insert into employee_record values( 1011,1019);<br/>insert into employee_record values( 1010,1003);<br/>insert into employee_record values( 1001,1003);<br/>insert into employee_record values( 1004,1003);<br/>insert into employee_record values( 1012,1004);<br/>insert into employee_record values( 1002,1004);<br/>insert into employee_record values( 1015,1004);<br/>insert into employee_record values( 1003,404);<br/>insert into employee_record values( 1019,404);<br/>insert into employee_record values( 1016,404);<br/>insert into employee_record values( 1008,1019);<br/>insert into employee_record values( 1006,1019);</span></pre><p id="417c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">下面是结果集的截图:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lj"><img src="../Images/5eab10ba120302c5b8aecd2a00163d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*soAxtE3POTh9hhwxYaov1w.png"/></div></div></figure><p id="f7eb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该表代表了雇员和其经理之间的关系，简单地说，对于一个特定的组织，经理是雇员的经理，经理的经理。这意味着该表包含雇员-经理数据的层次结构。</p><p id="e843" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们更明白这一点。请看下图，其中包含了看起来像是层级结构的雇员。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lk"><img src="../Images/c8195f174d01dd96c6ca8c5cadefbcf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NbnU3IS0QyOKhaK34Kosng.png"/></div></div></figure><p id="9902" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是我们的SQL递归查询，它检索直接或间接向employee_number = 404的经理报告的所有雇员的雇员号:</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="b8a0" class="jw jx hh lb b fi lf lg l lh li">WITH CTE_Recursive AS <br/>(<br/>  select er.employee_number from employee_record er<br/>  where er.manager_employee_number = 404<br/>  UNION ALL<br/>  select emprec.employee_number from CTE_Recursive crec<br/>  inner join employee_record emprec<br/>  on crec.employee_number = emprec.manager_employee_number<br/>)<br/>select * from CTE_Recursive order by employee_number</span></pre><p id="7d06" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">上述查询的输出如下:</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es ll"><img src="../Images/30ebb163de8d3fad2e258823512a2fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*pAie7mTFFpNCUmKAxMvUMw.png"/></div></figure><p id="ffae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在上面的查询中，before <code class="du lm ln lo lb b">UNION ALL</code>是雇员编号为404的经理的直接雇员，而after union all充当一个‘iterator’语句。在CTE，我们使用同一个CTE，它将一直运行，直到在员工编号为404的经理手下获得直接和间接员工。</p></div><div class="ab cl jp jq go jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="ha hb hc hd he"><h2 id="35ef" class="jw jx hh bd jy jz ka kb kc kd ke kf kg ja kh ki kj je kk kl km ji kn ko kp kq bi translated">PySpark代码来识别数据的层次结构</h2><p id="2f08" class="pw-post-body-paragraph ip iq hh ir b is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji kv jk jl jm ha bi translated">在<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a>中，我将使用<a class="ae jn" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank"> Dataframe </a>操作、<a class="ae jn" href="https://docs.python.org/3/tutorial/datastructures.html#tut-listcomps" rel="noopener ugc nofollow" target="_blank">列表理解</a>，以及使用Lambda表达式的迭代映射函数来识别数据的层次结构，并以列表的形式获得输出。</p><p id="de08" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，让我们创建模式。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="828f" class="jw jx hh lb b fi lf lg l lh li"># Employee DF<br/>schema = 'EMPLOYEE_NUMBER int, MANAGER_EMPLOYEE_NUMBER int'<br/>employees_df = spark.createDataFrame(<br/>[[404,None],<br/>[1016,404],<br/>[1003,404],<br/>[1019,404],<br/>[1010,1003],<br/>[1004,1003],<br/>[1001,1003],<br/>[1012,1004],<br/>[1002,1004],<br/>[1015,1004],<br/>[1008,1019],<br/>[1006,1019],<br/>[1014,1019],<br/>[1011,1019]], schema=schema)</span></pre><p id="f59b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我已经创建了一个用户定义的函数(UDF)，该函数将一个列表作为输入，并在迭代完成时返回一个完整的列表集。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="21d1" class="jw jx hh lb b fi lf lg l lh li">def get_emp_recursive_udf(x) :<br/>  emp_list = []<br/>  direct_emp_list = list(employees_df<br/>    .select('EMPLOYEE_NUMBER')<br/>    .filter(employees_df.MANAGER_EMPLOYEE_NUMBER == x) <br/>    .toPandas()['EMPLOYEE_NUMBER'])<br/>  driterative_list = list(map(lambda y :<br/>    get_emp_recursive_udf(y), direct_emp_list))<br/>  final_lst = direct_emp_list + driterative_list<br/>  emp_list.append(final_lst)<br/>  return emp_list</span></pre><p id="5be8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，让我们使用UDF。一旦我们得到函数的输出，我们将把它转换成一个格式良好的二维列表</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="5c6a" class="jw jx hh lb b fi lf lg l lh li">#Enter the Input Employee List<br/>input_emp_list = [404]<br/>employee_list = list(map(lambda y : get_emp_recursive_udf(y),input_emp_list))<br/>employee_list_str = str(employee_list)<br/>employee_list_str = employee_list_str.replace("[","").replace("]","").replace("'","").replace(" ","").replace("None","").replace(',,',',')<br/>employee_list_str_formatted = employee_list_str.replace(",,",",")<br/>employee_list_str_formatted = employee_list_str_formatted.split(",")<br/>list_of_employee = list(dict.fromkeys(employee_list_str_formatted))<br/>final_list_of_employee = list(filter(None, list_of_employee))<br/>final_list_of_employee</span></pre><p id="0319" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">运行完整的<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a>代码后，下面是我们得到的结果集——我们在SQL CTE递归查询中得到的输出的完整副本。</p><figure class="kw kx ky kz fd ii er es paragraph-image"><div class="er es lp"><img src="../Images/3ce7d387312b8c1b665071f2d1658ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*DKuF7G_OugAETf4MR751bw.png"/></div></figure><p id="b627" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">太好了！代码如预期的那样运行良好。厉害！</p></div><div class="ab cl jp jq go jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="ha hb hc hd he"><h2 id="48bc" class="jw jx hh bd jy jz ka kb kc kd ke kf kg ja kh ki kj je kk kl km ji kn ko kp kq bi translated">结论</h2><p id="f52d" class="pw-post-body-paragraph ip iq hh ir b is kr iu iv iw ks iy iz ja kt jc jd je ku jg jh ji kv jk jl jm ha bi translated">在这篇博客中，我们能够展示如何将简单的递归CTE查询转换成等价的<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a>代码。借助这种方法，<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a>用户也可以找到递归元素，就像传统关系数据库中的<a class="ae jn" href="https://docs.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql?view=sql-server-ver16#guidelines-for-defining-and-using-recursive-common-table-expressions" rel="noopener ugc nofollow" target="_blank">递归CTE </a>方法一样。<a class="ae jn" href="https://spark.apache.org/docs/latest/api/python/#:~:text=PySpark%20is%20an%20interface%20for,data%20in%20a%20distributed%20environment." rel="noopener ugc nofollow" target="_blank"> PySpark </a>用户可以从Spark SQL <a class="ae jn" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html" rel="noopener ugc nofollow" target="_blank"> Dataframe </a>中找到递归元素，以优化时间性能的方式提供一个精细且易于实现的解决方案。</p></div></div>    
</body>
</html>