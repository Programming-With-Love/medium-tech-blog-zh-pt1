<html>
<head>
<title>K-Nearest Neighbors Algorithm Using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Python的k-最近邻算法</h1>
<blockquote>原文：<a href="https://medium.com/edureka/k-nearest-neighbors-algorithm-b87ee824b860?source=collection_archive---------7-----------------------#2018-07-26">https://medium.com/edureka/k-nearest-neighbors-algorithm-b87ee824b860?source=collection_archive---------7-----------------------#2018-07-26</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/7381b802fac0a5052c72aa8adad2bef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*Dme6pkfvBEoVnC22fgoP4A.jpeg"/></div></figure><p id="cc72" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">随着商业世界完全围绕着数据科学，它已经成为除了领域之外最重要的领域之一。在这篇关于KNN算法的文章中，您将了解KNN算法是如何工作的，以及如何使用Python来实现它。</p><h1 id="788d" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">什么是KNN算法？</h1><p id="3258" class="pw-post-body-paragraph il im hh in b io kh iq ir is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji ha bi translated">“K最近邻或KNN算法是一种简单的算法，在其训练阶段使用整个数据集。每当一个看不见的数据实例需要预测时，它会在整个训练数据集中搜索k个最相似的实例，具有最相似实例的数据最终作为预测返回。</p><p id="fc7d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">KNN经常用于搜索应用程序，在那里你要寻找<strong class="in hi">相似的</strong>物品，比如<strong class="in hi">寻找与这个相似的物品。</strong></p><p id="e262" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">该算法表明，如果<strong class="in hi"> <em class="km">你和你的邻居相似，那么你就是他们中的一员</em> </strong>。例如，如果苹果看起来更像桃子、梨和樱桃(水果)，而不是猴子、猫或老鼠(动物)，那么苹果很可能是一种水果。</p><h1 id="a661" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">KNN算法是如何工作的？</h1><p id="d442" class="pw-post-body-paragraph il im hh in b io kh iq ir is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji ha bi translated">k-最近邻算法使用一种非常简单的方法来执行分类。当使用新示例进行测试时，它会查看训练数据，并找到与新示例最接近的k个训练示例。然后，它将最常见的类标签(在那些k-training示例中)分配给测试示例。</p><h1 id="182f" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">KNN算法中的“k”代表什么？</h1><figure class="ko kp kq kr fd ii er es paragraph-image"><div class="er es kn"><img src="../Images/dd2a7e429218b5709ddb66fe17bcb919.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*2S4-xzWxjpQbNNPvNrWd1Q.png"/></div></figure><p id="4de6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">KNN算法中的k代表投票支持新测试数据类的最近邻点的数量。</p><p id="5a72" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果k=1，则测试示例被赋予与训练集中最接近的示例相同的标签。</p><p id="f6b4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">如果k=3，则检查三个最接近的类的标签，并分配最常见的(即，至少出现两次)标签，对于较大的ks，依此类推。</p><h1 id="2681" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">KNN算法手动实现</h1><p id="d326" class="pw-post-body-paragraph il im hh in b io kh iq ir is ki iu iv iw kj iy iz ja kk jc jd je kl jg jh ji ha bi translated">让我们考虑这个例子，</p><p id="5dfa" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">假设我们有几个顾客的身高和体重及其相应的t恤尺寸。你的任务是预测安娜的t恤尺寸，她的身高是161厘米，体重是61公斤。</p><figure class="ko kp kq kr fd ii er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ks"><img src="../Images/9584053b958ea4782a844e26a2826af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ri3NnAzRTiAv_CxWud5lNQ.png"/></div></div></figure><p id="cc5b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">步骤1: </strong>计算新点和现有点之间的欧几里德距离<br/>例如，点P1(1，1)和P2(5，4)之间的欧几里德距离为:</p><figure class="ko kp kq kr fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/1bfdd5fd02d31b4c63c2edb2a2970110.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*DBJmZMdcXqTqNyeNwS-Nwg.png"/></div></figure><figure class="ko kp kq kr fd ii er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ks"><img src="../Images/1f0408a67da658393c51f972b100b37d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Tr_IwLNhEesucM2Y99Fcg.png"/></div></div></figure><p id="dc15" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">第二步:</strong>选择K的值，选择K个邻居到新点的最近距离。<br/>在这种情况下，选择具有最小欧几里德距离的前5个参数</p><figure class="ko kp kq kr fd ii er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ks"><img src="../Images/11c54f5f36123ee334f034df4c9ad116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JLvDhxJB0HPVN7uoYG1B2g.png"/></div></div></figure><p id="4735" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">步骤3: </strong>统计所有K个邻居/预测值的投票<br/>因为对于K = 5，我们有4件尺寸为M的t恤衫，因此根据kNN算法，身高161 cm、体重61kg的Anna将适合尺寸为M的t恤衫。</p><figure class="ko kp kq kr fd ii er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ks"><img src="../Images/a386262a9ed19a2ab6e92f96083e3082.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*disnjwJGkyKoZMy0X_069A.png"/></div></div></figure><h1 id="0db8" class="jj jk hh bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">用Python实现kNN算法</h1><ul class=""><li id="9885" class="ky kz hh in b io kh is ki iw la ja lb je lc ji ld le lf lg bi translated">处理数据</li><li id="dc1d" class="ky kz hh in b io lh is li iw lj ja lk je ll ji ld le lf lg bi translated">计算距离</li><li id="3b75" class="ky kz hh in b io lh is li iw lj ja lk je ll ji ld le lf lg bi translated">找到k个最近点</li><li id="a00a" class="ky kz hh in b io lh is li iw lj ja lk je ll ji ld le lf lg bi translated">预测班级</li><li id="2f8c" class="ky kz hh in b io lh is li iw lj ja lk je ll ji ld le lf lg bi translated">检查准确性</li></ul><p id="ecd2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">“不要光看，要实践！”</p><p id="a7ed" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">第一步:</strong>处理数据</p><p id="4b1f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">第一步是使用<a class="ae lm" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>处理笔数据集。O <em class="km">打开功能</em>，用csv模块下的<em class="km">读取功能</em>读取数据线。</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="f7c4" class="ls jk hh lo b fi lt lu l lv lw"><strong class="lo hi">import</strong> <!-- -->csv<br/>with open(r'C:UsersAtul HarshaDocumentsiris.data.txt') as csvfile:<br/>     lines <strong class="lo hi">=</strong> <!-- -->csv.reader(csvfile)<br/>     <strong class="lo hi">for</strong> <!-- -->row <strong class="lo hi">in</strong> <!-- -->lines:<br/>          print<!-- --> <!-- -->(', '.join(row))</span></pre><p id="f318" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在，您需要将数据分为训练数据集(用于进行预测)和测试数据集(用于评估模型的准确性)。</p><p id="1183" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在继续之前，请将作为字符串加载的flower measures转换为数字。接下来，将数据集随机分为训练数据集和测试数据集。通常，67/33的标准比率用于测试/训练分割</p><p id="2db7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">综上所述，让我们定义一个函数<strong class="in hi"> <em class="km"> handleDataset </em> </strong>，当提供了确切的文件名时，该函数将加载CSV，并使用提供的拆分比率将其随机拆分为训练和测试数据集。</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="2470" class="ls jk hh lo b fi lt lu l lv lw"><strong class="lo hi">import</strong> <!-- -->csv<br/><strong class="lo hi">import</strong> <!-- -->random<br/><strong class="lo hi">def</strong> <!-- -->handleDataset(filename, split, trainingSet<strong class="lo hi">=</strong>[] , testSet<strong class="lo hi">=</strong>[]):<br/>    with open(filename, 'r') as csvfile:<br/>         lines <strong class="lo hi">=</strong> <!-- -->csv.reader(csvfile)<br/>         dataset <strong class="lo hi">=</strong> <!-- -->list(lines)<br/>         <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(len(dataset)<strong class="lo hi">-</strong>1):<br/>             <strong class="lo hi">for</strong> <!-- -->y <strong class="lo hi">in</strong> <!-- -->range(4):<br/>                 dataset[x][y] <strong class="lo hi">=</strong> <!-- -->float(dataset[x][y])<br/>         <strong class="lo hi">if</strong> <!-- -->random.random() &lt; split:<br/>            trainingSet.append(dataset[x])<br/>         <strong class="lo hi">else</strong>:<br/>            testSet.append(dataset[x])</span></pre><p id="f46d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们检查一下上面的函数，看看它是否工作正常，</p><p id="2cb3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">测试<strong class="in hi"> handleDataset </strong>函数</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="caa7" class="ls jk hh lo b fi lt lu l lv lw">trainingSet<strong class="lo hi">=</strong>[]<br/>testSet<strong class="lo hi">=</strong>[]<br/>handleDataset(r'iris.data.', 0.66, trainingSet, testSet)<br/>print<!-- --> <!-- -->('Train: '<!-- --> <strong class="lo hi">+</strong> <!-- -->repr(len(trainingSet)))<br/>print<!-- --> <!-- -->('Test: '<!-- --> <strong class="lo hi">+</strong> <!-- -->repr(len(testSet)))</span></pre><p id="dbf9" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">步骤2: </strong>计算距离<br/>为了进行任何预测，你必须计算新点和现有点之间的距离，因为你将需要k个最近的点。</p><p id="d5ba" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在这种情况下，为了计算距离，我们将使用欧几里得距离。这被定义为两个数字数组的平方差之和的<em class="km">平方根</em></p><p id="35dd" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">具体来说，我们只需要前4个属性(特征)来计算距离，因为最后一个属性是类标签。因此，其中一种方法是将欧几里得距离限制为固定长度，从而忽略最终维数。</p><p id="5ff4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">综上所述，我们定义<strong class="in hi"> <em class="km">欧几里德距离</em> </strong>函数如下:</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="d042" class="ls jk hh lo b fi lt lu l lv lw"><strong class="lo hi">import</strong> <!-- -->math<br/><strong class="lo hi">def</strong> <!-- -->euclideanDistance(instance1, instance2, length):<br/>    distance <strong class="lo hi">=</strong> <!-- -->0<br/>    <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(length):<br/>        distance <strong class="lo hi">+=</strong> <!-- -->pow((instance1[x] <strong class="lo hi">-</strong> <!-- -->instance2[x]), 2)<br/>    <strong class="lo hi">return</strong> <!-- -->math.sqrt(distance)</span></pre><p id="8990" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">测试<strong class="in hi"> <em class="km">欧几里德距离</em> </strong>函数，</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="00f1" class="ls jk hh lo b fi lt lu l lv lw">data1 <strong class="lo hi">=</strong> <!-- -->[2, 2, 2, 'a']<br/>data2 <strong class="lo hi">=</strong> <!-- -->[4, 4, 4, 'b']<br/>distance <strong class="lo hi">=</strong> <!-- -->euclideanDistance(data1, data2, 3)<br/>print<!-- --> <!-- -->('Distance: '<!-- --> <strong class="lo hi">+</strong> <!-- -->repr(distance))</span></pre><p id="bb61" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">第三步:</strong>找到k个最近点</p><p id="f842" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">既然您已经计算了每个点的距离，我们可以使用它来收集给定测试数据/实例的k个最相似的点/实例。</p><p id="fd3b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这是一个简单的过程:计算所有实例之间的距离，并选择具有最小欧几里德距离的子集。</p><p id="c188" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们创建一个<strong class="in hi"><em class="km">getKNeighbors</em></strong>函数，它从给定测试实例的训练集中返回k个最相似的邻居</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="cfee" class="ls jk hh lo b fi lt lu l lv lw"><strong class="lo hi">import</strong> <!-- -->operator<br/><strong class="lo hi">def</strong> <!-- -->getKNeighbors(trainingSet, testInstance, k):<br/>    distances <strong class="lo hi">=</strong> <!-- -->[]<br/>    length <strong class="lo hi">=</strong> <!-- -->len(testInstance)<strong class="lo hi">-</strong>1<br/>    <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(len(trainingSet)):<br/>        dist <strong class="lo hi">=</strong> <!-- -->euclideanDistance(testInstance, trainingSet[x], length)<br/>        distances.append((trainingSet[x], dist))<br/>    distances.sort(key<strong class="lo hi">=</strong>operator.itemgetter(1))<br/>    neighbors <strong class="lo hi">=</strong> <!-- -->[]<br/>    <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(k):<br/>        neighbors.append(distances[x][0])<br/>    <strong class="lo hi">return</strong> <!-- -->neighbors</span></pre><p id="01d6" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">测试<strong class="in hi"> getKNeighbors </strong>函数</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="157f" class="ls jk hh lo b fi lt lu l lv lw">trainSet <strong class="lo hi">=</strong> <!-- -->[[2, 2, 2, 'a'], [4, 4, 4, 'b']]<br/>testInstance <strong class="lo hi">=</strong> <!-- -->[5, 5, 5]<br/>k <strong class="lo hi">=</strong> <!-- -->1<br/>neighbors <strong class="lo hi">=</strong> <!-- -->getNeighbors(trainSet, testInstance, 1)<br/>print(neighbors)</span></pre><p id="92d3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">第四步:</strong>预测班级</p><p id="811d" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在您已经有了给定测试实例的k个最近点/邻居，下一个任务是根据这些邻居预测响应</p><p id="dd5b" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">为此，您可以允许每个邻居为其类属性投票，并将多数投票作为预测。</p><p id="366f" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们创建一个<em class="km"> getResponse </em>函数来获得来自多个邻居的多数投票响应。</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="180f" class="ls jk hh lo b fi lt lu l lv lw"><strong class="lo hi">import</strong> <!-- -->operator<br/><strong class="lo hi">def</strong> <!-- -->getResponse(neighbors):<br/>    classVotes <strong class="lo hi">=</strong> <!-- -->{}<br/>    <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(len(neighbors)):<br/>        response <strong class="lo hi">=</strong> <!-- -->neighbors[x][<strong class="lo hi">-</strong>1]<br/>        <strong class="lo hi">if</strong> <!-- -->response <strong class="lo hi">in</strong> <!-- -->classVotes:<br/>            classVotes[response] <strong class="lo hi">+=</strong> <!-- -->1<br/>    <strong class="lo hi">else</strong>:<br/>            classVotes[response] <strong class="lo hi">=</strong> <!-- -->1<br/>    sortedVotes <strong class="lo hi">=</strong> <!-- -->sorted(classVotes.items(),   key<strong class="lo hi">=</strong>operator.itemgetter(1), reverse<strong class="lo hi">=</strong>True)<br/>    <strong class="lo hi">return</strong> <!-- -->sortedVotes[0][0]</span></pre><p id="2ed8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">测试getResponse 功能</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="0b6b" class="ls jk hh lo b fi lt lu l lv lw">neighbors <strong class="lo hi">=</strong> <!-- -->[[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']]<br/>print(getResponse(neighbors))</span></pre><p id="e6f2" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><strong class="in hi">第五步:</strong>检查准确度</p><p id="5e68" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">现在我们已经有了kNN算法的所有部分。让我们检查一下我们的预测有多准确！</p><p id="ca67" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">评估模型准确性的一种简单方法是计算所有预测中正确预测总数的比率。</p><p id="1fb1" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们创建一个getAccuracy函数，该函数对所有正确预测进行求和，并返回正确分类百分比的准确性。</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="614f" class="ls jk hh lo b fi lt lu l lv lw"><strong class="lo hi">def</strong> <!-- -->getAccuracy(testSet, predictions):<br/>    correct <strong class="lo hi">=</strong> <!-- -->0<br/>    <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(len(testSet)):<br/>        <strong class="lo hi">if</strong> <!-- -->testSet[x][<strong class="lo hi">-</strong>1] <strong class="lo hi">is</strong> <!-- -->predictions[x]:<br/>            correct <strong class="lo hi">+=</strong> <!-- -->1<br/>     <strong class="lo hi">return</strong> <!-- -->(correct<strong class="lo hi">/</strong>float(len(testSet))) <strong class="lo hi">*</strong> <!-- -->100.0</span></pre><p id="09ef" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">测试<strong class="in hi"> getAccuracy </strong>功能</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="082a" class="ls jk hh lo b fi lt lu l lv lw">testSet <strong class="lo hi">=</strong> <!-- -->[[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']]<br/>predictions <strong class="lo hi">=</strong> <!-- -->['a', 'a', 'a']<br/>accuracy <strong class="lo hi">=</strong> <!-- -->getAccuracy(testSet, predictions)<br/>print(accuracy)</span></pre><p id="8a2a" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">既然我们已经创建了KNN算法的所有部分，让我们使用main函数将它们连接起来。</p><pre class="ko kp kq kr fd ln lo lp lq aw lr bi"><span id="919e" class="ls jk hh lo b fi lt lu l lv lw"># Example of kNN implemented from Scratch in Python</span><span id="f94e" class="ls jk hh lo b fi lx lu l lv lw"><strong class="lo hi">import</strong> <!-- -->csv<br/><strong class="lo hi">import</strong> <!-- -->random<br/><strong class="lo hi">import</strong> <!-- -->math<br/><strong class="lo hi">import</strong> <!-- -->operator</span><span id="e67d" class="ls jk hh lo b fi lx lu l lv lw"><strong class="lo hi">def</strong> <!-- -->handleDataset(filename, split, trainingSet<strong class="lo hi">=</strong>[] , testSet<strong class="lo hi">=</strong>[]):<br/>    with open(filename, 'rb') as csvfile:<br/>         lines <strong class="lo hi">=</strong> <!-- -->csv.reader(csvfile)<br/>         dataset <strong class="lo hi">=</strong> <!-- -->list(lines)<br/>         <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(len(dataset)<strong class="lo hi">-</strong>1):<br/>             <strong class="lo hi">for</strong> <!-- -->y <strong class="lo hi">in</strong> <!-- -->range(4):<br/>                  dataset[x][y] <strong class="lo hi">=</strong> <!-- -->float(dataset[x][y])</span><span id="9a57" class="ls jk hh lo b fi lx lu l lv lw"><strong class="lo hi">if</strong> <!-- -->random.random() &lt; split: trainingSet.append(dataset[x]) <strong class="lo hi">else</strong>: testSet.append(dataset[x]) <strong class="lo hi">def</strong> <!-- -->euclideanDistance(instance1, instance2, length): distance <strong class="lo hi">=</strong> <!-- -->0<!-- --> <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(length): distance <strong class="lo hi">+=</strong> <!-- -->pow((instance1[x] <strong class="lo hi">-</strong> <!-- -->instance2[x]), 2) <strong class="lo hi">return</strong> <!-- -->math.sqrt(distance) <strong class="lo hi">def</strong> <!-- -->getNeighbors(trainingSet, testInstance, k): distances <strong class="lo hi">=</strong> <!-- -->[] length <strong class="lo hi">=</strong> <!-- -->len(testInstance)<strong class="lo hi">-</strong>1<!-- --> <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(len(trainingSet)): dist <strong class="lo hi">=</strong> <!-- -->euclideanDistance(testInstance, trainingSet[x], length) distances.append((trainingSet[x], dist)) distances.sort(key<strong class="lo hi">=</strong>operator.itemgetter(1)) neighbors <strong class="lo hi">=</strong> <!-- -->[] <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(k): neighbors.append(distances[x][0]) <strong class="lo hi">return</strong> <!-- -->neighbors <strong class="lo hi">def</strong> <!-- -->getResponse(neighbors): classVotes <strong class="lo hi">=</strong> <!-- -->{} <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(len(neighbors)): response <strong class="lo hi">=</strong> <!-- -->neighbors[x][<strong class="lo hi">-</strong>1] <strong class="lo hi">if</strong> <!-- -->response <strong class="lo hi">in</strong> <!-- -->classVotes: classVotes[response] <strong class="lo hi">+=</strong> <!-- -->1<!-- --> <strong class="lo hi">else</strong>: classVotes[response] <strong class="lo hi">=</strong> <!-- -->1<!-- --> <!-- -->sortedVotes <strong class="lo hi">=</strong> <!-- -->sorted(classVotes.iteritems(), key<strong class="lo hi">=</strong>operator.itemgetter(1), reverse<strong class="lo hi">=</strong>True) <strong class="lo hi">return</strong> <!-- -->sortedVotes[0][0] <strong class="lo hi">def</strong> <!-- -->getAccuracy(testSet, predictions): correct <strong class="lo hi">=</strong> <!-- -->0<!-- --> <strong class="lo hi">for</strong> <!-- -->x <strong class="lo hi">in</strong> <!-- -->range(len(testSet)): <strong class="lo hi">if</strong> <!-- -->testSet[x][<strong class="lo hi">-</strong>1] <strong class="lo hi">==</strong> <!-- -->predictions[x]: correct <strong class="lo hi">+=</strong> <!-- -->1<!-- --> <strong class="lo hi">return</strong> <!-- -->(correct<strong class="lo hi">/</strong>float(len(testSet))) <strong class="lo hi">*</strong> <!-- -->100.0<!-- --> <strong class="lo hi">def</strong> <!-- -->main():</span><span id="ae98" class="ls jk hh lo b fi lx lu l lv lw"># prepare data trainingSet=[] testSet=[] split = 0.67 loadDataset('iris.data', split, trainingSet, testSet) print 'Train set: ' + repr(len(trainingSet)) print 'Test set: ' + repr(len(testSet))</span><span id="8ece" class="ls jk hh lo b fi lx lu l lv lw"># generate predictions predictions=[] k = 3 for x in range(len(testSet)): neighbors = getNeighbors(trainingSet, testSet[x], k) result = getResponse(neighbors) predictions.append(result) print('&gt; predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))</span><span id="5686" class="ls jk hh lo b fi lx lu l lv lw">accuracy <strong class="lo hi">=</strong> <!-- -->getAccuracy(testSet, predictions)<br/>print('Accuracy: '<!-- --> <strong class="lo hi">+</strong> <!-- -->repr(accuracy) <strong class="lo hi">+</strong> <!-- -->'%')<br/>main()</span></pre><p id="a26c" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这都是关于使用python的kNN算法。</p><p id="8d90" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">就这样，我们到了这篇文章的结尾。如果你对这个话题有任何疑问，请在下面留下评论，我们会尽快回复你。如果你想查看更多关于Python、DevOps、Ethical Hacking等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="e5d3" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">请留意本系列中的其他文章，它们将解释数据科学的各个方面。</p><blockquote class="ly lz ma"><p id="17b8" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 1。</em> <a class="ae lm" rel="noopener" href="/edureka/data-science-tutorial-484da1ff952b"> <em class="hh">数据科学教程</em> </a></p><p id="3d3d" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 2。</em> <a class="ae lm" rel="noopener" href="/edureka/math-and-statistics-for-data-science-1152e30cee73"> <em class="hh">数据科学的数学与统计</em> </a></p><p id="4ca8" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 3。</em><a class="ae lm" rel="noopener" href="/edureka/linear-regression-in-r-da3e42f16dd3"><em class="hh">R中的线性回归</em> </a></p><p id="d6e9" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 4。</em> <a class="ae lm" rel="noopener" href="/edureka/data-science-tutorial-484da1ff952b"> <em class="hh">数据科学教程</em> </a></p><p id="d6ca" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 5。</em><a class="ae lm" rel="noopener" href="/edureka/logistic-regression-in-r-2d08ac51cd4f"><em class="hh">R中的逻辑回归</em> </a></p><p id="a1f8" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 6。</em> <a class="ae lm" rel="noopener" href="/edureka/classification-algorithms-ba27044f28f1"> <em class="hh">分类算法</em> </a></p><p id="d387" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 7。</em> <a class="ae lm" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="991a" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 8。</em> <a class="ae lm" rel="noopener" href="/edureka/a-complete-guide-on-decision-tree-algorithm-3245e269ece"> <em class="hh">决策树中的R </em> </a></p><p id="7b60" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 9。</em> <a class="ae lm" rel="noopener" href="/edureka/introduction-to-machine-learning-97973c43e776"> <em class="hh">机器学习入门</em> </a></p><p id="f035" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 10。</em> <a class="ae lm" rel="noopener" href="/edureka/naive-bayes-in-r-37ca73f3e85c"> <em class="hh">朴素贝叶斯在R </em> </a></p><p id="91a0" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 11。</em> <a class="ae lm" rel="noopener" href="/edureka/statistics-and-probability-cf736d703703"> <em class="hh">统计与概率</em> </a></p><p id="1fba" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated">12。 <a class="ae lm" rel="noopener" href="/edureka/decision-trees-b00348e0ac89"> <em class="hh">如何创建一个完美的决策树？</em>T11】</a></p><p id="516d" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 13。</em> <a class="ae lm" rel="noopener" href="/edureka/data-scientists-myths-14acade1f6f7"> <em class="hh">关于数据科学家角色的十大误区</em> </a></p><p id="a47b" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 14。</em> <a class="ae lm" rel="noopener" href="/edureka/machine-learning-algorithms-29eea8b69a54"> <em class="hh">排名前5的机器学习算法</em> </a></p><p id="a359" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated">15。 <a class="ae lm" rel="noopener" href="/edureka/data-analyst-vs-data-engineer-vs-data-scientist-27aacdcaffa5"> <em class="hh">数据分析师vs数据工程师vs数据科学家</em> </a></p><p id="2206" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated">16。 <a class="ae lm" rel="noopener" href="/edureka/types-of-artificial-intelligence-4c40a35f784"> <em class="hh">人工智能的种类</em> </a></p><p id="0b21" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated">17。<a class="ae lm" rel="noopener" href="/edureka/r-vs-python-48eb86b7b40f"><em class="hh">R vs Python</em></a></p><p id="28fe" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 18。</em> <a class="ae lm" rel="noopener" href="/edureka/ai-vs-machine-learning-vs-deep-learning-1725e8b30b2e"> <em class="hh">人工智能vs机器学习vs深度学习</em> </a></p><p id="6fd6" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 19。</em> <a class="ae lm" rel="noopener" href="/edureka/machine-learning-projects-cb0130d0606f"> <em class="hh">机器学习项目</em> </a></p><p id="f91c" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated">20。 <a class="ae lm" rel="noopener" href="/edureka/data-analyst-interview-questions-867756f37e3d"> <em class="hh">数据分析师面试问答</em> </a></p><p id="8c2c" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 21。</em> <a class="ae lm" rel="noopener" href="/edureka/data-science-and-machine-learning-for-non-programmers-c9366f4ac3fb"> <em class="hh">面向非程序员的数据科学和机器学习工具</em> </a></p><p id="1d0f" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 22。</em> <a class="ae lm" rel="noopener" href="/edureka/top-10-machine-learning-frameworks-72459e902ebb"> <em class="hh">十大机器学习框架</em> </a></p><p id="9802" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 23。</em> <a class="ae lm" rel="noopener" href="/edureka/statistics-for-machine-learning-c8bc158bb3c8"> <em class="hh">用于机器学习的统计</em> </a></p><p id="46a2" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 24。</em> <a class="ae lm" rel="noopener" href="/edureka/random-forest-classifier-92123fd2b5f9"> <em class="hh">随机森林中的R </em> </a></p><p id="fb80" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 25。</em> <a class="ae lm" rel="noopener" href="/edureka/breadth-first-search-algorithm-17d2c72f0eaa"> <em class="hh">广度优先搜索算法</em> </a></p><p id="44f8" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 26。</em><a class="ae lm" rel="noopener" href="/edureka/linear-discriminant-analysis-88fa8ad59d0f"><em class="hh">R中的线性判别分析</em> </a></p><p id="85df" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 27。</em> <a class="ae lm" rel="noopener" href="/edureka/prerequisites-for-machine-learning-68430f467427"> <em class="hh">机器学习的先决条件</em> </a></p><p id="b306" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 28。</em> <a class="ae lm" rel="noopener" href="/edureka/r-shiny-tutorial-47b050927bd2"> <em class="hh">互动WebApps使用R闪亮</em> </a></p><p id="415d" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated">29。 <a class="ae lm" rel="noopener" href="/edureka/top-10-machine-learning-books-541f011d824e"> <em class="hh">机器学习十大书籍</em> </a></p><p id="4222" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh">三十。</em> <a class="ae lm" rel="noopener" href="/edureka/unsupervised-learning-40a82b0bac64"> <em class="hh">无监督学习</em> </a></p><p id="591d" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated"><em class="hh"> 31.1 </em> <a class="ae lm" rel="noopener" href="/edureka/10-best-books-data-science-9161f8e82aca"> <em class="hh"> 0最佳数据科学书籍</em> </a></p><p id="dd86" class="il im km in b io ip iq ir is it iu iv mb ix iy iz mc jb jc jd md jf jg jh ji ha bi translated">32。 <a class="ae lm" rel="noopener" href="/edureka/supervised-learning-5a72987484d0"> <em class="hh">监督学习</em> </a></p></blockquote></div><div class="ab cl me mf go mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ha hb hc hd he"><p id="68e8" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="km">原载于2018年7月26日</em><a class="ae lm" href="https://www.edureka.co/blog/k-nearest-neighbors-algorithm/" rel="noopener ugc nofollow" target="_blank"><em class="km">https://www.edureka.co</em></a><em class="km">。</em></p></div></div>    
</body>
</html>