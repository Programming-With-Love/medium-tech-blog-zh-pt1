<html>
<head>
<title>Pitfalls of Incorrectly Tuned XGBoost Hyperparameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">错误调整的XGBoost超参数的陷阱</h1>
<blockquote>原文：<a href="https://medium.com/capital-one-tech/pitfalls-of-incorrectly-tuned-xgboost-hyperparameters-d06e4e42a38f?source=collection_archive---------0-----------------------#2022-04-11">https://medium.com/capital-one-tech/pitfalls-of-incorrectly-tuned-xgboost-hyperparameters-d06e4e42a38f?source=collection_archive---------0-----------------------#2022-04-11</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="69af" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">不正确调整的基础分数如何破坏XGBoost版本中模型预测的数值稳定性</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/9ea7cdfca6ed003c38db47e61df4b3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Pm8asv_8FF5H0smH"/></div></div></figure><p id="3f79" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">机器学习从业者早就听说过这句格言<em class="ke">“垃圾进，垃圾出。”</em>它强调了将任何建模工作建立在一个被充分理解并适合手头问题的数据集上的重要性。建模的一个同样重要的相关方面是需要理解建模算法的<em class="ke">超参数</em>的细微差别。</p><p id="a67b" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">一般来说，<a class="ae kf" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">超参数是模型参数，其值不能从训练数据</a>中直接估计。它们控制模型训练过程的不同方面，并且需要预先指定。例如，<a class="ae kf" href="https://towardsdatascience.com/3-decision-tree-based-algorithms-for-machine-learning-75528a0f03d1?gi=a2dc9ae2535" rel="noopener" target="_blank">基于树的算法</a>通常有一个超参数来控制需要在模型训练开始之前指定的树的深度。在实践中，超参数通常基于以前的经验、成熟的启发式方法或技术(如<a class="ae kf" href="https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e" rel="noopener" target="_blank">网格搜索</a>)进行调整。</p><p id="ab1d" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在Capital One，超参数调整是整个模型构建过程中不可或缺的一步。一个执行良好的调优策略会带来一些好处，比如更好的预测性能和更快的训练时间，这反过来又有助于我们从数据中获取最大的价值。然而，尽管付出了巨大的努力，我们还是会在调优过程中遇到一些边缘情况，这些情况最终会以微妙和意想不到的方式影响建模结果。</p><p id="f878" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在这篇文章中，我们将深入研究<a class="ae kf" href="https://xgboost.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>，这是一个流行的开源建模包，它实现了<a class="ae kf" href="https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab" rel="noopener" target="_blank">梯度增强机器(GBM)算法</a>，并专注于一个特定的超参数<code class="du kg kh ki kj b">base_score</code>。具体来说，我们将展示一个场景，在这个场景中，一个用错误指定的基础分数训练的模型最终在XGBoost包的两个相邻版本中产生明显不同的预测。</p><p id="e77c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在这篇文章的结尾，你将会学到:</p><ul class=""><li id="34aa" class="kk kl hh jk b jl jm jo jp jr km jv kn jz ko kd kp kq kr ks bi translated">调整模型超参数时理解算法和计算含义的重要性。</li><li id="8b87" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated">基本分数在XGBoost模型预测过程中的作用。</li><li id="53f2" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated">为什么选择一个与目标变量的规模相称的基础分数是个好主意。</li><li id="a56c" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated">为什么选择一个范围非常不同的值可能会导致XGBoost版本之间细微的、意想不到的数字差异。</li></ul><h1 id="923e" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">XGBoost预测循环和基本分数的作用</h1><p id="f1d9" class="pw-post-body-paragraph ji jj hh jk b jl lq ii jn jo lr il jq jr ls jt ju jv lt jx jy jz lu kb kc kd ha bi translated">当谈到模型训练过程时，XGBoost附带了<a class="ae kf" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">各种各样的超参数</a>，人们可以通过调整来影响训练结果。一个这样的超参数是<code class="du kg kh ki kj b">base_score</code>,它允许用户向估计者提供有效的默认猜测。</p><p id="d1c3" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">一旦模型经过训练并准备好生成预测，该算法将首先对各个树的预测残差进行求和，然后在最后添加基本分数，以得出最终的预测值。在伪代码中，这个过程可以用一个简单的for循环来表示，如下面针对单个观察的说明。</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="d7e2" class="lz kz hh kj b fi ma mb l mc md">predicted_value = 0</span><span id="e0df" class="lz kz hh kj b fi me mb l mc md">for tree in trees:<br/>    # Predicted residual is found by first locating the leaf to<br/>    # which the observation of interest belongs, and then looking <br/>    # up the numerical value for said leaf<br/>    predicted_residual = …</span><span id="1288" class="lz kz hh kj b fi me mb l mc md">    predicted_value += predicted_residual</span><span id="78cf" class="lz kz hh kj b fi me mb l mc md"># Add base score at the end to get final prediction<br/>predicted_value += base_score</span></pre><p id="fa3f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">或者，我们可以设置运行总和等于基本分数开始，在这种情况下，我们不再需要在最后添加它。</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="c664" class="lz kz hh kj b fi ma mb l mc md"># Alternatively, set running sum equal to base score at the start<br/>predicted_value = base_score</span><span id="efd1" class="lz kz hh kj b fi me mb l mc md">for tree in trees:<br/>    predicted_residual = …<br/>    predicted_value += predicted_residual</span></pre><p id="6f78" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在XGBoost 1 . 3 . 0版之前，预测循环中的逻辑是运行总和从零开始。然而，在版本1.3.0中，<a class="ae kf" href="https://github.com/dmlc/xgboost/issues/6350#issuecomment-726053585" rel="noopener ugc nofollow" target="_blank">行为被修改</a>，使得运行总和在for循环开始时被有效地设置为等于基本分数。从数学上来说，这两种求和的方法是等价的。然而，从计算角度来说，情况并不总是如此。虽然这种细微差别预计不会在绝大多数用例中产生有意义的模型预测差异，但它的影响可能会被放大，并根据建模设置的具体情况被证明是有问题的。</p><p id="b614" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">为了强调这最后一点，我们现在将呈现一个场景，在该场景中，基本分数的值及其在预测循环中添加到运行总和的顺序最终会对XGBoost的两个相邻版本的模型预测的数值稳定性产生重大影响。</p><h1 id="cb32" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">基础分数对模型预测数值稳定性的影响</h1><p id="46a4" class="pw-post-body-paragraph ji jj hh jk b jl lq ii jn jo lr il jq jr ls jt ju jv lt jx jy jz lu kb kc kd ha bi translated">概括地说，我们将这样做:</p><ul class=""><li id="19a8" class="kk kl hh jk b jl jm jo jp jr km jv kn jz ko kd kp kq kr ks bi translated">使用特定的基本分数在XGBoost 1 . 2 . 0版中训练模型，保存模型对象，并根据训练数据生成预测。</li><li id="f29a" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated">在XGBoost版本1.3.0中加载<em class="ke">相同的</em>模型对象，在相同的训练数据上生成预测。</li><li id="bb9d" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated">检查两组模型预测是否相同。</li></ul><p id="e0af" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">为了设置这个实验，我们需要创建两个虚拟环境，在一个环境中安装XGBoost 1.2.0，在另一个环境中安装1.3.0。实现这一点的一个方法是通过conda环境/包管理器。详情请参考康达<a class="ae kf" href="https://conda.io/projects/conda/en/latest/user-guide/getting-started.html" rel="noopener ugc nofollow" target="_blank">官方文档</a>。</p><p id="a046" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">训练数据完全是合成的。具体来说，数据包括:</p><ul class=""><li id="b0fc" class="kk kl hh jk b jl jm jo jp jr km jv kn jz ko kd kp kq kr ks bi translated">1000次观察。</li><li id="1ac6" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated">一个目标变量加上五个特征，这六个特征都是在0到1之间随机生成的数字。</li></ul><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="d5a9" class="lz kz hh kj b fi ma mb l mc md">import numpy as np</span><span id="4013" class="lz kz hh kj b fi me mb l mc md"># Set random seed for reproducibility<br/>np.random.seed(2021)</span><span id="7b39" class="lz kz hh kj b fi me mb l mc md">data = np.random.rand(1000, 6)</span><span id="2c9f" class="lz kz hh kj b fi me mb l mc md"># Save to disk for later use<br/>np.savetxt('data.csv', data)</span></pre><p id="8b51" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们将在XGBoost 1.2.0中训练一个模型，并将拟合的模型与训练数据上的模型预测一起保存，供以后使用。</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="55c3" class="lz kz hh kj b fi ma mb l mc md"># Execute this snippet in xgboost 1.2.0 environment<br/>import xgboost as xgb</span><span id="9de4" class="lz kz hh kj b fi me mb l mc md">y, x = data[:, 0], data[:, 1:]</span><span id="bba1" class="lz kz hh kj b fi me mb l mc md"># Set hyperparameters for model training<br/>hyperparams = {<br/>    'base_score': 1000,<br/>    'n_estimators': 500,<br/>    'max_depth': 5,<br/>    'learning_rate': .05<br/>}</span><span id="911d" class="lz kz hh kj b fi me mb l mc md"># Train model and save<br/>model = xgb.XGBRegressor(**hyperparams)<br/>model.fit(x, y)<br/>model.save_model('model_v120.bin')</span><span id="dda7" class="lz kz hh kj b fi me mb l mc md"># Generate predictions on training data and save<br/>predicted_v120 = model.predict(x)<br/>np.savetxt('predicted_v120.csv', predicted_v120)</span></pre><p id="83e0" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">选择使用<code class="du kg kh ki kj b">XGBRegressor</code>作为特定模型在这里并不特别重要。然而，<em class="ke">和</em>重要的是，我们已经将基础分数设置得比目标变量大得多(再次回忆一下，目标变量在0和1之间)。正如我们将在后面的章节中讨论的那样，这将以一种微妙且有些出乎意料的方式影响模型预测的数值稳定性。</p><p id="4078" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">接下来，我们将把拟合的模型对象加载到XGBoost 1.3.0中，并再次对训练数据生成预测。</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="c00a" class="lz kz hh kj b fi ma mb l mc md"># Execute this snippet in xgboost 1.3.0 environment<br/>data = np.loadtxt('data.csv')<br/>x = data[:, 1:]</span><span id="51ad" class="lz kz hh kj b fi me mb l mc md"># Load previously trained model<br/>model = xgb.XGBRegressor()<br/>model.load_model('model_v120.bin')</span><span id="5a2b" class="lz kz hh kj b fi me mb l mc md">predicted_v130 = model.predict(x)<br/>predicted_v120 = np.loadtxt('predicted_v120.csv')</span></pre><p id="92be" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们现在有两组模型预测，它们都是由相同的基础模型对象根据相同的训练数据生成的。唯一的区别是使用的XGBoost版本——1 . 2 . 0和1.3.0。</p><p id="4224" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">凭直觉，我们会认为这两组预测几乎完全相同。为了验证事实是否如此，我们绘制了一个直方图，显示了两组之间的绝对百分比差异。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mf"><img src="../Images/80f32f255ca86a8c763be8ec62b8ff78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ftS0c10EYYHGZ7XP"/></div></div></figure><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="f605" class="lz kz hh kj b fi ma mb l mc md">Max absolute percent difference: 0.82%<br/>1.2.0: 0.05175781<br/>1.3.0: 0.05133431</span></pre><p id="183f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">如上所述，两组预测之间的差值可高达0.82%。虽然我们不一定期望预测与<em class="ke">完全相同(可能是由于浮点舍入误差)，但是观察到的差异比预期的要大得多；没有进一步的调查，我们不能简单地把这种数量级的变化忽略不计。特别是，如果模型的使用与治理过程联系在一起，这在大型企业中是常见的，这种差异肯定会引起风险管理部门的注意。</em></p><p id="d0a2" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">让我们用不同的基本分数重新训练模型，看看这是否会以任何方式影响delta。事实上，将基本分数修改为0.5(XGBoost中的默认值)会导致两个XGBoost版本中的模型预测几乎相同。</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="e408" class="lz kz hh kj b fi ma mb l mc md">Max percent difference: 0.00%<br/>1.2.0: 0.00867423<br/>1.3.0: 0.00867402</span></pre><p id="f6fb" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们将尝试其他一些基本分数，并评估它们对最大差异的影响。如下表所示，基本分数和最大差值之间似乎呈正相关。以极值25000为例。如果我们用这个基本分数训练一个模型，并比较两个XGBoost版本的模型预测，它们可能会相差30%。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mg"><img src="../Images/cf85ba1a8f9360c3a661d4eda105803b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NciG_-IMIPb-gN9N-WmMkw.png"/></div></div></figure><p id="3b3f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">那么结论是，一旦我们升级到软件包的新版本，XGBoost给出的模型预测就不可靠了吗？不会。我们很快就会看到，我们的特定建模设置会放大异常大的基础分数的影响。</p><p id="94d2" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">事实上，在我们最初的例子中，我们有目的地选择基础分数比目标变量的规模大几个数量级。在实践中，这可能是由于将超参数传递给XGBoost API时出现了输入错误，或者是由于数据科学家在重新调整目标变量时忘记相应地调整基本分数而导致的疏忽。</p><p id="8ed1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">不管它是如何出现的，要带走的关键信息是超参数可以以微妙和意想不到的方式影响建模结果，并且当修改它们的值时，理解算法和(我们将很快看到)计算的含义对我们来说很重要。</p><p id="7a20" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">对于手头的问题，我们需要研究为什么较大的基本分值会导致XGBoost版本中的模型预测在数值上不稳定。由于XGBoost中的<a class="ae kf" href="https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html#a-note-on-backward-compatibility-of-models-and-memory-snapshots" rel="noopener ugc nofollow" target="_blank">保证了向后兼容性</a>，数据科学家在一个版本的XGBoost中训练一个模型，保存该模型，然后在新版本的XGBoost中重用相同的模型，以便利用新版本中引入的额外功能和性能增强，这种情况并不罕见。如果这些改进是以数值不稳定和模型预测中无法解释的变化为代价的，那将是不幸的。</p><h1 id="a4b6" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">错误指定的基本分数如何破坏数值稳定性</h1><p id="d6fe" class="pw-post-body-paragraph ji jj hh jk b jl lq ii jn jo lr il jq jr ls jt ju jv lt jx jy jz lu kb kc kd ha bi translated">如前所述，在我们的原始示例中观察到的数字差异是我们选择基本分数1000的直接结果，该分数比目标变量的范围(介于0和1之间)大几个数量级。</p><p id="fead" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">为了理解这个问题是如何表现出来的，我们需要重温一下本文前面介绍的预测循环的伪代码。具体来说，我们关注XGBoost 1.3.0中引入的修改后的逻辑，在for循环开始时，运行总和被设置为等于基本分数。</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="ef23" class="lz kz hh kj b fi ma mb l mc md"># Start running sum at base score. New behavior introduced in 1.3.0<br/>predicted_value = base_score</span><span id="097a" class="lz kz hh kj b fi me mb l mc md">for tree in trees:<br/>    predicted_residual = …<br/>    predicted_value += predicted_residual</span></pre><p id="882c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">假设第一棵树的预测残差是-50.0341012。在循环的第一次迭代之后，我们期望<code class="du kg kh ki kj b">predicted_value</code>等于949.9658988。</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="7ed5" class="lz kz hh kj b fi ma mb l mc md">1000.0–50.0341012 = 949.9658988</span></pre><p id="881a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在引擎盖下，一台计算机使用<a class="ae kf" href="https://en.wikipedia.org/wiki/Floating-point_arithmetic" rel="noopener ugc nofollow" target="_blank">浮点运算</a>进行计算，类似于科学记数法的工作方式。虽然它不像上面的等式那么简单，但直觉上我们会认为它会产生相同的答案，如下图所示。(计算机以二为基数工作，而不是以十为基数，如图所示。然而，同样的原则也适用。)</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="6549" class="lz kz hh kj b fi ma mb l mc md">1000.0<br/> -50.0341012<br/>    ↓<br/>   1.0          * 10³ Convert to scientific notation<br/>  -5.00341012   * 10¹<br/>    ↓<br/>   1.0          * 10³<br/>  -0.0500341012 * 10³ Ensure the same exponent<br/>    ↓<br/>   0.9499658988 * 10³<br/>    ↓<br/> 949.9658988</span></pre><p id="367b" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">然而，现实中最终发生的事情略有不同。</p><p id="1faa" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">计算机能用来存储浮点数的位数是有限的。如果我们有一个小数点后有大量尾随数字的数字，可能没有足够的空间来存储它们。在某一点之后，一些数字将不得不被丢弃。</p><p id="3e64" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">让我们假设我们正在使用32位浮点数，它的精度限制大约是7位十进制数字。因为我们从比第一棵树的预测残差大得多的基础分数开始，这实际上导致较小的数字向右“移位”，导致最后三位数字在此过程中被丢弃。</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="5d97" class="lz kz hh kj b fi ma mb l mc md">  0.9499658988 * 10³<br/>   ↓<br/>  0.9499659xxx * 10³ Last 3 digits discarded<br/>   ↓<br/>949.9659             Less precision as a result</span></pre><p id="f5f5" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">相比之下，如果我们从零开始运行总和(XGBoost 1.2.0中的行为), for循环中的每个后续加法/减法将以更精确的方式执行，利用所有可用的位，而不会过早地丢弃尾部数字。</p><p id="e62f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">你可能想知道，<em class="ke">“如果我在这里或那里留下一些无关紧要的数字，会有什么不同？”你是对的——这通常没什么关系。然而，当目标变量很小时，每一点精度都很重要。如果我们执行大量的运算，每一次都牺牲了一点点精度，这些小误差就会叠加，并可能产生一个最终结果，这个结果与我们从零开始计算累计和的结果有很大的不同。这种现象是浮点计算的<a class="ae kf" href="https://en.wikipedia.org/wiki/Associative_property#Nonassociativity_of_floating_point_calculation" rel="noopener ugc nofollow" target="_blank">非关联性的直接结果——由于数学构造与其具体计算实现之间的偏差，这种属性经常让人们感到惊讶。</a></em></p><h1 id="3be5" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">如何补救由错误指定的基本分数导致的数值不稳定性</h1><p id="12f6" class="pw-post-body-paragraph ji jj hh jk b jl lq ii jn jo lr il jq jr ls jt ju jv lt jx jy jz lu kb kc kd ha bi translated">根据您的模型的具体用例，我们到目前为止讨论的数字差异可能并不重要。然而，如果数值稳定性和精度<em class="ke">对您来说很重要的话，有一些选项可以减少前面描述的浮点运算的细微差别所导致的数值差异。</em></p><p id="e754" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">最稳健的补救措施是使用适当选择的基础分数重新训练模型，该分数与目标变量的规模相当。</p><p id="70fe" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在重新训练模型不可行的情况下，我们可以通过复制XGBoost早期版本中的预测逻辑来消除数值差异。</p><pre class="ix iy iz ja fd lv kj lw lx aw ly bi"><span id="3b47" class="lz kz hh kj b fi ma mb l mc md">def v120_predict_replica(model, x):<br/>    """Replica of the prediction logic in xgboost version 1.2.0.</span><span id="1c1d" class="lz kz hh kj b fi me mb l mc md">    Specifically, base score will be added _at the end_ of the <br/>    prediction loop, rather than at the start. This is achieved via  <br/>    setting `base_margin` to be zero, which forces the base score to <br/>    be ignored.<br/><a class="ae kf" href="https://xgboost.readthedocs.io/en/latest/prediction.html#base-margin" rel="noopener ugc nofollow" target="_blank">https://xgboost.readthedocs.io/en/latest/prediction.html#base-<br/>margin</a><br/>    """</span><span id="0653" class="lz kz hh kj b fi me mb l mc md">    float32 = np.float32 # xgboost uses 32-bit floats<br/>    base_margin = np.zeros(len(x), dtype=float32)<br/>    dmatrix = xgb.DMatrix(x, base_margin=base_margin)<br/>    base_score = model.get_xgb_params()[‘base_score’]<br/>    booster = model.get_booster()<br/>    predicted_value = booster.predict(dmatrix) + float32(base_score)<br/>    return predicted_value</span></pre><p id="c22f" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">也就是说，这不应该被视为一个永久的解决方案——它只是一个权宜之计，为我们争取更多的时间，让我们为重新训练模型的更强大的解决方案制定计划。</p><h1 id="8680" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">结论</h1><p id="0872" class="pw-post-body-paragraph ji jj hh jk b jl lq ii jn jo lr il jq jr ls jt ju jv lt jx jy jz lu kb kc kd ha bi translated">在这篇文章中，我们研究了一个场景，在这个场景中，一个用错误指定的基础分数训练的XGBoost模型最终在两个相邻的XGBoost版本之间产生数值不稳定的预测。尽管我们只关注单个超参数，但关键信息是通用的。</p><p id="bde8" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">将一个不合适的数据集输入到一个算法中可能会导致一个几乎无用的模型。类似地，试图在没有充分考虑算法和计算含义的情况下调整模型的超参数，可能最终导致细微的、意想不到的、难以调试的建模结果。</p><p id="d63e" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">如果您想了解XGBoost中可用的超参数的更多信息，那么<a class="ae kf" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">官方文档</a>和之前的Capital One博客文章— <a class="ae kf" href="https://www.capitalone.com/tech/machine-learning/how-to-control-your-xgboost-model/" rel="noopener ugc nofollow" target="_blank">“如何控制您的XGBoost模型”</a> —是帮助您入门的好资源。</p><h1 id="8403" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">承认</h1><p id="f768" class="pw-post-body-paragraph ji jj hh jk b jl lq ii jn jo lr il jq jr ls jt ju jv lt jx jy jz lu kb kc kd ha bi translated">这篇文章是我在Capital One的同事们之前工作和贡献的结晶(按字母顺序排列):</p><ul class=""><li id="4935" class="kk kl hh jk b jl jm jo jp jr km jv kn jz ko kd kp kq kr ks bi translated"><a class="ae kf" href="https://www.linkedin.com/in/aron-ahmadia/" rel="noopener ugc nofollow" target="_blank">阿隆·艾哈迈迪</a></li><li id="7cab" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://www.linkedin.com/in/hanying-chen-22794181/" rel="noopener ugc nofollow" target="_blank">陈</a></li><li id="8d41" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://www.linkedin.com/in/jeff4rest/" rel="noopener ugc nofollow" target="_blank">杰夫·福里斯特</a></li><li id="6bbd" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated">Khatereh Mohajery </li><li id="d53b" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://www.linkedin.com/in/spencer-bailey-b4b683a0/" rel="noopener ugc nofollow" target="_blank">斯潘塞·贝利</a></li><li id="5739" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://www.linkedin.com/in/sunandkrishnan/" rel="noopener ugc nofollow" target="_blank">苏南和克里希南</a></li><li id="937f" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://www.linkedin.com/in/ting-shao-cfa-719aab79/" rel="noopener ugc nofollow" target="_blank">邵婷</a></li></ul><h1 id="d764" class="ky kz hh bd la lb lc ld le lf lg lh li in lj io lk iq ll ir lm it ln iu lo lp bi translated">参考资料和资源</h1><ul class=""><li id="688d" class="kk kl hh jk b jl lq jo lr jr mh jv mi jz mj kd kp kq kr ks bi translated">https://xgboost.readthedocs.io/en/latest/parameter.html<a class="ae kf" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="f854" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html#a-note-on-backward-compatibility-of-models-and-memory-snapshots" rel="noopener ugc nofollow" target="_blank">https://xgboost . readthedocs . io/en/latest/tutorials/saving _ model . html # a-note-on-backward-compatibility-of-models-and-memory-snapshots</a></li><li id="519e" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://github.com/dmlc/xgboost/issues/6350#issuecomment-726053585" rel="noopener ugc nofollow" target="_blank">https://github.com/dmlc/xgboost/issues/6350</a></li><li id="bb5a" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://github.com/dmlc/xgboost/issues/799" rel="noopener ugc nofollow" target="_blank">https://github.com/dmlc/xgboost/issues/799</a></li><li id="5933" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://www.capitalone.com/tech/machine-learning/how-to-control-your-xgboost-model/" rel="noopener ugc nofollow" target="_blank">https://www . capital one . com/tech/machine-learning/how-to-control-your-xgboost-model/</a></li><li id="6e2b" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/difference-a-parameter-and-a-hyperparameter/</a></li><li id="1d2a" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Hyperparameter _(machine _ learning)</a></li><li id="f7ce" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Floating-point_arithmetic" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Floating-point_arithmetic</a></li><li id="f427" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://en.wikipedia.org/wiki/Associative_property#Nonassociativity_of_floating_point_calculation" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Associative _ property # non Associative _ of _ floating _ point _ calculation</a></li><li id="fcd3" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://floating-point-gui.de/" rel="noopener ugc nofollow" target="_blank">https://floating-point-gui.de/</a></li><li id="72a5" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://ciechanow.ski/exposing-floating-point/" rel="noopener ugc nofollow" target="_blank">https://ciechanow.ski/exposing-floating-point/</a></li><li id="bd0c" class="kk kl hh jk b jl kt jo ku jr kv jv kw jz kx kd kp kq kr ks bi translated"><a class="ae kf" href="https://conda.io/projects/conda/en/latest/user-guide/getting-started.html" rel="noopener ugc nofollow" target="_blank">https://conda . io/projects/conda/en/latest/user-guide/getting-started . html</a></li></ul></div><div class="ab cl mk ml go mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ha hb hc hd he"><p id="cadf" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><em class="ke">披露声明:2022资本一。观点是作者个人的观点。除非本帖中另有说明，否则Capital One不隶属于所提及的任何公司，也不被这些公司认可。使用或展示的所有商标和其他知识产权是其各自所有者的财产。</em></p></div></div>    
</body>
</html>