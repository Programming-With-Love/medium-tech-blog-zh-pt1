<html>
<head>
<title>MTet: Multi-domain Translation for English and Vietnamese</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MTet:英语和越南语的多领域翻译</h1>
<blockquote>原文：<a href="https://medium.com/google-developer-experts/mtet-multi-domain-translation-for-english-and-vietnamese-2091116f0771?source=collection_archive---------6-----------------------#2022-03-17">https://medium.com/google-developer-experts/mtet-multi-domain-translation-for-english-and-vietnamese-2091116f0771?source=collection_archive---------6-----------------------#2022-03-17</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="01ab" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">收集高质量的数据，并为越南语训练最先进的神经机器翻译模型。</h2></div><h1 id="1185" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">介绍</h1><p id="2b2f" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">机器翻译(MT)，即由计算机自动将内容从一种语言映射到另一种语言的任务，无疑是自然语言处理(NLP)最重要的应用之一。尽管使用人工智能，特别是深度学习和神经网络的机器翻译取得了很大进展，但英语-越南语翻译质量仍然落后，主要原因是缺乏大规模的高质量数据集。最近，<a class="ae kk" href="https://vietai.org/" rel="noopener ugc nofollow" target="_blank"> VietAI </a>已经<a class="ae kk" href="https://blog.vietai.org/sat/" rel="noopener ugc nofollow" target="_blank">向研究社区发布了</a>一个包含330万个例子的高质量英语-越南语翻译语料库，以促进研究的进一步发展。</p><p id="6f08" class="pw-post-body-paragraph jo jp hh jq b jr kl ii jt ju km il jw jx kn jz ka kb ko kd ke kf kp kh ki kj ha bi translated">继续这一努力，我们很高兴地介绍我们的第二个版本VietAI的<strong class="jq hi"> MTet </strong>项目，它代表<strong class="jq hi"> M </strong>多域<strong class="jq hi"> T </strong>翻译为<strong class="jq hi">E</strong>English和Vie <strong class="jq hi"> T </strong> namese。在此版本中，我们进一步改进了有史以来第一个大规模的多领域英语-越南语翻译数据集，发布了跨11个领域的多达420万个示例。此外，我们在<a class="ae kk" href="https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1" rel="noopener ugc nofollow" target="_blank">IWS lt’15</a>上展示了最先进的结果(英语-越南语+3.5 BLEU)。我们希望我们的努力将激发人们为越南NLP社区不断增长的高质量数据集库做出更多贡献。</p><blockquote class="kq kr ks"><p id="287a" class="jo jp kt jq b jr kl ii jt ju km il jw ku kn jz ka kv ko kd ke kw kp kh ki kj ha bi translated">VietAI是一个非营利组织，其使命是在越南建立一个由世界级人工智能专家组成的社区。VietAI培养和培训了数千名AI领域的学生和专家，其中3人是越南首批机器学习领域的谷歌开发者专家。</p></blockquote><h1 id="f7fc" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">资料组</h1><p id="aa6e" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">在此版本中，我们对第一个版本的数据集(datav1)进行了清理和重复数据删除，同时添加了来自各种文本源的120万个训练文本对。这使我们的数据集在datav2中从3.0M增长到接近4.2M的训练文本对。</p><p id="ff8b" class="pw-post-body-paragraph jo jp hh jq b jr kl ii jt ju km il jw jx kn jz ka kb ko kd ke kf kp kh ki kj ha bi translated">附加数据来自两个来源。首先，我们使用modelv1对来自现有大型嘈杂来源(OpenSubtitles、MultiCCAligned和Wikilingua)的高质量数据进行评分、过滤和配对，这些数据尚未纳入datav1。其次，我们从30个公共网站中混合执行自动和手动搜集，涵盖多个不同的领域，如医学出版物、宗教文本、工程文章、文学、新闻和诗歌。我们还校准了之前的测试集，以平衡不同的翻译域。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es kx"><img src="../Images/a58e996d0aab4ca42f76c32bc6a325a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rXpaKu_Q5AWeOyV3F7UCxA.png"/></div></div></figure><h1 id="4000" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">利用谷歌云平台、TPUs和Tensorflow</h1><p id="8170" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">通过使用<strong class="jq hi">TPU v3–8和TPU v3–32</strong>，我们能够训练<strong class="jq hi">更大的</strong>变压器模型，并在多个测试集上获得最先进的结果。利用谷歌云存储的区域灵活性，我们还能够分发我们的数据管道，用于每个特定变压器型号和TPU(v2–8、v3–8和v3–32)的培训。TPU v3训练速度的提高(与TPU v2–8相比)也使我们能够更长时间、更快地训练大型模型。</p><p id="ab01" class="pw-post-body-paragraph jo jp hh jq b jr kl ii jt ju km il jw jx kn jz ka kb ko kd ke kf kp kh ki kj ha bi translated">感谢<strong class="jq hi"> Google Cloud </strong>慷慨提供的<strong class="jq hi"> GCP信用</strong>，我们能够并行运行10个TPUv2，从非常大但有噪音的翻译数据集(如MultiCC(2000万句)和open字幕语料库(350万句))中评分和过滤高质量的训练数据，最终贡献了近一半的新数据。</p><h1 id="894b" class="iw ix hh bd iy iz ja jb jc jd je jf jg in jh io ji iq jj ir jk it jl iu jm jn bi translated">模型和结果</h1><p id="1043" class="pw-post-body-paragraph jo jp hh jq b jr js ii jt ju jv il jw jx jy jz ka kb kc kd ke kf kg kh ki kj ha bi translated">除了改善数据集大小和多样性，我们还采用了具有Transformer-tall18设置的普通Transformer架构，与Transformer-tall9相比，其大小增加了一倍，并且在IWSLT2015基准测试中的翻译质量更好。</p><p id="816d" class="pw-post-body-paragraph jo jp hh jq b jr kl ii jt ju km il jw jx kn jz ka kb ko kd ke kf kp kh ki kj ha bi translated">一些好的结果:</p><pre class="ky kz la lb fd lj lk ll lm aw ln bi"><span id="ac15" class="lo ix hh lk b fi lp lq l lr ls"><strong class="lk hi">English</strong></span><span id="4f62" class="lo ix hh lk b fi lt lq l lr ls">Without arguments, 'print' displays the entire partition table. However with the following arguments it performs various other actions.</span><span id="1fab" class="lo ix hh lk b fi lt lq l lr ls"><strong class="lk hi">Vietnamese</strong></span><span id="11d9" class="lo ix hh lk b fi lt lq l lr ls">Khi không có đối số, " print " hiển thị toàn bộ bảng phân vùng. Nếu đưa ra các đối số theo sau, thì nó làm một số hành vi khác.</span><span id="2091" class="lo ix hh lk b fi lt lq l lr ls"><strong class="lk hi">English</strong></span><span id="844f" class="lo ix hh lk b fi lt lq l lr ls">We report a seven-year-old female presenting with fever, dry cough, and abdominal pain after that.</span><span id="8d3f" class="lo ix hh lk b fi lt lq l lr ls"><strong class="lk hi">Vietnamese</strong></span><span id="9102" class="lo ix hh lk b fi lt lq l lr ls">Chúng tôi báo cáo một trường hợp bệnh nhi nữ, 7 tuổi vào viện với triệu chứng ho khan, sốt và đau bụng dữ dội sau đó.</span></pre><p id="b719" class="pw-post-body-paragraph jo jp hh jq b jr kl ii jt ju km il jw jx kn jz ka kb ko kd ke kf kp kh ki kj ha bi translated">在Google Cloud TPU v3和Google Cloud Platform基础设施的帮助下，我们能够更快、更长时间地训练大型模型，最终在我们的Transformer models v2上实现最先进的翻译质量。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lu"><img src="../Images/cc3281ec0a5389e06006c51b5e3e9a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rdxf2DxKsdS5TYfRE8yMBQ.png"/></div></div></figure><p id="45a0" class="pw-post-body-paragraph jo jp hh jq b jr kl ii jt ju km il jw jx kn jz ka kb ko kd ke kf kp kh ki kj ha bi translated">在图1中，我们报告了IWSLT2015翻译语料库上的MTet模型的结果。MTet Transformer-tall18型号实现了最先进的越南语和英语翻译。在更大的MTet数据集上进行训练也有显著的改善，对En-Vi和Vi-En分别贡献了0.7%和1.6%。</p><p id="b984" class="pw-post-body-paragraph jo jp hh jq b jr kl ii jt ju km il jw jx kn jz ka kb ko kd ke kf kp kh ki kj ha bi translated">我们的Transformer-tall18模型在我们新发布的高质量VietAI翻译数据集上进行训练，优于现有的M2M100，后者是在更大但噪声更大的数据集上训练的更大模型(图2)。我们在En-Vi和Vi-En任务上都取得了最先进的结果(分别比M2M100高5.8%和12.4%)，而体积却小得多。</p><p id="53d9" class="pw-post-body-paragraph jo jp hh jq b jr kl ii jt ju km il jw jx kn jz ka kb ko kd ke kf kp kh ki kj ha bi translated">这项工作是由VietAI研究小组(Chinh Ngo，Hieu Tran，Long Phan，Trieu H. Trinh，Hieu Nguyen，阮明，Minh-Thang Luong)进行的。</p><p id="cb34" class="pw-post-body-paragraph jo jp hh jq b jr kl ii jt ju km il jw jx kn jz ka kb ko kd ke kf kp kh ki kj ha bi translated">要查看更多细节，请查看<a class="ae kk" href="https://translate.vietai.org/" rel="noopener ugc nofollow" target="_blank"> VietAI翻译项目</a>。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es lv"><img src="../Images/b620c5311679ac11e51a5a8f3d766612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*H_I4Er2DKQD9mB1N3qdHPQ.png"/></div><figcaption class="lw lx et er es ly lz bd b be z dx">Figure 1. Improvements made from our first release to this second release on on IWSLT15 test set.</figcaption></figure><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es ma"><img src="../Images/0101a2f0891e6fe82da4392035f72431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N-TQ8C6H2Y1qLfTgdfX1hQ.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">Figure 2. Comparison between pre-trained translation models and our purely supervised MTet.</figcaption></figure></div></div>    
</body>
</html>