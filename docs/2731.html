<html>
<head>
<title>Import/Export Data Between HDFS and RDBMS Using Apache Sqoop</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Sqoop在HDFS和RDBMS之间导入/导出数据</h1>
<blockquote>原文：<a href="https://medium.com/edureka/apache-sqoop-tutorial-431ed0af69ee?source=collection_archive---------3-----------------------#2017-11-21">https://medium.com/edureka/apache-sqoop-tutorial-431ed0af69ee?source=collection_archive---------3-----------------------#2017-11-21</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/a72a1f728f716c8b0f53e6bd5bf1bd80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*wWhg48Gd04ecNf57wkz0CA.png"/></div><figcaption class="il im et er es in io bd b be z dx">Apache Sqoop Tutorial - Edureka</figcaption></figure><p id="84f2" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在开始这篇Apache Sqoop教程之前，让我们后退一步。你还记得数据摄取的重要性吗，正如我们在之前的博客<strong class="ir hi"> <em class="jn"> Apache Flume </em> </strong>中所讨论的。现在，正如我们所知，Apache Flume是一个用于非结构化数据源的数据摄取工具，但是组织将其运营数据存储在关系数据库中。因此，需要一种可以从关系数据库导入和导出数据的工具。这也是Apache Sqoop诞生的原因。Sqoop可以轻松地与Hadoop集成，并从HDFS上的关系数据库中转储结构化数据，从而增强了Hadoop的能力。这就是为什么Apache Flume是<strong class="ir hi"> <em class="jn"> Hadoop生态系统</em> </strong>的重要组成部分。</p><p id="2a2d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最初，Sqoop由Cloudera开发和维护。后来，在2011年7月23日，它被阿帕奇孵化。2012年4月，Sqoop项目被提升为Apache的顶级项目。</p><p id="0e09" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇Apache Flume教程博客中，我们将讨论:</p><ul class=""><li id="9c7f" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">Sqoop简介</li><li id="a79c" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">为什么选择Sqoop</li><li id="74d0" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Sqoop特性</li><li id="b46b" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Flume vs Sqoop</li><li id="a4f9" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Sqoop架构和工作</li><li id="a294" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">Sqoop命令</li></ul><p id="c50d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将从介绍Apache Sqoop开始这个Apache Sqoop教程。接下来，我们将了解使用Apache Sqoop的优势。</p><h1 id="5059" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Sqoop简介</h1><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es la"><img src="../Images/3aaed2cfef2cc535c620825c11d0c7f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*fciSdDmwHl6bjFnm6colJw.png"/></div></figure><p id="05a9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">通常，应用程序使用RDBMS与关系数据库进行交互，因此这使得关系数据库成为生成大数据的最重要来源之一。这种数据以关系结构存储在RDB服务器中。在这里，Apache Sqoop在<strong class="ir hi"> <em class="jn"> Hadoop生态系统</em> </strong>中扮演着重要的角色，在关系数据库服务器和HDFS之间提供可行的交互。</p><p id="dc60" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，Apache Sqoop是<strong class="ir hi"> <em class="jn"> Hadoop生态系统</em> </strong>中的一个工具，用于在<strong class="ir hi"><em class="jn">【HDFS】</em></strong>(Hadoop存储)和关系数据库服务器(如MySQL、Oracle RDB、SQLite、Teradata、Netezza、Postgres等)之间传输数据。Apache Sqoop将数据从关系数据库导入到HDFS，将数据从HDFS导出到关系数据库。它在<a class="ae lf" href="https://www.edureka.co/blog/hadoop-tutorial?utm_source=medium&amp;utm_medium=content-link&amp;utm_campaign=apache-sqoop-tutorial" rel="noopener ugc nofollow" target="_blank"> Hadoop </a>和外部数据存储(如企业数据仓库、关系数据库等)之间高效地传输批量数据。</p><blockquote class="lg lh li"><p id="cb2d" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">这就是Sqoop得名的原因——“SQ<strong class="ir hi">SQ</strong>L to Had<strong class="ir hi">OOP</strong>T34]Hadoop to SQL”。</p></blockquote><p id="997d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">此外，Sqoop用于将数据从外部数据存储导入Hadoop生态系统的工具，如<strong class="ir hi"><em class="jn">Hive</em></strong>&amp;<strong class="ir hi"><em class="jn">h base</em></strong>。</p><p id="4dba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们知道什么是Apache Sqoop。因此，让我们继续学习Apache Sqoop教程，理解为什么组织广泛使用Sqoop。</p><h1 id="1f93" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">为什么选择Sqoop？</h1><p id="8a9c" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">对于Hadoop开发者来说，在数据被加载到HDFS之后，真正的游戏才开始。他们玩弄这些数据，以便获得隐藏在HDFS存储的数据中的各种洞察力。</p><p id="4cfb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，对于这个分析，驻留在关系数据库管理系统中的数据需要转移到HDFS。为从关系数据库向HDFS导入和导出数据而编写<strong class="ir hi"> <em class="jn"> MapReduce </em> </strong>代码的任务是无趣的&amp;乏味的。这就是Apache Sqoop来拯救他们并消除他们的痛苦的地方。它自动执行导入&amp;导出数据的过程。</p><p id="b1bb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Sqoop通过提供导入和导出数据的CLI简化了开发人员的工作。他们只需提供基本信息，如数据库认证、来源、目的地、操作等。它负责剩下的部分。</p><p id="484d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Sqoop在内部将命令转换成MapReduce任务，然后通过HDFS执行这些任务。它使用YARN框架来导入和导出数据，这在并行性之上提供了容错。</p><p id="3895" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇Sqoop教程博客中，我们将了解Sqoop的关键特性，然后我们将继续讨论Apache Sqoop架构。</p><h1 id="f46c" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Sqoop的主要特性</h1><p id="ac6d" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">Sqoop提供了许多显著的特性，例如:</p><blockquote class="lg lh li"><p id="e1d0" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated"><strong class="ir hi">满载</strong> : Apache Sqoop可以通过一条命令装载整个表。您还可以使用一个命令从数据库中加载所有的表。</p><p id="3171" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated"><strong class="ir hi">增量</strong> <strong class="ir hi">加载</strong> : Apache Sqoop还提供了增量加载的功能，每当表被更新时，就可以加载表的一部分。</p><p id="e3d3" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated"><strong class="ir hi">并行</strong></p><p id="64a3" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated"><strong class="ir hi">导入</strong> <strong class="ir hi">结果</strong> <strong class="ir hi">的</strong><strong class="ir hi">SQL</strong><strong class="ir hi">查询</strong>:您也可以导入HDFS SQL查询返回的结果。</p><p id="f204" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated"><strong class="ir hi">压缩</strong>:您可以通过使用deflate(gzip)算法和–compress参数，或者通过指定–Compression-codec参数来压缩您的数据。也可以在<strong class="ir hi"> Apache Hive </strong>中加载压缩表。</p><p id="133f" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated"><strong class="ir hi">连接器</strong> <strong class="ir hi">用于</strong> <strong class="ir hi">所有</strong> <strong class="ir hi">专业</strong> <strong class="ir hi"> RDBMS </strong> <strong class="ir hi">数据库</strong> : Apache Sqoop提供了多个RDBMS数据库的连接器，几乎覆盖了整个圆周。</p><p id="db2b" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated"><strong class="ir hi"> Kerberos </strong>Sqoop支持Kerberos认证。</p><p id="d125" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated"><strong class="ir hi">将</strong> <strong class="ir hi">数据</strong> <strong class="ir hi">直接</strong> <strong class="ir hi">加载到</strong> <strong class="ir hi"> HIVE/HBase </strong>中:您可以将数据直接加载到<strong class="ir hi"> Apache Hive </strong>中进行分析，也可以将您的数据转储到HBase中，这是一个NoSQL数据库。</p><p id="ef4e" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated"><strong class="ir hi">支持</strong> <strong class="ir hi"> Accumulo </strong>的 <strong class="ir hi">:也可以指示Sqoop导入Accumulo中的表，而不是HDFS的目录。</strong></p></blockquote><p id="cfbc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">该架构为Apache Sqoop提供了这些优势。现在，我们知道了Apache Sqoop的特性，让我们继续了解Apache Sqoop的架构和工作方式。</p><h1 id="546a" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Sqoop架构和工作</h1><p id="5a96" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">让我们使用下图来了解Apache Sqoop的工作原理:</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/668400c121464ea49a15e215fb696d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rfFz4s5_W_mxY2Cy5G6XsA.png"/></div></div></figure><p id="5b5d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">导入工具将单个表从RDBMS导入到HDFS。在HDFS，表中的每一行都被视为一条记录。</p><p id="e2ec" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当我们提交Sqoop命令时，我们的主任务被分成子任务，这些子任务由单独的Map任务在内部处理。Map任务是子任务，它将部分数据导入Hadoop生态系统。总的来说，所有地图任务都会导入全部数据。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/049ab8bf71da82481cd18b4e84746689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ahN8AKOUusrdiICX23jiTg.png"/></div></div></figure><p id="8ece" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">导出也以类似的方式工作。</p><p id="1c8a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">导出工具将一组文件从HDFS导出回RDBMS。作为Sqoop输入的文件包含记录，这些记录在表中称为行。</p><p id="01ca" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当我们提交作业时，它被映射到地图任务中，这些任务从HDFS带来大量数据。这些块被导出到结构化数据目标。结合所有这些导出的数据块，我们在目的地接收整个数据，在大多数情况下，目的地是RDBMS (MYSQL/Oracle/SQL Server)。</p><p id="63ce" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在聚合的情况下，需要减少阶段。但是，Apache Sqoop只是导入和导出数据；它不执行任何聚合。映射作业根据用户定义的数量启动多个映射器。对于Sqoop导入，将为每个映射器任务分配一部分要导入的数据。Sqoop在映射器之间平均分配输入数据以获得高性能。然后，每个映射器使用JDBC创建与数据库的连接，并获取由Sqoop分配的数据部分，并根据CLI中提供的参数将其写入HDFS或配置单元或HBase。</p><p id="dcf5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们已经了解了Apache Sqoop的架构和工作方式，让我们来了解一下Apache Flume和Apache Sqoop之间的区别。</p><h1 id="775b" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Flume vs Sqoop</h1><p id="b549" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">Flume和Sqoop的主要区别在于:</p><ul class=""><li id="5a87" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">Flume只将非结构化数据或半结构化数据引入HDFS。</li><li id="ab40" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">而Sqoop可以将结构化数据从RDBMS或企业数据仓库导入和导出到HDFS，反之亦然。</li></ul><p id="6bc5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，在我们的Apache Sqoop教程中，是时候学习Apache Sqoop命令了。</p><h1 id="0897" class="kc kd hh bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Sqoop命令</h1><h2 id="4fde" class="lw kd hh bd ke lx ly lz ki ma mb mc km ja md me kq je mf mg ku ji mh mi ky mj bi translated">Sqoop —导入命令</h2><p id="51d9" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">Import命令用于将关系数据库中的表导入HDFS。在我们的例子中，我们将把表从MySQL数据库导入到HDFS。</p><p id="27f0" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如您在下图中看到的，我们在雇员数据库中有雇员表，我们将把它导入到HDFS。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mk"><img src="../Images/baf2cf86800ec0e6a8fdecb1ce5a6888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*5vaQG35MouKUcZZe3kV7Ug.png"/></div></figure><p id="bce8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">导入表格的命令是:</p><pre class="lb lc ld le fd ml mm mn mo aw mp bi"><span id="3315" class="lw kd hh mm b fi mq mr l ms mt">sqoop import --connect jdbc:<!-- -->mysql://localhost/employees<!-- --> --username edureka --table employees</span></pre><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mu"><img src="../Images/fbdb57fdf96d6a2ce3802260e82342c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1UIPRbyZaJz3yFZh3ovcUQ.png"/></div></div></figure><p id="cbf6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如您在下图中看到的，在执行此命令后，地图任务将在后端执行。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mv"><img src="../Images/288146bb4c9e30c7d4eaaaaae1ffb746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AvaS0ZIUXyZhGiuKJrQbFA.png"/></div></div></figure><p id="aa25" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">代码执行后，您可以检查HDFS的Web UI，即数据导入的localhost:50070。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mw"><img src="../Images/4778e843ecae2fa6f230dcc30ba01161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iqlXRFWUkaTHx3FGpbN9fA.png"/></div></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/199039db97e43bcd7ab31602bb7f6a37.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*6uLEffS-1pqzit4UbSApaw.png"/></div></figure><h2 id="b551" class="lw kd hh bd ke lx ly lz ki ma mb mc km ja md me kq je mf mg ku ji mh mi ky mj bi translated">Sqoop —导入目标目录的命令</h2><p id="ed13" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">您也可以使用下面的命令将该表导入到HDFS的特定目录中:</p><pre class="lb lc ld le fd ml mm mn mo aw mp bi"><span id="f48e" class="lw kd hh mm b fi mq mr l ms mt">sqoop import --connect jdbc:<!-- -->mysql://localhost/employees<!-- --> --username edureka --table employees --m 1 --target-dir /employees</span></pre><p id="bc37" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Sqoop从大多数数据库源并行导入数据。<em class="jn"> -m </em>属性用于指定要执行的映射器数量。</p><p id="08c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Sqoop从大多数数据库源并行导入数据。您可以通过使用<em class="jn"> -m </em>或<em class="jn">–num-mappers</em>参数来指定用于执行导入的映射任务(并行进程)的数量。这些参数中的每一个都采用一个与要使用的并行度相对应的整数值。</p><p id="84c4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以独立于目录中的文件数量来控制映射器的数量。导出性能取决于并行度。默认情况下，Sqoop将在导出过程中并行使用四个任务。这可能不是最佳选择，您需要尝试自己的特定设置。额外的任务可能提供更好的并发性，但是如果数据库已经在更新索引、调用触发器等方面遇到瓶颈，那么额外的负载可能会降低性能。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es my"><img src="../Images/d6f941be5feb45eebd5c8017849bccc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HbNrqyLdxGBGnRdAHljzAA.png"/></div></div></figure><p id="f0e4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以在下图中看到，映射器任务的数量为1。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/a3fa6a480ec302a0d21ee5a48c3b3a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*15R47ANUdB1wbRBf7XFnaw.png"/></div></figure><p id="a344" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">导入MySQL表时创建的文件数等于创建的映射器数。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es na"><img src="../Images/e5ad66f1d044324524c081dc73dcb1c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kGsIpz1_vKRBoEkFp-1NSA.png"/></div></div></figure><h2 id="697e" class="lw kd hh bd ke lx ly lz ki ma mb mc km ja md me kq je mf mg ku ji mh mi ky mj bi translated">Sqoop —带有Where子句的导入命令</h2><p id="ae95" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">您可以使用Sqoop导入工具中的“where”子句导入表的子集。它在各自的数据库服务器中执行相应的SQL查询，并将结果存储在HDFS的目标目录中。您可以使用以下命令通过'<em class="jn"> where </em>子句导入数据:</p><pre class="lb lc ld le fd ml mm mn mo aw mp bi"><span id="7210" class="lw kd hh mm b fi mq mr l ms mt">sqoop import --connect jdbc:mysql://localhost/employees --username edureka --table employees --m 3 --where "emp_no &amp;gt; 49000" --target-dir /Latest_Employees</span></pre><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es my"><img src="../Images/cb22dee7f1b183de8cf520e7292052aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HZXUFQcrYTyWdKHTQtFr5Q.png"/></div></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es nb"><img src="../Images/eafa68834cf9553e2f4c0000a98ac1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wiO8QOS-iEInpBVP7Sii9w.png"/></div></div></figure><h2 id="5ec9" class="lw kd hh bd ke lx ly lz ki ma mb mc km ja md me kq je mf mg ku ji mh mi ky mj bi translated">Sqoop —增量导入</h2><p id="42df" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">Sqoop提供了一种增量导入模式，该模式可用于仅检索比先前导入的一组行更新的行。Sqoop支持两种类型的增量导入:<em class="jn"> append </em>和<em class="jn"> lastmodified </em>。您可以使用–incremental参数来指定要执行的增量导入的类型。</p><p id="4613" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当导入一个表时，您应该指定<em class="jn"> append </em>模式，在该表中，新行随着行id值的增加而不断增加。用<em class="jn">–check-column</em>指定包含行id的列。Sqoop导入校验列的值大于用<em class="jn">–最后一个值</em>指定的值的行。</p><p id="c1c3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">Sqoop支持的另一种表更新策略称为<em class="jn"> lastmodified </em>模式。当源表的行可能被更新时，应该使用这种方法，每次更新都会将last-modified列的值设置为当前时间戳。</p><p id="9170" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">运行后续导入时，您应该以这种方式指定<em class="jn">–最后值</em>，以确保您只导入新的或更新的数据。这是通过将增量导入创建为保存的作业来自动处理的，这是执行重复增量导入的首选机制。</p><p id="9ade" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，我们插入一个新行，它将在我们的HDFS中更新。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es my"><img src="../Images/0cfb18ebca0ce1cacb031f0a294a9447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4hfKZcM-TJScIM9HfglcQ.png"/></div></div></figure><p id="1943" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">增量导入的命令是:</p><pre class="lb lc ld le fd ml mm mn mo aw mp bi"><span id="e01a" class="lw kd hh mm b fi mq mr l ms mt">sqoop import --connect jdbc:mysql://localhost/employees --username edureka --table employees --target-dir /Latest_Employees --incremental append --check-column emp_no --last-value 499999</span></pre><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mu"><img src="../Images/8662d5abf08a039bb11d98b2105dd606.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QpMKLne6fKLydjUqn2tJvQ.png"/></div></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/f1e711e8b8cc9199fd5cbcad0382feaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*5l008DNK0VrV4fiS0bFtow.png"/></div></figure><p id="a445" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以在下图中看到，使用更新的数据创建了一个新文件。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es nd"><img src="../Images/5d6454a54083426f0cebc338f337672c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mmZFqS0RxWsttrgAAAJjsQ.png"/></div></div></figure><h2 id="6dc7" class="lw kd hh bd ke lx ly lz ki ma mb mc km ja md me kq je mf mg ku ji mh mi ky mj bi translated">Sqoop —导入所有表</h2><p id="aa44" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">您可以将所有表从RDBMS数据库服务器导入到HDFS。每个表数据存储在单独的目录中，目录名与表名相同。数据库中的每个表都必须有一个主键字段，这是强制性的。从数据库导入所有表的命令是:</p><pre class="lb lc ld le fd ml mm mn mo aw mp bi"><span id="15a7" class="lw kd hh mm b fi mq mr l ms mt">sqoop import-all-tables --connect jdbc:mysql://localhost/employees --username edureka</span></pre><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mu"><img src="../Images/caed1c3aa849b0072c8e16c6f3b5f160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jg9hZNp_g1XTOHyvBmo1Eg.png"/></div></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es ne"><img src="../Images/871b4a4db5207fb3f1f7d04a89fa9f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kmy2B9pdap4gFLVAoJXHLw.png"/></div></div></figure><h2 id="161d" class="lw kd hh bd ke lx ly lz ki ma mb mc km ja md me kq je mf mg ku ji mh mi ky mj bi translated">Sqoop —列出数据库</h2><p id="3af8" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">您可以使用Sqoop列出关系数据库中存在的数据库。Sqoop list-databases工具针对数据库服务器解析并执行“显示数据库”查询。列出数据库的命令是:</p><pre class="lb lc ld le fd ml mm mn mo aw mp bi"><span id="0191" class="lw kd hh mm b fi mq mr l ms mt">sqoop list-databases --connect jdbc:mysql://localhost/ --username edureka</span></pre><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mu"><img src="../Images/7af0a312d8136b701eed50bbd2ca959a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lh6kfo25pFPapXD8Ubg_lg.png"/></div></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/64152d8cfe9f47d780b1cb440790c315.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*rpJk7KiqUGAeaYOm4CZV7Q.png"/></div></figure><h2 id="bac8" class="lw kd hh bd ke lx ly lz ki ma mb mc km ja md me kq je mf mg ku ji mh mi ky mj bi translated">Sqoop —列表</h2><p id="b890" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">您还可以使用Sqoop列出MySQL数据库服务器中特定数据库的表。Sqoop list-tables工具解析并执行“显示表格”查询。在数据库中列出表格的命令是:</p><pre class="lb lc ld le fd ml mm mn mo aw mp bi"><span id="67e7" class="lw kd hh mm b fi mq mr l ms mt">sqoop list-tables --connect jdbc:mysql://localhost/employees --username edureka</span></pre><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es ng"><img src="../Images/87302798e1bbba50d84eddc5b225ea20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r4dIAiaLjMam-cB5yOIZoA.png"/></div></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es nh"><img src="../Images/711e317e7e454fdd66bc830b6fc48571.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*nLHKzqUcNFzg5FKqp4RiVw.png"/></div></figure><p id="cd09" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">正如我们上面讨论的，您还可以将数据从HDFS导出到RDBMS数据库。目标表必须存在于目标数据库中。这些数据作为记录储存在HDFS。这些记录被读取、解析并用用户指定的分隔符分隔。默认操作是使用insert语句将输入文件中的所有记录插入到数据库表中。在更新模式下，Sqoop生成update语句，将现有记录替换到数据库中。</p><p id="10e9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因此，首先，我们创建一个空表，我们将在其中导出数据。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mu"><img src="../Images/c1468ec5fde3363b224c6856af1fa917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jDFg0OgA_9X3rgeOVve9vw.png"/></div></div></figure><p id="0e79" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将数据从HDFS导出到关系数据库的命令是:</p><pre class="lb lc ld le fd ml mm mn mo aw mp bi"><span id="cd4e" class="lw kd hh mm b fi mq mr l ms mt">sqoop export --connect jdbc:mysql://localhost/employees --username edureka --table emp --export-dir /user/edureka/employees</span></pre><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es ni"><img src="../Images/6d9cf9d0b4e6ffcdd1695b3f0ca60485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m7iTZ44c2_PM75PkOQ_N4Q.png"/></div></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div class="er es nj"><img src="../Images/8eca89b71873423384a4c0eeb234a3e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*WrOqIvkstVSzLGi6eEJO6A.png"/></div></figure><h2 id="3cb5" class="lw kd hh bd ke lx ly lz ki ma mb mc km ja md me kq je mf mg ku ji mh mi ky mj bi translated">Sqoop — Codegen</h2><p id="dcfc" class="pw-post-body-paragraph ip iq hh ir b is lm iu iv iw ln iy iz ja lo jc jd je lp jg jh ji lq jk jl jm ha bi translated">在面向对象的应用程序中，每个数据库表都有一个数据访问对象类，它包含初始化对象的“getter”和“setter”方法。Codegen自动生成DAO类。它基于表模式结构用Java生成DAO类。</p><pre class="lb lc ld le fd ml mm mn mo aw mp bi"><span id="6ebb" class="lw kd hh mm b fi mq mr l ms mt">sqoop codegen --connect jdbc:mysql://localhost/employees --username edureka --table employees</span></pre><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es nk"><img src="../Images/98ad7b51ace862a8b0b81baef5c82b92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W6EXxOOlxsRzV12vGXC6qA.png"/></div></div></figure><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mu"><img src="../Images/5ddf6e938d57a12938e454b68f3d737a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tbPXltO0jznKev0rE8LaNg.png"/></div></div></figure><p id="cc74" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以在上面的图像中看到生成代码的路径。让我们走这条路，检查创建的文件。</p><figure class="lb lc ld le fd ii er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mu"><img src="../Images/ae95a58fb719fab805bcb3ac271abf63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Ra1Wh8HxUAkVG8CTjUcpA.png"/></div></div></figure><p id="122e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我希望这篇文章能给你带来信息和附加值。</p><p id="ce38" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果你想查看更多关于人工智能、Python、道德黑客等市场最热门技术的文章，你可以参考Edureka的官方网站。</p><p id="832b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">请留意本系列中解释大数据其他各方面的其他文章。</p><blockquote class="lg lh li"><p id="2bbd" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">1.<a class="ae lf" rel="noopener" href="/edureka/hadoop-tutorial-24c48fbf62f6"> Hadoop教程</a></p><p id="f020" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">2.<a class="ae lf" rel="noopener" href="/edureka/hive-tutorial-b980dfaae765">蜂巢教程</a></p><p id="5875" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">3.<a class="ae lf" rel="noopener" href="/edureka/pig-tutorial-2baab2f0a5b0">养猪教程</a></p><p id="b51b" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">4.<a class="ae lf" rel="noopener" href="/edureka/mapreduce-tutorial-3d9535ddbe7c">地图缩小教程</a></p><p id="e928" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">5.<a class="ae lf" rel="noopener" href="/edureka/hbase-tutorial-bdc36ab32dc0"> HBase教程</a></p><p id="4746" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">6.<a class="ae lf" rel="noopener" href="/edureka/hdfs-tutorial-f8c4af1c8fde"> HDFS教程</a></p><p id="83e9" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">7.<a class="ae lf" rel="noopener" href="/edureka/hadoop-3-35e7fec607a"> Hadoop 3 </a></p><p id="bc01" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">8.<a class="ae lf" rel="noopener" href="/edureka/big-data-tutorial-b664da0bb0c8">大数据教程</a></p><p id="f5f6" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">9.<a class="ae lf" rel="noopener" href="/edureka/apache-flume-tutorial-6f7150210c76">水槽教程</a></p><p id="53e8" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">10.<a class="ae lf" rel="noopener" href="/edureka/apache-oozie-tutorial-d8f7bbbe1591"> Oozie教程</a></p><p id="365a" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">11.<a class="ae lf" rel="noopener" href="/edureka/hadoop-ecosystem-2a5fb6740177"> Hadoop生态系统</a></p><p id="97e1" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">12.<a class="ae lf" rel="noopener" href="/edureka/hive-commands-b70045a5693a">HQL顶级配置单元命令及示例</a></p><p id="757d" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">13.<a class="ae lf" rel="noopener" href="/edureka/create-hadoop-cluster-with-amazon-emr-f4ce8de30fd"> Hadoop集群搭配亚马逊EMR？</a></p><p id="ed33" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">14.<a class="ae lf" rel="noopener" href="/edureka/big-data-engineer-resume-7bc165fc8d9d">大数据工程师简历</a></p><p id="3778" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">15.<a class="ae lf" rel="noopener" href="/edureka/hadoop-developer-cc3afc54962c"> Hadoop开发人员-工作趋势和薪水</a></p><p id="bc6a" class="ip iq jn ir b is it iu iv iw ix iy iz lj jb jc jd lk jf jg jh ll jj jk jl jm ha bi translated">16.<a class="ae lf" rel="noopener" href="/edureka/hadoop-interview-questions-55b8e547dd5c"> Hadoop面试问题</a></p></blockquote><p id="864d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="jn">原载于2014年10月9日www.edureka.co</em><a class="ae lf" href="https://www.edureka.co/blog/hadoop-tutorial/" rel="noopener ugc nofollow" target="_blank"><em class="jn"/></a><em class="jn">。</em></p></div></div>    
</body>
</html>