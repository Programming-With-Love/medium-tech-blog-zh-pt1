<html>
<head>
<title>A Beginner’s Tale of A First Computer Vision Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第一个计算机视觉项目的初学者故事</h1>
<blockquote>原文：<a href="https://medium.com/compendium/a-beginners-tale-of-a-first-computer-vision-project-bf4180a18c1a?source=collection_archive---------1-----------------------#2018-08-02">https://medium.com/compendium/a-beginners-tale-of-a-first-computer-vision-project-bf4180a18c1a?source=collection_archive---------1-----------------------#2018-08-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="f4da" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">让机器人能够看见——第一部分</p><h1 id="a22c" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">介绍</h1><p id="ac14" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">本文将是两部分系列的第一部分，描述一个结合了深度学习(DL)，更具体地说是计算机视觉和机器人操作系统(ROS)这一极其热门的现象的项目。这种结合产生了模型训练和来自相机流的物体分类的流水线，如下图所示。这第一篇文章将描述模型训练部分，更具体地说是在图像中的“fast.ai”脚本人物中使用的模型的训练。第二部分将描述系统的ROS部分，更具体地说是它的发布/订阅特性的初始化，以及映像中其余的设置。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kf"><img src="../Images/6f2aa8c818e67776fde36e06113a4a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YmPBctQ_Vok26tYO"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx">Highly sophisticated drawing of the components used in the project, and how they interact.</figcaption></figure><p id="7542" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">该项目的文档可以在GitHub上找到。查看自述文件，了解如何复制该模型培训。此外，本文将使用代码和输出的图片。任何愿意的人都可以克隆这个库，并亲自试用。需要注意的是，“fast.ai”模块是一个<a class="ae kv" href="https://git-scm.com/book/en/v2/Git-Tools-Submodules" rel="noopener ugc nofollow" target="_blank">子模块</a>——需要自己更新。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kw"><img src="../Images/eb95f8e47c61d68783ee9d4e5b566466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*puA9JNDkxmBbqDcZ"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx"><a class="ae kv" href="https://makeameme.org/meme/buzzwords-buzzwords-everywhere-5af7a9" rel="noopener ugc nofollow" target="_blank">Make a Meme</a></figcaption></figure><p id="d06c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">至于任何关于DL的文章，我们将不得不使用一些技术术语。这些将不详细解释，并且将努力使这一数量最小化，因为目的是给出项目和所使用的技术的简要和简单的概述。但是，我们为感兴趣的读者提供了链接。</p><p id="ee5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个项目和来自硅谷的<a class="ae kv" href="https://hub.packtpub.com/the-software-behind-silicon-valley-emmy-nominated-not-hotdog-app/" rel="noopener ugc nofollow" target="_blank">【非热狗】</a>的“艾美奖”提名软件有很多相似之处(除了这个模型可以分类多个标签，而不仅仅是二进制)，客观上很酷。另一个类似的项目，作为灵感，可以在这里找到<a class="ae kv" href="https://www.oreilly.com/learning/how-to-build-a-robot-that-sees-with-100-and-tensorflow" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="caaa" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">数据集</h1><p id="e1bc" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">那么，训练这样一个模型从哪里开始呢？首先要做的是定义一个数据集，包含图像及其各自的标签，描述图像的正确分类，使用所谓的监督学习。有几种方法可以定义这样的数据集；你可以手动下载相关的图片，并把它们放在一个以图片标签命名的文件夹中，或者你可以找到/制作一个脚本来完成这项工作。后一种选择工作量更少，更有趣，也更具可伸缩性。下面是一个这样的<a class="ae kv" href="https://github.com/hardikvasa/google-images-download" rel="noopener ugc nofollow" target="_blank">脚本</a>被实现的例子，它指定了data/‘label _ name’的文件夹结构。</p><pre class="kg kh ki kj fd kx ky kz la aw lb bi"><span id="69f7" class="lc jd hh ky b fi ld le l lf lg">def download_images(searchword, form="jpg", lim=100, directory= "data"):<br/>    if not os.path.isdir(directory):<br/>        ! mkdir $directory<br/>    ! googleimagesdownload --keywords $searchword --format $form --limit $lim --output_directory $directory <br/>    src_path = os.path.join(directory, searchword)<br/>    if not os.path.isdir(src_path):<br/>        ! mkdir $src_path</span></pre><p id="73cc" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">完整的方法请参见前面提到的GitHub项目，该项目也将集合分为<a class="ae kv" href="https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set" rel="noopener ugc nofollow" target="_blank">训练和验证集合</a>，比例分别为70/30。</p><p id="91b5" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">今天的DL模型能够用小数据集获得相当好的结果，在这种情况下，大约600幅图像用于训练集，200幅图像用于验证集。然而，我们使用了<a class="ae kv" href="https://searchcio.techtarget.com/definition/transfer-learning" rel="noopener ugc nofollow" target="_blank">迁移学习</a>，显著提高了结果，同时也减少了培训时间。此外，所使用的DL框架<a class="ae kv" href="http://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>提供了一种内置的数据增强方法，它基本上可以改变图像的方向，水平、垂直翻转等。，同时保持内容不变。这导致数据集增加了大约4倍(在运行时)。</p><h1 id="dc6c" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">路径和数据清理</h1><p id="8f3a" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">前面提到的DL框架fast.ai就是建立在<a class="ae kv" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>之上的框架。它实现了训练和验证集的相同默认路径，更具体地说是“数据/训练”和“数据/验证”，这就是为什么“图像下载”单元也将此作为默认路径，同时在<em class="lh">训练</em>和<em class="lh">验证</em>文件夹中为每个搜索字符串创建一个文件夹。通过遵循这些规则，可以很容易地告诉图像分类器对象在哪里寻找数据。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es li"><img src="../Images/b8e4ed5702c0439cdf1a3653c502f482.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/0*Q3J42c4uLx5JkYoE"/></div><figcaption class="kr ks et er es kt ku bd b be z dx"><a class="ae kv" href="https://www.business2community.com/marketing/much-dirty-data-costing-01241847" rel="noopener ugc nofollow" target="_blank">How Much is Dirty Data Costing You?</a></figcaption></figure><p id="2cd4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">很多情感类的电影，广告等。提出问题<em class="lh">“如果你可以告诉年轻时的自己任何事情，你会说什么？</em>“经过大约六个月的DL实践，可以有把握地说，许多人与DL相关联的部分，即模型的训练，确实是容易的部分，而困难的部分是首先获得数据，然后验证和清理数据。</p><p id="7e6f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">对于这个项目，这表现为下载的图像不是JPEG图像——这很奇怪，因为下载时需要指定图像的文件类型。当然有很多方法可以转换成jpeg格式，但是由于时间的原因，一个简单的解决方法是删除所有不相关类型的数据，也就是任何扩展名不是. JPEG的数据。</p><pre class="kg kh ki kj fd kx ky kz la aw lb bi"><span id="bf00" class="lc jd hh ky b fi ld le l lf lg">for path in file_paths:<br/>    for files in os.listdir(path):<br/>        file_path = os.path.join(path, files)<br/>        if imghdr.what(file_path != 'jpeg':<br/>            os.remove(file_path);</span></pre><h1 id="8cd9" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">培养</h1><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lj"><img src="../Images/f3a3d90fa14de76ccaf522efe4d03875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DIJMxDMwZu5RxAYu"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx"><a class="ae kv" href="https://adhdrollercoaster.org/tools-and-strategies/brain-training-games-deemed-ineffective/" rel="noopener ugc nofollow" target="_blank">“Brain-training” Games Ineffective for ADHD</a></figcaption></figure><p id="d8aa" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">既然数据集已经被定义、分离和清理，那么实际的模型训练就可以开始了。使用fast.ai框架使这变得非常容易:只需要4行代码:</p><pre class="kg kh ki kj fd kx ky kz la aw lb bi"><span id="e078" class="lc jd hh ky b fi ld le l lf lg">tfms = tfms_from_model(arch, sz, aug_tfms=augmentation, max_zoom=1.1)<br/>data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, num_workers=1)<br/>learn = ConvLearner.pretrained(arch, data, ps=0.4)<br/>learn.fit(1e-3, 1)</span></pre><p id="9ebd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">解释这些参数的功能需要一整篇<a class="ae kv" href="https://becominghuman.ai/3-lines-of-code-deciphering-fast-ai-658e79151af8" rel="noopener ugc nofollow" target="_blank">新文章</a>，然而，在高层次上，它是这样做的:</p><ol class=""><li id="bdd6" class="lk ll hh ig b ih ii il im ip lm it ln ix lo jb lp lq lr ls bi translated">定义一个transforms对象，如前所述，它会在运行时增加数据集</li><li id="b4be" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">定义一个图像分类器对象，它保存数据集并应用数据转换</li><li id="8d89" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">定义一个学习对象，如前所述，它使用迁移学习</li><li id="c696" class="lk ll hh ig b ih lt il lu ip lv it lw ix lx jb lp lq lr ls bi translated">通过模型运行数据，即训练模型。</li></ol><p id="76f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">就这样——完成，终结，完成！模型现在已经训练好了，可以使用了。然而，如果你想接近这个框架和模型的潜力，下一步就是超参数调整和更多的训练。</p><h1 id="5857" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">超参数调整(和更多培训)</h1><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es ly"><img src="../Images/ebb215c926eae1c847324f743a9b49c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/0*o6JBaWwctzNaiJhY"/></div><figcaption class="kr ks et er es kt ku bd b be z dx"><a class="ae kv" href="https://apk-dl.com/car-photo-tuning-professional-virtual-tuning/com.Andrey.Pimpmyride" rel="noopener ugc nofollow" target="_blank">Car Photo Tuning — Professional Virtual Tuning 2.2 APK</a></figcaption></figure><p id="7d04" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有相当多的可能的超参数需要优化，但是对于本文来说，焦点主要集中在<a class="ae kv" href="https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b" rel="noopener ugc nofollow" target="_blank">学习率</a>和<a class="ae kv" rel="noopener" href="/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5">辍学率</a>上。批次和图像大小也进行了调整，但是，这主要是因为Cuda-内存不足错误(最大GPU内存)。</p><p id="6004" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了找到最佳学习率，fast.ai框架提供了一个巧妙的技巧:</p><pre class="kg kh ki kj fd kx ky kz la aw lb bi"><span id="a38b" class="lc jd hh ky b fi ld le l lf lg">learn.lr_find()<br/>learn.sched.plot()</span></pre><p id="2700" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">它基本上描绘了不同的学习速度和它们导致的损失之间的关系。这里需要注意的一点是，对于像这个项目这样的有限数据集来说，这变得很棘手——学习率查找器绘制了一条非常不稳定的曲线。然而，一般的经验法则是<em class="lh">1e–2</em>是一个很好的起点，然后人们可以尝试用不同的值来训练模型，但是使用前面提到的值作为参考点。</p><p id="df5a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">此外，当在fast.ai中使用预训练模型时，它将冻结网络中三分之二的层，这实质上意味着它们在训练期间不会更新。人们可以解冻这些，以便将模型专门化到自己的数据集，并可能获得更好的学习率曲线。</p><pre class="kg kh ki kj fd kx ky kz la aw lb bi"><span id="903c" class="lc jd hh ky b fi ld le l lf lg">learn.unfreeze()</span></pre><p id="cd5d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这也可能导致过度拟合，本质上是<em class="lh">训练损失</em> &lt; <em class="lh">验证损失</em>，这就是辍学的原因。经验法则:当一个模型过度拟合时，增加辍学率，甚至尝试差分辍学(与<a class="ae kv" href="https://towardsdatascience.com/transfer-learning-using-differential-learning-rates-638455797f00" rel="noopener" target="_blank">差分学习率</a>原理相同)。</p><p id="43cd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">最后，保存模型，以便可用于简介中图像的fast.ai脚本:</p><pre class="kg kh ki kj fd kx ky kz la aw lb bi"><span id="799d" class="lc jd hh ky b fi ld le l lf lg">learn.save("Resnet34_multiclass")</span></pre><h1 id="1595" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">结论</h1><p id="a486" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">来自MultiClassClassification的第一个模型Resnet34获得了大约82.7%的准确度(在大约1000张图像上)。标签)。这是一个公平的结果，考虑到数据(你试过在谷歌图片上转到第20页以上吗？)、数据量以及超参数调整是手动完成的事实。在一个完美的世界中，每个类将有超过10k个图像，而不必删除它们中的任何一个并通过网格搜索或随机搜索来调整超参数。</p><p id="2bc0" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">也就是说，为了让机器人“能够看见”，82.7%是一个令人满意的数字，这也是第二部分将重点关注的内容——使用训练和保存的模型对网络摄像头传输的图像进行分类。</p></div></div>    
</body>
</html>