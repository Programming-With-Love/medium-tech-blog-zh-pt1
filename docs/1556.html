<html>
<head>
<title>The NLP Playbook, Part 1: Deep Dive into Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP剧本，第1部分:深入研究文本分类</h1>
<blockquote>原文：<a href="https://medium.com/capital-one-tech/the-nlp-playbook-part-1-deep-dive-into-text-classification-61503144de78?source=collection_archive---------0-----------------------#2018-05-02">https://medium.com/capital-one-tech/the-nlp-playbook-part-1-deep-dive-into-text-classification-61503144de78?source=collection_archive---------0-----------------------#2018-05-02</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es hf"><img src="../Images/fd1f98fd41757afd647c04c893657a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlpfdH5fh7o7qjuxU3RXuw.jpeg"/></div></div></figure><div class=""/><p id="66a1" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">Mackenzie Sweeney和Bayan Bruss，机器学习工程师，Capital One </p><h1 id="00ab" class="jp jq hs bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">介绍</h1><p id="a502" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">Capital One在2016年成立了机器学习中心(C4ML)，以促进机器学习在整个业务中的采用；在这种情况下，我们的团队成为机器学习产品交付、创新、教育和合作伙伴关系的内部、企业范围的咨询和卓越中心。随着C4ML的迅速扩展，我们已经解决了各种领域的问题，总是寻求平衡使用最先进的技术和对开发速度、可伸缩性和可解释性的实际关注。C4ML经常运作的一个领域是自然语言处理(NLP)。去年，我们开始提炼最新的文献，向我们在C4ML的同事介绍各种NLP问题的最佳技术，我们希望通过这里的一系列博客帖子将我们的讨论扩展到更广泛的ML实践者社区。虽然结构可能会有所不同，但我们在每篇文章中的目标是提供一些NLP问题类别中最佳模型的整洁比较。与此同时，我们将讨论Capital One为使用和推进这些技术正在进行的一些工作。</p><p id="2bd3" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在这篇文章中，我们从自然语言处理中的一个典型问题开始:文本分类。最简单地说，这需要训练一个模型来学习区分许多已知类别的语言模式。这可以是两类(二进制)或更多类(多类)。对于分类的一个很好的介绍，请查看<a class="ae ks" rel="noopener" href="/simple-ai/classification-versus-regression-intro-to-machine-learning-5-5566efd4cb83">这篇博客</a>由大卫·福莫发表的文章。</p><p id="98e9" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">下表提供了Capital One文本分类的一些重要商业应用示例。</p><figure class="ku kv kw kx fd hj er es paragraph-image"><a href="https://devblogs.nvidia.com/malware-detection-neural-networks/"><div class="er es kt"><img src="../Images/ba68d2f54d244049c79a1a4c35095f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-P9CgKcd8kf781dXZeZDg.jpeg"/></div></a></figure><p id="94b2" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">具体来说，我们将关注情感分析和主题分类。我们将首先解释表述这些问题的各种方法。为了让您能够为这些任务开发自己的强大系统，我们将提供一个决策树来帮助您在近年来涌现的四种不同方法中进行选择。</p><h1 id="bc06" class="jp jq hs bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">应用程序</h1><h2 id="b405" class="ky jq hs bd jr kz la lb jv lc ld le jz jb lf lg kd jf lh li kh jj lj lk kl ll bi translated">情感分析和主题分类</h2><p id="3f27" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">情感分析试图确定文本的情感。根据你对“情感”和“文本”的工作定义，你可以用几种方式来表述这个问题</p><p id="98fa" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">情感可以是二元的、分类的、顺序的或连续的。当建模为连续时，情绪通常被称为“极性”，类似于正负电荷。下图说明了这些不同的选项，并提供了每个选项的示例。</p><figure class="ku kv kw kx fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lm"><img src="../Images/e3d35e70bb5c8edfac360d1140dd4e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P5mOEUJ_h4rahnvPQcgirA.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Sentiment Modeling Options</figcaption></figure><p id="6a0c" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">“文本”的定义是你所关注的写作水平:文档、句子、实体或方面水平。最后两个最好用例子来解释。想象一下对智能手机的回顾，讨论它的优点和缺点。评论者可以讨论相机的正面和负面。也许它的分辨率很高，但变焦距离有限。在这个例子中，手机的相机是实体，人们对相机的多个方面——分辨率和变焦距离——表达了情感。虽然这种情感理解水平通常是理想的，但准确建模也是最具挑战性的。完成这一挑战性任务的一种方法是将文档级情感分析与另一种称为主题分类的分类方法相结合。</p><h2 id="28df" class="ky jq hs bd jr kz la lb jv lc ld le jz jb lf lg kd jf lh li kh jj lj lk kl ll bi translated">最先进方法入门</h2><p id="69a6" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">您可能想知道从哪里开始解决您自己的NLP问题。Capital One中的机器学习项目通常从对最先进解决方案的文献综述开始；然而，学术文献的实验很少与具体的商业问题相匹配。我们必须从既定的一流方法转化为针对我们特定用例的定制解决方案。下面的诊断树代表了我们的典型翻译过程，我们提供该过程来帮助您开始您自己的翻译过程。它只需要了解您的数据特征和您自己的时间限制。本节的其余部分将讨论此诊断树中列出的方法，以便您可以快速开始应用您认为最适合您的问题的方法。</p><figure class="ku kv kw kx fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lr"><img src="../Images/f21523004bc1388802c98335a2770592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nd4PjxdElJk3EEFfJBfgow.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Which Model to Use? Your Decision Tree</figcaption></figure><p id="c0a3" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">让我们先从最简单的模型开始。2016年，Joulin等人提供了经验证据，证明对于更小、更简单的数据集，在术语频率-反向文档频率(TFIDF)矩阵上训练的逻辑回归模型表现与最佳神经网络模型一样好；它的训练速度也快了一个数量级，实现起来也简单得多。虽然随着数据量的增加，性能会急剧下降，但这个模型仍然是一个很好的基线，任何更复杂的模型都应该超越它。有很多很好的资源可以解释<a class="ae ks" rel="noopener" href="/@acrosson/summarize-documents-using-tf-idf-bdee8f60b71">什么是TFIDF</a>、<a class="ae ks" href="http://blog.yhat.com/posts/logistic-regression-python-rodeo.html" rel="noopener ugc nofollow" target="_blank">逻辑回归是如何工作的</a>、<a class="ae ks" href="http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/" rel="noopener ugc nofollow" target="_blank">如何将它们用于文本分类</a>。</p><p id="0b80" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">在过去十年左右的时间里，深度学习模型在NLP的几乎每个任务上都表现出了领先的性能。所有这些模型的起点都有一个共同的任务:将文本表示嵌入到神经网络可以使用的向量中。大多数技术通过<a class="ae ks" href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization" rel="noopener ugc nofollow" target="_blank">将输入单元分解成密集的嵌入矩阵</a>来实现这一点。当这些单元是单词时，这种方法很好地处理了由大多数自然语言词汇的庞大规模所导致的维数灾难。</p><h2 id="1ed8" class="ky jq hs bd jr kz la lb jv lc ld le jz jb lf lg kd jf lh li kh jj lj lk kl ll bi translated">字符CNN</h2><p id="5977" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">虽然这种因式分解方法在字符级仍然有用，但最近的研究表明，对于良好的性能来说，这并不是必需的。在字符级工作还有其他好处:它消除了对预处理的需要，可以处理词汇表之外的单词，并且是一种语言无关的方法。例如，张的角色CNN (2015)跳过了因式分解步骤，使用量化程序嵌入文本——这是第一个公开的这样做的方法。下面的GIF演示了这是如何工作的。</p><figure class="ku kv kw kx fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ls"><img src="../Images/d154f0a78c1d229bd664e6176e7f8a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*L3NkoodUg7RL8LPejaPSyw.gif"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Character Quantization</figcaption></figure><figure class="ku kv kw kx fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lt"><img src="../Images/ef56b6cb33238011f1a4c6aba924aae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ex23TSDq8hvL36pwpSCR5w.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Character CNN Architecture (reference 1)</figcaption></figure><p id="9bfa" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">字符CNN由以下阶段组成:</p><p id="8eff" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">1.选择您的字符集并执行字符量化。</p><p id="dc6a" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">2.为您的文本选择一个合适的长度，并将所有输入内容截断/填充到该长度。张等人选择了1014，他说“看起来1014个字符已经可以捕获大部分感兴趣的文本。”</p><p id="ec2c" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">3.通过6个卷积层馈送结果向量，最大池在之间。池层的大小为3，并且不重叠。在前2层使用大小为7的内核，然后在最后4层使用大小为3的内核。</p><p id="2d01" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">4.通过3个全连接层馈送卷积模块的输出，每个层之间有50/50的压差。</p><p id="88e9" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">5.通过您的分类图层输入最终输出。</p><h2 id="7e39" class="ky jq hs bd jr kz la lb jv lc ld le jz jb lf lg kd jf lh li kh jj lj lk kl ll bi translated">字符CRNN</h2><p id="3bf0" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">Xiao和Cho的字符级CRNN (2016)将张模型中的一些卷积层替换为递归层，并在嵌入层中添加回。他们声称这可以更有效地捕捉文本序列中的冗长依赖关系。这种方法将所需的参数减少了1.35到90倍，并且似乎提高了小数据集的性能。该模型中使用的循环层使用长短期记忆(LSTM)单位。</p><p id="502f" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">字符CRNN由以下阶段组成:</p><p id="b962" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">1.一键编码字符，保留您选择的字符集。</p><p id="197c" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">2.将编码字符嵌入到大小为8的密集实值向量中。</p><p id="bcfe" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">3.应用2-3个卷积层；在最后一层使用3的内核大小，在前面的层使用5的内核大小。在大小为3的卷积之间使用最大池。</p><p id="965a" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">4.施加50/50的压差，然后通过尺寸为128的双向LSTM馈电；再次应用50/50退出。</p><p id="d960" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">5.取两个方向的最后一个隐藏状态，连接起来形成一个二维向量。把它输入你的分类层。</p><figure class="ku kv kw kx fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lu"><img src="../Images/e0a3af0c04bafc80002a2d82f68ae3f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZnNyO3vRVco0ir_SPq42ug.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Character CRNN Architecture (reference 2)</figcaption></figure><h2 id="1056" class="ky jq hs bd jr kz la lb jv lc ld le jz jb lf lg kd jf lh li kh jj lj lk kl ll bi translated">非常深入的CNN</h2><p id="ad95" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">Conneau等人(2017)也受到张工作的启发，在字符层面构建了一个文本分类模型。像肖和赵一样，他们认为难以捕捉文本中的长期依存关系是张的字符CNN的一个限制，但他们采取了一种非常不同的方法来解决这个问题，认为“LSTMs是用于序列处理的通用学习机器，缺乏特定于任务的结构……文本具有相似的属性:字符组合形成n-grams、词干、单词、短语、句子等。”</p><p id="4b88" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">受用最新和最伟大的从业者技巧(Simonyan和Zisserman，2014年，he等人，2016年)连接和训练的大量卷积的成功应用的启发，Conneau和他的合作者设计了他们自己的用于文本分类的“非常深度的CNN”(VD CNN)。这个巨大的模型使用29个卷积层(比张的模型增加了5倍)和分解的字符级嵌入来征服其他层。通过堆叠许多具有较小(大小为3)内核的卷积，他们声称他们的网络可以自行学习将这些“3-gram特征”组合成新的分层结构特征的最佳方式。他们的实验表明，他们的方法在大型数据集上确实优于字符级CNN和CRNN，但在较小的数据集上表现不佳。再多层还是买不到免费的午餐。</p><h2 id="a9b6" class="ky jq hs bd jr kz la lb jv lc ld le jz jb lf lg kd jf lh li kh jj lj lk kl ll bi translated">快速文本(Joulin等人，2016年)</h2><p id="3852" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">2016年，脸书人工智能研究院发布了一个名为<a class="ae ks" href="https://github.com/facebookresearch/fastText" rel="noopener ugc nofollow" target="_blank"> FastText </a>的模型。FastText利用许多已知的性能增强技术来获得接近最先进的精度，与上面讨论的其他方法相比，在训练速度上有显著的提高。FastText的核心依赖于Mikolov的Word2Vec论文(2013)中引入的连续词袋(CBOW)模型。在最初的CBOW模型中，一个句子中的几个单词被传递到一个单层前馈神经网络中，该模型试图预测应该在这些单词中间的单词。FastText用预测类别取代了预测单词的目标。这些单层模型的训练速度非常快，伸缩性也非常好。</p><figure class="ku kv kw kx fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lv"><img src="../Images/03f09c2c82ea065bd00eb288dd848800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sfpY0f6i05jTHj68eNcAiw.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">FastText Model Architecture (reference 4)</figcaption></figure><p id="391e" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">除了将CBOW模型重新用于文本分类任务之外，作者还使用了许多“技巧”来提高速度和准确性。两个主要的技巧是用一个<a class="ae ks" href="http://ruder.io/word-embeddings-softmax/index.html#hierarchicalsoftmax" rel="noopener ugc nofollow" target="_blank">层次化的softmax </a>替换类别上的softmax，以及结合降维<a class="ae ks" href="https://www.quora.com/Can-you-explain-feature-hashing-in-an-easily-understandable-way/answer/Luis-Argerich?share=03fb46a5&amp;srid=kpBo" rel="noopener ugc nofollow" target="_blank">散列技巧</a>使用n-gram特征。</p><p id="b185" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">FastText在许多其他任务上也表现得非常好。我们将在接下来的文章中讨论这些。我们通常对FastText背后的简单性和良好的工程精神非常满意。正如伍迪·格思里曾经说过的，“任何傻瓜都能把事情变得复杂。只有天才才能让它变得简单。”</p><h1 id="d793" class="jp jq hs bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">构建决策树</h1><p id="917d" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">那么，我们如何理解这四种方法来构建我们的决策树呢？张等人收集、整理并发布了一组基准数据集，对文本分类方法进行了有效的比较。用他们的话说:</p><p id="bc21" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">“…大多数用于文本分类的开放数据集都非常小…因此，我们没有使用它们来混淆我们的社区，而是为我们的实验建立了几个大规模的数据集，从几十万到几百万个样本不等。”</p><p id="9d21" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">所有四种方法的作者都使用这些数据集来评估它们的性能，从而能够进行简单的比较。下表提供了每个数据集的一些一般特征。</p><figure class="ku kv kw kx fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lw"><img src="../Images/01b01e9a7784ae65f09fba071718803b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XtFxD_-ONPTPlNB0TJm44Q.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Datasets (reference 1)</figcaption></figure><p id="e3d3" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">虽然所有模型都使用这些数据集，但FastText是最后发布的，它的比较包括这里讨论的所有其他方法。所以我们包括了FastText论文的结果，如下所示。</p><figure class="ku kv kw kx fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es lx"><img src="../Images/e87d8626b05aeb320a9be64b9c780e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ijy1bh8uKbymBtiLki67-w.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Accuracy (reference 4)</figcaption></figure><p id="e65c" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">FastText在文本分类任务中表现非常好。从这些结果中可以看出，在较小的数据集上，它始终优于字符CNN和CRNN。在较大的数据集上，VDCNN优于FastText。然而，决策并不像“对小数据集使用FastText，对大数据集使用VDCNN”那么简单。VDCNN是一个复杂得多的模型，这种复杂性带来了培训成本。下表显示了字符CNN、VDCNN和FastText之间的速度比较。虽然字符CNN可能需要几天，而VDCNN可能需要几个小时，但FastText即使在最大的数据集上也只需几秒钟。</p><figure class="ku kv kw kx fd hj er es paragraph-image"><div role="button" tabindex="0" class="hk hl di hm bf hn"><div class="er es ca"><img src="../Images/8d03986aaddb4509ec84ba4589b49830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P3W12IOtw3swzF7D0RI__Q.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx">Speed Comparison (reference 4)</figcaption></figure><p id="07ff" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">最后，我们必须记住，FastText是在单词级别操作的，而其他方法都是在字符级别操作的。虽然FastText的最新版本采用子词“语素”(Bojanowski，2016年)来处理词汇外的词，但它仍然不是语言不可知的。特别是，FastText的单词嵌入要求用户使用特定于语言的预处理来处理经常有噪声的数据。</p><h1 id="c4b8" class="jp jq hs bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论</h1><p id="79ee" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">我们希望这篇文章给了你一个有用的剧本来解决你自己的文本分类问题。虽然我们不认为这些方法中的任何一种已经“解决”了这些问题，但我们确实相信，将它们放在您的工具包中，并知道如何以及何时应用它们，将有助于您为自己的业务问题设计出有效的解决方案。在以后的文章中，我们计划为其他NLP问题提供类似的工具包和剧本。</p><h1 id="f382" class="jp jq hs bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">参考</h1><p id="a486" class="pw-post-body-paragraph iq ir hs is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ha bi translated">1.张、项、赵军波、杨乐存。2015."用于文本分类的字符级卷积网络."神经信息处理系统进展，649–657。</p><p id="442c" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">2.肖，怡君和赵京贤。2016."结合卷积和递归层的有效字符级文档分类."arXiv:1602.00367 [Cs]，一月。<a class="ae ks" href="http://arxiv.org/abs/1602.00367." rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1602.00367.</a></p><p id="33a2" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">3.Conneau、Alexis、Holger Schwenk、loc Barrault和Yann Lecun。2017."用于文本分类的非常深的卷积网络."计算语言学协会欧洲分会第15届会议论文集:第1卷，长论文，1:1107–1116。</p><p id="eb64" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">4.朱林、阿曼德、爱德华·格雷夫、皮奥特·博亚诺夫斯基和托马斯·米科洛夫。2016."高效文本分类的锦囊妙计."arXiv:1607.01759 [Cs]，七月。http://arxiv.org/abs/1607.01759.<a class="ae ks" href="http://arxiv.org/abs/1607.01759." rel="noopener ugc nofollow" target="_blank"/></p><p id="4a33" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">5.何、、、任、。2016."深层剩余网络中的身份映射."arXiv:1603.05027 [Cs]，三月。【http://arxiv.org/abs/1603.05027. T4】</p><p id="3c79" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">6.西蒙扬，凯伦和安德鲁·齐泽曼。2014."用于大规模图像识别的非常深的卷积网络."arXiv:1409.1556 [Cs]，九月。<a class="ae ks" href="http://arxiv.org/abs/1409.1556." rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1409.1556.</a></p><p id="5a94" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">7.米科洛夫、托马斯、伊利亚·苏茨基弗、程凯、格雷戈·科拉多和杰弗里·迪安。2013."单词和短语的分布式表示及其组合性."arXiv:1310.4546 [Cs，Stat]，10月。<a class="ae ks" href="http://arxiv.org/abs/1310.4546." rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1310.4546.</a></p><p id="7513" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">8.博雅诺斯基，皮奥特，爱德华·格雷夫，阿曼德·朱林和托马斯·米科洛夫。2016."用子词信息丰富词向量."arXiv:1607.04606 [Cs]，七月。<a class="ae ks" href="http://arxiv.org/abs/1607.04606." rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1607.04606.</a></p><p id="99fc" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">9.本吉奥、约舒阿、雷让·杜查姆、帕斯卡尔·文森特和克里斯蒂安·贾乌文。2003."一个神经概率语言模型."机器学习研究杂志3(二月):1137–1155。</p><p id="5028" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">10.布雷，大卫m，安德鲁Y. Ng，迈克尔乔丹。2003."潜在的狄利克雷分配."机器学习研究杂志3(1月):993–1022。</p><p id="22d7" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">11.加尔，亚林和邹斌·格拉马尼。2015."作为贝叶斯近似的辍学:表示深度学习中的模型不确定性."arXiv:1506.02142 [Cs，Stat]，6月。<a class="ae ks" href="http://arxiv.org/abs/1506.02142." rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1506.02142.</a></p></div><div class="ab cl ly lz go ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ha hb hc hd he"><p id="3de5" class="pw-post-body-paragraph iq ir hs is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn ha bi translated">这些是作者的观点。除非本帖中另有说明，否则Capital One不属于所提及的任何公司，也不被其认可。使用或展示的所有商标和其他知识产权都是其各自所有者的所有权。本文为2018首都一。</p></div></div>    
</body>
</html>