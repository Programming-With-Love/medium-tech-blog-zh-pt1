<html>
<head>
<title>Creating your first spider — 01 — Python scrapy tutorial for beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为初学者创建您的第一个spider-01-Python scrapy教程</h1>
<blockquote>原文：<a href="https://medium.com/quick-code/python-scrapy-tutorial-for-beginners-01-creating-your-first-spider-13b1297886a2?source=collection_archive---------1-----------------------#2019-09-01">https://medium.com/quick-code/python-scrapy-tutorial-for-beginners-01-creating-your-first-spider-13b1297886a2?source=collection_archive---------1-----------------------#2019-09-01</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/d3c654d1f930b89b5612f4ed550f1d11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YzmdfUoqxdxM-8D2V2BC7A.png"/></div></div></figure><p id="2f8f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">了解如何使用Python和Scrapy框架在几分钟内获取任何网站的数据。在“Python scrapy初学者教程”的第一课，我们将从一个<a class="ae jn" href="http://books.toscrape.com" rel="noopener ugc nofollow" target="_blank">书店</a>中抓取数据，提取所有信息并存储在一个文件中。</p><p id="6286" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这篇文章中，你将学到:</p><ul class=""><li id="8a13" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">准备您的环境并安装一切</li><li id="4058" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">如何创建一个Scrapy项目和蜘蛛</li><li id="6826" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">如何从HTML中获取数据</li><li id="4d77" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">操作数据并提取您想要的数据</li><li id="d6b1" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">如何将数据存储到. json中。csv和。xml文件</li></ul><figure class="kc kd ke kf fd ii"><div class="bz dy l di"><div class="kg kh l"/></div></figure><blockquote class="ki kj kk"><p id="45e9" class="ip iq kl ir b is it iu iv iw ix iy iz km jb jc jd kn jf jg jh ko jj jk jl jm ha bi translated">本文的视频版本</p></blockquote><h1 id="a5dd" class="kp kq hh bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">准备您的环境并安装一切</h1><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/980c63c8f40f5944bf08cc005d22a79f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*Yl4_LP5EAdSENx3a"/></div></figure><p id="db92" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在做任何事情之前，我们需要准备好我们的环境并安装好一切。</p><p id="0a2e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="kl">在Python中，我们创建虚拟环境来拥有一个具有不同依赖关系的独立环境。</em></p><p id="9e95" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="kl">比如Project1有Python 3.4和Scrapy 1.2，Project2有Python 3.7.4和Scrapy 1.7.3。</em></p><p id="2a73" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">由于我们保持独立的环境，一个项目一个环境，我们永远不会因为拥有不同版本的包而产生冲突。</p><p id="3576" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以使用<a class="ae jn" href="https://docs.conda.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Conda </a>、<a class="ae jn" href="https://virtualenv.pypa.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> virtualenv </a>或<a class="ae jn" href="https://github.com/pypa/pipenv" rel="noopener ugc nofollow" target="_blank"> Pipenv </a>来创建虚拟环境。在本课程中，我将使用pipenv。你只需要用<em class="kl"> pip install </em>安装它，并用<em class="kl"> pipenv shell </em>创建一个新的虚拟环境。</p><p id="1def" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一旦设置好，用<em class="kl"> pip安装scrapy </em>安装Scrapy。这就是你所需要的。</p><p id="9a94" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">是时候创建项目和你的蜘蛛了。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/8685f7e89d65a86e984437a73bacabe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*dhufNp-b2XkH7TBP"/></div></figure></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h1 id="4186" class="kp kq hh bd kr ks lv ku kv kw lw ky kz la lx lc ld le ly lg lh li lz lk ll lm bi translated">创建项目和蜘蛛——以及它们是什么</h1><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/14f5c3b72d0cf7ef186c8190d9207235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*VOybjWrsGKtF1E1w"/></div></figure><p id="63d7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在此之前，我们需要创建一个Scrapy项目。在当前文件夹中，输入:</p><pre class="kc kd ke kf fd ma mb mc md aw me bi"><span id="ab2e" class="mf kq hh mb b fi mg mh l mi mj">scrapy startproject books</span></pre><p id="b86d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这将创建一个名为“books”的项目。在里面你会找到一些文件。我将在更详细的帖子中解释它们。</p><p id="f405" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">创建项目后，导航到创建的项目(cd books ),进入文件夹后，通过传递名称和根URL(不带“www ”)创建一个蜘蛛:</p><pre class="kc kd ke kf fd ma mb mc md aw me bi"><span id="152e" class="mf kq hh mb b fi mg mh l mi mj">scrapy genspider spider books.toscrape.com</span></pre><p id="a06f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们有我们的蜘蛛在蜘蛛文件夹里！你会得到这样的东西:</p><figure class="kc kd ke kf fd ii"><div class="bz dy l di"><div class="mk kh l"/></div></figure><p id="6937" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，我们进口scrapy。然后，从Scrapy继承“Spider”创建了一个类。这个类有3个变量和一个方法。</p><p id="a937" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">变量是蜘蛛的名字，允许的域名和起始网址。不言自明。这个名字是我们马上要用来运行蜘蛛的，allowed_domains限制了抓取过程的范围(它不能超出这里没有指定的任何URL ), start _ URLs是scrapy蜘蛛的起点。在这种情况下，只有一个。</p><p id="7fa5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当我们启动Scrapy spider时，会在内部调用parse方法。现在只有“通过”:它什么也不做。让我们解决这个问题。</p></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h1 id="f12e" class="kp kq hh bd kr ks lv ku kv kw lw ky kz la lx lc ld le ly lg lh li lz lk ll lm bi translated">如何从HTML中获取数据</h1><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ml"><img src="../Images/fbc35a5ae90664816f7ec5993b5c5553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*pzxyJugsX1XLNtBL"/></div></figure><p id="97c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们将查询HTML，为此我们需要Xpath，一种查询语言。不要担心，即使一开始看起来很奇怪，也很容易学会，因为你需要的只是一些函数。</p></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h2 id="a9d0" class="mf kq hh bd kr mm mn mo kv mp mq mr kz ja ms mt ld je mu mv lh ji mw mx ll my bi translated">解析方法</h2><p id="8817" class="pw-post-body-paragraph ip iq hh ir b is mz iu iv iw na iy iz ja nb jc jd je nc jg jh ji nd jk jl jm ha bi translated">但是首先，让我们看看我们有什么关于“解析”方法。</p><p id="7a82" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">解析它在Scrapy spider启动时被自动调用。作为参数，我们有(类的实例)和一个<em class="kl">响应</em>。<em class="kl">响应</em>是当我们请求一个HTML时服务器返回的结果。在这个类中，我们请求h<em class="kl">TTP://books . toscrape . com</em>，在这个类中，我们有一个包含所有HTML、状态消息等等的对象。</p><p id="8e07" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将“pass”替换为<em class="kl">print(response . status)</em>并运行spider:</p><pre class="kc kd ke kf fd ma mb mc md aw me bi"><span id="b886" class="mf kq hh mb b fi mg mh l mi mj">scrapy crawl spider</span></pre><p id="c342" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这是我们得到的:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/ae40ce51436c7e9d7c6038c7f5ab33b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*RphDxPKRacappq-D"/></div></figure><p id="5021" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在很多信息之间，我们看到我们已经抓取了start_url，得到了200 HTTP消息(成功)然后蜘蛛停止了。</p><p id="61ae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">除了‘地位’，我们蜘蛛还有很多方法。我们现在要用的是“xpath”。</p></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h2 id="8eb4" class="mf kq hh bd kr mm mn mo kv mp mq mr kz ja ms mt ld je mu mv lh ji mw mx ll my bi translated">Xpath的第一步</h2><p id="6e8f" class="pw-post-body-paragraph ip iq hh ir b is mz iu iv iw na iy iz ja nb jc jd je nc jg jh ji nd jk jl jm ha bi translated">打开起始网址，<a class="ae jn" href="http://books.toscrape.com/" rel="noopener ugc nofollow" target="_blank">http://books.toscrape.com/</a>，右键- &gt;查看任意一本书。将打开一个带有网站HTML结构的侧菜单(如果没有，请确保您选择了“元素”选项卡)。你会看到这样的东西:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/47a9d1bb912043429d35fc42e14b9389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*0ZZMtEUNrryLJIwA"/></div></figure><p id="d768" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们可以看到每个“文章”标签包含了我们想要的所有信息。</p><p id="2440" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">计划是抓取所有的文章，然后一篇接一篇地从每本书中获取所有的信息。</p><p id="d8c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，让我们看看如何选择所有文章。</p><p id="bbd4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们点击HTML侧边菜单并按Control + F，搜索菜单打开:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/a153cf552220c96a98cbea508d699699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*y7mkhGIGThPtWHrp"/></div></figure><p id="84fa" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在右下角，你可以看到“通过字符串、选择器或Xpath查找”。Scrapy用的是Xpath，我们就用它吧。</p><p id="3390" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">要用Xpath开始一个查询，先写“//”，然后是您想要查找的内容。我们想抓取所有的文章，所以键入“//article”。我们希望更准确，所以我们抓取所有属性为' class = product_pod '的文章。若要指定属性，请在方括号中键入，如下所示:“//article[@ class = " product _ pod "]”。</p><p id="3cbc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你现在可以看到，我们已经选择了20个元素:20本初始书籍。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/da061b1c95b45585e9cf30d35ce8f630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*I7lTk9WfjIvUrADp"/></div></figure><p id="7411" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">好像我们成功了！让我们复制Xpath指令并使用它来选择我们蜘蛛中的文章。然后，我们储存所有的书。</p><figure class="kc kd ke kf fd ii"><div class="bz dy l di"><div class="mk kh l"/></div></figure></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h1 id="6b31" class="kp kq hh bd kr ks lv ku kv kw lw ky kz la lx lc ld le ly lg lh li lz lk ll lm bi translated">提取数据</h1><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/ee19363731bb03be91ec9a78d377c2e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/0*utyaSBoCHpzZwvHY"/></div></figure><p id="8c7b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一旦我们有了所有的书，我们想要在每本书里面寻找我们想要的信息。先说题目。请访问您的URL，搜索完整标题所在的位置。右键单击任何标题，然后选择“检查”。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/659a99ce75cd3b944867038fa10803d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/0*UMponeiH1hLbe8vC"/></div></figure><p id="efd3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在h3标签中，有一个“a”标签，将图书标题作为“title”属性。让我们把这些书翻一遍，然后摘录下来。</p><figure class="kc kd ke kf fd ii"><div class="bz dy l di"><div class="mk kh l"/></div></figure><p id="d155" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们得到所有的书，对于每一本书，我们搜索“h3”标签，然后搜索“a”标签，并选择@title属性。我们需要该文本，所以我们使用'<em class="kl"> extract_first 【T1 ')(我们也可以'使用extract '来提取所有文本)。</em></p><p id="e209" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">因为我们抓取的不是整个HTML，而是一个小的子集(book中的子集)，所以我们需要在Xpath函数的开头放一个点。记住:'//'代表整个HTML响应，'。//'作为我们已经提取HTML的子集。</p><p id="220f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们有标题，现在去的价格。右键点击价格并检查。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/a8173fa18f55c9983363c3c5f8b9c740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*e0F2QlYzuK3hjiti"/></div></figure><p id="6dcb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们想要的文本在一个“p”标记内，而“price_color”类在一个“div”标记内。在标题后添加以下内容:</p><pre class="kc kd ke kf fd ma mb mc md aw me bi"><span id="d655" class="mf kq hh mb b fi mg mh l mi mj">price = book.xpath('.//div/p[@class="price_color"]/text()').extract_first()</span></pre><p id="e4b9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们转到任何一个“div ”,带有一个包含“price_color”类的“p”子元素，然后我们使用“text()”函数来获取文本。然后，我们<em class="kl">提取_first() </em>我们的选择。</p><p id="4d50" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">让我们看看我们有什么？打印价格和标题，并运行蜘蛛。</p><figure class="kc kd ke kf fd ii"><div class="bz dy l di"><div class="mk kh l"/></div></figure><pre class="kc kd ke kf fd ma mb mc md aw me bi"><span id="b6db" class="mf kq hh mb b fi mg mh l mi mj">scrapy crawl spider</span></pre><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ng"><img src="../Images/b0297a5433072360bb85cdb55b251194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/0*FoSkFRNV3Ha6ZzA4"/></div></figure><p id="d084" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">一切都在按计划进行。让我们把图片的网址也带上。右键单击图像，检查它:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nf"><img src="../Images/18f5b3a771bb1401ba44f9ed579e7152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/0*QxDI_xMn4gR-eGNd"/></div></div></figure><p id="ad92" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们这里没有网址，只有一部分。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es nh"><img src="../Images/f3564f42708bb4617196321ebf5f1e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/0*5USAsGilQ6XMXRaB"/></div></figure><p id="8f71" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">“src”属性具有相对URL，而不是完整的URL。“books.toscrape.com”丢失。嗯，我们只需要添加它。把这个加到你方法的底部。</p><figure class="kc kd ke kf fd ii"><div class="bz dy l di"><div class="mk kh l"/></div></figure><p id="10bb" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们用类“thumbnail”获得“img”标签，用“src”获得相对URL，然后添加第一个(也是唯一的)start_url。同样，让我们打印结果。再次运行蜘蛛。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/89b56c6e2bdc05dd33b4bd4bcf50b8e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*CcaB-SpHkp3pATyl"/></div></figure><p id="a6c3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">看起来不错！打开任何一个网址，你都会看到封面的缩略图。</p><p id="89d9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在让我们提取网址，这样我们就可以购买任何感兴趣的书。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ni"><img src="../Images/5a4e6df69f48f1f97e7ed1c2c26993aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/0*dAeeNpm-kBKlQhfS"/></div></figure><p id="8c76" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">图书URL存储在标题和缩略图的href中。两者都可以。</p><figure class="kc kd ke kf fd ii"><div class="bz dy l di"><div class="mk kh l"/></div></figure><p id="6f06" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">再次运行蜘蛛:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/9f2bb0c6a2cd9dd095bfd2fabe5bf677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/0*5ptOBBGxoWnFOuu9"/></div></figure><p id="a7d3" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">点击任何一个网址，你就会进入那本书的网站。</p><p id="74c9" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们选择了我们想要的所有字段，但是我们没有对它做任何事情，对吗？我们需要“让出”(或“归还”)它们。对于每本书，我们将返回它的标题，价格，图像和图书的网址。</p><p id="cbc6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">删除所有的打印，并产生像字典一样的项目:</p><figure class="kc kd ke kf fd ii"><div class="bz dy l di"><div class="mk kh l"/></div></figure><p id="c668" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">运行蜘蛛程序，查看终端:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/4d19f50aca362cbc2763379be7817230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*E4molaBJsN9CD5P-"/></div></figure></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h1 id="49dd" class="kp kq hh bd kr ks lv ku kv kw lw ky kz la lx lc ld le ly lg lh li lz lk ll lm bi translated">将数据保存到文件中</h1><p id="8112" class="pw-post-body-paragraph ip iq hh ir b is mz iu iv iw na iy iz ja nb jc jd je nc jg jh ji nd jk jl jm ha bi translated">虽然在终端上看起来很酷，但是没有任何用处。我们为什么不把它存储到一个文件中，以便以后使用呢？</p><p id="1c28" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">当我们运行蜘蛛时，我们有可选的参数。其中之一是您想要存储的文件的名称。运行这个。</p><pre class="kc kd ke kf fd ma mb mc md aw me bi"><span id="3f98" class="mf kq hh mb b fi mg mh l mi mj">scrapy crawl spider -o books.json</span></pre><p id="e062" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">等到完成…一个新的文件出现了！双击它将其打开。</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/c4170bde7af114853f36a3663f85b781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*86NSAVghxeJb0GPu"/></div></figure><p id="41fc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们在终端上看到的所有信息现在都存储在一个“books.json”中。是不是很酷？我们也可以这样做。csv和。xml文件:</p><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es ln"><img src="../Images/c320ef7e01323a12936acf22c46570ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*F07upsH1B8dLdEGy"/></div></figure><figure class="kc kd ke kf fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/96aedab82b3f9cfe93cd54ed10da6733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/0*O2nuOGMsWXCUZT7R"/></div></figure></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><h1 id="44ca" class="kp kq hh bd kr ks lv ku kv kw lw ky kz la lx lc ld le ly lg lh li lz lk ll lm bi translated">结论</h1><p id="2e4f" class="pw-post-body-paragraph ip iq hh ir b is mz iu iv iw na iy iz ja nb jc jd je nc jg jh ji nd jk jl jm ha bi translated">我知道第一次很棘手，但你已经学到了Scrapy的基本知识。您知道如何:</p><ul class=""><li id="1b9d" class="jo jp hh ir b is it iw ix ja jq je jr ji js jm jt ju jv jw bi translated">创建一个Scrapy蜘蛛导航一个网址</li><li id="9e13" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">一个杂乱的项目是结构化的</li><li id="228d" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">使用Xpath提取数据</li><li id="42e7" class="jo jp hh ir b is jx iw jy ja jz je ka ji kb jm jt ju jv jw bi translated">将数据存储在中。json，。csv和。xml文件</li></ul><p id="9fae" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">建议你继续训练。寻找一个你想要抓取的URL，尝试提取几个字段，就像你在<a class="ae jn" href="https://letslearnabout.net/python/beautiful-soup/your-first-web-scraping-script-with-python-beautiful-soup/" rel="noopener ugc nofollow" target="_blank">美汤教程</a>中所做的那样。Scrapy的诀窍是学习Xpath是如何工作的。</p><p id="0e35" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">但是…你记得每本书都有一个像这样的URL<a class="ae jn" href="http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html" rel="noopener ugc nofollow" target="_blank"/>吗？</p><p id="09b8" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们收集的每一件物品中，我们可以获取更多的信息。我们将在本系列的<a class="ae jn" href="https://letslearnabout.net/tutorial/scrapy-tutorial/python-scrapy-tutorial-for-beginners-02-extract-all-the-data/" rel="noopener ugc nofollow" target="_blank">第二课</a>中进行讲解。</p></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><p id="fac4" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae jn" href="https://github.com/david1707/scrapy_tutorial/tree/01_lesson" rel="noopener ugc nofollow" target="_blank">Github上的最终代码</a></p><p id="cb11" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae jn" href="https://twitter.com/DavidMM1707" rel="noopener ugc nofollow" target="_blank">在Twitter上联系我</a></p><p id="6223" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae jn" href="https://www.youtube.com/channel/UC9OLm6YFRzr4yjlw4xNWYvg?sub_confirmation=1" rel="noopener ugc nofollow" target="_blank">我的Youtube教程视频</a></p><p id="389a" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><a class="ae jn" href="https://letslearnabout.net/python/beautiful-soup/your-first-web-scraping-script-with-python-beautiful-soup/" rel="noopener ugc nofollow" target="_blank">你的第一个用Python和美汤编写的网页抓取脚本</a></p></div><div class="ab cl lo lp go lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="ha hb hc hd he"><p id="6f79" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><em class="kl">原载于2019年9月1日</em><a class="ae jn" href="https://letslearnabout.net/tutorial/scrapy-tutorial/python-scrapy-tutorial-for-beginners-01-creating-your-first-spider/" rel="noopener ugc nofollow" target="_blank"><em class="kl">https://letslearnabout.net</em></a><em class="kl">。</em></p></div></div>    
</body>
</html>