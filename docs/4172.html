<html>
<head>
<title>Exploring Firebase MLKit on Android: Face Detection (Part Two)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索Android上的Firebase MLKit:人脸检测(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/google-developer-experts/exploring-firebase-mlkit-on-android-face-detection-part-two-de7e307c52e0?source=collection_archive---------1-----------------------#2018-05-31">https://medium.com/google-developer-experts/exploring-firebase-mlkit-on-android-face-detection-part-two-de7e307c52e0?source=collection_archive---------1-----------------------#2018-05-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/e41f2854e9debac82686bf4bedad1a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IPlWWwb7lHVCRv2U8gE4HQ.png"/></div></div></figure><p id="2564" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在今年的Google I/O上，我们看到了Firebase MLKit的推出，这是Firebase套件的一部分，旨在让我们的应用程序能够更轻松地支持智能功能。随之而来的是面部识别功能，让我们能够识别面部以及这些面部的“标志”(鼻子、眼睛等)和表情。在这篇文章中，我想深入探讨如何在我们的应用程序中实现这个特性。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="8aea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在我们的应用程序中有很多情况下，我们可能希望利用人脸检测。也许你想进行面部验证，给照片加标签或者给相机添加滤镜——有很多可能性。现在这一功能已经成为MLKit库的一部分，将它添加到我们的应用程序中不会有很多障碍。说到面部识别，MLKit允许我们检测以下特征:</p><ul class=""><li id="e2f9" class="ju jv hh ir b is it iw ix ja jw je jx ji jy jm jz ka kb kc bi translated">鼻子底部的坐标</li><li id="01c8" class="ju jv hh ir b is kd iw ke ja kf je kg ji kh jm jz ka kb kc bi translated">嘴右侧的坐标</li><li id="5989" class="ju jv hh ir b is kd iw ke ja kf je kg ji kh jm jz ka kb kc bi translated">嘴左侧的坐标</li><li id="760e" class="ju jv hh ir b is kd iw ke ja kf je kg ji kh jm jz ka kb kc bi translated">嘴底部的坐标</li><li id="5ea1" class="ju jv hh ir b is kd iw ke ja kf je kg ji kh jm jz ka kb kc bi translated">脸在微笑的概率</li><li id="2c06" class="ju jv hh ir b is kd iw ke ja kf je kg ji kh jm jz ka kb kc bi translated">右眼睁开的概率</li><li id="1ab4" class="ju jv hh ir b is kd iw ke ja kf je kg ji kh jm jz ka kb kc bi translated">左眼睁开的概率</li><li id="4b78" class="ju jv hh ir b is kd iw ke ja kf je kg ji kh jm jz ka kb kc bi translated">检测到的人脸的边界</li><li id="a1fc" class="ju jv hh ir b is kd iw ke ja kf je kg ji kh jm jz ka kb kc bi translated">被检测面的旋转角度</li><li id="3354" class="ju jv hh ir b is kd iw ke ja kf je kg ji kh jm jz ka kb kc bi translated">被检测面部的倾斜角度</li></ul><p id="1d2c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">哇，这是一个非常好的数据列表，我们可以检索检测到的人脸！现在，需要注意的是，MLKit的人脸识别功能<strong class="ir hi">仅作为设备上识别功能</strong>提供，您无法在云端执行人脸识别。就用户的个人数据而言，这可能是一件好事，但也很重要，因为它降低了实现它的复杂性。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="1c0d" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">MLKit人脸识别包含三个核心概念:</p><figure class="kj kk kl km fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ki"><img src="../Images/0d4e0daaa0ea549dac4d6f565feeabed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pGTmmFuys35d7rZj7LNCIg.png"/></div></div></figure><p id="a072" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">首先，<strong class="ir hi">面部跟踪</strong>是基于面部特征跟踪面部的概念。如果识别出一张人脸，那么它可以跨图像(甚至视频帧)再次被识别。这本身不是面部识别，而是识别中的一个概念，允许我们在媒体的一些过程中跟踪面部。</p><p id="7dcc" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">还有不同的<strong class="ir hi">地标</strong>点组成一张脸。这些东西包括单眼(无论是左眼还是右眼)、右脸颊、嘴巴的左侧——所有这些都是已识别人脸上的标志，所有这些都可以使用MLKit进行识别。可以检测到的界标取决于面部的欧拉Y角(当面部转向左侧时为正Y角，当面部转向右侧时为负Y角)。基于这些值，我们可以检测到以下内容:</p><figure class="kj kk kl km fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kn"><img src="../Images/5773107854ada9bb922d62431a2f6c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g6o1tmKRuoByGaF0WsW3gw.png"/></div></div></figure><p id="ae1b" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">您可以看到，当头部从一侧转到另一侧时，可用于检测的特征会发生变化。这一点值得记住——重要的是欧拉角可以从已识别的人脸中获得，因此可以提示用户将头转向所需的方向，以改善识别过程。<strong class="ir hi">注:</strong>这些欧拉角只有在探测器处于<strong class="ir hi">精确</strong>模式下才可用。</p><p id="1e76" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">最后，<strong class="ir hi">分类</strong>是分析一个特定特征是否存在于一张脸上的概念。例如，在MLKit的情况下，我们可以检查面部微笑的概率。这一分类过程只适用于正面人脸，即具有小欧拉Y角的人脸。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="ded7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在开始使用MLKit的人脸识别功能之前，我们需要先将依赖项添加到我们的项目级build.gradle文件中:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="eca2" class="kt ku hh kp b fi kv kw l kx ky">implementation 'com.google.firebase:firebase-ml-vision:16.0.0'</span></pre><p id="047f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，如果您希望在应用程序安装时下载MLKit的人脸识别部分，那么您可以在清单文件的应用程序标记中添加以下代码片段。否则，MLKit库的人脸识别部分将在您的应用程序中需要时下载。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="e844" class="kt ku hh kp b fi kv kw l kx ky">&lt;meta-data<br/>      android:name="com.google.firebase.ml.vision.DEPENDENCIES"<br/>      android:value="face" /&gt;</span></pre><p id="f8a1" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在我们已经完成了上面的工作，我们准备将人脸识别添加到我们的应用程序中。说到面部识别，我们可以为识别过程配置一些不同的选项。这是通过一个<a class="ae kz" href="https://firebase.google.com/docs/reference/android/com/google/firebase/ml/vision/face/FirebaseVisionFaceDetectorOptions.Builder" rel="noopener ugc nofollow" target="_blank">firebasevisionfacededetectoroptions</a>实例来完成的。我们可以使用类生成器创建一个新的实例:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="323d" class="kt ku hh kp b fi kv kw l kx ky">val options = FirebaseVisionFaceDetectorOptions.Builder()</span></pre><p id="70ea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后可以用一组不同的属性进行配置:</p><ul class=""><li id="0ee7" class="ju jv hh ir b is it iw ix ja jw je jx ji jy jm jz ka kb kc bi translated"><strong class="ir hi">检测模式</strong>——用于说明识别过程应该偏向速度还是准确度，可以设置为<strong class="ir hi">ACCURATE _ MODE</strong>T12】或T14】FAST _ MODE。这默认为FAST_MODE。</li></ul><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="58d1" class="kt ku hh kp b fi kv kw l kx ky">.setModeType(FirebaseVisionFaceDetectorOptions.<em class="la">ACCURATE_MODE</em>)</span><span id="a086" class="kt ku hh kp b fi lb kw l kx ky">.setModeType(FirebaseVisionFaceDetectorOptions.<em class="la">FAST_MODE</em>)</span></pre><ul class=""><li id="2bab" class="ju jv hh ir b is it iw ix ja jw je jx ji jy jm jz ka kb kc bi translated"><strong class="ir hi">界标检测</strong> —用于声明识别过程是否应识别面部界标，如鼻子、眼睛、嘴巴等。这默认为NO_LANDMARKS <em class="la">。</em></li></ul><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="0497" class="kt ku hh kp b fi kv kw l kx ky">.setLandmarkType(FirebaseVisionFaceDetectorOptions.<em class="la">ALL_LANDMARKS</em>)</span><span id="18cb" class="kt ku hh kp b fi lb kw l kx ky">.setLandmarkType(FirebaseVisionFaceDetectorOptions.NO<em class="la">_LANDMARKS</em>)</span></pre><ul class=""><li id="773d" class="ju jv hh ir b is it iw ix ja jw je jx ji jy jm jz ka kb kc bi translated"><strong class="ir hi">特征分类</strong> —用于声明识别过程是否应该对面部特征进行分类，例如面部是否在微笑或眼睛是否睁开。这默认为NO_CLASSIFICATIONS。</li></ul><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="8c70" class="kt ku hh kp b fi kv kw l kx ky">.setClassificationType(FirebaseVisionFaceDetectorOptions.<em class="la">ALL_CLASSIFICATIONS</em>)</span><span id="fb7c" class="kt ku hh kp b fi lb kw l kx ky">.setClassificationType(FirebaseVisionFaceDetectorOptions.<em class="la">NO_CLASSIFICATIONS</em>)</span></pre><ul class=""><li id="9e76" class="ju jv hh ir b is it iw ix ja jw je jx ji jy jm jz ka kb kc bi translated"><strong class="ir hi">最小面部尺寸</strong> —用于定义待检测面部的最小尺寸(相对于给定图像)。该值默认为0.1f。</li></ul><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="8eb2" class="kt ku hh kp b fi kv kw l kx ky">.setMinFaceSize(0.15f)</span></pre><ul class=""><li id="f374" class="ju jv hh ir b is it iw ix ja jw je jx ji jy jm jz ka kb kc bi translated"><strong class="ir hi">启用面部跟踪</strong> —用于声明是否应为面部分配ID，以便在图像间跟踪面部。这默认为假。</li></ul><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="e38e" class="kt ku hh kp b fi kv kw l kx ky">.setTrackingEnabled(true)</span><span id="c17f" class="kt ku hh kp b fi lb kw l kx ky">.setTrackingEnabled(false)</span></pre><p id="99ea" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">将所有这些放在一起，您将会有大致如下的内容:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="36bc" class="kt ku hh kp b fi kv kw l kx ky">val options = FirebaseVisionFaceDetectorOptions.Builder()<br/>        .setModeType(FirebaseVisionFaceDetectorOptions.<em class="la">FAST_MODE</em>)<br/>        .setLandmarkType(<br/>            FirebaseVisionFaceDetectorOptions.<em class="la">ALL_LANDMARKS</em>)      <br/>        .setClassificationType(<br/>            FirebaseVisionFaceDetectorOptions.<em class="la">ALL_CLASSIFICATIONS</em>)<br/>        .setMinFaceSize(0.15f)<br/>        .setTrackingEnabled(true)<br/>        .build()</span></pre><p id="1804" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果您没有用构建器设置任何选项，那么它们将被设置为上面提到的默认值。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="abe6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然我们已经建立了选项，我们可以继续在我们的识别流程中使用它们。我们希望使用这些选项来创建一个<a class="ae kz" href="https://firebase.google.com/docs/reference/android/com/google/firebase/ml/vision/common/FirebaseVisionImage" rel="noopener ugc nofollow" target="_blank"> FirebaseVisionImage </a>的实例——这是一个保存图像数据的类，为识别过程做好准备。现在，我们需要这个实例，然后才能执行任何形式的识别，为了创建这个实例，我们需要使用我们的图像数据，这可以通过以下五种方式之一来完成:</p><figure class="kj kk kl km fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lc"><img src="../Images/faf1963b0d715ecaefb892b04bf508e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-7EDr_BOVoEoGHKlevaUA.png"/></div></div></figure><h2 id="3a93" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">位图</h2><p id="b968" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">首先，我们可以使用一个位图实例创建一个<a class="ae kz" href="https://firebase.google.com/docs/reference/android/com/google/firebase/ml/vision/common/FirebaseVisionImage" rel="noopener ugc nofollow" target="_blank"> FirebaseVisionImage </a>的实例。我们可以通过将一个<strong class="ir hi">直立</strong>位图传递到fromBitmap()函数中来实现——这将返回一个<a class="ae kz" href="https://firebase.google.com/docs/reference/android/com/google/firebase/ml/vision/common/FirebaseVisionImage" rel="noopener ugc nofollow" target="_blank"> FirebaseVisionImage </a></p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="62be" class="kt ku hh kp b fi kv kw l kx ky">val image = FirebaseVisionImage.fromBitmap(bitmap);</span></pre><h2 id="bf28" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">媒体。图像</h2><p id="36b8" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们也可以通过媒体来实现。图像实例—这可能发生在从设备的摄像头捕捉图像时。这样做时，我们必须传递这个图像的实例以及它的旋转，所以这必须在调用fromMediaImage()函数之前进行计算。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="e87e" class="kt ku hh kp b fi kv kw l kx ky">val image = FirebaseVisionImage.fromMediaImage(mediaImage,    <br/>                rotation);</span></pre><h2 id="f7db" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">字节缓冲器</h2><p id="cccd" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">也可以使用ByteBuffer创建实例。为此，我们必须首先创建一个FirebaseVisionImageMetadata的实例。这包含构建视觉图像所需的数据，例如旋转和测量。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="48df" class="kt ku hh kp b fi kv kw l kx ky">FirebaseVisionImageMetadata metadata = new <br/>    FirebaseVisionImageMetadata.Builder()<br/>        .setWidth(1280)<br/>        .setHeight(720)<br/>        .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21)<br/>        .setRotation(rotation)<br/>        .build();</span></pre><p id="26ba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">然后，我们可以将它与ByteBuffer一起传递，以创建实例:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="43ca" class="kt ku hh kp b fi kv kw l kx ky">val image = FirebaseVisionImage.fromByteBuffer(buffer, metadata);</span></pre><h2 id="b7a3" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">ByteArray</h2><p id="25aa" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">从ByteArray创建图像的方式与ByteBuffer相同，只是我们必须使用fromByteArray()函数:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="4eb3" class="kt ku hh kp b fi kv kw l kx ky">val image = FirebaseVisionImage.fromByteArray(byteArray, metadata);</span></pre><h2 id="2df7" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">文件</h2><p id="db97" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">通过使用上下文和期望的URI调用fromFilePath()函数，可以从文件创建视觉图像实例。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="a030" class="kt ku hh kp b fi kv kw l kx ky">val image: FirebaseVisionImage?<br/>try {<br/>    image = FirebaseVisionImage.fromFilePath(context, uri);<br/>} catch (IOException e) {<br/>    e.printStackTrace();<br/>}</span></pre></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="a322" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">用于检索FirebaseVisionImage实例的方法将取决于您的应用程序以及您处理图像的方式。无论如何，此时您应该可以访问FirebaseVisionImage类的一个实例。接下来我们需要做的是检索<code class="du mb mc md kp b"><a class="ae kz" href="https://firebase.google.com/docs/reference/android/com/google/firebase/ml/vision/face/FirebaseVisionFaceDetector" rel="noopener ugc nofollow" target="_blank">FirebaseVisionFaceDetector</a></code>类的一个实例——这个类用于在我们给定的图像中查找<a class="ae kz" href="https://firebase.google.com/docs/reference/android/com/google/firebase/ml/vision/face/FirebaseVisionFace.html" rel="noopener ugc nofollow" target="_blank"> FirebaseVisionFace </a>的任何实例。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="23f9" class="kt ku hh kp b fi kv kw l kx ky">val detector = FirebaseVision.getInstance()<br/>        .getVisionFaceDetector(options);</span></pre><p id="8264" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，我们可以不带任何选项调用getVisionFaceDetector()函数，该类将使用默认选项。或者，我们可以传入我们之前创建的firebasevisionfacededetectoroptions实例:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="86d4" class="kt ku hh kp b fi kv kw l kx ky">val options = FirebaseVisionFaceDetectorOptions.Builder()<br/>        .setModeType(FirebaseVisionFaceDetectorOptions.<em class="la">FAST_MODE</em>)<br/>        .setLandmarkType(<br/>            FirebaseVisionFaceDetectorOptions.<em class="la">ALL_LANDMARKS</em>)      <br/>        .setClassificationType(<br/>            FirebaseVisionFaceDetectorOptions.<em class="la">ALL_CLASSIFICATIONS</em>)<br/>        .setMinFaceSize(0.15f)<br/>        .setTrackingEnabled(true)<br/>        .build()</span></pre><p id="4e7e" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然我们的检测器已经配置好了，我们可以继续使用这个FirebaseVisionFaceDetector实例来检测图像中的人脸。这可以通过调用detectInImage()函数，传入我们的FirebaseVisionImage实例来实现:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="09a2" class="kt ku hh kp b fi kv kw l kx ky">val result = detector.detectInImage(image)<br/>        .addOnSuccessListener <strong class="kp hi">{<br/>            </strong><br/>        <strong class="kp hi">}<br/>        </strong>.addOnFailureListener <strong class="kp hi">{</strong></span><span id="324b" class="kt ku hh kp b fi lb kw l kx ky"><strong class="kp hi">        }</strong></span></pre><p id="e398" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在<strong class="ir hi">addonfailulelistener</strong>中，我们将处理人脸检测中的错误，并让用户知道我们无法完成操作。另一方面，<strong class="ir hi"> addOnSuccessListener </strong>我们将可以访问我们所请求的数据。</p><p id="35c6" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">在这个监听器中，我们将收到一个<strong class="ir hi">列表&lt; FirebaseVisionFace &gt; </strong>实例——我们可以遍历这个列表来检索每个检测到的人脸的数据。这是我在面部检测过程中运行的一张图片，以及从中检索到的数据:</p><figure class="kj kk kl km fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es me"><img src="../Images/915b4dedb8139e2629cbceaec61841db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0IfXRmhHmwEg4GOSnPh6aw.png"/></div></div></figure><p id="be2f" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated"><strong class="ir hi">注意:</strong>这张图片没有展示所有的识别功能。</p><p id="e544" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们可以在这里看到一组不同的数据，让我们来分解一下:</p><h2 id="0823" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFace.boundingBox</h2><p id="38ac" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">使用我们的face实例，我们得到了一个边界框，它表示检测到的人脸的边界。这是以一个<a class="ae kz" href="https://developer.android.com/reference/android/graphics/Rect.html" rel="noopener ugc nofollow" target="_blank"> Rect </a>实例的形式，所以可以用来很容易地在画布上绘制盒子。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="bed6" class="kt ku hh kp b fi kv kw l kx ky">val bounds = face.<em class="la">boundingBox</em></span></pre><p id="7aba" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">我们还可以根据面部是否倾斜来检索面部的旋转——如果是，我们可以使用这些来改变我们处理FirebaseVisionFace实例提供给我们的其他属性的方式。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="e965" class="kt ku hh kp b fi kv kw l kx ky">val rotationY = face.<em class="la">headEulerAngleY</em><br/>val rotationZ = face.<em class="la">headEulerAngleZ</em></span></pre><h2 id="d3e8" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">firebasevisionface . lefteyeopenprobability</h2><p id="60cb" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们可以使用face实例上的<strong class="ir hi"> leftEyeOpenProbability </strong>属性来检索脸部左眼睁开的概率。我们必须首先使用<em class="la"> FirebaseVisionFace检查该属性是否未被计算。UNCOMPUTED_PROBABILITY </em>字段，那么我们可以检索概率值:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="fc16" class="kt ku hh kp b fi kv kw l kx ky">if (face.<em class="la">leftEyeOpenProbability </em>!=<br/>        FirebaseVisionFace.<em class="la">UNCOMPUTED_PROBABILITY</em>) {<br/>    val leftEyeOpenProb = face.<em class="la">leftEyeOpenProbability<br/>}</em></span></pre><p id="fa90" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里返回的值将是介于0.0和1.0之间的概率。</p><h2 id="6329" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">firebasevisionface . rightyeopenprobability</h2><p id="5ac3" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们可以使用我们的face实例上的<strong class="ir hi">rightyeopenprobability</strong>属性来检索人脸睁开右眼的概率。我们必须首先使用<em class="la"> FirebaseVisionFace检查属性是否未被计算。UNCOMPUTED_PROBABILITY </em>字段，那么我们可以检索概率值:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="44df" class="kt ku hh kp b fi kv kw l kx ky">if (face.right<em class="la">EyeOpenProbability </em>!=<br/>        FirebaseVisionFace.<em class="la">UNCOMPUTED_PROBABILITY</em>) {<br/>    val rightEyeOpenProb = face.right<em class="la">EyeOpenProbability<br/>}</em></span></pre><p id="8443" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里返回的值将是介于0.0和1.0之间的概率。</p><h2 id="7c17" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFace.smilingProbability概率</h2><p id="6682" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们可以使用face实例上的<strong class="ir hi"> smilingProbability </strong>属性来检索脸部微笑的概率。我们必须首先使用<em class="la"> FirebaseVisionFace检查属性是否未被计算。UNCOMPUTED_PROBABILITY </em>字段，那么我们可以检索概率值:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="47ea" class="kt ku hh kp b fi kv kw l kx ky">if (face.<em class="la">smilingProbability </em>!=   <br/>        FirebaseVisionFace.<em class="la">UNCOMPUTED_PROBABILITY</em>) {<br/>    val smileProb = face.<em class="la">smilingProbability</em><br/>}</span></pre><p id="dbb5" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">这里返回的值将是介于0.0和1.0之间的概率。</p><h2 id="431b" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFaceLandmark。左嘴巴</h2><p id="1ab6" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们可以通过检索<strong class="ir hi"> FirebaseVisionFaceLandmark来检索嘴巴左侧的位置。来自视觉人脸实例的LEFT_MOUTH </strong>地标。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="2512" class="kt ku hh kp b fi kv kw l kx ky">val leftMouth = face.getLandmark(    <br/>        FirebaseVisionFaceLandmark.<em class="la">LEFT_MOUTH</em>)</span><span id="3eb9" class="kt ku hh kp b fi lb kw l kx ky">leftMouth?.let {<br/>    val leftMouthPos = leftMouth.<em class="la">position</em><br/>}</span></pre><h2 id="bc52" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFaceLandmark。右口</h2><p id="3a41" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们可以通过检索<strong class="ir hi"> FirebaseVisionFaceLandmark来检索嘴右侧的位置。来自视觉面实例的RIGHT_MOUTH </strong>地标。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="7e44" class="kt ku hh kp b fi kv kw l kx ky">val rightMouth = face.getLandmark(    <br/>        FirebaseVisionFaceLandmark.RIGHT<em class="la">_MOUTH</em>)</span><span id="6578" class="kt ku hh kp b fi lb kw l kx ky">rightMouth?.let {<br/>    val rightMouthPos = rightMouth.<em class="la">position</em><br/>}</span></pre><h2 id="c28e" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFaceLandmark。底部_口</h2><p id="e213" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们也可以通过检索<strong class="ir hi"> FirebaseVisionFaceLandmark来检索嘴底部的位置。来自视觉面实例的BOTTOM_MOUTH </strong>标志。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="3a33" class="kt ku hh kp b fi kv kw l kx ky">val bottomMouth = face.getLandmark(    <br/>        FirebaseVisionFaceLandmark.BOTTOM<em class="la">_MOUTH</em>)</span><span id="5695" class="kt ku hh kp b fi lb kw l kx ky">bottomMouth?.let {<br/>    val bottomMouthPos = bottomMouth.<em class="la">position</em><br/>}</span></pre><h2 id="c744" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFaceLandmark。左耳</h2><p id="3d76" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们可以通过检索<strong class="ir hi"> FirebaseVisionFaceLandmark来检索左耳的位置。来自视觉人脸实例的LEFT_EAR </strong>地标。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="8c2c" class="kt ku hh kp b fi kv kw l kx ky">val leftEar = face.getLandmark(    <br/>        FirebaseVisionFaceLandmark.<em class="la">LEFT_EAR</em>)</span><span id="4bc7" class="kt ku hh kp b fi lb kw l kx ky">leftEar?.let {<br/>    val leftEarPos = leftEar.<em class="la">position</em><br/>}</span></pre><h2 id="c8d6" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFaceLandmark。右耳</h2><p id="a8c8" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们可以通过检索<strong class="ir hi"> FirebaseVisionFaceLandmark来检索右耳的位置。来自视觉人脸实例的右耳</strong>标志。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="2dae" class="kt ku hh kp b fi kv kw l kx ky">val rightEar = face.getLandmark(    <br/>        FirebaseVisionFaceLandmark.RIGHT<em class="la">_EAR</em>)</span><span id="f54d" class="kt ku hh kp b fi lb kw l kx ky">rightEar?.let {<br/>    val rightEarPos = rightEar.<em class="la">position</em><br/>}</span></pre><h2 id="837f" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFaceLandmark。左脸颊</h2><p id="0390" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们可以通过检索<strong class="ir hi"> FirebaseVisionFaceLandmark来检索左脸颊的位置。来自视觉人脸实例的LEFT_CHEEK </strong>地标。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="f8bd" class="kt ku hh kp b fi kv kw l kx ky">val leftCheek = face.getLandmark(    <br/>        FirebaseVisionFaceLandmark.<em class="la">LEFT_CHEEK</em>)</span><span id="d1ee" class="kt ku hh kp b fi lb kw l kx ky">leftCheek?.let {<br/>    val leftCheekPos = leftCheek.<em class="la">position</em><br/>}</span></pre><h2 id="414d" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFaceLandmark。右脸颊</h2><p id="eb86" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们可以通过检索<strong class="ir hi"> FirebaseVisionFaceLandmark来检索右脸颊的位置。来自视觉人脸实例的RIGHT_CHEEK </strong>地标。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="4c6f" class="kt ku hh kp b fi kv kw l kx ky">val rightCheek = face.getLandmark(    <br/>        FirebaseVisionFaceLandmark.RIGHT<em class="la">_CHEEK</em>)</span><span id="78bf" class="kt ku hh kp b fi lb kw l kx ky">rightCheek?.let {<br/>    val rightCheekPos = rightCheek.<em class="la">position</em><br/>}</span></pre><h2 id="87eb" class="kt ku hh bd ld le lf lg lh li lj lk ll ja lm ln lo je lp lq lr ji ls lt lu lv bi translated">FirebaseVisionFaceLandmark。鼻子底部</h2><p id="0e22" class="pw-post-body-paragraph ip iq hh ir b is lw iu iv iw lx iy iz ja ly jc jd je lz jg jh ji ma jk jl jm ha bi translated">我们还可以通过检索<strong class="ir hi"> FirebaseVisionFaceLandmark来检索鼻子底部的位置。来自视觉面实例的NOSE_BASE </strong>地标。</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="1301" class="kt ku hh kp b fi kv kw l kx ky">val noseBase = face.getLandmark(    <br/>        FirebaseVisionFaceLandmark.NOSE_BASE)</span><span id="3ef0" class="kt ku hh kp b fi lb kw l kx ky">noseBase?.let {<br/>    val noseBasePos = noseBase.<em class="la">position</em><br/>}</span></pre></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="8f54" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">如果我们决定启用面部跟踪，那么我们可能想要检索检测到的面部的ID——我们可以使用面部实例的<strong class="ir hi"> trackingId </strong>来这样做。我们可以首先检查这不等于<a class="ae kz" href="https://firebase.google.com/docs/reference/android/com/google/firebase/ml/vision/face/FirebaseVisionFace.html#INVALID_ID" rel="noopener ugc nofollow" target="_blank"> INVALID_ID </a>值(如果我们不跟踪人脸，这是返回的值),然后检索实际值:</p><pre class="kj kk kl km fd ko kp kq kr aw ks bi"><span id="a477" class="kt ku hh kp b fi kv kw l kx ky">if (face.<em class="la">trackingId </em>!= FirebaseVisionFace.<em class="la">INVALID_ID</em>) {<br/>    val faceId = face.<em class="la">trackingId<br/></em>}</span></pre></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="b159" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">现在，如果我们在执行视觉分析的图像中有多个人脸，会发生什么呢？就像我之前提到的，我们得到了一个<strong class="ir hi">列表&lt; FirebaseVisionFace &gt; </strong>实例——这意味着我们可以简单地遍历这些面，并对每个面执行所需的操作。这是我在这个过程中看到的一张包含多张面孔的图片:</p><figure class="kj kk kl km fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/d225f3ab2b6449a80cfe18b47b201807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rG-Ezg1okQx2Pb_PzCkGtA.png"/></div></div></figure><p id="8d7c" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">你可以看到，除了一个低头看着笔记本电脑的人之外，大多数人脸都被检测到了。我们还可以看到，并非所有的特征都被检测到——我运行了与第一张图片完全相同的代码，但这可能与照片中人脸的角度和焦点有关(我认为这是一种预期行为！).</p><p id="0d63" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">既然游戏中有多个面，我们可以很容易地处理每个面的不同属性，因为我们得到了单独的视觉面实例。如果我们通过ID来跟踪人脸，那么人脸的管理也会变得更加简单。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="0cf7" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">需要注意的是，当您完成识别过程时，您应该调用<strong class="ir hi">firebasevisionfacedevictor</strong>实例上的close()来释放它的资源。</p></div><div class="ab cl jn jo go jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ha hb hc hd he"><p id="0719" class="pw-post-body-paragraph ip iq hh ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm ha bi translated">哇，用MLKit进行人脸识别真是太棒了，不是吗！从这篇文章中，我希望您已经能够看到Firebase为我们的应用程序做出了多么简单的认可。你有什么想法？如果您有任何意见或问题，请联系我们🙂</p><div class="mg mh ez fb mi mj"><a href="https://twitter.com/hitherejoe" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hi fi z dy mo ea eb mp ed ef hg bi translated">乔·伯奇(@hitherejoe) |推特</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">乔伯奇的最新推文(@hitherejoe)。Android Lead &amp;高级工程师@Buffer。谷歌开发专家…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">twitter.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx in mj"/></div></div></a></div></div></div>    
</body>
</html>