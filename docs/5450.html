<html>
<head>
<title>Lessons From Implementing AlphaZero</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实施AlphaZero的经验教训</h1>
<blockquote>原文：<a href="https://medium.com/oracledevs/lessons-from-implementing-alphazero-7e36e9054191?source=collection_archive---------0-----------------------#2018-06-05">https://medium.com/oracledevs/lessons-from-implementing-alphazero-7e36e9054191?source=collection_archive---------0-----------------------#2018-06-05</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><figure class="hg hh ez fb hi hj er es paragraph-image"><div class="er es hf"><img src="../Images/5ccf34e017a2733dbb53045e4ad0ace1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*yzPRrbo45BwX9N4iq6S_ew.png"/></div></figure><div class=""/><p id="9174" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">DeepMind的<a class="ae jj" href="https://arxiv.org/abs/1712.01815" rel="noopener ugc nofollow" target="_blank"> AlphaZero出版物</a>是棋盘游戏强化学习(RL)的一个里程碑。该算法在国际象棋、shogi和go中实现了超人的性能，每个游戏都有不到24小时的自我游戏，除了规则之外，几乎没有使用任何专业或硬编码的游戏知识。</p><p id="f053" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们认为以新的方式复制和扩展他们的结果是有益的，在这个过程中发现各种选择如何影响算法的性能。机器学习作为一个整体只是慢慢地被放在坚实的理论基础上，所以探索新的方向通常会有很多收获。</p><p id="ae39" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">本文是我们分享探索过程中收集的见解的系列文章中的第一篇。</p></div><div class="ab cl jk jl go jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="ha hb hc hd he"><p id="f3c9" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">对于这第一篇文章，我们将只给出一个未经修改的AlphaZero算法的概述。我们不会让你自己去阅读这篇论文，而是尝试用一种相对来说是RL新手也能理解的方式来展示它，链接到外部资源以获得更多细节。</p><p id="4332" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在高层次上，它使用由深度残差神经网络(或“ResNet”)支持的修改的蒙特卡罗树搜索(MCTS)。AlphaZero与自己对弈，每一方选择MCTS选定的棋步。这些自己玩的游戏的结果被用于不断改进ResNet。自我游戏和ResNet训练同时进行，相互促进。</p><p id="8532" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">让我们打开包装，从ResNet开始。</p></div><div class="ab cl jk jl go jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="ha hb hc hd he"><h1 id="b911" class="jr js ho bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">神经网络</strong></h1><p id="ecb8" class="pw-post-body-paragraph il im ho in b io kp iq ir is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ha bi translated"><a class="ae jj" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet架构</a>是一种训练深度网络的流行方式，通常用于图像识别。对于AlphaZero的网络，输入是板状态，有两个输出:</p><ol class=""><li id="a52b" class="ku kv ho in b io ip is it iw kw ja kx je ky ji kz la lb lc bi translated">头寸的估计<strong class="in hp">值</strong> (v)，范围从1(赢)到-1(输)。</li><li id="5dac" class="ku kv ho in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">用于进行每个下一个可能动作的先验概率的向量。</li></ol><p id="39c6" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">网络被随机初始化。在一些自我游戏发生后，训练数据包括从玩的游戏中随机选择的棋盘位置，以(棋盘状态、游戏结果、来自MCTS的儿童访问计数)的形式。</p><h1 id="676e" class="jr js ho bd jt ju li jw jx jy lj ka kb kc lk ke kf kg ll ki kj kk lm km kn ko bi translated">MCTS</h1><p id="08a8" class="pw-post-body-paragraph il im ho in b io kp iq ir is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ha bi translated"><a class="ae jj" href="https://www.youtube.com/watch?v=UXW2yZndl7U" rel="noopener ugc nofollow" target="_blank">这个视频</a>很好地展示了标准MCTS。为了在给定棋盘位置的情况下选择一步棋，我们要重复800次下面的算法。我们构建了一个树，最初只有一个节点(代表当前的板状态)。</p><ol class=""><li id="3f4e" class="ku kv ho in b io ip is it iw kw ja kx je ky ji kz la lb lc bi translated">从根节点开始(当前板状态)。走向在利用当前信息和探索新动作之间给出最佳权衡的孩子(由UCB1公式形式化)。递归直到你碰到一个叶子节点。</li><li id="98e3" class="ku kv ho in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">如果这是第一次访问该节点，执行一个卷展栏:随机模拟移动直到游戏结束，然后使用游戏结果更新从叶到根的所有节点的值。</li><li id="4a60" class="ku kv ho in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">如果是第二次访问，展开它(即创建它的子对象)，访问+展示其中一个。</li><li id="eed9" class="ku kv ho in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">没有第三次访问，因为它不再是叶节点。</li></ol><p id="bf0d" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在AlphaZero中，滚动由从神经网络获取预测代替，UCB1由PUCT(多项式上置信树)代替。算法看起来是这样的:</p><ol class=""><li id="cf68" class="ku kv ho in b io ip is it iw kw ja kx je ky ji kz la lb lc bi translated">从根开始。走向得分最高的孩子。递归直到你碰到一个叶子节点。</li><li id="06b5" class="ku kv ho in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">在第一次访问时，调用神经网络获取(1)估计的游戏分数(或<em class="ln">值</em>、<strong class="in hp"> v </strong>)和(2)访问每个孩子的建议概率(<strong class="in hp"> p </strong>)，以用于PUCT。创建孩子，但不要访问他们。</li><li id="7e56" class="ku kv ho in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">没有第二次访问。</li></ol><h1 id="6908" class="jr js ho bd jt ju li jw jx jy lj ka kb kc lk ke kf kg ll ki kj kk lm km kn ko bi translated">把它绑在一起</h1><p id="88ac" class="pw-post-body-paragraph il im ho in b io kp iq ir is kq iu iv iw kr iy iz ja ks jc jd je kt jg jh ji ha bi translated">MCTS迭代800次后，通过选择被访问最多的孩子来选择一步棋。然后另一方用同样的方法玩。这种情况一直持续到游戏结束。在这一点上，游戏中的每个棋盘位置都标有游戏分数，并且儿童访问计数。这些样本被添加到ResNet的训练集中。随着ResNet的训练，它被用于随后的MCTS自我发挥。</p><p id="77a0" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这种在MCTS自弹自唱和ResNet培训之间的来回是非常强大的。ResNet有助于归纳从MCTS学到的知识，而self-play使用这些归纳的知识来更深入地学习游戏。正如我们在论文中看到的，这使得AlphaZero基本上胜过了所有现有的算法——其中最好的算法是在各自游戏大师的帮助下精心手工调整的。</p></div><div class="ab cl jk jl go jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="ha hb hc hd he"><p id="bd42" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">这是对AlphaZero培训的一个非常粗略的概述。我们已经忽略了过多的细节，我们将在接下来的几集里探讨。</p><p id="2a8c" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">敬请期待！</p><p id="1570" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><em class="ln">零件</em><a class="ae jj" rel="noopener" href="/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68"><em class="ln">2</em></a><em class="ln"/><a class="ae jj" rel="noopener" href="/oracledevs/lessons-from-alphazero-part-3-parameter-tweaking-4dceb78ed1e5"><em class="ln">3</em></a><em class="ln"/><a class="ae jj" rel="noopener" href="/oracledevs/lessons-from-alphazero-part-4-improving-the-training-target-6efba2e71628"><em class="ln">4</em></a><em class="ln"/><a class="ae jj" rel="noopener" href="/oracledevs/lessons-from-alpha-zero-part-5-performance-optimization-664b38dc509e"><em class="ln">5</em></a><em class="ln">，以及</em> <a class="ae jj" rel="noopener" href="/oracledevs/lessons-from-alpha-zero-part-6-hyperparameter-tuning-b1cfcbe4ca9a"> <em class="ln"> 6 </em> </a> <em class="ln">现已出。</em></p></div></div>    
</body>
</html>