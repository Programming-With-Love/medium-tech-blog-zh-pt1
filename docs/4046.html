<html>
<head>
<title>Streaming data in a Delta table using Spark Structured Streaming</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Spark结构化流传输增量表中的数据流</h1>
<blockquote>原文：<a href="https://medium.com/globant/streaming-data-in-a-delta-table-using-spark-structured-streaming-19bd6f3ea37b?source=collection_archive---------0-----------------------#2022-12-14">https://medium.com/globant/streaming-data-in-a-delta-table-using-spark-structured-streaming-19bd6f3ea37b?source=collection_archive---------0-----------------------#2022-12-14</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="5247" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">实时数据处理的解决方案</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es iw"><img src="../Images/392f594326334612173fe801e07f28d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pzoWKEITTb04Zms3"/></div></div></figure><p id="35c5" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">作为数据工程师，你必须和大数据技术打交道，比如<a class="ae ke" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">星火流</a>、<a class="ae ke" href="https://kafka.apache.org/documentation/" rel="noopener ugc nofollow" target="_blank">卡夫卡</a>、<a class="ae ke" href="https://delta.io" rel="noopener ugc nofollow" target="_blank">三角洲湖</a>。然而，当大规模地结合这些技术时，您会发现自己在寻找一个覆盖所有复杂生产用例的解决方案。在这篇博文中，我将分享如何通过结合这三种技术来构建实时数据处理和分析，从而利用数据平台的分析能力。</p><h1 id="d9ff" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated"><strong class="ak">问题陈述</strong></h1><p id="1f66" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">这是一个普遍存在的问题，我们需要在您的数据平台中进行低延迟分析。通常，数据在三角洲湖中通过批处理作业进行处理，以生成着陆和最终处理的数据图层。由于批处理的性质，这会增加处理时间，并且数据会在下一个计划的作业中到达。</p><p id="a9ff" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">批处理会造成延迟，影响业务快速决策。此外，批处理过程的执行依赖于类似<a class="ae ke" href="https://airflow.apache.org/docs/" rel="noopener ugc nofollow" target="_blank">气流</a>的编排器。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lc"><img src="../Images/71f9d337683943638bac842611fbfe44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nL1Ui9W2i3B2BZnPAO9XCA.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">Batch processing architecture</figcaption></figure><h1 id="8c4a" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated"><strong class="ak">提议的解决方案</strong></h1><p id="03f2" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">Spark流作业的理念是它总是在运行。它不断地从Kafka主题中读取事件，处理它们，并将输出写入Delta Lake表。这项工作不应停止。</p><p id="c303" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">正如事件驱动架构通过Kafka主题近实时处理数据一样，我们将使用Delta Lake表来桥接近实时数据处理管道。</p><h1 id="a16c" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated"><strong class="ak">架构</strong></h1><p id="2b2f" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">基于卡夫卡事件的实时分析系统。以下架构中的第一个数据入口点是Kafka，由Spark流作业使用，并以Delta Lake表的形式写入。让我们一个一个地看每个组件。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es lh"><img src="../Images/976e540c1fb232fe31c0afb877f3fed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-PmOVSkUN1D5BGcu92gxA.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">Event-driven Architecture</figcaption></figure><h2 id="1a52" class="li kg hh bd kh lj lk ll kl lm ln lo kp jr lp lq kr jv lr ls kt jz lt lu kv lv bi translated"><strong class="ak">事件商店(卡夫卡)</strong></h2><p id="299a" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">我们使用Kafka作为我们的事件存储来处理实时数据。Kafka是一个社区分布式流媒体平台，每天能够处理数万亿个事件。它发布和订阅记录流，还用于容错存储。它存储、读取和分析流数据。它用于消息传递、网站活动跟踪和日志。</p><p id="17b5" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">一个Kafka集群由一个或多个名为<a class="ae ke" href="https://developer.confluent.io/learn-kafka/apache-kafka/brokers/" rel="noopener ugc nofollow" target="_blank"> Kafka Brokers </a>的服务器组成，在这些Brokers里面有<a class="ae ke" href="https://developer.confluent.io/learn-kafka/apache-kafka/topics/#:~:text=As%20a%20developer%20using%20Kafka,is%20a%20log%20of%20events." rel="noopener ugc nofollow" target="_blank"> Kafka Topics </a>。Kafka主题包含正在生产和消费的数据。</p><h2 id="7dc4" class="li kg hh bd kh lj lk ll kl lm ln lo kp jr lp lq kr jv lr ls kt jz lt lu kv lv bi translated"><strong class="ak"> Spark结构化流媒体</strong></h2><p id="bcec" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated"><a class="ae ke" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">结构化流</a>是一个基于Spark SQL引擎的可扩展和容错的流处理引擎。Spark SQL引擎将不断递增地运行它，并随着流数据的不断到达更新最终结果。</p><p id="cce3" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在内部，默认情况下，使用微批处理引擎处理结构化流查询，该引擎将数据流作为一系列小批量作业进行处理，从而实现低至100毫秒的端到端延迟和<a class="ae ke" href="https://learn.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-streaming-exactly-once" rel="noopener ugc nofollow" target="_blank">恰好一次</a>容错保证。然而，自<a class="ae ke" href="https://spark.apache.org/docs/2.3.0/" rel="noopener ugc nofollow" target="_blank"> Spark 2.3 </a>以来，引入了一种新的低延迟处理模式，称为连续处理，可以在至少一次保证的情况下实现低至一毫秒的端到端延迟。在不改变查询中的数据集/数据框操作的情况下，我们可以根据应用程序要求选择模式。</p><h2 id="cc9b" class="li kg hh bd kh lj lk ll kl lm ln lo kp jr lp lq kr jv lr ls kt jz lt lu kv lv bi translated"><strong class="ak">三角洲湖泊</strong></h2><p id="fd92" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">Delta Lake是一个集中式存储库，允许您存储任何规模的所有结构化和非结构化数据。我们可以按原样存储我们的数据，而不必首先对数据进行结构化，并运行不同类型的分析，从仪表板和可视化到大数据处理和实时分析。</p><p id="988d" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">为什么我们需要三角洲湖？有几个原因:</p><ul class=""><li id="7192" class="lw lx hh jk b jl jm jo jp jr ly jv lz jz ma kd mb mc md me bi translated">数据在同一路径上“覆盖”,在作业失败时导致增量丢失。</li><li id="6b08" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">数据在同一路径上“覆盖”,在作业失败时导致增量丢失。</li><li id="4bbf" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">编写<a class="ae ke" href="https://avro.apache.org/docs/" rel="noopener ugc nofollow" target="_blank"> AVRO </a>和<a class="ae ke" href="https://parquet.apache.org/docs/" rel="noopener ugc nofollow" target="_blank">拼花</a>文件的普通火花码没有<a class="ae ke" href="https://docs.delta.io/latest/concurrency-control.html" rel="noopener ugc nofollow" target="_blank"> ACID </a>事务。</li><li id="f16a" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">在Spark中，不支持模式进化。如果我们尝试合并两个具有不同方案的数据框，作业将会失败。</li><li id="cdbb" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">不维护数据版本；一旦数据被删除，我们就无法回滚到较旧的数据。</li></ul><p id="638c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">以下是三角洲湖的主要特征:</p><ul class=""><li id="bd32" class="lw lx hh jk b jl jm jo jp jr ly jv lz jz ma kd mb mc md me bi translated">ACID事务——使用Delta Lake，我们不需要编写任何代码——事务被自动写入博客。这个事务日志是关键，它代表了唯一的真实来源。</li><li id="cef3" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">可扩展的元数据处理——激发分布式处理能力，轻松处理数十亿个文件的Pb级表的所有元数据。</li><li id="1f8d" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">统一的批处理和流——不再需要使用不同的体系结构来读取数据流和批处理数据。三角洲湖表是一个批处理和流式信源和信宿。</li><li id="3d3b" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">模式执行——如果您将一个模式放在一个Delta Lake表中，并试图向该表中写入与该模式不一致的数据，它将向您提供一个错误，并不允许您写入该数据，从而防止您写错。</li><li id="70d5" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">模式演化——模式演化是一个特性，它允许用户轻松地更改表的当前模式，以适应随时间变化的数据。通过将<em class="mk"> </em> <code class="du ml mm mn mo b">.option("mergeSchema", "true")</code>添加到我们的<code class="du ml mm mn mo b">.write</code>或<code class="du ml mm mn mo b">.writeStream</code> Spark命令来激活模式进化。</li><li id="a406" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">增量插入和删除-增量允许我们非常容易地做增量插入或合并。我们可以将另一个数据框中的数据合并到您的表中，并进行更新、插入和删除。</li><li id="1f87" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated">Delta Lake中的压缩——如果我们不断地向Delta Lake表中写入数据，随着时间的推移，将会累积许多文件，特别是当我们小批量地添加数据时。我们可以通过将一个表重新分区为数量较少的大文件来压缩它。</li></ul><h1 id="7cd3" class="kf kg hh bd kh ki kj kk kl km kn ko kp in kq io kr iq ks ir kt it ku iu kv kw bi translated">实施提议的解决方案</h1><p id="b3ad" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">需要采取以下步骤。</p><p id="ba0a" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi"> 1。在Kafka集群上创建一个T</strong><a class="ae ke" href="https://kafka.apache.org/quickstart" rel="noopener ugc nofollow" target="_blank"><strong class="jk hi">opic</strong></a><strong class="jk hi">。</strong> <code class="du ml mm mn mo b">--bootstrap.server</code>是以逗号分隔的主机和端口对列表，这些主机和端口对是Kafka客户端最初连接到的“bootstrap”<a class="ae ke" href="https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-brokers.html" rel="noopener ugc nofollow" target="_blank">Kafka集群</a>中Kafka代理的地址，用于引导自身。在我们写第一个事件之前，我们需要创建一个主题。让我们创建employee_topic。打开终端会话并运行以下命令:</p><pre class="ix iy iz ja fd mp mo mq bn mr ms bi"><span id="1668" class="mt kg hh mo b be mu mv l mw mx">sh kafka-topics.sh –create –topic topic_name –bootstrap-server <br/>{bootstrap-server}</span></pre><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es my"><img src="../Images/ea8cfeff05836bc81b784abf1b610745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gV4ktmHnEqZAFxBGXE0tng.png"/></div></div></figure><p id="1a94" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi"> 2。发布数据到Kafka主题</strong>。Kafka客户端通过网络与Kafka代理通信，以写入(或读取)事件。一旦接收到事件，代理就会以持久和容错的方式存储事件，只要我们需要，甚至是永远。</p><p id="f52b" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">在终端会话上运行控制台生成器客户端，将一些事件写入您的主题。默认情况下，您输入的每一行都会导致一个单独的事件写入主题。</p><pre class="ix iy iz ja fd mp mo mq bn mr ms bi"><span id="8888" class="mt kg hh mo b be mu mv l mw mx">sh kafka-console-producer.sh -broker-list {bootstrap-server} -topic topic_name</span></pre><p id="12e1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">让我们将数据发布到employee_topic。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="jc jd di je bf jf"><div class="er es mz"><img src="../Images/e83e33862a56bd5948768e51579f843e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_EiEzsSMtCVD0sHIjSG8A.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx">JSON events in the Kafka topic</figcaption></figure><p id="6db1" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi"> 3。模式</strong>。Kafka主题事件具有JSON格式。在我们以流的方式读取Kafka主题事件之前，我们只需要提供一个schema.json文件。该文件包含Kafka主题中可用的事件数据的元数据信息。让我们从一些导入开始，创建Spark会话并将schema.json加载到一个模式中。</p><pre class="ix iy iz ja fd mp mo mq bn mr ms bi"><span id="7eac" class="mt kg hh mo b be mu mv l mw mx">from delta. tables import *<br/>from pyspark.sql.functions import *<br/>from pyspark.sql.types import *<br/>from pyspark.sql import *<br/><br/>spark = SparkSession.builder.appName('job_name').getOrCreate()<br/>schema_file = "schema.json"<br/>data = open(schema_file)<br/>schema = StructType.fromJson(json.load(data))</span></pre><p id="ca3e" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi"> 4。Spark流作业从Kafka主题中读取数据。</strong>我们需要使用<code class="du ml mm mn mo b">format("kafka")</code>明确说明我们从哪里开始流式传输，并且应该提供Kafka服务器，并使用该选项订阅我们正在流式传输的主题。</p><p id="9ee6" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">Spark Streaming使用<a class="ae ke" href="https://sparkbyexamples.com/spark/how-to-create-a-sparksession-and-spark-context/" rel="noopener ugc nofollow" target="_blank"> SparkSession </a>上的<code class="du ml mm mn mo b">readStream()</code>从Kafka加载流数据集。选项<code class="du ml mm mn mo b">kafka.bootstrap.servers</code>将提供Kafka引导服务器的详细信息，在<code class="du ml mm mn mo b">subscribe </code>中将提供Kafka主题名称。选项<code class="du ml mm mn mo b">startingOffsets earliest</code>用于在查询开始时读取Kafka中所有可用的数据；我们可能不会经常使用这个选项，默认值<code class="du ml mm mn mo b">startingOffsets</code>是<code class="du ml mm mn mo b">latest</code>，它只读取尚未处理的新数据。</p><pre class="ix iy iz ja fd mp mo mq bn mr ms bi"><span id="f625" class="mt kg hh mo b be mu mv l mw mx">data_df=spark.readstream.format("kafka")\<br/>  .option("kafka.bootstrap.servers","{bootstrap-server}")\<br/>  .option("subscribe","{topic_name}")\<br/>  .option("failOnDataLoss","False")\<br/>  .option("startingOffsets", "earliest")\<br/>  .load()\<br/>  .selectExpr(schema)<br/><br/>data_df.printSchema()</span></pre><p id="1a9e" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi"> 5。将最后的</strong> <code class="du ml mm mn mo b"><strong class="jk hi">data_df</strong></code> <strong class="jk hi">写成三角洲湖表</strong>。使用结构化流，我们可以将数据流写入Delta Lake表。Delta的事务日志保证每条消息将被精确地处理一次。它还支持将多个流或批处理作业并发写入同一个表。默认情况下，流中的消息将被追加到Delta Lake表中。</p><pre class="ix iy iz ja fd mp mo mq bn mr ms bi"><span id="9fa0" class="mt kg hh mo b be mu mv l mw mx">data_df.writeStream.format("delta")\<br/>  .outputMode("append")\<br/>  .option("checkpoint_location","{checkpoint-location}")\<br/>  .trigger("processing=30 seconds")\<br/>  .start("{Delta table path}")</span></pre><p id="3780" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">6。将三角洲湖表读作溪流。当我们加载一个Delta Lake表作为流源并在流查询中使用它时，该查询处理表中存在的所有数据以及流启动后到达的任何新数据。</p><p id="389c" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated">我们还可以通过设置<code class="du ml mm mn mo b">maxFilesPerTrigger</code>选项来控制任何微批量Delta Lake给定流的最大大小。这指定了每个触发器中要考虑的新文件的最大数量。默认值为1000。流中的选项<code class="du ml mm mn mo b">ignoreChanges</code>不会因删除或更新源表而中断。</p><pre class="ix iy iz ja fd mp mo mq bn mr ms bi"><span id="a7f5" class="mt kg hh mo b be mu mv l mw mx">delta_df=spark.readStream.format("delta")\<br/>  .option("maxFilesPerTrigger",5)\<br/>  .option("ignoreChanges","true")\<br/>  .load("{Delta table path}")</span></pre><p id="32f6" class="pw-post-body-paragraph ji jj hh jk b jl jm ii jn jo jp il jq jr js jt ju jv jw jx jy jz ka kb kc kd ha bi translated"><strong class="jk hi"> 7。将数据写入增量表</strong>。我们可以使用结构化流将数据流写入增量表。Delta的事务日志保证每条消息将被精确地处理一次。它还支持将多个流或批处理作业并发写入同一个表。默认情况下，流中的消息将被追加到增量表中。</p><pre class="ix iy iz ja fd mp mo mq bn mr ms bi"><span id="a609" class="mt kg hh mo b be mu mv l mw mx">delta_df.writeStream.format("delta")\<br/>  .outputMode("append")\<br/>  .option("checkpoint_location","{checkpoint-location}")\<br/>  .trigger("processing=30 seconds")\<br/>  .start("{Delta table </span></pre></div><div class="ab cl na nb go nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ha hb hc hd he"><h1 id="affe" class="kf kg hh bd kh ki nh kk kl km ni ko kp in nj io kr iq nk ir kt it nl iu kv kw bi translated">结论</h1><p id="50b3" class="pw-post-body-paragraph ji jj hh jk b jl kx ii jn jo ky il jq jr kz jt ju jv la jx jy jz lb kb kc kd ha bi translated">在本文中，我已经介绍了Delta Lake特性的实时数据处理。借助上述事件驱动架构，解决Spark Streaming和Kafka之间的集成问题是构建我们的实时分析仪表板的一个重要里程碑。</p></div><div class="ab cl na nb go nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="ha hb hc hd he"><h1 id="f91d" class="kf kg hh bd kh ki nh kk kl km ni ko kp in nj io kr iq nk ir kt it nl iu kv kw bi translated">参考</h1><ul class=""><li id="fbc2" class="lw lx hh jk b jl kx jo ky jr nm jv nn jz no kd mb mc md me bi translated">美国<a class="ae ke" href="https://www.databricks.com/dataaisummit/session/streaming-data-delta-lake-rust-and-kafka" rel="noopener ugc nofollow" target="_blank">治疗-数据-三角洲-湖泊-铁锈-卡夫卡</a></li><li id="5537" class="lw lx hh jk b jl mf jo mg jr mh jv mi jz mj kd mb mc md me bi translated"><a class="ae ke" href="https://docs.delta.io/0.3.0/delta-streaming.html" rel="noopener ugc nofollow" target="_blank">德尔塔火花流</a></li></ul></div></div>    
</body>
</html>